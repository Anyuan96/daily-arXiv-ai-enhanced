<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 76]
- [cs.CL](#cs.CL) [Total: 82]
- [cs.SD](#cs.SD) [Total: 9]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Vision-Based Perception for Autonomous Vehicles in Off-Road Environment Using Deep Learning](https://arxiv.org/abs/2509.19378)
*Nelson Alves Ferreira Neto*

Main category: cs.CV

TL;DR: 本研究提出了CMSNet框架，用于在不规则地形中实现自主驾驶的实时图像分割，包含新的Kamino数据集和算法验证。


<details>
  <summary>Details</summary>
Motivation: 针对自主驾驶在开放矿区及发展中国家不规则地形上的低延迟智能系统的需求，开发一套新颖的感知体系。

Method: 提出了CMSNet框架，并通过深度学习进行障碍物和可行地面区域的分割，结合TensorRT等技术实现实时推断。

Result: CMSNet在各种不良条件下（如夜间、雨天和尘土飞扬）成功地进行了实时语义分割，并通过Kamino数据集进行了验证。

Conclusion: CMSNet在不规则地形上实现了有效的实时语义分割，验证了其在自主驾驶中应用的潜力。

Abstract: Low-latency intelligent systems are required for autonomous driving on
non-uniform terrain in open-pit mines and developing countries. This work
proposes a perception system for autonomous vehicles on unpaved roads and
off-road environments, capable of navigating rough terrain without a predefined
trail. The Configurable Modular Segmentation Network (CMSNet) framework is
proposed, facilitating different architectural arrangements. CMSNet
configurations were trained to segment obstacles and trafficable ground on new
images from unpaved/off-road scenarios with adverse conditions (night, rain,
dust). We investigated applying deep learning to detect drivable regions
without explicit track boundaries, studied algorithm behavior under visibility
impairment, and evaluated field tests with real-time semantic segmentation. A
new dataset, Kamino, is presented with almost 12,000 images from an operating
vehicle with eight synchronized cameras. The Kamino dataset has a high number
of labeled pixels compared to similar public collections and includes images
from an off-road proving ground emulating a mine under adverse visibility. To
achieve real-time inference, CMSNet CNN layers were methodically removed and
fused using TensorRT, C++, and CUDA. Empirical experiments on two datasets
validated the proposed system's effectiveness.

</details>


### [2] [Overview of LifeCLEF Plant Identification task 2020](https://arxiv.org/abs/2509.19402)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: 本研究探讨了在植物识别挑战中，利用 herbarium 数据集如何有效提升在生物多样性丰富而数据缺乏地区的自动识别能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决自动识别数据不足地区植物种类的问题，利用 herbarium 收藏丰富的植物图片资源来提升识别精度。

Method: 通过使用包含数十万 herbarium 标本和几千张野外照片的数据集，对植物进行跨领域分类。

Result: 各参与研究小组采用了不同的方法和系统，对评估结果进行了分析，显示出 herbarium 数据集在植物识别上的有效性。

Conclusion: 使用 herbarium 数据集可以显著提高南美古安盾地区植物的自动识别能力。

Abstract: Automated identification of plants has improved considerably thanks to the
recent progress in deep learning and the availability of training data with
more and more photos in the field. However, this profusion of data only
concerns a few tens of thousands of species, mostly located in North America
and Western Europe, much less in the richest regions in terms of biodiversity
such as tropical countries. On the other hand, for several centuries, botanists
have collected, catalogued and systematically stored plant specimens in
herbaria, particularly in tropical regions, and the recent efforts by the
biodiversity informatics community made it possible to put millions of
digitized sheets online. The LifeCLEF 2020 Plant Identification challenge (or
"PlantCLEF 2020") was designed to evaluate to what extent automated
identification on the flora of data deficient regions can be improved by the
use of herbarium collections. It is based on a dataset of about 1,000 species
mainly focused on the South America's Guiana Shield, an area known to have one
of the greatest diversity of plants in the world. The challenge was evaluated
as a cross-domain classification task where the training set consist of several
hundred thousand herbarium sheets and few thousand of photos to enable learning
a mapping between the two domains. The test set was exclusively composed of
photos in the field. This paper presents the resources and assessments of the
conducted evaluation, summarizes the approaches and systems employed by the
participating research groups, and provides an analysis of the main outcomes.

</details>


### [3] [iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam Video Reasoning](https://arxiv.org/abs/2509.19552)
*Manyi Yao,Bingbing Zhuang,Sparsh Garg,Amit Roy-Chowdhury,Christian Shelton,Manmohan Chandraker,Abhishek Aich*

Main category: cs.CV

TL;DR: iFinder是一个新提出的框架，通过将行车记录仪视频转化为层次化的数据结构，支持大型语言模型的推理能力，解决了在没有额外传感器情况下进行视频分析的挑战，并在事故推理准确性上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 将大型语言模型应用于特定领域任务（如后续分析行车记录仪视频）面临挑战，尤其是在视觉模态由于缺乏激光雷达、GPS等数据时，现有的视频-视觉-语言模型在空间推理、因果推断和事件可解释性方面存在困难。

Method: iFinder是一个结构化的语义基础框架，通过将行车记录仪视频转化为分层可解释的数据结构，为大型语言模型提供支持。它采用了模块化的无训练管道，利用预训练的视觉模型提取关键线索，并采用三段式提示策略，促进一步步的、扎根的推理。

Result: iFinder在四个公共行车记录仪视频基准上评估时，提出的基础与特定领域线索相结合，特别是物体朝向和全球上下文，显著优于端到端视频-视觉-语言模型，并在事故推理准确性上提升了多达39%。

Conclusion: iFinder通过将大型语言模型与特定驾驶领域的表示相结合，为后续驾驶视频理解提供了一种零次学习、可解释、可靠的替代方案，显著提升了事故推理的准确性。

Abstract: Grounding large language models (LLMs) in domain-specific tasks like post-hoc
dash-cam driving video analysis is challenging due to their general-purpose
training and lack of structured inductive biases. As vision is often the sole
modality available for such analysis (i.e., no LiDAR, GPS, etc.), existing
video-based vision-language models (V-VLMs) struggle with spatial reasoning,
causal inference, and explainability of events in the input video. To this end,
we introduce iFinder, a structured semantic grounding framework that decouples
perception from reasoning by translating dash-cam videos into a hierarchical,
interpretable data structure for LLMs. iFinder operates as a modular,
training-free pipeline that employs pretrained vision models to extract
critical cues -- object pose, lane positions, and object trajectories -- which
are hierarchically organized into frame- and video-level structures. Combined
with a three-block prompting strategy, it enables step-wise, grounded reasoning
for the LLM to refine a peer V-VLM's outputs and provide accurate reasoning.
Evaluations on four public dash-cam video benchmarks show that iFinder's
proposed grounding with domain-specific cues, especially object orientation and
global context, significantly outperforms end-to-end V-VLMs on four zero-shot
driving benchmarks, with up to 39% gains in accident reasoning accuracy. By
grounding LLMs with driving domain-specific representations, iFinder offers a
zero-shot, interpretable, and reliable alternative to end-to-end V-VLMs for
post-hoc driving video understanding.

</details>


### [4] [CURE: Centroid-guided Unsupervised Representation Erasure for Facial Recognition Systems](https://arxiv.org/abs/2509.19562)
*Fnu Shivam,Nima Najafzadeh,Yenumula Reddy,Prashnna Gyawali*

Main category: cs.CV

TL;DR: 本研究提出CURE框架，解决面部识别系统中无监督机器遗忘的问题，提升遗忘效率，同时引入了UES评估标准。


<details>
  <summary>Details</summary>
Motivation: 解决现有机器遗忘技术在隐私受限环境和大型嘈杂数据集中的实用性问题。

Method: 提出CURE框架，通过无监督方式在没有身份标签的情况下进行机器遗忘。

Result: CURE显著超越了现有无监督遗忘方法，并提出了新的评估标准UES。

Conclusion: CURE框架在面部识别系统中实现了无监督的数据遗忘，显著提升了模型的性能和可靠性。

Abstract: In the current digital era, facial recognition systems offer significant
utility and have been widely integrated into modern technological
infrastructures; however, their widespread use has also raised serious privacy
concerns, prompting regulations that mandate data removal upon request. Machine
unlearning has emerged as a powerful solution to address this issue by
selectively removing the influence of specific user data from trained models
while preserving overall model performance. However, existing machine
unlearning techniques largely depend on supervised techniques requiring
identity labels, which are often unavailable in privacy-constrained situations
or in large-scale, noisy datasets. To address this critical gap, we introduce
CURE (Centroid-guided Unsupervised Representation Erasure), the first
unsupervised unlearning framework for facial recognition systems that operates
without the use of identity labels, effectively removing targeted samples while
preserving overall performance. We also propose a novel metric, the Unlearning
Efficiency Score (UES), which balances forgetting and retention stability,
addressing shortcomings in the current evaluation metrics. CURE significantly
outperforms unsupervised variants of existing unlearning methods. Additionally,
we conducted quality-aware unlearning by designating low-quality images as the
forget set, demonstrating its usability and benefits, and highlighting the role
of image quality in machine unlearning.

</details>


### [5] [Synthesizing Artifact Dataset for Pixel-level Detection](https://arxiv.org/abs/2509.19589)
*Dennis Menn,Feng Liang,Diana Marculescu*

Main category: cs.CV

TL;DR: 本论文提出了一种新的伪影检测器训练方法，通过自动注入伪影生成标注数据，显著提升了检测器性能，减少了对人工标注的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有的伪标签方法由于噪声标签导致效果不佳，并且高质量的人工标注成本高。

Method: 通过一种自动引入伪影的过程，在高质量合成图像的特定区域中注入伪影，从而产生像素级的标注数据。

Result: 所提出的方法在ConvNeXt和Swin-T模型上分别提升了13.2%和3.7%的性能，验证是在人工标注数据集上进行的。

Conclusion: 提出的方法使得可以在没有人工标注的情况下生成像素级的伪标签，从而有效提升了伪标签的质量和检测器的性能。

Abstract: Artifact detectors have been shown to enhance the performance of
image-generative models by serving as reward models during fine-tuning. These
detectors enable the generative model to improve overall output fidelity and
aesthetics. However, training the artifact detector requires expensive
pixel-level human annotations that specify the artifact regions. The lack of
annotated data limits the performance of the artifact detector. A naive
pseudo-labeling approach-training a weak detector and using it to annotate
unlabeled images-suffers from noisy labels, resulting in poor performance. To
address this, we propose an artifact corruption pipeline that automatically
injects artifacts into clean, high-quality synthetic images on a predetermined
region, thereby producing pixel-level annotations without manual labeling. The
proposed method enables training of an artifact detector that achieves
performance improvements of 13.2% for ConvNeXt and 3.7% for Swin-T, as verified
on human-labeled data, compared to baseline approaches. This work represents an
initial step toward scalable pixel-level artifact annotation datasets that
integrate world knowledge into artifact detection.

</details>


### [6] [Parameter-Efficient Multi-Task Learning via Progressive Task-Specific Adaptation](https://arxiv.org/abs/2509.19602)
*Neeraj Gangwar,Anshuka Rangi,Rishabh Deshmukh,Holakou Rahmanian,Yesh Dattatreya,Nickvash Kani*

Main category: cs.CV

TL;DR: 介绍了一种新的渐进式参数高效多任务适应方法，通过适配器模块解决多任务学习中的任务干扰问题，显著减少可训练参数并提高性能。


<details>
  <summary>Details</summary>
Motivation: 通过在初始层进行跨任务迁移学习，减轻任务之间的冲突，同时在预测头部实现特定任务学习。

Method: 提出了一种渐进的任务特定多任务适应方法，通过在预训练模型中引入适配器模块，初始层共享，后续层逐步特定于任务。

Result: 实验结果显示，方法在PASCAL和NYUD-v2数据集上表现优于完全微调的多任务模型，且仅需要五分之一的可训练参数。

Conclusion: 该方法在参数效率和多任务学习方面超过当前最先进的方法，并在样本测试中表现出色。

Abstract: Parameter-efficient fine-tuning methods have emerged as a promising solution
for adapting pre-trained models to various downstream tasks. While these
methods perform well in single-task learning, extending them to multi-task
learning exacerbates common challenges, such as task interference and negative
transfer, due to the limited number of trainable parameters. To address these
issues, we introduce progressive task-specific multi-task adaptation, a novel
parameter-efficient approach for multi-task learning. This approach introduces
adapter modules in a pre-trained model such that these modules are shared
across all tasks in the initial layers and become progressively more
task-specific in the later layers. The motivation is to reduce the conflicts
among tasks by allowing transfer learning across all tasks in the initial
layers and enabling task-specific learning toward the prediction heads.
Additionally, we propose a gradient-based approach for computing task
similarity and use this measure to allocate similar tasks to the shared adapter
modules. Our task similarity method introduces minimal overhead in the
pipeline. We evaluate our approach by adapting the Swin Transformer for dense
prediction tasks. Experiments on the PASCAL and NYUD-v2 datasets demonstrate
that our approach outperforms a fully fine-tuned multi-task model while
requiring only one-fifth of the trainable parameters. This approach achieves
better relative improvement to single-task fine-tuning while reducing the
number of trainable parameters and surpasses the current state-of-the-art
methods for parameter-efficient multi-task learning.

</details>


### [7] [Raw-JPEG Adapter: Efficient Raw Image Compression with JPEG](https://arxiv.org/abs/2509.19624)
*Mahmoud Afifi,Ran Zhang,Michael S. Brown*

Main category: cs.CV

TL;DR: 为了解决原始图像与JPEG压缩格式之间的适配问题，本文提出了RawJPEG Adapter，它通过轻量化的预处理方法有效提高了JPEG压缩的重建精度与兼容性。


<details>
  <summary>Details</summary>
Motivation: 在有限存储场景下，传统的原始数据存储格式如DNG容量大，不实用，而JPEG格式虽然支持广泛却不适合原始存储，因此需要一种新的方法来平衡这两者。

Method: 利用轻量化、可学习和可逆的预处理管道，对原始图像进行空间和可选频域转换，并将紧凑参数存储在JPEG注释字段中。

Result: 实验表明，RawJPEG Adapter在多个数据集上的表现优于直接JPEG存储，支持其他编解码器，且在压缩率和重建精度之间实现了良好的权衡。

Conclusion: RawJPEG Adapter提供了一种有效的方式，将原始图像适配为JPEG压缩格式，保持高重建精度，同时实现较好的压缩率。

Abstract: Digital cameras digitize scene light into linear raw representations, which
the image signal processor (ISP) converts into display-ready outputs. While raw
data preserves full sensor information--valuable for editing and vision
tasks--formats such as Digital Negative (DNG) require large storage, making
them impractical in constrained scenarios. In contrast, JPEG is a widely
supported format, offering high compression efficiency and broad compatibility,
but it is not well-suited for raw storage. This paper presents RawJPEG Adapter,
a lightweight, learnable, and invertible preprocessing pipeline that adapts raw
images for standard JPEG compression. Our method applies spatial and optional
frequency-domain transforms, with compact parameters stored in the JPEG comment
field, enabling accurate raw reconstruction. Experiments across multiple
datasets show that our method achieves higher fidelity than direct JPEG
storage, supports other codecs, and provides a favorable trade-off between
compression ratio and reconstruction accuracy.

</details>


### [8] [The Impact of 2D Segmentation Backbones on Point Cloud Predictions Using 4D Radar](https://arxiv.org/abs/2509.19644)
*William L. Muckelroy III,Mohammed Alsakabi,John M. Dolan,Ozan K. Tonguz*

Main category: cs.CV

TL;DR: 本研究探索了高容量分割网络对生成点云质量的影响，发现最优配置可提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LiDAR点云能够显著提高道路安全性，但其高成本限制了先进自动驾驶系统的广泛应用。

Method: 利用高容量的分割骨干网络来探讨对生成点云质量的影响。

Result: 经过测试发现，过高容量的模型可能会影响性能，而选择最佳的分割骨干网则能实现显著的改善。

Conclusion: 选择最佳的分割骨干网可以显著提升点云质量，提供超过23.7%的改进。

Abstract: LiDAR's dense, sharp point cloud (PC) representations of the surrounding
environment enable accurate perception and significantly improve road safety by
offering greater scene awareness and understanding. However, LiDAR's high cost
continues to restrict the broad adoption of high-level Autonomous Driving (AD)
systems in commercially available vehicles. Prior research has shown progress
towards circumventing the need for LiDAR by training a neural network, using
LiDAR point clouds as ground truth (GT), to produce LiDAR-like 3D point clouds
using only 4D Radars. One of the best examples is a neural network created to
train a more efficient radar target detector with a modular 2D convolutional
neural network (CNN) backbone and a temporal coherence network at its core that
uses the RaDelft dataset for training (see arXiv:2406.04723). In this work, we
investigate the impact of higher-capacity segmentation backbones on the quality
of the produced point clouds. Our results show that while very high-capacity
models may actually hurt performance, an optimal segmentation backbone can
provide a 23.7% improvement over the state-of-the-art (SOTA).

</details>


### [9] [Bias in the Picture: Benchmarking VLMs with Social-Cue News Images and LLM-as-Judge Assessment](https://arxiv.org/abs/2509.19659)
*Aravind Narayanan,Vahid Reza Khazaie,Shaina Raza*

Main category: cs.CV

TL;DR: 研究表明，视觉语言模型在处理图像和文本时易于吸收社会刻板印象，尤其在性别和职业方面的偏见风险高，提出了新的基准数据集以促进公正性评估。


<details>
  <summary>Details</summary>
Motivation: 研究大规模视觉语言模型（VLMs）在理解图像和文本时，可能会吸收并复制有害的社会刻板印象。

Method: 构建了一个包含1343组图像和问题的基准数据集，评估不同的最新视觉语言模型，并利用大型语言模型进行人类验证。

Result: 发现视觉上下文会系统性地影响模型输出，不同属性和模型的偏见普遍性存在差异，且更高的输出可靠性不一定意味着偏见更少。

Conclusion: 多模态模型对视觉上下文的敏感性会导致输出的偏见，特别是在性别和职业方面的风险较高。

Abstract: Large vision-language models (VLMs) can jointly interpret images and text,
but they are also prone to absorbing and reproducing harmful social stereotypes
when visual cues such as age, gender, race, clothing, or occupation are
present. To investigate these risks, we introduce a news-image benchmark
consisting of 1,343 image-question pairs drawn from diverse outlets, which we
annotated with ground-truth answers and demographic attributes (age, gender,
race, occupation, and sports). We evaluate a range of state-of-the-art VLMs and
employ a large language model (LLM) as judge, with human verification. Our
findings show that: (i) visual context systematically shifts model outputs in
open-ended settings; (ii) bias prevalence varies across attributes and models,
with particularly high risk for gender and occupation; and (iii) higher
faithfulness does not necessarily correspond to lower bias. We release the
benchmark prompts, evaluation rubric, and code to support reproducible and
fairness-aware multimodal assessment.

</details>


### [10] [MoTiC: Momentum Tightness and Contrast for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2509.19664)
*Zeyu He,Shuai Huang,Yuwu Lu,Ming Zhao*

Main category: cs.CV

TL;DR: 本文提出的MoTiC框架通过结合贝叶斯分析和大规模对比学习等方法，显著提高了新类原型的准确性并减少了估计偏差，在增量学习中展现出优异性能。


<details>
  <summary>Details</summary>
Motivation: 针对现有FSCIL方法在新类原型估计偏差和老类知识保留上的不足，提出了一种新的框架以提高增量学习的准确性和鲁棒性。

Method: 通过贝叶斯分析对新类先验与旧类统计信息进行对齐，结合大规模对比学习和动量自监督等方法，形成了MoTiC框架。

Result: 在三个FSCIL基准测试中的实验表明，所提出的方法在性能上达到了最新的 state-of-the-art，特别是在CUB-200这一细粒度任务上表现出色。

Conclusion: 本文提出的MoTiC框架通过引入动量自监督和虚拟类别，增强了特征空间的表现力，同时减少了新类原型的估计偏差，提升了增量学习的鲁棒性，尤其在细粒度任务CUB-200上的表现优越。

Abstract: Few-Shot Class-Incremental Learning (FSCIL) must contend with the dual
challenge of learning new classes from scarce samples while preserving old
class knowledge. Existing methods use the frozen feature extractor and
class-averaged prototypes to mitigate against catastrophic forgetting and
overfitting. However, new-class prototypes suffer significant estimation bias
due to extreme data scarcity, whereas base-class prototypes benefit from
sufficient data. In this work, we theoretically demonstrate that aligning the
new-class priors with old-class statistics via Bayesian analysis reduces
variance and improves prototype accuracy. Furthermore, we propose large-scale
contrastive learning to enforce cross-category feature tightness. To further
enrich feature diversity and inject prior information for new-class prototypes,
we integrate momentum self-supervision and virtual categories into the Momentum
Tightness and Contrast framework (MoTiC), constructing a feature space with
rich representations and enhanced interclass cohesion. Experiments on three
FSCIL benchmarks produce state-of-the-art performances, particularly on the
fine-grained task CUB-200, validating our method's ability to reduce estimation
bias and improve incremental learning robustness.

</details>


### [11] [Deep Learning for Clouds and Cloud Shadow Segmentation in Methane Satellite and Airborne Imaging Spectroscopy](https://arxiv.org/abs/2509.19665)
*Manuel Perez-Carrasco,Maya Nasr,Sebastien Roche,Chris Chan Miller,Zhan Zhang,Core Francisco Park,Eleanor Walker,Cecilia Garraffo,Douglas Finkbeiner,Ritesh Gautam,Steven Wofsy*

Main category: cs.CV

TL;DR: 本研究利用机器学习方法，尤其深度学习模型提升了高分辨率传感器云和云影检测的效果，强调了光谱注意力在特定数据集上的优势。


<details>
  <summary>Details</summary>
Motivation: 有效的云和云影检测是准确检索大气甲烷浓度的重要前提，尤其适用于MethaneSAT及其航空伙伴任务MethaneAIR。

Method: 应用机器学习方法，包括传统的迭代逻辑回归（ILR）、多层感知器（MLP），及先进的深度学习架构UNet和光谱通道注意力网络（SCAN）。

Result: 传统方法在空间一致性和边界定义上存在不足，导致云和云影检测效果不佳；而深度学习模型在检测质量上显著增强。

Conclusion: 深度学习模型显著提升了云和云影的检测质量，特别是SCAN在MethaneSAT数据上表现优异，强调了结合光谱注意力的重要性。

Abstract: Effective cloud and cloud shadow detection is a critical prerequisite for
accurate retrieval of concentrations of atmospheric methane or other trace
gases in hyperspectral remote sensing. This challenge is especially pertinent
for MethaneSAT and for its airborne companion mission, MethaneAIR. In this
study, we use machine learning methods to address the cloud and cloud shadow
detection problem for sensors with these high spatial resolutions instruments.
Cloud and cloud shadows in remote sensing data need to be effectively screened
out as they bias methane retrievals in remote sensing imagery and impact the
quantification of emissions. We deploy and evaluate conventional techniques
including Iterative Logistic Regression (ILR) and Multilayer Perceptron (MLP),
with advanced deep learning architectures, namely UNet and a Spectral Channel
Attention Network (SCAN) method. Our results show that conventional methods
struggle with spatial coherence and boundary definition, affecting the
detection of clouds and cloud shadows. Deep learning models substantially
improve detection quality: UNet performs best in preserving spatial structure,
while SCAN excels at capturing fine boundary details. Notably, SCAN surpasses
UNet on MethaneSAT data, underscoring the benefits of incorporating spectral
attention for satellite specific features. This in depth assessment of various
disparate machine learning techniques demonstrates the strengths and
effectiveness of advanced deep learning architectures in providing robust,
scalable solutions for clouds and cloud shadow screening towards enhancing
methane emission quantification capacity of existing and next generation
hyperspectral missions. Our data and code is publicly available at
https://doi.org/10.7910/DVN/IKLZOJ

</details>


### [12] [Enhancing Transformer-Based Vision Models: Addressing Feature Map Anomalies Through Novel Optimization Strategies](https://arxiv.org/abs/2509.19687)
*Sumit Mamtani*

Main category: cs.CV

TL;DR: 提出STA和ANF两种轻量级优化技术，以提升ViTs的可解释性，改善特征图中的结构化噪声问题，经过验证具有实际有效性。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在计算机视觉任务中表现出色，但特征图中的结构化噪声伪影限制了其下游应用的潜力。

Method: 采用结构化令牌增强（STA）和自适应噪声过滤（ANF）两种优化技术，STA通过空间扰动增强令牌多样性，ANF则在变换层之间应用可学习的内联去噪。

Result: 在标准基准上（包括ImageNet、Ade20k和NYUv2）评估后，实验结果显示视觉质量和任务表现均有持续改进。

Conclusion: 提出的结构化令牌增强（STA）和自适应噪声过滤（ANF）技术在视觉质量和任务表现上均取得一致改进，具有实际有效性。

Abstract: Vision Transformers (ViTs) have demonstrated superior performance across a
wide range of computer vision tasks. However, structured noise artifacts in
their feature maps hinder downstream applications such as segmentation and
depth estimation. We propose two novel and lightweight optimisation techniques-
Structured Token Augmentation (STA) and Adaptive Noise Filtering (ANF)- to
improve interpretability and mitigate these artefacts. STA enhances token
diversity through spatial perturbations during tokenisation, while ANF applies
learnable inline denoising between transformer layers. These methods are
architecture-agnostic and evaluated across standard benchmarks, including
ImageNet, Ade20k, and NYUv2. Experimental results show consistent improvements
in visual quality and task performance, highlighting the practical
effectiveness of our approach.

</details>


### [13] [From Prompt to Progression: Taming Video Diffusion Models for Seamless Attribute Transition](https://arxiv.org/abs/2509.19690)
*Ling Lo,Kelvin C. K. Chan,Wen-Huang Cheng,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 提出了一种新方法通过逐帧指导改善视频生成中的属性过渡，推出了CAT-Bench基准以评估不同模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有模型在生成具有渐变属性过渡的视频时，往往难以处理复杂的时间变化，导致一致性问题明显。

Method: 通过在去噪过程中引入逐帧指导构建数据特定的过渡方向，平滑一致地引导属性过渡。

Result: 实验结果显示，我们的方法相较于现有基线具有优势，能够保持视觉保真度并实现属性的平滑过渡。

Conclusion: 我们的方法在生成视觉质量、与文本提示对齐及提供无缝属性过渡方面表现优异。

Abstract: Existing models often struggle with complex temporal changes, particularly
when generating videos with gradual attribute transitions. The most common
prompt interpolation approach for motion transitions often fails to handle
gradual attribute transitions, where inconsistencies tend to become more
pronounced. In this work, we propose a simple yet effective method to extend
existing models for smooth and consistent attribute transitions, through
introducing frame-wise guidance during the denoising process. Our approach
constructs a data-specific transitional direction for each noisy latent,
guiding the gradual shift from initial to final attributes frame by frame while
preserving the motion dynamics of the video. Moreover, we present the
Controlled-Attribute-Transition Benchmark (CAT-Bench), which integrates both
attribute and motion dynamics, to comprehensively evaluate the performance of
different models. We further propose two metrics to assess the accuracy and
smoothness of attribute transitions. Experimental results demonstrate that our
approach performs favorably against existing baselines, achieving visual
fidelity, maintaining alignment with text prompts, and delivering seamless
attribute transitions. Code and CATBench are released:
https://github.com/lynn-ling-lo/Prompt2Progression.

</details>


### [14] [Anatomically Constrained Transformers for Cardiac Amyloidosis Classification](https://arxiv.org/abs/2509.19691)
*Alexander Thorley,Agis Chartsias,Jordan Strom,Roberto Lang,Jeremy Slivnick,Jamie O'Driscoll,Rajan Sharma,Dipak Kotecha,Jinming Duan,Alberto Gomez*

Main category: cs.CV

TL;DR: 本研究提出一种受解剖学约束的变换器模型，以提高心脏淀粉样变性的分类性能，同时确保分类依赖于临床特征。


<details>
  <summary>Details</summary>
Motivation: 为确保分类与临床相关特征相关，驱动我们将模型应用于定量特征，如心肌的整体纵向应变。

Method: 通过将变换器模型约束到心肌的解剖区域，并在自监督学习中仅遮掩和重建解剖图块。

Result: 我们的模型相比于传统完整视频变换器在心脏淀粉样变性分类任务中表现出更高的性能。

Conclusion: 使用解剖学约束和遮掩重建方法的变换器模型在心脏淀粉样变性分类任务中表现优越，且能够可视化变换器的注意力分数。

Abstract: Cardiac amyloidosis (CA) is a rare cardiomyopathy, with typical abnormalities
in clinical measurements from echocardiograms such as reduced global
longitudinal strain of the myocardium. An alternative approach for detecting CA
is via neural networks, using video classification models such as convolutional
neural networks. These models process entire video clips, but provide no
assurance that classification is based on clinically relevant features known to
be associated with CA. An alternative paradigm for disease classification is to
apply models to quantitative features such as strain, ensuring that the
classification relates to clinically relevant features. Drawing inspiration
from this approach, we explicitly constrain a transformer model to the
anatomical region where many known CA abnormalities occur -- the myocardium,
which we embed as a set of deforming points and corresponding sampled image
patches into input tokens. We show that our anatomical constraint can also be
applied to the popular self-supervised learning masked autoencoder
pre-training, where we propose to mask and reconstruct only anatomical patches.
We show that by constraining both the transformer and pre-training task to the
myocardium where CA imaging features are localized, we achieve increased
performance on a CA classification task compared to full video transformers.
Our model provides an explicit guarantee that the classification is focused on
only anatomical regions of the echo, and enables us to visualize transformer
attention scores over the deforming myocardium.

</details>


### [15] [Learning to Stop: Reinforcement Learning for Efficient Patient-Level Echocardiographic Classification](https://arxiv.org/abs/2509.19694)
*Woo-Jin Cho Kim,Jorge Oliveira,Arian Beqiri,Alex Thorley,Jordan Strom,Jamie O'Driscoll,Rajan Sharma,Jeremy Slivnick,Roberto Lang,Alberto Gomez,Agisilaos Chartsias*

Main category: cs.CV

TL;DR: 本研究通过强化学习提出一种方法，以选择最佳视频片段子集进行心脏疾病分类，显著降低了计算成本并提高了性能。


<details>
  <summary>Details</summary>
Motivation: 选择最佳的片段子集以最大化特定任务（基于图像的疾病分类）的性能，因为依赖单个片段会忽视其他片段提供的互补信息，而使用所有片段在计算上代价高昂，可能不利于临床应用。

Method: 通过强化学习优化的方法，学习在处理特定视图的片段以降低疾病分类的不确定性，或者在分类信心达到足够时停止处理。还提出了一种可学习的基于注意力的聚合方法，以灵活融合多个片段的信息。

Result: 在检测心脏淀粉样变的任务中，所提议的方法在仅使用30%的所有片段时取得了AUC 0.91的结果。

Conclusion: 提议的方法在仅使用30%的视频片段时，心脏淀粉样变检测任务的AUC达到了0.91，超越了使用所有片段和其他基准所取得的表现。

Abstract: Guidelines for transthoracic echocardiographic examination recommend the
acquisition of multiple video clips from different views of the heart,
resulting in a large number of clips. Typically, automated methods, for
instance disease classifiers, either use one clip or average predictions from
all clips. Relying on one clip ignores complementary information available from
other clips, while using all clips is computationally expensive and may be
prohibitive for clinical adoption.
  To select the optimal subset of clips that maximize performance for a
specific task (image-based disease classification), we propose a method
optimized through reinforcement learning. In our method, an agent learns to
either keep processing view-specific clips to reduce the disease classification
uncertainty, or stop processing if the achieved classification confidence is
sufficient. Furthermore, we propose a learnable attention-based aggregation
method as a flexible way of fusing information from multiple clips. The
proposed method obtains an AUC of 0.91 on the task of detecting cardiac
amyloidosis using only 30% of all clips, exceeding the performance achieved
from using all clips and from other benchmarks.

</details>


### [16] [Towards Robust In-Context Learning for Medical Image Segmentation via Data Synthesis](https://arxiv.org/abs/2509.19711)
*Jiesi Hu,Yanwu Yang,Zhiyu Ye,Chenfei Ye,Hanyang Peng,Jianfeng Cao,Ting Ma*

Main category: cs.CV

TL;DR: SynthICL是一个新数据合成框架，通过结合解剖先验，解决了医学图像分割中数据匮乏的问题，显著提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决医学图像分割中数据匮乏的问题，满足ICL对大规模多样化数据集的需求。

Method: SynthICL是一个基于领域随机化的全新数据合成框架。

Result: 在四个保留数据集上的实验表明，使用SynthICL生成的数据使模型在平均Dice上提升了63%，同时在未见解剖域上增强了泛化能力。

Conclusion: SynthICL通过结合真实数据的解剖先验，生成多样化的解剖结构并建模个体差异，显著提升了ICL的医学图像分割性能。

Abstract: The rise of In-Context Learning (ICL) for universal medical image
segmentation has introduced an unprecedented demand for large-scale, diverse
datasets for training, exacerbating the long-standing problem of data scarcity.
While data synthesis offers a promising solution, existing methods often fail
to simultaneously achieve both high data diversity and a domain distribution
suitable for medical data. To bridge this gap, we propose \textbf{SynthICL}, a
novel data synthesis framework built upon domain randomization. SynthICL
ensures realism by leveraging anatomical priors from real-world datasets,
generates diverse anatomical structures to cover a broad data distribution, and
explicitly models inter-subject variations to create data cohorts suitable for
ICL. Extensive experiments on four held-out datasets validate our framework's
effectiveness, showing that models trained with our data achieve performance
gains of up to 63\% in average Dice and substantially enhanced generalization
to unseen anatomical domains. Our work helps mitigate the data bottleneck for
ICL-based segmentation, paving the way for robust models. Our code and the
generated dataset are publicly available at
https://github.com/jiesihu/Neuroverse3D.

</details>


### [17] [VIMD: Monocular Visual-Inertial Motion and Depth Estimation](https://arxiv.org/abs/2509.19713)
*Saimouli Katragadda,Guoquan Huang*

Main category: cs.CV

TL;DR: 本文提出了一种新型的单目视觉惯性运动和深度估计学习框架，名为VIMD，能够在资源受限的情况下实现高效的深度估计，并具备良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 开发准确且高效的稠密度量深度估计方法，以提高机器人和增强现实中的3D视觉感知能力。

Method: 通过基于MSCKF的单目视觉惯性运动跟踪，开发了一种单目视觉惯性运动和深度估计学习框架，利用多视角信息迭代精细化每个像素的尺度。

Result: 在TartanAir和VOID数据集上进行了广泛评估，并在AR Table数据集上展示了零样本泛化能力，结果表明即使使用极少的深度点，VIMD也能实现卓越的准确性和鲁棒性。

Conclusion: VIMD框架具备出色的精度和鲁棒性，适合在资源受限环境中应用，且具有良好的泛化能力，适用于多种场景。

Abstract: Accurate and efficient dense metric depth estimation is crucial for 3D visual
perception in robotics and XR. In this paper, we develop a monocular
visual-inertial motion and depth (VIMD) learning framework to estimate dense
metric depth by leveraging accurate and efficient MSCKF-based monocular
visual-inertial motion tracking. At the core the proposed VIMD is to exploit
multi-view information to iteratively refine per-pixel scale, instead of
globally fitting an invariant affine model as in the prior work. The VIMD
framework is highly modular, making it compatible with a variety of existing
depth estimation backbones. We conduct extensive evaluations on the TartanAir
and VOID datasets and demonstrate its zero-shot generalization capabilities on
the AR Table dataset. Our results show that VIMD achieves exceptional accuracy
and robustness, even with extremely sparse points as few as 10-20 metric depth
points per image. This makes the proposed VIMD a practical solution for
deployment in resource constrained settings, while its robust performance and
strong generalization capabilities offer significant potential across a wide
range of scenarios.

</details>


### [18] [Frequency-domain Multi-modal Fusion for Language-guided Medical Image Segmentation](https://arxiv.org/abs/2509.19719)
*Bo Yu,Jianhua Yang,Zetao Du,Yan Huang,Chenglong Li,Liang Wang*

Main category: cs.CV

TL;DR: FMISeg是一种新型的频域多模态交互模型，通过双向和语言引导的特征交互，提升了医学图像的分割效果。


<details>
  <summary>Details</summary>
Motivation: 针对现有医学图像分割方法在处理复杂病变形态和视觉-语言模态间的语义差距的问题。

Method: 提出一种频域多模态交互模型（FMISeg），通过频域特征双向交互模块（FFBI）和语言引导频域特征交互模块（LFFI）进行语言引导医学图像分割。

Result: 在QaTa-COV19和MosMedData+数据集上的实验表明，该方法在定性和定量表现上均超过了现有方法。

Conclusion: FMISeg模型在质和量上均优于现有最先进方法，展示了其对医学图像分割的有效性。

Abstract: Automatically segmenting infected areas in radiological images is essential
for diagnosing pulmonary infectious diseases. Recent studies have demonstrated
that the accuracy of the medical image segmentation can be improved by
incorporating clinical text reports as semantic guidance. However, the complex
morphological changes of lesions and the inherent semantic gap between
vision-language modalities prevent existing methods from effectively enhancing
the representation of visual features and eliminating semantically irrelevant
information, ultimately resulting in suboptimal segmentation performance. To
address these problems, we propose a Frequency-domain Multi-modal Interaction
model (FMISeg) for language-guided medical image segmentation. FMISeg is a late
fusion model that establishes interaction between linguistic features and
frequency-domain visual features in the decoder. Specifically, to enhance the
visual representation, our method introduces a Frequency-domain Feature
Bidirectional Interaction (FFBI) module to effectively fuse frequency-domain
features. Furthermore, a Language-guided Frequency-domain Feature Interaction
(LFFI) module is incorporated within the decoder to suppress semantically
irrelevant visual features under the guidance of linguistic information.
Experiments on QaTa-COV19 and MosMedData+ demonstrated that our method
outperforms the state-of-the-art methods qualitatively and quantitatively.

</details>


### [19] [PolGS: Polarimetric Gaussian Splatting for Fast Reflective Surface Reconstruction](https://arxiv.org/abs/2509.19726)
*Yufei Han,Bowen Tie,Heng Guo,Youwei Lyu,Si Li,Boxin Shi,Yunpeng Jia,Zhanyu Ma*

Main category: cs.CV

TL;DR: PolGS是一个有效的极化高斯点云模型，通过结合极化约束，提升了复杂反射表面重建的速度与质量。


<details>
  <summary>Details</summary>
Motivation: 提高复杂反射特性表面的重建质量，解决现有方法在重建速度和质量上的不足。

Method: 提出了一种结合极化约束的3D Gaussian Splatting框架。

Result: 在合成和真实世界数据集上的实验结果证明了方法的有效性，能够在10分钟内快速重建反射表面。

Conclusion: PolGS模型显著提升了复杂反射表面重建的质量，尤其在快速反射表面重建方面表现优异。

Abstract: Efficient shape reconstruction for surfaces with complex reflectance
properties is crucial for real-time virtual reality. While 3D Gaussian
Splatting (3DGS)-based methods offer fast novel view rendering by leveraging
their explicit surface representation, their reconstruction quality lags behind
that of implicit neural representations, particularly in the case of recovering
surfaces with complex reflective reflectance. To address these problems, we
propose PolGS, a Polarimetric Gaussian Splatting model allowing fast reflective
surface reconstruction in 10 minutes. By integrating polarimetric constraints
into the 3DGS framework, PolGS effectively separates specular and diffuse
components, enhancing reconstruction quality for challenging reflective
materials. Experimental results on the synthetic and real-world dataset
validate the effectiveness of our method.

</details>


### [20] [CAMILA: Context-Aware Masking for Image Editing with Language Alignment](https://arxiv.org/abs/2509.19731)
*Hyunseung Kim,Chiho Choi,Srikanth Malla,Sai Prahladh Padmanabhan,Saurabh Bagchi,Joon Hee Choi*

Main category: cs.CV

TL;DR: 本文提出了一种上下文感知的图像编辑方法CAMILA，有效提高了对复杂指令的处理能力，确保编辑一致性和图像完整性。


<details>
  <summary>Details</summary>
Motivation: 解决现有图像编辑模型在处理用户指令时的局限性，尤其是针对不合理或矛盾的指令导致的 nonsensical output 问题。

Method: 提出了一种名为CAMILA的上下文感知图像编辑方法，通过上下文验证确保仅对相关区域应用编辑，同时忽略不可执行的指令。

Result: CAMILA在单指令和多指令图像编辑任务中表现优于现有最先进模型，显示出更高的性能和语义对齐。

Conclusion: CAMILA方法在处理复杂指令时表现出色，能够有效提高图像编辑的语义一致性和对图像完整性的保护。

Abstract: Text-guided image editing has been allowing users to transform and synthesize
images through natural language instructions, offering considerable
flexibility. However, most existing image editing models naively attempt to
follow all user instructions, even if those instructions are inherently
infeasible or contradictory, often resulting in nonsensical output. To address
these challenges, we propose a context-aware method for image editing named as
CAMILA (Context-Aware Masking for Image Editing with Language Alignment).
CAMILA is designed to validate the contextual coherence between instructions
and the image, ensuring that only relevant edits are applied to the designated
regions while ignoring non-executable instructions. For comprehensive
evaluation of this new method, we constructed datasets for both single- and
multi-instruction image editing, incorporating the presence of infeasible
requests. Our method achieves better performance and higher semantic alignment
than state-of-the-art models, demonstrating its effectiveness in handling
complex instruction challenges while preserving image integrity.

</details>


### [21] [Robust RGB-T Tracking via Learnable Visual Fourier Prompt Fine-tuning and Modality Fusion Prompt Generation](https://arxiv.org/abs/2509.19733)
*Hongtao Yang,Bineng Zhong,Qihua Liang,Zhiruo Zhu,Yaozong Zheng,Ning Li*

Main category: cs.CV

TL;DR: VFPTrack是一种结合空间和频域信息的新方法，显著提高了RGB-T热成像跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 针对现有RGB-T跟踪方法仅依赖空间域信息的问题，引入频域信息以提高提示学习效果。

Method: 利用快速傅里叶变换(FFT)提取频域特征，并结合空间域的视觉提示进行多模态特征融合。

Result: 在三个热门RGB-T跟踪基准上进行了大量实验，结果表明VFPTrack方法表现优异。

Conclusion: VFPTrack方法通过结合空间和频域信息 显著提升了RGB-T热成像跟踪的性能。

Abstract: Recently, visual prompt tuning is introduced to RGB-Thermal (RGB-T) tracking
as a parameter-efficient finetuning (PEFT) method. However, these PEFT-based
RGB-T tracking methods typically rely solely on spatial domain information as
prompts for feature extraction. As a result, they often fail to achieve optimal
performance by overlooking the crucial role of frequency-domain information in
prompt learning. To address this issue, we propose an efficient Visual Fourier
Prompt Tracking (named VFPTrack) method to learn modality-related prompts via
Fast Fourier Transform (FFT). Our method consists of symmetric feature
extraction encoder with shared parameters, visual fourier prompts, and Modality
Fusion Prompt Generator that generates bidirectional interaction prompts
through multi-modal feature fusion. Specifically, we first use a frozen feature
extraction encoder to extract RGB and thermal infrared (TIR) modality features.
Then, we combine the visual prompts in the spatial domain with the frequency
domain prompts obtained from the FFT, which allows for the full extraction and
understanding of modality features from different domain information. Finally,
unlike previous fusion methods, the modality fusion prompt generation module we
use combines features from different modalities to generate a fused modality
prompt. This modality prompt is interacted with each individual modality to
fully enable feature interaction across different modalities. Extensive
experiments conducted on three popular RGB-T tracking benchmarks show that our
method demonstrates outstanding performance.

</details>


### [22] [Rectified Decoupled Dataset Distillation: A Closer Look for Fair and Comprehensive Evaluation](https://arxiv.org/abs/2509.19743)
*Xinhao Zhong,Shuoyang Sun,Xulin Gu,Chenyang Zhu,Bin Chen,Yaowei Wang*

Main category: cs.CV

TL;DR: 本研究提出RD$^3$，通过建立标准化评估协议，解决了现有数据集精炼方法中性能评估不一致的问题，并改进了精炼数据集的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前已有的方法在后评估协议上不一致，阻碍了领域的进展，因此需要一种新方法来改进数据集精炼过程。

Method: 提出了Rectified Decoupled Dataset Distillation (RD$^3$)，并系统地研究了不同后评估设置如何影响测试准确性。

Result: 研究表明，现有方法间的性能差异主要源于评估不一致，而非合成数据的内在质量差异。同时发现了一些通用策略，提升了不同设置下的精炼数据集有效性。

Conclusion: RD$^3$建立了一个标准化基准和严格的评估协议，为未来数据集精炼研究提供了公平和可重复的比较基础。

Abstract: Dataset distillation aims to generate compact synthetic datasets that enable
models trained on them to achieve performance comparable to those trained on
full real datasets, while substantially reducing storage and computational
costs. Early bi-level optimization methods (e.g., MTT) have shown promising
results on small-scale datasets, but their scalability is limited by high
computational overhead. To address this limitation, recent decoupled dataset
distillation methods (e.g., SRe$^2$L) separate the teacher model pre-training
from the synthetic data generation process. These methods also introduce random
data augmentation and epoch-wise soft labels during the post-evaluation phase
to improve performance and generalization. However, existing decoupled
distillation methods suffer from inconsistent post-evaluation protocols, which
hinders progress in the field. In this work, we propose Rectified Decoupled
Dataset Distillation (RD$^3$), and systematically investigate how different
post-evaluation settings affect test accuracy. We further examine whether the
reported performance differences across existing methods reflect true
methodological advances or stem from discrepancies in evaluation procedures.
Our analysis reveals that much of the performance variation can be attributed
to inconsistent evaluation rather than differences in the intrinsic quality of
the synthetic data. In addition, we identify general strategies that improve
the effectiveness of distilled datasets across settings. By establishing a
standardized benchmark and rigorous evaluation protocol, RD$^3$ provides a
foundation for fair and reproducible comparisons in future dataset distillation
research.

</details>


### [23] [nnFilterMatch: A Unified Semi-Supervised Learning Framework with Uncertainty-Aware Pseudo-Label Filtering for Efficient Medical Segmentation](https://arxiv.org/abs/2509.19746)
*Yi Yang*

Main category: cs.CV

TL;DR: 本研究提出的nnFilterMatch框架通过结合SSL与伪标签过滤，提供了高效的医学图像分割解决方案，减少了标注需求且保持了准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的SSL-AL混合方法依赖于迭代与循环重训练，造成计算开销，限制了临床应用的可扩展性。

Method: 提出了一个新颖的深度分割框架nnFilterMatch，结合了半监督学习（SSL）与基于熵的伪标签过滤机制。

Result: 通过选择性排除高置信度的伪标签，nnFilterMatch无须进行重训练循环，并且在保持不确定性引导学习优点的同时，实现了高效的注释需求降低。

Conclusion: 本研究提出的nnFilterMatch框架在多个临床分割基准上验证了其有效性，表现超过或可比完全监督模型，且仅需5%-20%的标注数据。

Abstract: Semi-supervised learning (SSL) has emerged as a promising paradigm in medical
image segmentation, offering competitive performance while substantially
reducing the need for extensive manual annotation. When combined with active
learning (AL), these strategies further minimize annotation burden by
selectively incorporating the most informative samples. However, conventional
SSL_AL hybrid approaches often rely on iterative and loop-based retraining
cycles after each annotation round, incurring significant computational
overhead and limiting scalability in clinical applications. In this study, we
present a novel, annotation-efficient, and self-adaptive deep segmentation
framework that integrates SSL with entropy-based pseudo-label filtering
(FilterMatch), an AL-inspired mechanism, within the single-pass nnU-Net
training segmentation framework (nnFilterMatch). By selectively excluding
high-confidence pseudo-labels during training, our method circumvents the need
for retraining loops while preserving the benefits of uncertainty-guided
learning. We validate the proposed framework across multiple clinical
segmentation benchmarks and demonstrate that it achieves performance comparable
to or exceeding fully supervised models, even with only 5\%--20\% labeled data.
This work introduces a scalable, end-to-end learning strategy for reducing
annotation demands in medical image segmentation without compromising accuracy.
Code is available here: https://github.com/Ordi117/nnFilterMatch.git.

</details>


### [24] [Talking Head Generation via AU-Guided Landmark Prediction](https://arxiv.org/abs/2509.19749)
*Shao-Yu Chang,Jingyi Xu,Hieu Le,Dimitris Samaras*

Main category: cs.CV

TL;DR: 提出一种基于面部动作单位的两阶段谈话头生成框架，改善了表达准确性和视觉真实感。


<details>
  <summary>Details</summary>
Motivation: 通过显式映射面部动作单位（AUs）到二维面部标记，实现更精确的表达控制。

Method: 采用两阶段框架：第一阶段使用变分运动生成器预测面部标记序列，第二阶段使用扩散合成器生成真实视频。

Result: 在MEAD数据集上测试结果表明，本文方法在多个指标上优于现有技术。

Conclusion: 该方法在表现力丰富的谈话头生成方面超越了现有的最先进技术，验证了显式AU与面部标记建模的有效性。

Abstract: We propose a two-stage framework for audio-driven talking head generation
with fine-grained expression control via facial Action Units (AUs). Unlike
prior methods relying on emotion labels or implicit AU conditioning, our model
explicitly maps AUs to 2D facial landmarks, enabling physically grounded,
per-frame expression control. In the first stage, a variational motion
generator predicts temporally coherent landmark sequences from audio and AU
intensities. In the second stage, a diffusion-based synthesizer generates
realistic, lip-synced videos conditioned on these landmarks and a reference
image. This separation of motion and appearance improves expression accuracy,
temporal stability, and visual realism. Experiments on the MEAD dataset show
that our method outperforms state-of-the-art baselines across multiple metrics,
demonstrating the effectiveness of explicit AU-to-landmark modeling for
expressive talking head generation.

</details>


### [25] [ExpFace: Exponential Angular Margin Loss for Deep Face Recognition](https://arxiv.org/abs/2509.19753)
*Jinhui Zheng,Xueyuan Gong*

Main category: cs.CV

TL;DR: ExpFace提出了一种新的损失函数，通过强调干净样本并抑制噪声样本，改善面部识别中的表现。


<details>
  <summary>Details</summary>
Motivation: 观察到干净样本在角度空间中主要集中于中心区域，而噪声样本倾向于移向边缘区域，因此需要强调干净样本并抑制噪声样本。

Method: 提出一种新的Exponential Angular Margin Loss (ExpFace)，通过引入角度指数项作为损失的边距。

Result: ExpFace实现了最新的最先进性能，克服了传统Margin-based softmax损失的一些限制。

Conclusion: ExpFace在面部识别任务中表现出色，超越了现有Margin-based softmax损失，且具有更好的稳定性和一致性。

Abstract: Face recognition is an open-set problem requiring high discriminative power
to ensure that intra-class distances remain smaller than inter-class distances.
Margin-based softmax losses, such as SphereFace, CosFace, and ArcFace, have
been widely adopted to enhance intra-class compactness and inter-class
separability, yet they overlook the impact of noisy samples. By examining the
distribution of samples in the angular space, we observe that clean samples
predominantly cluster in the center region, whereas noisy samples tend to shift
toward the peripheral region. Motivated by this observation, we propose the
Exponential Angular Margin Loss (ExpFace), which introduces an angular
exponential term as the margin. This design applies a larger penalty in the
center region and a smaller penalty in the peripheral region within the angular
space, thereby emphasizing clean samples while suppressing noisy samples. We
present a unified analysis of ExpFace and classical margin-based softmax losses
in terms of margin embedding forms, similarity curves, and gradient curves,
showing that ExpFace not only avoids the training instability of SphereFace and
the non-monotonicity of ArcFace, but also exhibits a similarity curve that
applies penalties in the same manner as the decision boundary in the angular
space. Extensive experiments demonstrate that ExpFace achieves state-of-the-art
performance. To facilitate future research, we have released the source code
at: https://github.com/dfr-code/ExpFace.

</details>


### [26] [Logics-Parsing Technical Report](https://arxiv.org/abs/2509.19760)
*Xiangyang Chen,Shuzhao Li,Xiuwen Zhu,Yongfan Chen,Fan Yang,Cheng Fang,Lin Qu,Xiaoxiao Xu,Hu Wei,Minggang Wu*

Main category: cs.CV

TL;DR: 本文提出了一种增强的LVLM模型，通过引入强化学习和多样数据类型，提升了复杂文档解析的能力，并在相应基准上展示了其先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前LVLM在处理复杂文档类型（如多列报纸或海报）时，缺乏明确的分析阶段，限制了其能力。

Method: 提出了一种基于强化学习的端到端LVLM模型，并设计了奖励机制以优化复杂的布局分析和阅读顺序推断。

Result: 通过在LogicsParsingBench上的全面实验，我们验证了所提模型的有效性和性能。

Conclusion: 我们的模型在各种文档分析场景中表现出卓越的效果和先进的性能。

Abstract: Recent advances in Large Vision-Language models (LVLM) have spurred
significant progress in document parsing task. Compared to traditional
pipeline-based methods, end-to-end paradigms have shown their excellence in
converting PDF images into structured outputs through integrated Optical
Character Recognition (OCR), table recognition, mathematical formula
recognition and so on. However, the absence of explicit analytical stages for
document layouts and reading orders limits the LVLM's capability in handling
complex document types such as multi-column newspapers or posters. To address
this limitation, we propose in this report Logics-Parsing: an end-to-end
LVLM-based model augmented with reinforcement learning. Our model incorporates
meticulously designed reward mechanisms to optimize complex layout analysis and
reading order inference. In addition, we expand the model's versatility by
incorporating diverse data types such as chemical formulas and handwritten
Chinese characters into supervised fine-tuning. Finally, to enable rigorous
evaluation of our approach, we introduce LogicsParsingBench, a curated set of
1,078 page-level PDF images spanning nine major categories and over twenty
sub-categories, which will be released later. Comprehensive experiments
conducted on LogicsParsingBench have validated the efficacy and
State-of-the-art (SOTA) performance of our proposed model across diverse
document analysis scenarios. Project Page:
https://github.com/alibaba/Logics-Parsing

</details>


### [27] [Sex-based Bias Inherent in the Dice Similarity Coefficient: A Model Independent Analysis for Multiple Anatomical Structures](https://arxiv.org/abs/2509.19778)
*Hartmut Häntze,Myrthe Buser,Alessa Hering,Lisa C. Adams,Keno K. Bressem*

Main category: cs.CV

TL;DR: 研究表明，Dice相似系数对分割性能的评估存在性别偏差，特别是在器官大小的差异影响下，因此在医学图像分析中需要更准确公正的评估方法。


<details>
  <summary>Details</summary>
Motivation: 探讨DSC作为评估指标在分割性能中的性别偏差，特别是在不同器官大小对DSC的影响。

Method: 本研究采用均匀大小的合成错误，对50名参与者的手动MRI注释进行了量化分析，以实现性别基础的可比性。

Result: 产生的系统性DSC性别差异显示，对于小结构而言，平均差异约为0.03；中等结构约为0.01；而大型结构大致不受影响，性别之间的DSC差异接近零。

Conclusion: 本研究揭示了Dice相似系数（DSC）在医学图像分割中引入的性别偏差，强调了在使用DSC作为评估指标时，男性和女性之间的评分不应被视为完全一致。

Abstract: Overlap-based metrics such as the Dice Similarity Coefficient (DSC) penalize
segmentation errors more heavily in smaller structures. As organ size differs
by sex, this implies that a segmentation error of equal magnitude may result in
lower DSCs in women due to their smaller average organ volumes compared to men.
While previous work has examined sex-based differences in models or datasets,
no study has yet investigated the potential bias introduced by the DSC itself.
This study quantifies sex-based differences of the DSC and the normalized DSC
in an idealized setting independent of specific models. We applied
equally-sized synthetic errors to manual MRI annotations from 50 participants
to ensure sex-based comparability. Even minimal errors (e.g., a 1 mm boundary
shift) produced systematic DSC differences between sexes. For small structures,
average DSC differences were around 0.03; for medium-sized structures around
0.01. Only large structures (i.e., lungs and liver) were mostly unaffected,
with sex-based DSC differences close to zero. These findings underline that
fairness studies using the DSC as an evaluation metric should not expect
identical scores between men and women, as the metric itself introduces bias. A
segmentation model may perform equally well across sexes in terms of error
magnitude, even if observed DSC values suggest otherwise. Importantly, our work
raises awareness of a previously underexplored source of sex-based differences
in segmentation performance. One that arises not from model behavior, but from
the metric itself. Recognizing this factor is essential for more accurate and
fair evaluations in medical image analysis.

</details>


### [28] [EfficienT-HDR: An Efficient Transformer-Based Framework via Multi-Exposure Fusion for HDR Reconstruction](https://arxiv.org/abs/2509.19779)
*Yu-Shen Huang,Tzu-Han Chen,Cheng-Yen Hsiao,Shaou-Gang Miaou*

Main category: cs.CV

TL;DR: 本研究提出了一种轻量级的HDR成像方法，解决了计算成本高和鬼影问题，适用于边缘设备，实验表明性能优异。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上实现高质量HDR成像，以促进智能监控和自动驾驶等下游任务。

Method: 利用Context-Aware Vision Transformer并引入IAAF模块来减少鬼影，同时通过IRE、DyT和E-MSDC模块降低计算复杂性。

Result: 该方法的主版本在计算量减少约67%和提升5倍CPU推理速度的同时，还实现了高视觉质量，轻量版在计算效率方面表现优异。

Conclusion: 本研究提出了一种轻量级的高动态范围（HDR）成像解决方案，兼具出色的性能和图像质量，适合资源受限的边缘设备。

Abstract: Achieving high-quality High Dynamic Range (HDR) imaging on
resource-constrained edge devices is a critical challenge in computer vision,
as its performance directly impacts downstream tasks such as intelligent
surveillance and autonomous driving. Multi-Exposure Fusion (MEF) is a
mainstream technique to achieve this goal; however, existing methods generally
face the dual bottlenecks of high computational costs and ghosting artifacts,
hindering their widespread deployment. To this end, this study proposes a
light-weight Vision Transformer architecture designed explicitly for HDR
reconstruction to overcome these limitations. This study is based on the
Context-Aware Vision Transformer and begins by converting input images to the
YCbCr color space to separate luminance and chrominance information. It then
employs an Intersection-Aware Adaptive Fusion (IAAF) module to suppress
ghosting effectively. To further achieve a light-weight design, we introduce
Inverted Residual Embedding (IRE), Dynamic Tanh (DyT), and propose Enhanced
Multi-Scale Dilated Convolution (E-MSDC) to reduce computational complexity at
multiple levels. Our study ultimately contributes two model versions: a main
version for high visual quality and a light-weight version with advantages in
computational efficiency, both of which achieve an excellent balance between
performance and image quality. Experimental results demonstrate that, compared
to the baseline, the main version reduces FLOPS by approximately 67% and
increases inference speed by more than fivefold on CPU and 2.5 times on an edge
device. These results confirm that our method provides an efficient and
ghost-free HDR imaging solution for edge devices, demonstrating versatility and
practicality across various dynamic scenarios.

</details>


### [29] [BiTAA: A Bi-Task Adversarial Attack for Object Detection and Depth Estimation via 3D Gaussian Splatting](https://arxiv.org/abs/2509.19793)
*Yixun Zhang,Feng Zhou,Jianqin Yin*

Main category: cs.CV

TL;DR: 研究提出了一种新的双任务对抗攻击BiTAA，有效地影响了自动驾驶中的物体检测与深度估计，展示了多任务感知的风险及防御的必要性。


<details>
  <summary>Details</summary>
Motivation: 针对现有的2D/3D攻击方法在任务间的互动缺乏研究，尤其是检测与深度估计的交互，提出了一种新的双任务对抗攻击方法。

Method: 提出了一种基于3D高斯点云的双任务对抗攻击框架，支持全图和区域的攻击，并设计了复合损失函数以控制深度偏差，同时进行检测抑制。

Result: 通过统一的评估协议和实世界测试，结果显示检测和深度估计之间存在一致的跨任务降级，特别是在从检测到深度和反向的转移中表现出明显的不对称性。

Conclusion: 该研究展示了在自动驾驶领域中，双任务对抗攻击BiTAA的有效性，强调了多任务相机感知的实际风险并激励进一步的防御研究。

Abstract: Camera-based perception is critical to autonomous driving yet remains
vulnerable to task-specific adversarial manipulations in object detection and
monocular depth estimation. Most existing 2D/3D attacks are developed in task
silos, lack mechanisms to induce controllable depth bias, and offer no
standardized protocol to quantify cross-task transfer, leaving the interaction
between detection and depth underexplored. We present BiTAA, a bi-task
adversarial attack built on 3D Gaussian Splatting that yields a single
perturbation capable of simultaneously degrading detection and biasing
monocular depth. Specifically, we introduce a dual-model attack framework that
supports both full-image and patch settings and is compatible with common
detectors and depth estimators, with optional expectation-over-transformation
(EOT) for physical reality. In addition, we design a composite loss that
couples detection suppression with a signed, magnitude-controlled log-depth
bias within regions of interest (ROIs) enabling controllable near or far
misperception while maintaining stable optimization across tasks. We also
propose a unified evaluation protocol with cross-task transfer metrics and
real-world evaluations, showing consistent cross-task degradation and a clear
asymmetry between Det to Depth and from Depth to Det transfer. The results
highlight practical risks for multi-task camera-only perception and motivate
cross-task-aware defenses in autonomous driving scenarios.

</details>


### [30] [StrCGAN: A Generative Framework for Stellar Image Restoration](https://arxiv.org/abs/2509.19805)
*Shantanusinh Parmar*

Main category: cs.CV

TL;DR: StrCGAN是一种针对低分辨率天文摄影图像增强的生成模型，通过结合3D卷积、多光谱融合和天体物理正则化，显著提升了图像重建质量。


<details>
  <summary>Details</summary>
Motivation: 动机在于低解析度天文图像重建困难，因此开发新模型以提高小型望远镜观察图像的质量。

Method: 通过扩展CycleGAN框架，结合3D卷积层、多光谱融合及天体物理正则化模块，提升图像重建效果。

Result: StrCGAN在天文图像增强任务中表现出色，生成的重建图像在视觉和物理一致性上优于现有标准模型。

Conclusion: StrCGAN能够生成更清晰且物理上一致的天文图像重建，优于传统的生成对抗网络模型。

Abstract: We introduce StrCGAN (Stellar Cyclic GAN), a generative model designed to
enhance low-resolution astrophotography images. Our goal is to reconstruct
high-fidelity ground truth-like representations of celestial objects, a task
that is challenging due to the limited resolution and quality of
small-telescope observations such as the MobilTelesco dataset. Traditional
models such as CycleGAN provide a foundation for image-to-image translation but
are restricted to 2D mappings and often distort the morphology of stars and
galaxies. To overcome these limitations, we extend the CycleGAN framework with
three key innovations: 3D convolutional layers to capture volumetric spatial
correlations, multi-spectral fusion to align optical and near-infrared (NIR)
domains, and astrophysical regularization modules to preserve stellar
morphology. Ground-truth references from multi-mission all-sky surveys spanning
optical to NIR guide the training process, ensuring that reconstructions remain
consistent across spectral bands. Together, these components allow StrCGAN to
generate reconstructions that are not only visually sharper but also physically
consistent, outperforming standard GAN models in the task of astrophysical
image enhancement.

</details>


### [31] [Adaptive Model Ensemble for Continual Learning](https://arxiv.org/abs/2509.19819)
*Yuchuan Mao,Zhi Gao,Xiaomeng Fan,Yuwei Wu,Yunde Jia,Chenchen Jing*

Main category: cs.CV

TL;DR: 本论文提出了一种适应性知识融合方法meta-weight-ensembler，能有效缓解持续学习中的灾难性遗忘问题，并提升现有方法性能。


<details>
  <summary>Details</summary>
Motivation: 现有的模型集合方法在任务和层级上通常会遇到知识冲突问题，导致新旧任务的学习性能受损。

Method: 通过元学习训练混合系数生成器，为模型集合生成适当的混合系数，以解决任务级知识冲突。

Result: meta-weight-ensembler通过为每一层单独生成混合系数来解决层级知识冲突，实现了高效的学习。

Conclusion: meta-weight-ensembler有效缓解了灾难性遗忘并达到了最先进的性能。

Abstract: Model ensemble is an effective strategy in continual learning, which
alleviates catastrophic forgetting by interpolating model parameters, achieving
knowledge fusion learned from different tasks. However, existing model ensemble
methods usually encounter the knowledge conflict issue at task and layer
levels, causing compromised learning performance in both old and new tasks. To
solve this issue, we propose meta-weight-ensembler that adaptively fuses
knowledge of different tasks for continual learning. Concretely, we employ a
mixing coefficient generator trained via meta-learning to generate appropriate
mixing coefficients for model ensemble to address the task-level knowledge
conflict. The mixing coefficient is individually generated for each layer to
address the layer-level knowledge conflict. In this way, we learn the prior
knowledge about adaptively accumulating knowledge of different tasks in a fused
model, achieving efficient learning in both old and new tasks.
Meta-weight-ensembler can be flexibly combined with existing continual learning
methods to boost their ability of alleviating catastrophic forgetting.
Experiments on multiple continual learning datasets show that
meta-weight-ensembler effectively alleviates catastrophic forgetting and
achieves state-of-the-art performance.

</details>


### [32] [ThinkFake: Reasoning in Multimodal Large Language Models for AI-Generated Image Detection](https://arxiv.org/abs/2509.19841)
*Tai-Ming Huang,Wei-Tung Lin,Kai-Lung Hua,Wen-Huang Cheng,Junichi Yamagishi,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 针对AI生成图像的检测问题，本研究提出了 ThinkFake 框架，利用多模态大语言模型和强化学习，显著提高了检测的解释性和适应性。


<details>
  <summary>Details</summary>
Motivation: AI生成图像真实性提高引发了对信息错误和隐私侵犯的关注，急需准确且可解释的检测方法。

Method: 利用多模态大语言模型和强化学习技术，结合设计良好的奖励函数，进行逐步推理并生成结构化输出。

Result: ThinkFake 在 GenImage 基准测试中优于最先进的方法，并在 LOKI 基准测试上表现出强大的零样本泛化能力。

Conclusion: ThinkFake 是一个有效且强健的框架，在 AI 生成图像检测上超越了现有技术，展示了良好的零样本泛化能力。

Abstract: The increasing realism of AI-generated images has raised serious concerns
about misinformation and privacy violations, highlighting the urgent need for
accurate and interpretable detection methods. While existing approaches have
made progress, most rely on binary classification without explanations or
depend heavily on supervised fine-tuning, resulting in limited generalization.
In this paper, we propose ThinkFake, a novel reasoning-based and generalizable
framework for AI-generated image detection. Our method leverages a Multimodal
Large Language Model (MLLM) equipped with a forgery reasoning prompt and is
trained using Group Relative Policy Optimization (GRPO) reinforcement learning
with carefully designed reward functions. This design enables the model to
perform step-by-step reasoning and produce interpretable, structured outputs.
We further introduce a structured detection pipeline to enhance reasoning
quality and adaptability. Extensive experiments show that ThinkFake outperforms
state-of-the-art methods on the GenImage benchmark and demonstrates strong
zero-shot generalization on the challenging LOKI benchmark. These results
validate our framework's effectiveness and robustness. Code will be released
upon acceptance.

</details>


### [33] [PersONAL: Towards a Comprehensive Benchmark for Personalized Embodied Agents](https://arxiv.org/abs/2509.19843)
*Filippo Ziliotto,Jelin Raphael Akkara,Alessandro Daniele,Lamberto Ballan,Luciano Serafini,Tommaso Campari*

Main category: cs.CV

TL;DR: PersONAL基准旨在改进具身AI在个性化任务中的表现，展示了当前技术在家庭环境中处理用户偏好的不足。


<details>
  <summary>Details</summary>
Motivation: 目前在具身AI的实际应用中，特别是在家庭环境中，代理难以建模个体用户的偏好和行为，因此需要一个有效的基准来研究个性化问题。

Method: 建立了PersONAL基准，通过自然语言查询，要求代理识别、检索和导航至与特定用户关联的物体，并进行了两种评估模式的测试。

Result: PersONAL基准包含2000多个高质量的场景，由30多个逼真的家庭构成，实验揭示了与人类表现之间的显著差距。

Conclusion: 研究表明，现有的具身智能代理在满足用户个性化需求方面仍存在显著差距，呼吁发展能够感知、推理和记忆个性化信息的能力，以促进现实生活中的辅助机器人应用。

Abstract: Recent advances in Embodied AI have enabled agents to perform increasingly
complex tasks and adapt to diverse environments. However, deploying such agents
in realistic human-centered scenarios, such as domestic households, remains
challenging, particularly due to the difficulty of modeling individual human
preferences and behaviors. In this work, we introduce PersONAL (PERSonalized
Object Navigation And Localization, a comprehensive benchmark designed to study
personalization in Embodied AI. Agents must identify, retrieve, and navigate to
objects associated with specific users, responding to natural-language queries
such as "find Lily's backpack". PersONAL comprises over 2,000 high-quality
episodes across 30+ photorealistic homes from the HM3D dataset. Each episode
includes a natural-language scene description with explicit associations
between objects and their owners, requiring agents to reason over user-specific
semantics. The benchmark supports two evaluation modes: (1) active navigation
in unseen environments, and (2) object grounding in previously mapped scenes.
Experiments with state-of-the-art baselines reveal a substantial gap to human
performance, highlighting the need for embodied agents capable of perceiving,
reasoning, and memorizing over personalized information; paving the way towards
real-world assistive robot.

</details>


### [34] [FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models](https://arxiv.org/abs/2509.19870)
*Xin Wang,Jie Li,Zejia Weng,Yixu Wang,Yifeng Gao,Tianyu Pang,Chao Du,Yan Teng,Yingchun Wang,Zuxuan Wu,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本文研究了Vision-Language-Action模型中的一种新兴对抗脆弱性，提出了FreezeVLA框架，并展示了其在攻击成功率和对抗图像迁移性上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 尽管VLA模型在机器人技术中取得了快速进展，但其对敌对攻击的安全性和鲁棒性尚未得到充分研究。

Method: 本文提出FreezeVLA框架，通过最小-最大双层优化生成和评估行动冻结攻击。

Result: 在对三种最先进的VLA模型和四个机器人基准的实验中，FreezeVLA的平均攻击成功率达到76.2%，显著优于现有方法，且其生成的对抗图像具有较强的迁移性。

Conclusion: 本研究揭示了Vision-Language-Action模型中的一种关键安全风险，并强调了需要加强防御机制的紧迫性。

Abstract: Vision-Language-Action (VLA) models are driving rapid progress in robotics by
enabling agents to interpret multimodal inputs and execute complex,
long-horizon tasks. However, their safety and robustness against adversarial
attacks remain largely underexplored. In this work, we identify and formalize a
critical adversarial vulnerability in which adversarial images can "freeze" VLA
models and cause them to ignore subsequent instructions. This threat
effectively disconnects the robot's digital mind from its physical actions,
potentially inducing inaction during critical interventions. To systematically
study this vulnerability, we propose FreezeVLA, a novel attack framework that
generates and evaluates action-freezing attacks via min-max bi-level
optimization. Experiments on three state-of-the-art VLA models and four robotic
benchmarks show that FreezeVLA attains an average attack success rate of 76.2%,
significantly outperforming existing methods. Moreover, adversarial images
generated by FreezeVLA exhibit strong transferability, with a single image
reliably inducing paralysis across diverse language prompts. Our findings
expose a critical safety risk in VLA models and highlight the urgent need for
robust defense mechanisms.

</details>


### [35] [Adaptive Guidance Semantically Enhanced via Multimodal LLM for Edge-Cloud Object Detection](https://arxiv.org/abs/2509.19875)
*Yunqing Hu,Zheming Yang,Chang Zhao,Wen Ji*

Main category: cs.CV

TL;DR: 本文提出了一种基于自适应引导的语义增强边缘云协同目标检测方法，利用多模态大型语言模型，在复杂场景下显著提升了检测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测方法在低光照和重遮挡等复杂场景中面临性能下降的问题，缺乏高层语义理解。

Method: 采用多模态大型语言模型，通过指令微调生成结构化场景描述，并设计自适应映射机制将语义信息转换为边缘检测器的参数调整信号。

Result: 实验表明，该方法在低光照和高度遮挡的场景中能将延迟减少超过79%，计算成本降低70%，并保持准确性。

Conclusion: 该方法在复杂场景中显著提高了检测精度和效率。

Abstract: Traditional object detection methods face performance degradation challenges
in complex scenarios such as low-light conditions and heavy occlusions due to a
lack of high-level semantic understanding. To address this, this paper proposes
an adaptive guidance-based semantic enhancement edge-cloud collaborative object
detection method leveraging Multimodal Large Language Models (MLLM), achieving
an effective balance between accuracy and efficiency. Specifically, the method
first employs instruction fine-tuning to enable the MLLM to generate structured
scene descriptions. It then designs an adaptive mapping mechanism that
dynamically converts semantic information into parameter adjustment signals for
edge detectors, achieving real-time semantic enhancement. Within an edge-cloud
collaborative inference framework, the system automatically selects between
invoking cloud-based semantic guidance or directly outputting edge detection
results based on confidence scores. Experiments demonstrate that the proposed
method effectively enhances detection accuracy and efficiency in complex
scenes. Specifically, it can reduce latency by over 79% and computational cost
by 70% in low-light and highly occluded scenes while maintaining accuracy.

</details>


### [36] [Generalized Shortest Path-based Superpixels for 3D Spherical Image Segmentation](https://arxiv.org/abs/2509.19895)
*Rémi Giraud,Rodrigo Borba Pinheiro,Yannick Berthoumieu*

Main category: cs.CV

TL;DR: SphSPS是一种新方法，专注于处理360度全景图像的超像素分割，通过考虑三维几何，有效提升分割准确性与规则性。


<details>
  <summary>Details</summary>
Motivation: 随着广角图像捕获设备的广泛使用，对快速、准确的图像分析需求增加，迫切需要专门的解决方案。

Method: 提出了一种新的超像素方法SphSPS，基于球面最短路径的概念，适用于360度全景图像的分割。

Result: SphSPS方法在分割准确性和超像素形状规则性方面进行了验证，结果显示其在360度图像分割范围内超越了现有方法。

Conclusion: SphSPS方法在360度全景图像的分割准确性、抗噪声能力和形状规则性方面显著优于现有的平面和球面方法，为360度图像的超像素应用提供了有效工具。

Abstract: The growing use of wide angle image capture devices and the need for fast and
accurate image analysis in computer visions have enforced the need for
dedicated under-representation approaches. Most recent decomposition methods
segment an image into a small number of irregular homogeneous regions, called
superpixels. Nevertheless, these approaches are generally designed to segment
standard 2D planar images, i.e., captured with a 90o angle view without
distortion. In this work, we introduce a new general superpixel method called
SphSPS (for Spherical Shortest Path-based Superpixels)1 , dedicated to wide
360o spherical or omnidirectional images. Our method respects the geometry of
the 3D spherical acquisition space and generalizes the notion of shortest path
between a pixel and a superpixel center, to fastly extract relevant clustering
features. We demonstrate that considering the geometry of the acquisition space
to compute the shortest path enables to jointly improve the segmentation
accuracy and the shape regularity of superpixels. To evaluate this regularity
aspect, we also generalize a global regularity metric to the spherical space,
addressing the limitations of the only existing spherical compactness measure.
Finally, the proposed SphSPS method is validated on the reference 360o
spherical panorama segmentation dataset and on synthetic road omnidirectional
images. Our method significantly outperforms both planar and spherical
state-of-the-art approaches in terms of segmentation accuracy,robustness to
noise and regularity, providing a very interesting tool for superpixel-based
applications on 360o images.

</details>


### [37] [Efficient Cell Painting Image Representation Learning via Cross-Well Aligned Masked Siamese Network](https://arxiv.org/abs/2509.19896)
*Pin-Jui Huang,Yu-Hsuan Liao,SooHeon Kim,NoSeong Park,JongBae Park,DongMyung Shin*

Main category: cs.CV

TL;DR: CWA-MSN是一种新颖的细胞图像表示学习框架，能有效应对遗传和化学干扰的数据问题，同时在有限的数据和参数预算下，超越了现有的自监督和对比学习方法。


<details>
  <summary>Details</summary>
Motivation: 计算模型能够预测细胞对化学和遗传扰动的表型反应，从而加速药物发现，但提取具有生物学意义且具有批处理健壮性的细胞绘画表征仍然具有挑战性。

Method: 提出了一种称为Cross-Well Aligned Masked Siamese Network（CWA-MSN）的新型表示学习框架，通过对不同井中同一扰动下的细胞嵌入进行对齐，强制保持语义一致性。

Result: CWA-MSN在基因-基因关系检索基准测试中超越了最先进的自监督和对比学习方法，提高了基准得分，且在训练数据和模型规模上大幅度减少。

Conclusion: CWA-MSN是一种简单而有效的细胞图像表示学习框架，能够在数据和参数预算有限的情况下，实现高效的表型建模。

Abstract: Computational models that predict cellular phenotypic responses to chemical
and genetic perturbations can accelerate drug discovery by prioritizing
therapeutic hypotheses and reducing costly wet-lab iteration. However,
extracting biologically meaningful and batch-robust cell painting
representations remains challenging. Conventional self-supervised and
contrastive learning approaches often require a large-scale model and/or a huge
amount of carefully curated data, still struggling with batch effects. We
present Cross-Well Aligned Masked Siamese Network (CWA-MSN), a novel
representation learning framework that aligns embeddings of cells subjected to
the same perturbation across different wells, enforcing semantic consistency
despite batch effects. Integrated into a masked siamese architecture, this
alignment yields features that capture fine-grained morphology while remaining
data- and parameter-efficient. For instance, in a gene-gene relationship
retrieval benchmark, CWA-MSN outperforms the state-of-the-art publicly
available self-supervised (OpenPhenom) and contrastive learning (CellCLIP)
methods, improving the benchmark scores by +29\% and +9\%, respectively, while
training on substantially fewer data (e.g., 0.2M images for CWA-MSN vs. 2.2M
images for OpenPhenom) or smaller model size (e.g., 22M parameters for CWA-MSN
vs. 1.48B parameters for CellCLIP). Extensive experiments demonstrate that
CWA-MSN is a simple and effective way to learn cell image representation,
enabling efficient phenotype modeling even under limited data and parameter
budgets.

</details>


### [38] [Aerial-Ground Image Feature Matching via 3D Gaussian Splatting-based Intermediate View Rendering](https://arxiv.org/abs/2509.19898)
*Jiangxue Yu,Hui Wang,San Jiang,Xing Zhang,Dejin Zhang,Qingquan Li*

Main category: cs.CV

TL;DR: 本研究提出了一种新的特征匹配算法，通过创建中间视图来提高航空与地面图像的匹配可靠性，为3D建模提供支持。


<details>
  <summary>Details</summary>
Motivation: 解决3D建模中航空和地面图像的可靠对应关系问题，以提升复杂场景的建模精度。

Method: 通过生成中间视图以缓解视角扭曲，实现航空图像与地面图像之间的特征匹配。

Result: 实验结果显示，所提方案在特征匹配数量和质量上优于广泛使用的方法，支持准确的ISfM重建和完整的基于3DGS的场景渲染。

Conclusion: 该研究提出的特征匹配算法有效提高了航空和地面图像之间的特征匹配可靠性，并在图像重建和场景渲染中取得了较好的效果。

Abstract: The integration of aerial and ground images has been a promising solution in
3D modeling of complex scenes, which is seriously restricted by finding
reliable correspondences. The primary contribution of this study is a feature
matching algorithm for aerial and ground images, whose core idea is to generate
intermediate views to alleviate perspective distortions caused by the extensive
viewpoint changes. First, by using aerial images only, sparse models are
reconstructed through an incremental SfM (Structure from Motion) engine due to
their large scene coverage. Second, 3D Gaussian Splatting is then adopted for
scene rendering by taking as inputs sparse points and oriented images. For
accurate view rendering, a render viewpoint determination algorithm is designed
by using the oriented camera poses of aerial images, which is used to generate
high-quality intermediate images that can bridge the gap between aerial and
ground images. Third, with the aid of intermediate images, reliable feature
matching is conducted for match pairs from render-aerial and render-ground
images, and final matches can be generated by transmitting correspondences
through intermediate views. By using real aerial and ground datasets, the
validation of the proposed solution has been verified in terms of feature
matching and scene rendering and compared comprehensively with widely used
methods. The experimental results demonstrate that the proposed solution can
provide reliable feature matches for aerial and ground images with an obvious
increase in the number of initial and refined matches, and it can provide
enough matches to achieve accurate ISfM reconstruction and complete 3DGS-based
scene rendering.

</details>


### [39] [CapStARE: Capsule-based Spatiotemporal Architecture for Robust and Efficient Gaze Estimation](https://arxiv.org/abs/2509.19936)
*Miren Samaniego,Igor Rodriguez,Elena Lazkano*

Main category: cs.CV

TL;DR: CapStARE是一种胶囊基础的时空架构，优化了注视估计技术，性能优越且适合实时应用。


<details>
  <summary>Details</summary>
Motivation: 针对注视估计提高性能，特别是在慢速和快速注视动态建模上，并保持实时推理能力。

Method: 采用Capsule结构的时空架构，结合ConvNeXt主干网、注意力路由的胶囊形成以及双GRU解码器。

Result: 在ETH-XGaze和MPIIFaceGaze上取得了最先进的表现，并在Gaze360和RT-GENE上同样表现优异，具有更少的参数和更好的可解释性。

Conclusion: CapStARE是一个有效且可靠的实时注视估计解决方案，适用于互动系统。

Abstract: We introduce CapStARE, a capsule-based spatio-temporal architecture for gaze
estimation that integrates a ConvNeXt backbone, capsule formation with
attention routing, and dual GRU decoders specialized for slow and rapid gaze
dynamics. This modular design enables efficient part-whole reasoning and
disentangled temporal modeling, achieving state-of-the-art performance on
ETH-XGaze (3.36) and MPIIFaceGaze (2.65) while maintaining real-time inference
(< 10 ms). The model also generalizes well to unconstrained conditions in
Gaze360 (9.06) and human-robot interaction scenarios in RT-GENE (4.76),
outperforming or matching existing methods with fewer parameters and greater
interpretability. These results demonstrate that CapStARE offers a practical
and robust solution for real-time gaze estimation in interactive systems. The
related code and results for this article can be found on:
https://github.com/toukapy/capsStare

</details>


### [40] [GS-RoadPatching: Inpainting Gaussians via 3D Searching and Placing for Driving Scenes](https://arxiv.org/abs/2509.19937)
*Guo Chen,Jiarun Liu,Sicong Du,Chenming Wu,Deqi Li,Shi-Sheng Huang,Guofeng Zhang,Sheng Yang*

Main category: cs.CV

TL;DR: GS-RoadPatching是一种新的基于3D高斯溅射的驾驶场景填充方法，具有优越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的3DGS插补方法依赖于2D视角生成，其预测的外观或深度线索有限，因此我们寻求一种更有效的3D插补方式。

Method: 通过3D高斯溅射（3DGS）进行基于结构的补丁搜索和替代融合优化来完成场景填充。

Result: 我们的实验表明，GS-RoadPatching在质量和互操作性方面超越了基线方法，同时在多个公开数据集上表现出色。

Conclusion: 我们的方法在驾驶场景的填充任务中表现出色，达到了最先进的性能，同时在一般场景中也显示了适用性。

Abstract: This paper presents GS-RoadPatching, an inpainting method for driving scene
completion by referring to completely reconstructed regions, which are
represented by 3D Gaussian Splatting (3DGS). Unlike existing 3DGS inpainting
methods that perform generative completion relying on 2D perspective-view-based
diffusion or GAN models to predict limited appearance or depth cues for missing
regions, our approach enables substitutional scene inpainting and editing
directly through the 3DGS modality, extricating it from requiring
spatial-temporal consistency of 2D cross-modals and eliminating the need for
time-intensive retraining of Gaussians. Our key insight is that the highly
repetitive patterns in driving scenes often share multi-modal similarities
within the implicit 3DGS feature space and are particularly suitable for
structural matching to enable effective 3DGS-based substitutional inpainting.
Practically, we construct feature-embedded 3DGS scenes to incorporate a patch
measurement method for abstracting local context at different scales and,
subsequently, propose a structural search method to find candidate patches in
3D space effectively. Finally, we propose a simple yet effective
substitution-and-fusion optimization for better visual harmony. We conduct
extensive experiments on multiple publicly available datasets to demonstrate
the effectiveness and efficiency of our proposed method in driving scenes, and
the results validate that our method achieves state-of-the-art performance
compared to the baseline methods in terms of both quality and interoperability.
Additional experiments in general scenes also demonstrate the applicability of
the proposed 3D inpainting strategy. The project page and code are available
at: https://shanzhaguoo.github.io/GS-RoadPatching/

</details>


### [41] [Interpreting ResNet-based CLIP via Neuron-Attention Decomposition](https://arxiv.org/abs/2509.19943)
*Edmund Bu,Yossi Gandelsman*

Main category: cs.CV

TL;DR: 该研究提出了一种新技术，通过分析CLIP-ResNet中神经元的输出贡献，揭示可解释的计算路径，并利用这些发现改善语义分割和监测数据集分布变化。


<details>
  <summary>Details</summary>
Motivation: 为了更好地解释CLIP-ResNet中神经元的贡献，并探讨如何将这些贡献应用于实际任务。

Method: 通过分析CLIP的注意力池层中的神经元和注意力头的组合，解构其对输出的贡献。

Result: 发现神经元-头对可近似为CLIP-ResNet图像-文本嵌入空间中的单一方向，并能用于训练免费语义分割和监测数据集分布变化。

Conclusion: 通过分析CLIP-ResNet中神经元对输出的贡献，研究表明单个计算路径可以揭示可解释的单位，并可用于下游任务。

Abstract: We present a novel technique for interpreting the neurons in CLIP-ResNet by
decomposing their contributions to the output into individual computation
paths. More specifically, we analyze all pairwise combinations of neurons and
the following attention heads of CLIP's attention-pooling layer. We find that
these neuron-head pairs can be approximated by a single direction in
CLIP-ResNet's image-text embedding space. Leveraging this insight, we interpret
each neuron-head pair by associating it with text. Additionally, we find that
only a sparse set of the neuron-head pairs have a significant contribution to
the output value, and that some neuron-head pairs, while polysemantic,
represent sub-concepts of their corresponding neurons. We use these
observations for two applications. First, we employ the pairs for training-free
semantic segmentation, outperforming previous methods for CLIP-ResNet. Second,
we utilize the contributions of neuron-head pairs to monitor dataset
distribution shifts. Our results demonstrate that examining individual
computation paths in neural networks uncovers interpretable units, and that
such units can be utilized for downstream tasks.

</details>


### [42] [When Words Can't Capture It All: Towards Video-Based User Complaint Text Generation with Multimodal Video Complaint Dataset](https://arxiv.org/abs/2509.19952)
*Sarmistha Das,R E Zera Marveen Lyngkhoi,Kirtan Jain,Vinayak Goyal,Sriparna Saha,Manish Gupta*

Main category: cs.CV

TL;DR: 本文提出了一种新任务，通过视频帮助用户更好地表达投诉，构建了ComVID数据集和新模型。


<details>
  <summary>Details</summary>
Motivation: 由于用户在用文本表达投诉时存在难度，故而通过分析视频中的内容帮助用户清晰表达其投诉。

Method: 提出了一种多模态的Retrieval-Augmented Generation嵌入模型，结合用户情感状态生成投诉内容。

Result: 推出了包含1175个投诉视频及描述的ComVID数据集，并引入了新的投诉保留评价指标。

Conclusion: 本研究为用户通过视频表达投诉提供了新的研究方向与平台，并推出了相应的数据集与模型。

Abstract: While there exists a lot of work on explainable complaint mining,
articulating user concerns through text or video remains a significant
challenge, often leaving issues unresolved. Users frequently struggle to
express their complaints clearly in text but can easily upload videos depicting
product defects (e.g., vague text such as `worst product' paired with a
5-second video depicting a broken headphone with the right earcup). This paper
formulates a new task in the field of complaint mining to aid the common users'
need to write an expressive complaint, which is Complaint Description from
Videos (CoD-V) (e.g., to help the above user articulate her complaint about the
defective right earcup). To this end, we introduce ComVID, a video complaint
dataset containing 1,175 complaint videos and the corresponding descriptions,
also annotated with the emotional state of the complainer. Additionally, we
present a new complaint retention (CR) evaluation metric that discriminates the
proposed (CoD-V) task against standard video summary generation and description
tasks. To strengthen this initiative, we introduce a multimodal
Retrieval-Augmented Generation (RAG) embedded VideoLLaMA2-7b model, designed to
generate complaints while accounting for the user's emotional state. We conduct
a comprehensive evaluation of several Video Language Models on several tasks
(pre-trained and fine-tuned versions) with a range of established evaluation
metrics, including METEOR, perplexity, and the Coleman-Liau readability score,
among others. Our study lays the foundation for a new research direction to
provide a platform for users to express complaints through video. Dataset and
resources are available at: https://github.com/sarmistha-D/CoD-V.

</details>


### [43] [SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding](https://arxiv.org/abs/2509.19965)
*Phyo Thet Yee,Dimitrios Kollias,Sudeepta Mishra,Abhinav Dhall*

Main category: cs.CV

TL;DR: SynchroRaMa是一个新框架，通过结合文本和音频的情感信号，实现更丰富的谈话面部视频生成，并提升了自然性和视觉真实感。


<details>
  <summary>Details</summary>
Motivation: 现有的方法多依赖单一模态（音频或图像）进行情感嵌入，限制了捕捉细微情感线索的能力，同时基于单个参考图像进行建模，无法有效表现动态变化的动作或属性。

Method: SynchroRaMa框架通过多模态情感嵌入结合文本和音频的情感信号，并包含音频到动作模块（A2M），以确保自然的头部运动和准确的口型同步。

Result: 通过在基准数据集上的定量和定性实验，SynchroRaMa表现出明显的改进，尤其在图像质量、表情保持和动作真实感方面。

Conclusion: SynchroRaMa在图像质量、表情保持和动作真实感方面优于现有最先进的方法，并在用户研究中获得了更高的自然性、动作多样性和视频流畅性的主观评分。

Abstract: Audio-driven talking face generation has received growing interest,
particularly for applications requiring expressive and natural human-avatar
interaction. However, most existing emotion-aware methods rely on a single
modality (either audio or image) for emotion embedding, limiting their ability
to capture nuanced affective cues. Additionally, most methods condition on a
single reference image, restricting the model's ability to represent dynamic
changes in actions or attributes across time. To address these issues, we
introduce SynchroRaMa, a novel framework that integrates a multi-modal emotion
embedding by combining emotional signals from text (via sentiment analysis) and
audio (via speech-based emotion recognition and audio-derived valence-arousal
features), enabling the generation of talking face videos with richer and more
authentic emotional expressiveness and fidelity. To ensure natural head motion
and accurate lip synchronization, SynchroRaMa includes an audio-to-motion (A2M)
module that generates motion frames aligned with the input audio. Finally,
SynchroRaMa incorporates scene descriptions generated by Large Language Model
(LLM) as additional textual input, enabling it to capture dynamic actions and
high-level semantic attributes. Conditioning the model on both visual and
textual cues enhances temporal consistency and visual realism. Quantitative and
qualitative experiments on benchmark datasets demonstrate that SynchroRaMa
outperforms the state-of-the-art, achieving improvements in image quality,
expression preservation, and motion realism. A user study further confirms that
SynchroRaMa achieves higher subjective ratings than competing methods in
overall naturalness, motion diversity, and video smoothness. Our project page
is available at <https://novicemm.github.io/synchrorama>.

</details>


### [44] [OmniScene: Attention-Augmented Multimodal 4D Scene Understanding for Autonomous Driving](https://arxiv.org/abs/2509.19973)
*Pei Liu,Hongliang Lu,Haichao Liu,Haipeng Liu,Xin Liu,Ruoyu Yao,Shengbo Eben Li,Jun Ma*

Main category: cs.CV

TL;DR: 本论文提出OmniScene框架，通过结合视觉和语言的多模态学习，实现更为人性化的场景理解，并在自动驾驶中展现出显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前的自动驾驶系统在理解场景方面存在不足，主要依赖深度基于3D重建，而非真实场景理解，因此提出了一种新的框架来克服这些限制。

Method: 提出了一种人类般的框架OmniScene，通过OmniScene视觉-语言模型（OmniVLM）集成多视图和时间感知，实现整体4D场景理解，并运用教师-学生架构进行知识蒸馏，将文本表征与3D实例特征嵌入，以增强特征学习并捕捉人类般的注意力语义。

Result: 在nuScenes数据集上进行了全面评估，OmniScene在十余种前沿模型之间的多种任务上持续取得了优越的结果。

Conclusion: OmniScene在多种任务上展示了卓越的性能，并在感知、预测、规划和视觉问答方面建立了新的基准。

Abstract: Human vision is capable of transforming two-dimensional observations into an
egocentric three-dimensional scene understanding, which underpins the ability
to translate complex scenes and exhibit adaptive behaviors. This capability,
however, remains lacking in current autonomous driving systems, where
mainstream approaches primarily rely on depth-based 3D reconstruction rather
than true scene understanding. To address this limitation, we propose a novel
human-like framework called OmniScene. First, we introduce the OmniScene
Vision-Language Model (OmniVLM), a vision-language framework that integrates
multi-view and temporal perception for holistic 4D scene understanding. Then,
harnessing a teacher-student OmniVLM architecture and knowledge distillation,
we embed textual representations into 3D instance features for semantic
supervision, enriching feature learning, and explicitly capturing human-like
attentional semantics. These feature representations are further aligned with
human driving behaviors, forming a more human-like
perception-understanding-action architecture. In addition, we propose a
Hierarchical Fusion Strategy (HFS) to address imbalances in modality
contributions during multimodal integration. Our approach adaptively calibrates
the relative significance of geometric and semantic features at multiple
abstraction levels, enabling the synergistic use of complementary cues from
visual and textual modalities. This learnable dynamic fusion enables a more
nuanced and effective exploitation of heterogeneous information. We evaluate
OmniScene comprehensively on the nuScenes dataset, benchmarking it against over
ten state-of-the-art models across various tasks. Our approach consistently
achieves superior results, establishing new benchmarks in perception,
prediction, planning, and visual question answering.

</details>


### [45] [CamPVG: Camera-Controlled Panoramic Video Generation with Epipolar-Aware Diffusion](https://arxiv.org/abs/2509.19979)
*Chenhao Ji,Chaohui Yu,Junyao Gao,Fan Wang,Cairong Zhao*

Main category: cs.CV

TL;DR: 本研究提出了一种新颖的全景视频生成框架CamPVG，通过精确的相机位姿引导生成高质量一致的视频，克服了传统方法的限制。


<details>
  <summary>Details</summary>
Motivation: 当前全景视频生成方法在几何一致性方面存在挑战，尤其是在以球面投影进行相机控制时。

Method: 提出了一种基于扩散的全景视频生成框架CamPVG，结合精确的相机位姿指导，采用全景Pl"ucker嵌入和球面极线模块。

Result: 我们的实验表明CamPVG生成的全景视频在质量和一致性上显著优于现有方法。

Conclusion: 我们的方法在生成与相机轨迹一致的高质量全景视频方面表现优异，显著超越了现有的全景视频生成方法。

Abstract: Recently, camera-controlled video generation has seen rapid development,
offering more precise control over video generation. However, existing methods
predominantly focus on camera control in perspective projection video
generation, while geometrically consistent panoramic video generation remains
challenging. This limitation is primarily due to the inherent complexities in
panoramic pose representation and spherical projection. To address this issue,
we propose CamPVG, the first diffusion-based framework for panoramic video
generation guided by precise camera poses. We achieve camera position encoding
for panoramic images and cross-view feature aggregation based on spherical
projection. Specifically, we propose a panoramic Pl\"ucker embedding that
encodes camera extrinsic parameters through spherical coordinate
transformation. This pose encoder effectively captures panoramic geometry,
overcoming the limitations of traditional methods when applied to
equirectangular projections. Additionally, we introduce a spherical epipolar
module that enforces geometric constraints through adaptive attention masking
along epipolar lines. This module enables fine-grained cross-view feature
aggregation, substantially enhancing the quality and consistency of generated
panoramic videos. Extensive experiments demonstrate that our method generates
high-quality panoramic videos consistent with camera trajectories, far
surpassing existing methods in panoramic video generation.

</details>


### [46] [SDE-DET: A Precision Network for Shatian Pomelo Detection in Complex Orchard Environments](https://arxiv.org/abs/2509.19990)
*Yihao Hu,Pan Wang,Xiaodong Bai,Shijie Cai,Hang Wang,Huazhong Liu,Aiping Yang,Xiangxiang Li,Meiping Ding,Hongyan Liu,Jianguo Yao*

Main category: cs.CV

TL;DR: 本研究提出SDE-DET模型和STP-AgriData数据集，有效解决了沙田柚检测中的复杂环境挑战，并在检测性能上达到最新水平。


<details>
  <summary>Details</summary>
Motivation: 探讨在复杂果园环境中检测沙田柚的挑战，包括多尺度问题、树干和叶子遮挡及小物体检测难点。

Method: 构建STP-AgriData自定义数据集，提出SDE-DET模型，集成Star Block和可变形注意力机制以提高检测能力。

Result: SDE-DET模型在精准度、召回率、mAP和F1-score等方面 outperform了其他主流检测模型。

Conclusion: SDE-DET模型在STP-AgriData数据集上实现了先进的检测性能，为进一步发展自动采收机器人奠定了基础。

Abstract: Pomelo detection is an essential process for their localization, automated
robotic harvesting, and maturity analysis. However, detecting Shatian pomelo in
complex orchard environments poses significant challenges, including
multi-scale issues, obstructions from trunks and leaves, small object
detection, etc. To address these issues, this study constructs a custom dataset
STP-AgriData and proposes the SDE-DET model for Shatian pomelo detection.
SDE-DET first utilizes the Star Block to effectively acquire high-dimensional
information without increasing the computational overhead. Furthermore, the
presented model adopts Deformable Attention in its backbone, to enhance its
ability to detect pomelos under occluded conditions. Finally, multiple
Efficient Multi-Scale Attention mechanisms are integrated into our model to
reduce the computational overhead and extract deep visual representations,
thereby improving the capacity for small object detection. In the experiment,
we compared SDE-DET with the Yolo series and other mainstream detection models
in Shatian pomelo detection. The presented SDE-DET model achieved scores of
0.883, 0.771, 0.838, 0.497, and 0.823 in Precision, Recall, mAP@0.5,
mAP@0.5:0.95 and F1-score, respectively. SDE-DET has achieved state-of-the-art
performance on the STP-AgriData dataset. Experiments indicate that the SDE-DET
provides a reliable method for Shatian pomelo detection, laying the foundation
for the further development of automatic harvest robots.

</details>


### [47] [Improving Generalizability and Undetectability for Targeted Adversarial Attacks on Multimodal Pre-trained Models](https://arxiv.org/abs/2509.19994)
*Zhifang Zhang,Jiahan Zhang,Shengjie Zhou,Qi Wei,Shuo He,Feng Liu,Lei Feng*

Main category: cs.CV

TL;DR: 本研究提出了一种新的对抗攻击方法，解决了多模态预训练模型中现有攻击的可泛化性和不可检测性限制。


<details>
  <summary>Details</summary>
Motivation: 随着多模态预训练模型的广泛应用，针对其的安全问题，尤其是针对性对抗攻击，引起了严重的安全关注。

Method: 通过引入多种源模态和目标模态的代理，优化针对性对抗示例，从而使其在防御中保持隐蔽，并与多个潜在目标对齐。

Result: 实验结果表明，PTA方法在多个相关目标上取得高成功率，并且在多种异常检测方法面前仍保持不可检测性。

Conclusion: 提出的Proxy Targeted Attack方法能够在多种相关目标上实现高成功率，并且在多种异常检测方法面前保持不可检测性。

Abstract: Multimodal pre-trained models (e.g., ImageBind), which align distinct data
modalities into a shared embedding space, have shown remarkable success across
downstream tasks. However, their increasing adoption raises serious security
concerns, especially regarding targeted adversarial attacks. In this paper, we
show that existing targeted adversarial attacks on multimodal pre-trained
models still have limitations in two aspects: generalizability and
undetectability. Specifically, the crafted targeted adversarial examples (AEs)
exhibit limited generalization to partially known or semantically similar
targets in cross-modal alignment tasks (i.e., limited generalizability) and can
be easily detected by simple anomaly detection methods (i.e., limited
undetectability). To address these limitations, we propose a novel method
called Proxy Targeted Attack (PTA), which leverages multiple source-modal and
target-modal proxies to optimize targeted AEs, ensuring they remain evasive to
defenses while aligning with multiple potential targets. We also provide
theoretical analyses to highlight the relationship between generalizability and
undetectability and to ensure optimal generalizability while meeting the
specified requirements for undetectability. Furthermore, experimental results
demonstrate that our PTA can achieve a high success rate across various related
targets and remain undetectable against multiple anomaly detection methods.

</details>


### [48] [Anomaly Detection by Clustering DINO Embeddings using a Dirichlet Process Mixture](https://arxiv.org/abs/2509.19997)
*Nico Schulthess,Ender Konukoglu*

Main category: cs.CV

TL;DR: 本研究提出了一种基于DINOv2嵌入和Dirichlet过程混合模型的无监督异常检测方法，显示了在医学成像中的有效性和高效性。


<details>
  <summary>Details</summary>
Motivation: 在医学成像中进行无监督异常检测的需求，尤其是在面对小型和大型数据集时的计算效率问题。

Method: 通过使用Dirichlet过程混合模型 (DPMM) 对DINOv2嵌入的分布进行建模来进行异常检测。

Result: DINOv2嵌入在医学成像基准测试中实现了竞争力的异常检测性能，并且在推理时计算时间大幅减少。

Conclusion: 利用DINOv2嵌入和Dirichlet过程混合模型（DPMM），该方法在医学成像中的无监督异常检测中表现出色，同时计算时间至少减少了一半。

Abstract: In this work, we leverage informative embeddings from foundational models for
unsupervised anomaly detection in medical imaging. For small datasets, a
memory-bank of normative features can directly be used for anomaly detection
which has been demonstrated recently. However, this is unsuitable for large
medical datasets as the computational burden increases substantially.
Therefore, we propose to model the distribution of normative DINOv2 embeddings
with a Dirichlet Process Mixture model (DPMM), a non-parametric mixture model
that automatically adjusts the number of mixture components to the data at
hand. Rather than using a memory bank, we use the similarity between the
component centers and the embeddings as anomaly score function to create a
coarse anomaly segmentation mask. Our experiments show that through DPMM
embeddings of DINOv2, despite being trained on natural images, achieve very
competitive anomaly detection performance on medical imaging benchmarks and can
do this while at least halving the computation time at inference. Our analysis
further indicates that normalized DINOv2 embeddings are generally more aligned
with anatomical structures than unnormalized features, even in the presence of
anomalies, making them great representations for anomaly detection. The code is
available at https://github.com/NicoSchulthess/anomalydino-dpmm.

</details>


### [49] [Table Detection with Active Learning](https://arxiv.org/abs/2509.20003)
*Somraj Gautam,Nachiketa Purohit,Gaurav Harit*

Main category: cs.CV

TL;DR: 提出了一种新型主动学习方法，通过结合不确定性和多样性策略来提高目标检测任务的样本选择效率，并在有限预算内取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 解决物体检测任务中高标注成本的问题，尤其是在需要大量标记数据的情况下。

Method: 结合不确定性和多样性策略进行样本选择，以提高目标检测任务中的标注效率。

Result: 在两个基准数据集上验证了我们的方法，结果显示主动学习选择的样本明显优于随机抽样，降低了标注工作量。

Conclusion: 提出的主动学习方法在有限的标注预算内显著提升了样本选择效率，取得了较高的mAP得分，并且在性能上与全监督模型相当。

Abstract: Efficient data annotation remains a critical challenge in machine learning,
particularly for object detection tasks requiring extensive labeled data.
Active learning (AL) has emerged as a promising solution to minimize annotation
costs by selecting the most informative samples. While traditional AL
approaches primarily rely on uncertainty-based selection, recent advances
suggest that incorporating diversity-based strategies can enhance sampling
efficiency in object detection tasks. Our approach ensures the selection of
representative examples that improve model generalization. We evaluate our
method on two benchmark datasets (TableBank-LaTeX, TableBank-Word) using
state-of-the-art table detection architectures, CascadeTabNet and YOLOv9. Our
results demonstrate that AL-based example selection significantly outperforms
random sampling, reducing annotation effort given a limited budget while
maintaining comparable performance to fully supervised models. Our method
achieves higher mAP scores within the same annotation budget.

</details>


### [50] [Does the Manipulation Process Matter? RITA: Reasoning Composite Image Manipulations via Reversely-Ordered Incremental-Transition Autoregression](https://arxiv.org/abs/2509.20006)
*Xuekang Zhu,Ji-Zhe Zhou,Kaiwen Feng,Chenfan Qu,Yunfei Wang,Liting Zhou,Jian liu*

Main category: cs.CV

TL;DR: 本研究提出RITA框架，通过条件序列预测重新定义图像操纵定位，有效建模编辑过程的层次和时间关系，在性能上超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的图像操纵定位方法无法捕捉编辑过程的复杂性，直接将高维空间压缩为单一的二进制掩码，导致维度崩溃和内在任务特征的不匹配。

Method: 提出了RITA框架，将图像操纵定位重新表述为条件序列预测任务，逐层预测操纵区域。

Result: RITA在传统基准上达到了最先进的性能，并通过新基准HSIM和HSS度量评估顺序和层次对齐。

Conclusion: RITA框架通过层层预测操纵区域，有效建模了编辑操作间的时间依赖性和层次结构，在传统基准测试中表现出色，并为新颖的层次化定位任务奠定了良好的基础。

Abstract: Image manipulations often entail a complex manipulation process, comprising a
series of editing operations to create a deceptive image, exhibiting
sequentiality and hierarchical characteristics. However, existing IML methods
remain manipulation-process-agnostic, directly producing localization masks in
a one-shot prediction paradigm without modeling the underlying editing steps.
This one-shot paradigm compresses the high-dimensional compositional space into
a single binary mask, inducing severe dimensional collapse, thereby creating a
fundamental mismatch with the intrinsic nature of the IML task.
  To address this, we are the first to reformulate image manipulation
localization as a conditional sequence prediction task, proposing the RITA
framework. RITA predicts manipulated regions layer-by-layer in an ordered
manner, using each step's prediction as the condition for the next, thereby
explicitly modeling temporal dependencies and hierarchical structures among
editing operations.
  To enable training and evaluation, we synthesize multi-step manipulation data
and construct a new benchmark HSIM. We further propose the HSS metric to assess
sequential order and hierarchical alignment. Extensive experiments show RITA
achieves SOTA on traditional benchmarks and provides a solid foundation for the
novel hierarchical localization task, validating its potential as a general and
effective paradigm. The code and dataset will be publicly available.

</details>


### [51] [PS3: A Multimodal Transformer Integrating Pathology Reports with Histology Images and Biological Pathways for Cancer Survival Prediction](https://arxiv.org/abs/2509.20022)
*Manahil Raza,Ayesha Azam,Talha Qaiser,Nasir Rajpoot*

Main category: cs.CV

TL;DR: 本研究提出一种结合病理报告以增强生存预测的多模态融合方法PS3，通过原型生成和平衡表示，使用Transformer模型实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过融合病理报告、WSI和转录组数据进一步提升生存预测的准确性。

Method: 采用原型生成的方法形成平衡的表示，并使用基于Transformer的融合模型处理多模态数据。

Result: PS3模型通过处理病理报告、WSI及转录组数据，有效建模各模态之间的交互，获得更好的生存预测效果。

Conclusion: 所提出的PS3模型在六个来自TCGA的数据集上相较于其他方法表现出更优的生存预测能力。

Abstract: Current multimodal fusion approaches in computational oncology primarily
focus on integrating multi-gigapixel histology whole slide images (WSIs) with
genomic or transcriptomic data, demonstrating improved survival prediction. We
hypothesize that incorporating pathology reports can further enhance prognostic
performance. Pathology reports, as essential components of clinical workflows,
offer readily available complementary information by summarizing
histopathological findings and integrating expert interpretations and clinical
context. However, fusing these modalities poses challenges due to their
heterogeneous nature. WSIs are high-dimensional, each containing several
billion pixels, whereas pathology reports consist of concise text summaries of
varying lengths, leading to potential modality imbalance. To address this, we
propose a prototype-based approach to generate balanced representations, which
are then integrated using a Transformer-based fusion model for survival
prediction that we term PS3 (Predicting Survival from Three Modalities).
Specifically, we present: (1) Diagnostic prototypes from pathology reports,
leveraging self-attention to extract diagnostically relevant sections and
standardize text representation; (2) Histological prototypes to compactly
represent key morphological patterns in WSIs; and (3) Biological pathway
prototypes to encode transcriptomic expressions, accurately capturing cellular
functions. PS3, the three-modal transformer model, processes the resulting
prototype-based multimodal tokens and models intra-modal and cross-modal
interactions across pathology reports, WSIs and transcriptomic data. The
proposed model outperforms state-of-the-art methods when evaluated against
clinical, unimodal and multimodal baselines on six datasets from The Cancer
Genome Atlas (TCGA). The code is available at: https://github.com/manahilr/PS3.

</details>


### [52] [Generative Adversarial Networks Applied for Privacy Preservation in Biometric-Based Authentication and Identification](https://arxiv.org/abs/2509.20024)
*Lubos Mjachky,Ivan Homoliak*

Main category: cs.CV

TL;DR: 本文提出了一种基于生成对抗网络的新型身份验证方法，可有效保护用户隐私，避免数据泄露，同时保持身份验证的有效性。


<details>
  <summary>Details</summary>
Motivation: 旨在解决现有生物识别身份验证系统中用户对数据使用缺乏控制和数据泄露的隐患。

Method: 使用生成对抗网络（GAN）将人脸图像转换为视觉私密域，并在该域上训练身份验证分类器。

Result: 实验表明，该方法对攻击具有较强的抵御能力，并且仍能提供有意义的效用。

Conclusion: 提出的身份验证方法在确保用户隐私的同时，具有很好的鲁棒性和实用性。

Abstract: Biometric-based authentication systems are getting broadly adopted in many
areas. However, these systems do not allow participating users to influence the
way their data is used. Furthermore, the data may leak and can be misused
without the users' knowledge. In this paper, we propose a new authentication
method that preserves the privacy of individuals and is based on a generative
adversarial network (GAN). Concretely, we suggest using the GAN for translating
images of faces to a visually private domain (e.g., flowers or shoes).
Classifiers, which are used for authentication purposes, are then trained on
the images from the visually private domain. Based on our experiments, the
method is robust against attacks and still provides meaningful utility.

</details>


### [53] [Predictive Quality Assessment for Mobile Secure Graphics](https://arxiv.org/abs/2509.20028)
*Cas Steigstra,Sergey Milyaev,Shaodi You*

Main category: cs.CV

TL;DR: 本研究提出了一种新的框架来预测手机图像序列在安全图形验证中的有效性，结果表明，冻结的通用模型在应对不同印刷技术上的表现优于细致微调的模型。


<details>
  <summary>Details</summary>
Motivation: 为了缩小由手机图像获取不当引起的可靠性差距，提出了一种预测性框架，以估计图像帧在后续验证任务中的实用性。

Method: 采用轻量级模型来预测视频帧的质量评分，以评估其在下游验证任务中的适用性。通过重构的FNMR和ISRR指标对一个包含32,000多张图像的大规模数据集进行验证。

Result: 研究发现，冻结的ImageNet预训练网络的轻量级探针在未见过的印刷技术上表现出比完全微调模型更好的泛化能力。

Conclusion: 对于物理制造的领域转移，使用冻结的通用主干模型比进行完全微调更具鲁棒性，能更好地适应不同的印刷技术。

Abstract: The reliability of secure graphic verification, a key anti-counterfeiting
tool, is undermined by poor image acquisition on smartphones. Uncontrolled user
captures of these high-entropy patterns cause high false rejection rates,
creating a significant 'reliability gap'. To bridge this gap, we depart from
traditional perceptual IQA and introduce a framework that predictively
estimates a frame's utility for the downstream verification task. We propose a
lightweight model to predict a quality score for a video frame, determining its
suitability for a resource-intensive oracle model. Our framework is validated
using re-contextualized FNMR and ISRR metrics on a large-scale dataset of
32,000+ images from 105 smartphones. Furthermore, a novel cross-domain analysis
on graphics from different industrial printing presses reveals a key finding: a
lightweight probe on a frozen, ImageNet-pretrained network generalizes better
to an unseen printing technology than a fully fine-tuned model. This provides a
key insight for real-world generalization: for domain shifts from physical
manufacturing, a frozen general-purpose backbone can be more robust than full
fine-tuning, which can overfit to source-domain artifacts.

</details>


### [54] [SHMoAReg: Spark Deformable Image Registration via Spatial Heterogeneous Mixture of Experts and Attention Heads](https://arxiv.org/abs/2509.20073)
*Yuxi Zheng,Jianhui Feng,Tianran Li,Marius Staring,Yuchuan Qiao*

Main category: cs.CV

TL;DR: 本论文提出了SHMoAReg，一个基于Mixture of Experts机制的解码网络，在图像配准方面实现了显著性能提升，并增强了模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前的深度学习图像配准方法在特征提取和变形预测方面存在不足，需要更专门的特征提取和更灵活的变形预测机制。

Method: 采用Mixture of Experts（MoE）机制进行特征提取和变形场预测，结合了Mixture of Attention heads（MoA）和Spatial Heterogeneous Mixture of Experts（SHMoE）。

Result: 在两组公开数据集上的实验表明，SHMoAReg在Dice评分上从60.58%提高到65.58%，表现出一致的性能提升。

Conclusion: SHMoAReg网络在图像配准任务中取得了显著的性能提升，并提高了模型的可解释性。

Abstract: Encoder-Decoder architectures are widely used in deep learning-based
Deformable Image Registration (DIR), where the encoder extracts multi-scale
features and the decoder predicts deformation fields by recovering spatial
locations. However, current methods lack specialized extraction of features
(that are useful for registration) and predict deformation jointly and
homogeneously in all three directions. In this paper, we propose a novel
expert-guided DIR network with Mixture of Experts (MoE) mechanism applied in
both encoder and decoder, named SHMoAReg. Specifically, we incorporate Mixture
of Attention heads (MoA) into encoder layers, while Spatial Heterogeneous
Mixture of Experts (SHMoE) into the decoder layers. The MoA enhances the
specialization of feature extraction by dynamically selecting the optimal
combination of attention heads for each image token. Meanwhile, the SHMoE
predicts deformation fields heterogeneously in three directions for each voxel
using experts with varying kernel sizes. Extensive experiments conducted on two
publicly available datasets show consistent improvements over various methods,
with a notable increase from 60.58% to 65.58% in Dice score for the abdominal
CT dataset. Furthermore, SHMoAReg enhances model interpretability by
differentiating experts' utilities across/within different resolution layers.
To the best of our knowledge, we are the first to introduce MoE mechanism into
DIR tasks. The code will be released soon.

</details>


### [55] [Unleashing the Potential of the Semantic Latent Space in Diffusion Models for Image Dehazing](https://arxiv.org/abs/2509.20091)
*Zizheng Yang,Hu Yu,Bing Li,Jinghao Zhang,Jie Huang,Feng Zhao*

Main category: cs.CV

TL;DR: 本研究提出了一种新方法DiffLI$^2$D，通过利用预训练扩散模型的潜在表示，实现更高效的图像去雾，且在多种数据集上获得优秀结果。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像去雾中具有强大的生成能力，但由于其重训练的计算负担和推理过程中的多次采样限制了其广泛应用。

Method: 提出一种名为DiffLI$^2$D的网络，利用冻结的预训练扩散模型的语义潜在空间来进行图像去雾，避免了重新训练和迭代采样过程。

Result: 通过整合不同时间步的扩散潜在表示，DiffLI$^2$D为图像去雾提供了有效指导，并且在多个数据集上的实验结果表明其性能优于现有方法。

Conclusion: 该方法在多种数据集上显示出优于现有图像去雾方法的性能。

Abstract: Diffusion models have recently been investigated as powerful generative
solvers for image dehazing, owing to their remarkable capability to model the
data distribution. However, the massive computational burden imposed by the
retraining of diffusion models, coupled with the extensive sampling steps
during the inference, limit the broader application of diffusion models in
image dehazing. To address these issues, we explore the properties of hazy
images in the semantic latent space of frozen pre-trained diffusion models, and
propose a Diffusion Latent Inspired network for Image Dehazing, dubbed
DiffLI$^2$D. Specifically, we first reveal that the semantic latent space of
pre-trained diffusion models can represent the content and haze characteristics
of hazy images, as the diffusion time-step changes. Building upon this insight,
we integrate the diffusion latent representations at different time-steps into
a delicately designed dehazing network to provide instructions for image
dehazing. Our DiffLI$^2$D avoids re-training diffusion models and iterative
sampling process by effectively utilizing the informative representations
derived from the pre-trained diffusion models, which also offers a novel
perspective for introducing diffusion models to image dehazing. Extensive
experiments on multiple datasets demonstrate that the proposed method achieves
superior performance to existing image dehazing methods. Code is available at
https://github.com/aaaasan111/difflid.

</details>


### [56] [Hyperspectral Adapter for Semantic Segmentation with Vision Foundation Models](https://arxiv.org/abs/2509.20107)
*JuanaJuana Valeria Hurtado,Rohit Mohan,Abhinav Valada*

Main category: cs.CV

TL;DR: 本研究提出了一种新的超光谱适配器，通过结合预训练模型和新架构，在自动驾驶应用中显著提升了超光谱成像的语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 当前的超光谱成像语义分割方法由于依赖于针对RGB输入优化的架构和学习框架而表现不佳，激发了对超光谱数据进行有效学习的新方法的需求。

Method: 提出了一种新颖的超光谱适配器，该适配器利用预训练的视觉基础模型，通过光谱变换器和空间先验模块提取丰富的空间-光谱特征，并通过模态感知交互块有效集成超光谱表示和冻结的视觉变换器特征。

Result: 在三个基准自动驾驶数据集上的广泛评估表明，所提出的架构在使用HSI输入的情况下，实现了最先进的语义分割性能。

Conclusion: 本研究提出的超光谱适配器在直接使用超光谱输入的情况下，在自动驾驶数据集上实现了最先进的语义分割性能，超越了基于视觉的方法和超光谱分割方法。

Abstract: Hyperspectral imaging (HSI) captures spatial information along with dense
spectral measurements across numerous narrow wavelength bands. This rich
spectral content has the potential to facilitate robust robotic perception,
particularly in environments with complex material compositions, varying
illumination, or other visually challenging conditions. However, current HSI
semantic segmentation methods underperform due to their reliance on
architectures and learning frameworks optimized for RGB inputs. In this work,
we propose a novel hyperspectral adapter that leverages pretrained vision
foundation models to effectively learn from hyperspectral data. Our
architecture incorporates a spectral transformer and a spectrum-aware spatial
prior module to extract rich spatial-spectral features. Additionally, we
introduce a modality-aware interaction block that facilitates effective
integration of hyperspectral representations and frozen vision Transformer
features through dedicated extraction and injection mechanisms. Extensive
evaluations on three benchmark autonomous driving datasets demonstrate that our
architecture achieves state-of-the-art semantic segmentation performance while
directly using HSI inputs, outperforming both vision-based and hyperspectral
segmentation methods. We make the code available at
https://hyperspectraladapter.cs.uni-freiburg.de.

</details>


### [57] [A Simple Data Augmentation Strategy for Text-in-Image Scientific VQA](https://arxiv.org/abs/2509.20119)
*Belal Shoer,Yova Kementchedjhieva*

Main category: cs.CV

TL;DR: 本论文提出了一种新方法，通过合成数据集和微调模型来提高科学视觉问答的多语言性能。


<details>
  <summary>Details</summary>
Motivation: 科学图形和上下文的复杂性使得视觉-语言模型在视觉问答中的应用面临重大挑战，传统方法未能有效处理图形和文本的结合。

Method: 通过将现有的图像-文本对转换为统一图像的方式，合成新的数据集，并对小型多语言多模态模型进行微调。

Result: 在13种语言中，经过微调后，实现了显著的性能提升，并且具备良好的跨语言转移能力。

Conclusion: 针对科学视觉问答的研究中，融合视觉与文本信息的新训练方法显著提升了多语言模型的性能。

Abstract: Scientific visual question answering poses significant challenges for
vision-language models due to the complexity of scientific figures and their
multimodal context. Traditional approaches treat the figure and accompanying
text (e.g., questions and answer options) as separate inputs. EXAMS-V
introduced a new paradigm by embedding both visual and textual content into a
single image. However, even state-of-the-art proprietary models perform poorly
on this setup in zero-shot settings, underscoring the need for task-specific
fine-tuning. To address the scarcity of training data in this "text-in-image"
format, we synthesize a new dataset by converting existing separate image-text
pairs into unified images. Fine-tuning a small multilingual multimodal model on
a mix of our synthetic data and EXAMS-V yields notable gains across 13
languages, demonstrating strong average improvements and cross-lingual
transfer.

</details>


### [58] [EchoBench: Benchmarking Sycophancy in Medical Large Vision-Language Models](https://arxiv.org/abs/2509.20146)
*Botai Yuan,Yutian Zhou,Yingjie Wang,Fushuo Huo,Yongcheng Jing,Li Shen,Ying Wei,Zhiqi Shen,Ziwei Liu,Tianwei Zhang,Jie Yang,Dacheng Tao*

Main category: cs.CV

TL;DR: 本文研究了医疗LVLM中的恭维效应，提出了EchoBench基准，发现现有模型的恭维率显著，强调了在医疗应用中需重视模型的可靠性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的医疗LVLM基准主要关注准确率，而忽视了可靠性和安全性，本文研究了恭维效应在临床环境中的影响。

Method: 引入新的基准EchoBench，系统地评估医疗LVLM中的恭维效应，使用2122张图像和90个提示进行评估。

Result: 各类LVLM均表现出显著的恭维效应，部分模型的恭维率超过95%。引入高质量/多样化的数据和加强领域知识可减少恭维效应，简单的提示干预也能有效降低。

Conclusion: 研究表明，需要超越准确性的评估，以促进更安全、更可靠的医疗大规模视觉语言模型。

Abstract: Recent benchmarks for medical Large Vision-Language Models (LVLMs) emphasize
leaderboard accuracy, overlooking reliability and safety. We study sycophancy
-- models' tendency to uncritically echo user-provided information -- in
high-stakes clinical settings. We introduce EchoBench, a benchmark to
systematically evaluate sycophancy in medical LVLMs. It contains 2,122 images
across 18 departments and 20 modalities with 90 prompts that simulate biased
inputs from patients, medical students, and physicians. We evaluate
medical-specific, open-source, and proprietary LVLMs. All exhibit substantial
sycophancy; the best proprietary model (Claude 3.7 Sonnet) still shows 45.98%
sycophancy, and GPT-4.1 reaches 59.15%. Many medical-specific models exceed 95%
sycophancy despite only moderate accuracy. Fine-grained analyses by bias type,
department, perceptual granularity, and modality identify factors that increase
susceptibility. We further show that higher data quality/diversity and stronger
domain knowledge reduce sycophancy without harming unbiased accuracy. EchoBench
also serves as a testbed for mitigation: simple prompt-level interventions
(negative prompting, one-shot, few-shot) produce consistent reductions and
motivate training- and decoding-time strategies. Our findings highlight the
need for robust evaluation beyond accuracy and provide actionable guidance
toward safer, more trustworthy medical LVLMs.

</details>


### [59] [Smaller is Better: Enhancing Transparency in Vehicle AI Systems via Pruning](https://arxiv.org/abs/2509.20148)
*Sanish Suwal,Shaurya Garg,Dipkamal Bhusal,Michael Clifford,Nidhi Rastogi*

Main category: cs.CV

TL;DR: 本研究探讨了三种训练策略对交通标志分类器的后期解释质量的影响，表明修剪方法在提高AI模型的透明性和可靠性方面具有显著优势，尤其适用于资源受限的车辆AI系统。


<details>
  <summary>Details</summary>
Motivation: 由于后期解释的质量和可靠性常受到质疑，尤其是在自动驾驶领域，理解AI模型决策的透明性和安全性变得尤为重要。

Method: 系统比较自然训练、对抗训练和修剪三种训练方法对交通标志分类器后期解释质量的影响。

Result: 修剪显著提升了后期解释的可理解性和信赖度，并改善了模型的效率和学习表示的稀疏性。

Conclusion: 修剪策略有效提高了交通标志分类器的后期解释的可理解性和可靠性，且在资源有限的车载AI系统中具有潜力。

Abstract: Connected and autonomous vehicles continue to heavily rely on AI systems,
where transparency and security are critical for trust and operational safety.
Post-hoc explanations provide transparency to these black-box like AI models
but the quality and reliability of these explanations is often questioned due
to inconsistencies and lack of faithfulness in representing model decisions.
This paper systematically examines the impact of three widely used training
approaches, namely natural training, adversarial training, and pruning, affect
the quality of post-hoc explanations for traffic sign classifiers. Through
extensive empirical evaluation, we demonstrate that pruning significantly
enhances the comprehensibility and faithfulness of explanations (using saliency
maps). Our findings reveal that pruning not only improves model efficiency but
also enforces sparsity in learned representation, leading to more interpretable
and reliable decisions. Additionally, these insights suggest that pruning is a
promising strategy for developing transparent deep learning models, especially
in resource-constrained vehicular AI systems.

</details>


### [60] [C$^2$MIL: Synchronizing Semantic and Topological Causalities in Multiple Instance Learning for Robust and Interpretable Survival Analysis](https://arxiv.org/abs/2509.20152)
*Min Cen,Zhenfeng Zhuang,Yuzhe Zhang,Min Zeng,Baptiste Magnier,Lequan Yu,Hong Zhang,Liansheng Wang*

Main category: cs.CV

TL;DR: 本研究提出C$^2$MIL模型，旨在改善图基多实例学习在生存分析中的性能，解决语义偏差和因果光谱噪声的问题，实验结果表明其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决现有图基多实例学习方法在生存分析中面对的语义偏差和因果结构噪声问题。

Method: 提出了C$^2$MIL模型，结合了交叉尺度自适应特征解耦模块和伯努利可微因果子图采样方法，同时采用联合优化策略。

Result: C$^2$MIL在实验中比现有方法在泛化能力和可解释性上有显著提高。

Conclusion: C$^2$MIL模型在多个实例学习中提高了泛化能力和可解释性，能作为现有模型的因果增强方法。

Abstract: Graph-based Multiple Instance Learning (MIL) is widely used in survival
analysis with Hematoxylin and Eosin (H\&E)-stained whole slide images (WSIs)
due to its ability to capture topological information. However, variations in
staining and scanning can introduce semantic bias, while topological subgraphs
that are not relevant to the causal relationships can create noise, resulting
in biased slide-level representations. These issues can hinder both the
interpretability and generalization of the analysis. To tackle this, we
introduce a dual structural causal model as the theoretical foundation and
propose a novel and interpretable dual causal graph-based MIL model, C$^2$MIL.
C$^2$MIL incorporates a novel cross-scale adaptive feature disentangling module
for semantic causal intervention and a new Bernoulli differentiable causal
subgraph sampling method for topological causal discovery. A joint optimization
strategy combining disentangling supervision and contrastive learning enables
simultaneous refinement of both semantic and topological causalities.
Experiments demonstrate that C$^2$MIL consistently improves generalization and
interpretability over existing methods and can serve as a causal enhancement
for diverse MIL baselines. The code is available at
https://github.com/mimic0127/C2MIL.

</details>


### [61] [U-Mamba2-SSL for Semi-Supervised Tooth and Pulp Segmentation in CBCT](https://arxiv.org/abs/2509.20154)
*Zhi Qin Tan,Xiatian Zhu,Owen Addison,Yunpeng Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为U-Mamba2-SSL的半监督学习框架，显著提升了CBCT图像中牙齿和牙髓的分割精度。


<details>
  <summary>Details</summary>
Motivation: 牙齿和牙髓的准确分割对临床应用至关重要，但现有方法需大量专业知识且耗时，急需自动化算法处理无标签数据。

Method: 提出的U-Mamba2-SSL框架包括使用自监督方式预训练U-Mamba2，通过一致性正则化利用无标签数据，并实施伪标记策略以减少潜在错误的影响。

Result: U-Mamba2-SSL在验证集上的表现优越，达到了0.872的平均评分和0.969的DSC，证明了该方法的有效性。

Conclusion: U-Mamba2-SSL模型在牙齿和牙髓的分割任务中表现优越，验证集上取得了0.872的平均分和0.969的DSC，展示了其有效性。

Abstract: Accurate segmentation of teeth and pulp in Cone-Beam Computed Tomography
(CBCT) is vital for clinical applications like treatment planning and
diagnosis. However, this process requires extensive expertise and is
exceptionally time-consuming, highlighting the critical need for automated
algorithms that can effectively utilize unlabeled data. In this paper, we
propose U-Mamba2-SSL, a novel semi-supervised learning framework that builds on
the U-Mamba2 model and employs a multi-stage training strategy. The framework
first pre-trains U-Mamba2 in a self-supervised manner using a disruptive
autoencoder. It then leverages unlabeled data through consistency
regularization, where we introduce input and feature perturbations to ensure
stable model outputs. Finally, a pseudo-labeling strategy is implemented with a
reduced loss weighting to minimize the impact of potential errors. U-Mamba2-SSL
achieved an average score of 0.872 and a DSC of 0.969 on the validation
dataset, demonstrating the superior performance of our approach. The code is
available at https://github.com/zhiqin1998/UMamba2.

</details>


### [62] [Optical Ocean Recipes: Creating Realistic Datasets to Facilitate Underwater Vision Research](https://arxiv.org/abs/2509.20171)
*Patricia Schöntag,David Nakath,Judith Fischer,Rüdiger Röttgers,Kevin Köser*

Main category: cs.CV

TL;DR: 文章提出了一个新的框架以改进水下机器视觉的评估，允许在可控环境下生成真实数据集，解决了光学挑战和验证不足的问题。


<details>
  <summary>Details</summary>
Motivation: 为了克服水下机器视觉评估中的难题，如水环境的光学挑战与缺乏可控和验证的数据集，提出了新的数据生成框架。

Method: 通过使用经过校准的颜色和散射添加剂，在控制的水下环境中创建数据集。

Result: 开发了用于水下视觉任务的真实数据集，并演示了其在水参数估计、图像恢复、分割等任务中的应用。

Conclusion: 提出了一种名为“光学海洋配方”的框架，为在可控的水下条件下创建真实数据集提供了独特的方法，促进机器视觉在水下情境的分析和验证。

Abstract: The development and evaluation of machine vision in underwater environments
remains challenging, often relying on trial-and-error-based testing tailored to
specific applications. This is partly due to the lack of controlled,
ground-truthed testing environments that account for the optical challenges,
such as color distortion from spectrally variant light attenuation, reduced
contrast and blur from backscatter and volume scattering, and dynamic light
patterns from natural or artificial illumination. Additionally, the appearance
of ocean water in images varies significantly across regions, depths, and
seasons. However, most machine vision evaluations are conducted under specific
optical water types and imaging conditions, therefore often lack
generalizability. Exhaustive testing across diverse open-water scenarios is
technically impractical. To address this, we introduce the \textit{Optical
Ocean Recipes}, a framework for creating realistic datasets under controlled
underwater conditions. Unlike synthetic or open-water data, these recipes,
using calibrated color and scattering additives, enable repeatable and
controlled testing of the impact of water composition on image appearance.
Hence, this provides a unique framework for analyzing machine vision in
realistic, yet controlled underwater scenarios. The controlled environment
enables the creation of ground-truth data for a range of vision tasks,
including water parameter estimation, image restoration, segmentation, visual
SLAM, and underwater image synthesis. We provide a demonstration dataset
generated using the Optical Ocean Recipes and briefly demonstrate the use of
our system for two underwater vision tasks. The dataset and evaluation code
will be made available.

</details>


### [63] [Universal Camouflage Attack on Vision-Language Models for Autonomous Driving](https://arxiv.org/abs/2509.20196)
*Dehong Kong,Sifan Yu,Siyuan Liang,Jiawei Liang,Jianhou Gan,Aishan Liu,Wenqi Ren*

Main category: cs.CV

TL;DR: 该研究提出UCA框架，通过操作特征空间来提升VLM-AD的抗攻击能力，显示出在不同场景下引发错误驾驶命令的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于VLM-AD中的编码器和投影层的脆弱性，现有的攻击方法主要集中在数字级别，缺乏直接转移到VLM-AD系统的物理对抗攻击。

Method: UCA操作于特征空间，生成物理可实现的伪装纹理，利用特征差异损失和多尺度学习提升适应性和训练稳定性。

Result: UCA在各种VLM-AD模型和驾驶场景中超越现有最先进的攻击方法，改进了30%的3-P指标，表现出优越的攻击能力和鲁棒性。

Conclusion: UCA在不同VLM-AD模型和驾驶场景中都能引发错误的驾驶命令，并显示出较强的攻击鲁棒性，具有实际应用潜力。

Abstract: Visual language modeling for automated driving is emerging as a promising
research direction with substantial improvements in multimodal reasoning
capabilities. Despite its advanced reasoning abilities, VLM-AD remains
vulnerable to serious security threats from adversarial attacks, which involve
misleading model decisions through carefully crafted perturbations. Existing
attacks have obvious challenges: 1) Physical adversarial attacks primarily
target vision modules. They are difficult to directly transfer to VLM-AD
systems because they typically attack low-level perceptual components. 2)
Adversarial attacks against VLM-AD have largely concentrated on the digital
level. To address these challenges, we propose the first Universal Camouflage
Attack (UCA) framework for VLM-AD. Unlike previous methods that focus on
optimizing the logit layer, UCA operates in the feature space to generate
physically realizable camouflage textures that exhibit strong generalization
across different user commands and model architectures. Motivated by the
observed vulnerability of encoder and projection layers in VLM-AD, UCA
introduces a feature divergence loss (FDL) that maximizes the representational
discrepancy between clean and adversarial images. In addition, UCA incorporates
a multi-scale learning strategy and adjusts the sampling ratio to enhance its
adaptability to changes in scale and viewpoint diversity in real-world
scenarios, thereby improving training stability. Extensive experiments
demonstrate that UCA can induce incorrect driving commands across various
VLM-AD models and driving scenarios, significantly surpassing existing
state-of-the-art attack methods (improving 30\% in 3-P metrics). Furthermore,
UCA exhibits strong attack robustness under diverse viewpoints and dynamic
conditions, indicating high potential for practical deployment.

</details>


### [64] [PU-Gaussian: Point Cloud Upsampling using 3D Gaussian Representation](https://arxiv.org/abs/2509.20207)
*Mahmoud Khater,Mona Strauss,Philipp von Olshausen,Alexander Reiterer*

Main category: cs.CV

TL;DR: 本论文提出PU-Gaussian网络，通过模型局部邻域的3D高斯分布来有效上采样稀疏点云，实现了高保真度的3D表示和卓越性能。


<details>
  <summary>Details</summary>
Motivation: 针对稀疏和嘈杂的点云数据进行高保真度3D表示的挑战，尤其是在几何可解释性和对输入稀疏性的鲁棒性方面的不足。

Method: 提出了一种新颖的上采样网络PU-Gaussian，通过各点周围的各向异性3D高斯分布建模局部邻域，从而显式地在局部几何域进行点采样。

Result: PU-Gaussian在PU1K和PUGAN数据集的广泛测试表明其达到了最新的性能水平，并生成了更均匀的分布和更清晰的边缘。

Conclusion: PU-Gaussian在上采样任务中实现了卓越的性能，具有良好的几何可解释性和对输入稀疏性的鲁棒性。

Abstract: Point clouds produced by 3D sensors are often sparse and noisy, posing
challenges for tasks requiring dense and high-fidelity 3D representations.
Prior work has explored both implicit feature-based upsampling and
distance-function learning to address this, but often at the expense of
geometric interpretability or robustness to input sparsity. To overcome these
limitations, we propose PU-Gaussian, a novel upsampling network that models the
local neighborhood around each point using anisotropic 3D Gaussian
distributions. These Gaussians capture the underlying geometric structure,
allowing us to perform upsampling explicitly in the local geometric domain by
direct point sampling. The sampling process generates a dense, but coarse,
point cloud. A subsequent refinement network adjusts the coarse output to
produce a more uniform distribution and sharper edges. We perform extensive
testing on the PU1K and PUGAN datasets, demonstrating that PU-Gaussian achieves
state-of-the-art performance. We make code and model weights publicly available
at https://github.com/mvg-inatech/PU-Gaussian.git.

</details>


### [65] [ImageNet-trained CNNs are not biased towards texture: Revisiting feature reliance through controlled suppression](https://arxiv.org/abs/2509.20234)
*Tom Burgert,Oliver Stoll,Paolo Rota,Begüm Demir*

Main category: cs.CV

TL;DR: 本文通过一项新的框架分析CNN是否存在内在的纹理偏见，发现CNN主要依赖局部形状特征，并且在不同应用领域中，模型对于特征的依赖模式存在系统性的差异。


<details>
  <summary>Details</summary>
Motivation: 重新审视CNN内在纹理偏见的假设，解决Geirhos等人的线索冲突实验中的局限性。

Method: 提出一种领域无关的框架，通过系统性抑制形状、纹理和颜色线索，量化特征依赖性，并在受控条件下评估人类和神经网络的表现。

Result: 在不同领域（计算机视觉、医学成像、遥感）中，模型对特征的依赖模式不同：计算机视觉模型优先考虑形状，医学成像模型强调颜色，遥感模型则更依赖于纹理。

Conclusion: CNNs主要依靠局部形状特征，而非内在的纹理偏见，且这种依赖可通过现代训练策略和架构显著减轻。

Abstract: The hypothesis that Convolutional Neural Networks (CNNs) are inherently
texture-biased has shaped much of the discourse on feature use in deep
learning. We revisit this hypothesis by examining limitations in the
cue-conflict experiment by Geirhos et al. To address these limitations, we
propose a domain-agnostic framework that quantifies feature reliance through
systematic suppression of shape, texture, and color cues, avoiding the
confounds of forced-choice conflicts. By evaluating humans and neural networks
under controlled suppression conditions, we find that CNNs are not inherently
texture-biased but predominantly rely on local shape features. Nonetheless,
this reliance can be substantially mitigated through modern training strategies
or architectures (ConvNeXt, ViTs). We further extend the analysis across
computer vision, medical imaging, and remote sensing, revealing that reliance
patterns differ systematically: computer vision models prioritize shape,
medical imaging models emphasize color, and remote sensing models exhibit a
stronger reliance towards texture. Code is available at
https://github.com/tomburgert/feature-reliance.

</details>


### [66] [An Anisotropic Cross-View Texture Transfer with Multi-Reference Non-Local Attention for CT Slice Interpolation](https://arxiv.org/abs/2509.20242)
*Kwang-Hyun Uhm,Hyunjun Cho,Sung-Hoo Hong,Seung-Won Jung*

Main category: cs.CV

TL;DR: 本文提出一种创新的交叉视图纹理转移方法，旨在利用3D CT卷的各向异性特性提高CT切片的插值质量。


<details>
  <summary>Details</summary>
Motivation: CT成像中由于大切片厚度导致的各向异性特性使得疾病诊断面临挑战，急需改进切片分辨率。

Method: 设计了一种独特的框架，通过多参考非局部注意模块提取特征，将高分辨率的平面纹理细节转移到低分辨率的切片上。

Result: 在公共CT数据集上的大量实验中，结果显示我们的方法在CT切片插值方面显著优于现有方法。

Conclusion: 提出的交叉视图纹理转移方法在CT切片插值方面显著优于现有竞争方法。

Abstract: Computed tomography (CT) is one of the most widely used non-invasive imaging
modalities for medical diagnosis. In clinical practice, CT images are usually
acquired with large slice thicknesses due to the high cost of memory storage
and operation time, resulting in an anisotropic CT volume with much lower
inter-slice resolution than in-plane resolution. Since such inconsistent
resolution may lead to difficulties in disease diagnosis, deep learning-based
volumetric super-resolution methods have been developed to improve inter-slice
resolution. Most existing methods conduct single-image super-resolution on the
through-plane or synthesize intermediate slices from adjacent slices; however,
the anisotropic characteristic of 3D CT volume has not been well explored. In
this paper, we propose a novel cross-view texture transfer approach for CT
slice interpolation by fully utilizing the anisotropic nature of 3D CT volume.
Specifically, we design a unique framework that takes high-resolution in-plane
texture details as a reference and transfers them to low-resolution
through-plane images. To this end, we introduce a multi-reference non-local
attention module that extracts meaningful features for reconstructing
through-plane high-frequency details from multiple in-plane images. Through
extensive experiments, we demonstrate that our method performs significantly
better in CT slice interpolation than existing competing methods on public CT
datasets including a real-paired benchmark, verifying the effectiveness of the
proposed framework. The source code of this work is available at
https://github.com/khuhm/ACVTT.

</details>


### [67] [4D Driving Scene Generation With Stereo Forcing](https://arxiv.org/abs/2509.20251)
*Hao Lu,Zhuang Ma,Guangfeng Jiang,Wenhang Ge,Bohan Li,Yuzhan Cai,Wenzhao Zheng,Yunpeng Zhang,Yingcong Chen*

Main category: cs.CV

TL;DR: PhiGenesis 是一种统一的 4D 场景生成框架，结合了几何和时间一致性，解决了生成和新视图合成之间的挑战，展示了优越的性能。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型在同步支持时间外推和空间新视图合成方面面临挑战，特别是在缺乏每个场景优化的情况下。

Method: PhiGenesis 分为两个阶段：第一阶段使用预训练的视频 VAE 进行 4D 重建，第二阶段引入几何指导的视频扩散模型以生成未来视图。

Result: 实验结果表明，PhiGenesis 在外观和几何重建、时间生成和新视图合成任务中均达到最先进的性能，同时在下游评估中表现竞争力。

Conclusion: PhiGenesis 提供了一种先进的 4D 场景生成方法，在外观和几何重建，以及时间生成和新视图合成方面表现优异。

Abstract: Current generative models struggle to synthesize dynamic 4D driving scenes
that simultaneously support temporal extrapolation and spatial novel view
synthesis (NVS) without per-scene optimization. Bridging generation and novel
view synthesis remains a major challenge. We present PhiGenesis, a unified
framework for 4D scene generation that extends video generation techniques with
geometric and temporal consistency. Given multi-view image sequences and camera
parameters, PhiGenesis produces temporally continuous 4D Gaussian splatting
representations along target 3D trajectories. In its first stage, PhiGenesis
leverages a pre-trained video VAE with a novel range-view adapter to enable
feed-forward 4D reconstruction from multi-view images. This architecture
supports single-frame or video inputs and outputs complete 4D scenes including
geometry, semantics, and motion. In the second stage, PhiGenesis introduces a
geometric-guided video diffusion model, using rendered historical 4D scenes as
priors to generate future views conditioned on trajectories. To address
geometric exposure bias in novel views, we propose Stereo Forcing, a novel
conditioning strategy that integrates geometric uncertainty during denoising.
This method enhances temporal coherence by dynamically adjusting generative
influence based on uncertainty-aware perturbations. Our experimental results
demonstrate that our method achieves state-of-the-art performance in both
appearance and geometric reconstruction, temporal generation and novel view
synthesis (NVS) tasks, while simultaneously delivering competitive performance
in downstream evaluations. Homepage is at
\href{https://jiangxb98.github.io/PhiGensis}{PhiGensis}.

</details>


### [68] [A Versatile Foundation Model for AI-enabled Mammogram Interpretation](https://arxiv.org/abs/2509.20271)
*Fuxiang Huang,Jiayi Zhu,Yunfang Yu,Yu Xie,Yuan Guo,Qingcong Kong,Mingxiang Wu,Xinrui Jiang,Shu Yang,Jiabo Ma,Ziyi Liu,Zhe Xu,Zhixuan Chen,Yujie Tan,Zifan He,Luhui Mao,Xi Wang,Junlin Hou,Lei Zhang,Qiong Luo,Zhenhui Li,Herui Yao,Hao Chen*

Main category: cs.CV

TL;DR: 本研究提出VersaMammo，利用大型多机构乳腺X光图像数据集和创新的预训练策略，显著提升乳腺癌筛查的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 克服现有乳腺X光分析模型在数据多样性、通用性和临床评估方面的局限，推动乳腺癌早期检测。

Method: 采用两阶段的预训练策略，结合自监督学习和知识蒸馏，建立一个多任务基准。

Result: VersaMammo在68个内部任务中取得50个第一名，在24个外部验证任务中取得20个第一名，平均名次分别为1.5和1.2。

Conclusion: VersaMammo在乳腺癌筛查和诊断中表现出优越的泛化能力和临床实用性，是可靠且可扩展的进展。

Abstract: Breast cancer is the most commonly diagnosed cancer and the leading cause of
cancer-related mortality in women globally. Mammography is essential for the
early detection and diagnosis of breast lesions. Despite recent progress in
foundation models (FMs) for mammogram analysis, their clinical translation
remains constrained by several fundamental limitations, including insufficient
diversity in training data, limited model generalizability, and a lack of
comprehensive evaluation across clinically relevant tasks. Here, we introduce
VersaMammo, a versatile foundation model for mammograms, designed to overcome
these limitations. We curated the largest multi-institutional mammogram dataset
to date, comprising 706,239 images from 21 sources. To improve generalization,
we propose a two-stage pre-training strategy to develop VersaMammo, a mammogram
foundation model. First, a teacher model is trained via self-supervised
learning to extract transferable features from unlabeled mammograms. Then,
supervised learning combined with knowledge distillation transfers both
features and clinical knowledge into VersaMammo. To ensure a comprehensive
evaluation, we established a benchmark comprising 92 specific tasks, including
68 internal tasks and 24 external validation tasks, spanning 5 major clinical
task categories: lesion detection, segmentation, classification, image
retrieval, and visual question answering. VersaMammo achieves state-of-the-art
performance, ranking first in 50 out of 68 specific internal tasks and 20 out
of 24 external validation tasks, with average ranks of 1.5 and 1.2,
respectively. These results demonstrate its superior generalization and
clinical utility, offering a substantial advancement toward reliable and
scalable breast cancer screening and diagnosis.

</details>


### [69] [A co-evolving agentic AI system for medical imaging analysis](https://arxiv.org/abs/2509.20279)
*Songhao Li,Jonathan Xu,Tiancheng Bao,Yuxuan Liu,Yuchen Liu,Yihang Liu,Lilin Wang,Wenhui Lei,Sheng Wang,Yinuo Xu,Yan Cui,Jialu Yao,Shunsuke Koga,Zhi Huang*

Main category: cs.CV

TL;DR: TissueLab是一个集成多种医学领域工具的开源代理AI系统，能够实现实时分析、自动化工作流生成，且在医学影像领域表现出色。


<details>
  <summary>Details</summary>
Motivation: 虽然代理AI在医疗保健和生物医学研究中迅速发展，但在医学影像分析中的应用和性能仍然有限，主要是因为缺乏强大的生态系统和实时交互的专家反馈。

Method: TissueLab整合了病理学、放射学和空间组学领域的工具工厂，通过标准化输入、输出和不同工具的能力来实现自动化分析。

Result: TissueLab在多个任务中展示了与最先进的视觉-语言模型和其他代理AI系统相比较优越的性能，同时能够快速在未见过的疾病背景中提供准确结果。

Conclusion: TissueLab是一个开源的生态系统，旨在加速医学影像的计算研究和转化应用，同时为下一代医疗AI奠定基础。

Abstract: Agentic AI is rapidly advancing in healthcare and biomedical research.
However, in medical image analysis, their performance and adoption remain
limited due to the lack of a robust ecosystem, insufficient toolsets, and the
absence of real-time interactive expert feedback. Here we present "TissueLab",
a co-evolving agentic AI system that allows researchers to ask direct
questions, automatically plan and generate explainable workflows, and conduct
real-time analyses where experts can visualize intermediate results and refine
them. TissueLab integrates tool factories across pathology, radiology, and
spatial omics domains. By standardizing inputs, outputs, and capabilities of
diverse tools, the system determines when and how to invoke them to address
research and clinical questions. Across diverse tasks with clinically
meaningful quantifications that inform staging, prognosis, and treatment
planning, TissueLab achieves state-of-the-art performance compared with
end-to-end vision-language models (VLMs) and other agentic AI systems such as
GPT-5. Moreover, TissueLab continuously learns from clinicians, evolving toward
improved classifiers and more effective decision strategies. With active
learning, it delivers accurate results in unseen disease contexts within
minutes, without requiring massive datasets or prolonged retraining. Released
as a sustainable open-source ecosystem, TissueLab aims to accelerate
computational research and translational adoption in medical imaging while
establishing a foundation for the next generation of medical AI.

</details>


### [70] [HiPerformer: A High-Performance Global-Local Segmentation Model with Modular Hierarchical Fusion Strategy](https://arxiv.org/abs/2509.20280)
*Dayu Tan,Zhenpeng Xu,Yansen Su,Xin Peng,Chunhou Zheng,Weimin Zhong*

Main category: cs.CV

TL;DR: HiPerformer通过创新的模块化设计和特征融合模块，提高了医学图像分割的效果，是一种更精确和有效的分割方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN-Transformer混合架构的方法在特征融合方面存在信息冲突和损失的问题，急需改进的方法来提高医学图像分割的准确性。

Method: 提出了一种模块化的层次化编码器架构，通过动态并行融合多源特征，并设计了局部-全局特征融合模块和渐进金字塔聚合模块。

Result: 在十一种公开数据集上的实验结果显示，HiPerformer超越了现有分割技术，具有更高的分割准确性和鲁棒性。

Conclusion: HiPerformer方法在医学图像分割上显示出更高的准确性和鲁棒性。

Abstract: Both local details and global context are crucial in medical image
segmentation, and effectively integrating them is essential for achieving high
accuracy. However, existing mainstream methods based on CNN-Transformer hybrid
architectures typically employ simple feature fusion techniques such as serial
stacking, endpoint concatenation, or pointwise addition, which struggle to
address the inconsistencies between features and are prone to information
conflict and loss. To address the aforementioned challenges, we innovatively
propose HiPerformer. The encoder of HiPerformer employs a novel modular
hierarchical architecture that dynamically fuses multi-source features in
parallel, enabling layer-wise deep integration of heterogeneous information.
The modular hierarchical design not only retains the independent modeling
capability of each branch in the encoder, but also ensures sufficient
information transfer between layers, effectively avoiding the degradation of
features and information loss that come with traditional stacking methods.
Furthermore, we design a Local-Global Feature Fusion (LGFF) module to achieve
precise and efficient integration of local details and global semantic
information, effectively alleviating the feature inconsistency problem and
resulting in a more comprehensive feature representation. To further enhance
multi-scale feature representation capabilities and suppress noise
interference, we also propose a Progressive Pyramid Aggregation (PPA) module to
replace traditional skip connections. Experiments on eleven public datasets
demonstrate that the proposed method outperforms existing segmentation
techniques, demonstrating higher segmentation accuracy and robustness. The code
is available at https://github.com/xzphappy/HiPerformer.

</details>


### [71] [PerFace: Metric Learning in Perceptual Facial Similarity for Enhanced Face Anonymization](https://arxiv.org/abs/2509.20281)
*Haruka Kumagai,Leslie Wöhler,Satoshi Ikehata,Kiyoharu Aizawa*

Main category: cs.CV

TL;DR: 为了解决面部匿名化中的相似度问题，本文提出了一种新的人类感知基础的相似度度量方法，通过设计包含6400个三元组注释的数据集并应用度量学习，实现了在面部相似性预测和分类任务中显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着社会对隐私问题的关注增加，需要在面部匿名化技术中找到有效的面部交换方法，以平衡匿名性和自然性。

Method: 通过创建一个包含6400个三元组注释的数据集，并采用度量学习的方法来预测相似度。

Result: 实验结果显示与现有方法相比，在面部相似性预测和基于属性的面部分类任务上有显著改进。

Conclusion: 本研究提出了一种基于人类感知的面部相似度度量，显著提高了面部相似性预测和基于属性的面部分类任务的性能。

Abstract: In response to rising societal awareness of privacy concerns, face
anonymization techniques have advanced, including the emergence of
face-swapping methods that replace one identity with another. Achieving a
balance between anonymity and naturalness in face swapping requires careful
selection of identities: overly similar faces compromise anonymity, while
dissimilar ones reduce naturalness. Existing models, however, focus on binary
identity classification "the same person or not", making it difficult to
measure nuanced similarities such as "completely different" versus "highly
similar but different." This paper proposes a human-perception-based face
similarity metric, creating a dataset of 6,400 triplet annotations and metric
learning to predict the similarity. Experimental results demonstrate
significant improvements in both face similarity prediction and attribute-based
face classification tasks over existing methods.

</details>


### [72] [FAST: Foreground-aware Diffusion with Accelerated Sampling Trajectory for Segmentation-oriented Anomaly Synthesis](https://arxiv.org/abs/2509.20295)
*Xichen Xu,Yanshu Wang,Jinbao Wang,Xiaoning Lei,Guoyang Xie,Guannan Jiang,Zhichao Lu*

Main category: cs.CV

TL;DR: 本文提出了FAST框架，通过AIAS和FARM模块提高工业异常合成的效率和质量，解决了现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的工业异常合成方法在采样效率和生成质量之间难以取得平衡，并且忽视了异常和背景区域之间的统计差异。

Method: 提出了一种前景感知扩散框架，包含AIAS和FARM两个新模块。

Result: FAST能够在很少的步骤内合成出最先进的分割导向异常，且在各类实验中表现优于现有方法。

Conclusion: FAST方法在多个工业基准上显示出优越的性能，特别是在下游分割任务中。

Abstract: Industrial anomaly segmentation relies heavily on pixel-level annotations,
yet real-world anomalies are often scarce, diverse, and costly to label.
Segmentation-oriented industrial anomaly synthesis (SIAS) has emerged as a
promising alternative; however, existing methods struggle to balance sampling
efficiency and generation quality. Moreover, most approaches treat all spatial
regions uniformly, overlooking the distinct statistical differences between
anomaly and background areas. This uniform treatment hinders the synthesis of
controllable, structure-specific anomalies tailored for segmentation tasks. In
this paper, we propose FAST, a foreground-aware diffusion framework featuring
two novel modules: the Anomaly-Informed Accelerated Sampling (AIAS) and the
Foreground-Aware Reconstruction Module (FARM). AIAS is a training-free sampling
algorithm specifically designed for segmentation-oriented industrial anomaly
synthesis, which accelerates the reverse process through coarse-to-fine
aggregation and enables the synthesis of state-of-the-art segmentation-oriented
anomalies in as few as 10 steps. Meanwhile, FARM adaptively adjusts the
anomaly-aware noise within the masked foreground regions at each sampling step,
preserving localized anomaly signals throughout the denoising trajectory.
Extensive experiments on multiple industrial benchmarks demonstrate that FAST
consistently outperforms existing anomaly synthesis methods in downstream
segmentation tasks. We release the code at:
https://anonymous.4open.science/r/NeurIPS-938.

</details>


### [73] [A Comprehensive Evaluation of YOLO-based Deer Detection Performance on Edge Devices](https://arxiv.org/abs/2509.20318)
*Bishal Adhikari,Jiajia Li,Eric S. Michel,Jacob Dykes,Te-Ming Paul Tseng,Mary Love Tagert,Dong Chen*

Main category: cs.CV

TL;DR: 该研究介绍了一个新数据集和深度学习模型评估，强调了鹿检测在现代农业中重要性，提出小型YOLO模型在高效性和准确性上的最佳解决方案。


<details>
  <summary>Details</summary>
Motivation: 由于传统的鹿防护措施无法有效应对现代农业需求，急需智能自主解决方案以提高鹿检测的准确性和效率。

Method: 使用不同YOLO架构的深度学习模型进行鹿检测，进行了性能基准测试和比较分析。

Result: 该研究提供了一个由3,095张标注图像构成的数据集，并评估了12个模型变体的检测性能，揭示了适合边缘计算的平台和模型。

Conclusion: 该研究为鹿检测提供了一个新的数据集，并评估了多种深度学习模型的性能，发现小型先进模型在准确性和计算效率之间取得了最佳平衡。

Abstract: The escalating economic losses in agriculture due to deer intrusion,
estimated to be in the hundreds of millions of dollars annually in the U.S.,
highlight the inadequacy of traditional mitigation strategies since these
methods are often labor-intensive, costly, and ineffective for modern farming
systems. To overcome this, there is a critical need for intelligent, autonomous
solutions which require accurate and efficient deer detection. But the progress
in this field is impeded by a significant gap in the literature, mainly the
lack of a domain-specific, practical dataset and limited study on the on-field
deployability of deer detection systems. Addressing this gap, this study
presents a comprehensive evaluation of state-of-the-art deep learning models
for deer detection in challenging real-world scenarios. The contributions of
this work are threefold. First, we introduce a curated, publicly available
dataset of 3,095 annotated images with bounding-box annotations of deer,
derived from the Idaho Cameratraps project. Second, we provide an extensive
comparative analysis of 12 model variants across four recent YOLO
architectures(v8, v9, v10, and v11). Finally, we benchmarked performance on a
high-end NVIDIA RTX 5090 GPU and evaluated on two representative edge computing
platforms: Raspberry Pi 5 and NVIDIA Jetson AGX Xavier. Results show that the
real-time detection is not feasible in Raspberry Pi without hardware-specific
model optimization, while NVIDIA Jetson provides greater than 30 FPS with
GPU-accelerated inference on 's' and 'n' series models. This study also reveals
that smaller, architecturally advanced models such as YOLOv11n, YOLOv8s, and
YOLOv9s offer the optimal balance of high accuracy (AP@.5 > 0.85) and
computational efficiency (FPS > 30). To support further research, both the
source code and datasets are publicly available at
https://github.com/WinnerBishal/track-the-deer.

</details>


### [74] [Efficient Encoder-Free Pose Conditioning and Pose Control for Virtual Try-On](https://arxiv.org/abs/2509.20343)
*Qi Li,Shuwen Qiu,Julien Han,Xingzi Xu,Mehmet Saygin Seyfioglu,Kee Kiat Koo,Karim Bouyarmane*

Main category: cs.CV

TL;DR: 随着在线购物需求增长，这项研究聚焦于提升虚拟试衣中的姿势控制，通过姿势图拼接与混合掩膜训练策略，成功增强了产品集成的灵活性和输出的真实性。


<details>
  <summary>Details</summary>
Motivation: 随着在线购物的发展，消费者对虚拟试衣技术的需求不断增加，而姿势控制是实现有效虚拟试衣的关键挑战之一。

Method: 基于连结的虚拟试衣模型，通过空间拼接姿势数据，不添加额外参数或模块，比较姿势图和骨架的性能。

Result: 通过实验发现，姿势图拼接的表现优于其他方法，增强了姿势保持性和输出的现实感。同时，混合掩膜训练策略使模型在不同姿势条件下灵活集成产品。

Conclusion: 该研究表明，通过使用姿势图拼接技术，可以显著提高虚拟试衣效果的姿势保持性和输出现实感，同时引入混合掩膜训练策略增强了产品在多种姿势和条件下的灵活集成能力。

Abstract: As online shopping continues to grow, the demand for Virtual Try-On (VTON)
technology has surged, allowing customers to visualize products on themselves
by overlaying product images onto their own photos. An essential yet
challenging condition for effective VTON is pose control, which ensures
accurate alignment of products with the user's body while supporting diverse
orientations for a more immersive experience. However, incorporating pose
conditions into VTON models presents several challenges, including selecting
the optimal pose representation, integrating poses without additional
parameters, and balancing pose preservation with flexible pose control.
  In this work, we build upon a baseline VTON model that concatenates the
reference image condition without external encoder, control network, or complex
attention layers. We investigate methods to incorporate pose control into this
pure concatenation paradigm by spatially concatenating pose data, comparing
performance using pose maps and skeletons, without adding any additional
parameters or module to the baseline model. Our experiments reveal that pose
stitching with pose maps yields the best results, enhancing both pose
preservation and output realism. Additionally, we introduce a mixed-mask
training strategy using fine-grained and bounding box masks, allowing the model
to support flexible product integration across varied poses and conditions.

</details>


### [75] [PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation](https://arxiv.org/abs/2509.20358)
*Chen Wang,Chuhao Chen,Yiming Huang,Zhiyang Dou,Yuan Liu,Jiatao Gu,Lingjie Liu*

Main category: cs.CV

TL;DR: PhysCtrl是一个新颖的框架，专注于物理基础的图像到视频生成，利用生成物理网络和扩散模型，生成高质量、可控的物理可信视频。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成模型在生成逼真视频方面表现出色，但在物理可信度和三维可控性方面存在不足，因此提出了PhysCtrl以克服这些限制。

Method: 该框架利用生成物理网络，通过扩散模型处理物理参数和施加的力，学习四种材料（弹性、沙子、塑料泥和刚性）的物理动态分布。

Result: PhysCtrl在550K由物理模拟器生成的合成数据集上进行训练，通过引入一种新颖的时空注意力模块，能够实现更好的粒子交互和物理可信度的约束。

Conclusion: PhysCtrl能够生成真实且物理基础的运动轨迹，驱动图像到视频模型，生成在视觉质量和物理可信度上都优于现有方法的高保真可控视频。

Abstract: Existing video generation models excel at producing photo-realistic videos
from text or images, but often lack physical plausibility and 3D
controllability. To overcome these limitations, we introduce PhysCtrl, a novel
framework for physics-grounded image-to-video generation with physical
parameters and force control. At its core is a generative physics network that
learns the distribution of physical dynamics across four materials (elastic,
sand, plasticine, and rigid) via a diffusion model conditioned on physics
parameters and applied forces. We represent physical dynamics as 3D point
trajectories and train on a large-scale synthetic dataset of 550K animations
generated by physics simulators. We enhance the diffusion model with a novel
spatiotemporal attention block that emulates particle interactions and
incorporates physics-based constraints during training to enforce physical
plausibility. Experiments show that PhysCtrl generates realistic,
physics-grounded motion trajectories which, when used to drive image-to-video
models, yield high-fidelity, controllable videos that outperform existing
methods in both visual quality and physical plausibility. Project Page:
https://cwchenwang.github.io/physctrl

</details>


### [76] [EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning](https://arxiv.org/abs/2509.20360)
*Xuan Ju,Tianyu Wang,Yuqian Zhou,He Zhang,Qing Liu,Nanxuan Zhao,Zhifei Zhang,Yijun Li,Yuanhao Cai,Shaoteng Liu,Daniil Pakhomov,Zhe Lin,Soo Ye Kim,Qiang Xu*

Main category: cs.CV

TL;DR: EditVerse是一个统一的图像和视频生成及编辑框架，通过自注意力机制增强学习能力，提升了视频编辑效果，并超过了现有模型。


<details>
  <summary>Details</summary>
Motivation: 图像生成和编辑已实现统一框架，而视频生成和编辑仍然分散，亟需解决架构限制和数据稀缺问题。

Method: 通过统一的模型框架处理文本、图像和视频，以自注意力机制实现强大的上下文学习和跨模态知识迁移。

Result: EditVerse通过一个统一的模型实现了图像和视频的生成与编辑，并开发了232K的视频编辑样本用于联合训练。

Conclusion: EditVerse在各个模态之间展现出优越的编辑和生成能力，并在任务性能上超过现有模型。

Abstract: Recent advances in foundation models highlight a clear trend toward
unification and scaling, showing emergent capabilities across diverse domains.
While image generation and editing have rapidly transitioned from task-specific
to unified frameworks, video generation and editing remain fragmented due to
architectural limitations and data scarcity. In this work, we introduce
EditVerse, a unified framework for image and video generation and editing
within a single model. By representing all modalities, i.e., text, image, and
video, as a unified token sequence, EditVerse leverages self-attention to
achieve robust in-context learning, natural cross-modal knowledge transfer, and
flexible handling of inputs and outputs with arbitrary resolutions and
durations. To address the lack of video editing training data, we design a
scalable data pipeline that curates 232K video editing samples and combines
them with large-scale image and video datasets for joint training. Furthermore,
we present EditVerseBench, the first benchmark for instruction-based video
editing covering diverse tasks and resolutions. Extensive experiments and user
studies demonstrate that EditVerse achieves state-of-the-art performance,
surpassing existing open-source and commercial models, while exhibiting
emergent editing and generation abilities across modalities.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [77] [Automated Item Neutralization for Non-Cognitive Scales: A Large Language Model Approach to Reducing Social-Desirability Bias](https://arxiv.org/abs/2509.19314)
*Sirui Wu,Daijin Yang*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型在减少人格评估中的社会期望偏差方面的作用，结果显示中和方法有效但不完美。


<details>
  <summary>Details</summary>
Motivation: 评估通过大型语言模型辅助的项目中和，以减少人格评估中的社会期望偏差。

Method: 使用GPT-o3重写国际人格条目池五大人格测量（IPIP-BFM-50），并让203名参与者完成原始或中和版本的测量。

Result: 结果显示，在保留可靠性和五因素结构的同时，尽责性增加，宜人性和开放性下降，与社会期望的相关性在多个条目中有所下降，但不一致。配置不变性成立，但度量和标量不变性未能成立。

Conclusion: 人工智能中和可能是一种潜在但并不完美的偏差减少方法。

Abstract: This study evaluates item neutralization assisted by the large language model
(LLM) to reduce social desirability bias in personality assessment. GPT-o3 was
used to rewrite the International Personality Item Pool Big Five Measure
(IPIP-BFM-50), and 203 participants completed either the original or
neutralized form along with the Marlowe-Crowne Social Desirability Scale. The
results showed preserved reliability and a five-factor structure, with gains in
Conscientiousness and declines in Agreeableness and Openness. The correlations
with social desirability decreased for several items, but inconsistently.
Configural invariance held, though metric and scalar invariance failed.
Findings support AI neutralization as a potential but imperfect bias-reduction
method.

</details>


### [78] [FHIR-AgentBench: Benchmarking LLM Agents for Realistic Interoperable EHR Question Answering](https://arxiv.org/abs/2509.19319)
*Gyubok Lee,Elea Bach,Eric Yang,Tom Pollard,Alistair Johnson,Edward Choi,Yugang jia,Jong Ha Lee*

Main category: cs.CL

TL;DR: 本文引入FHIR-AgentBench基准，以评估LLM在面对HL7 FHIR标准下的复杂临床数据时的表现，揭示了数据检索和推理面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 最近转向HL7 FHIR标准的趋势要求LLM代理能够处理复杂的、基于资源的数据模型，现有基准测试无法有效评估LLM在互操作临床数据上的表现。

Method: 通过基于HL7 FHIR标准引导的2,931个真实世界的临床问题，系统评估了不同的代理框架和数据检索策略。

Result: 引入FHIR-AgentBench基准，比较了直接FHIR API调用和专用工具的数据检索策略、单回合与多回合的交互模式以及自然语言与代码生成的推理策略。

Conclusion: 通过FHIR-AgentBench基准测试，我们揭示了从复杂的FHIR资源中检索数据和推理的实际挑战，这些都显著影响了问答性能。

Abstract: The recent shift toward the Health Level Seven Fast Healthcare
Interoperability Resources (HL7 FHIR) standard opens a new frontier for
clinical AI, demanding LLM agents to navigate complex, resource-based data
models instead of conventional structured health data. However, existing
benchmarks have lagged behind this transition, lacking the realism needed to
evaluate recent LLMs on interoperable clinical data. To bridge this gap, we
introduce FHIR-AgentBench, a benchmark that grounds 2,931 real-world clinical
questions in the HL7 FHIR standard. Using this benchmark, we systematically
evaluate agentic frameworks, comparing different data retrieval strategies
(direct FHIR API calls vs. specialized tools), interaction patterns
(single-turn vs. multi-turn), and reasoning strategies (natural language vs.
code generation). Our experiments highlight the practical challenges of
retrieving data from intricate FHIR resources and the difficulty of reasoning
over them, both of which critically affect question answering performance. We
publicly release the FHIR-AgentBench dataset and evaluation suite
(https://github.com/glee4810/FHIR-AgentBench) to promote reproducible research
and the development of robust, reliable LLM agents for clinical applications.

</details>


### [79] [Readme_AI: Dynamic Context Construction for Large Language Models](https://arxiv.org/abs/2509.19322)
*Millie Vyas,Timothy Blattner,Alden Dima*

Main category: cs.CL

TL;DR: 通过Readme_AI协议，为大型语言模型提供动态上下文，提升其在专业数据上的响应能力和准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型经过大量数据训练，但在特定查询的语境下仍可能提供不准确或不可靠的信息。

Method: 创建并演示了一种原型Readme_AI模型上下文协议服务器，动态构建上下文并利用元数据进行推理。

Result: Readme_AI显著改善了LLM对NIST开发的Hedgehog库的理解，成功生成了相关代码，并减少了虚假信息。

Conclusion: 提出了一种可扩展的协议，用于动态为大型语言模型提供上下文，以提高响应的准确性和可靠性。

Abstract: Despite being trained on significant amounts of data, Large Language Models
(LLMs) can provide inaccurate or unreliable information in the context of a
user's specific query. Given query-specific context significantly improves the
usefulness of its responses. In this paper, we present a specification that can
be used to dynamically build context for data sources. The data source owner
creates the file containing metadata for LLMs to use when reasoning about
dataset-related queries. To demonstrate our proposed specification, we created
a prototype Readme_AI Model Context Protocol (MCP) server that retrieves the
metadata from the data source and uses it to dynamically build context. Some
features that make this specification dynamic are the extensible types that
represent crawling web-pages, fetching data from data repositories, downloading
and parsing publications, and general text. The context is formatted and
grouped using user-specified tags that provide clear contextual information for
the LLM to reason about the content. We demonstrate the capabilities of this
early prototype by asking the LLM about the NIST-developed Hedgehog library,
for which common LLMs often provides inaccurate and irrelevant responses
containing hallucinations. With Readme_AI, the LLM receives enough context that
it is now able to reason about the library and its use, and even generate code
interpolated from examples that were included in the Readme_AI file provided by
Hedgehog's developer. Our primary contribution is a extensible protocol for
dynamically grounding LLMs in specialized, owner-provided data, enhancing
responses from LLMs and reducing hallucinations. The source code for the
Readme_AI tool is posted here: https://github.com/usnistgov/readme_ai .

</details>


### [80] [Magnitude Matters: a Superior Class of Similarity Metrics for Holistic Semantic Understanding](https://arxiv.org/abs/2509.19323)
*V. S. Raghu Parupudi*

Main category: cs.CL

TL;DR: 本论文提出了新的参数无关的相似度度量，Overlap Similarity (OS) 和 Hyperbolic Tangent Similarity (HTS)，在处理整体语义理解任务时优于传统方法，但对组合语义的挑战仍未解决。


<details>
  <summary>Details</summary>
Motivation: 高维向量比较在自然语言处理中是一个基础任务，但现有的基线（原始点积和余弦相似度）各有不足，因此需要研究新的相似度度量。

Method: 通过对四种先进句子嵌入模型在八个标准NLP基准上的全面评估，使用Wilcoxon符号秩检验来测试统计显著性。

Result: 在需要整体语义理解的任务（如同义句重写和推理）中，OS和HTS在均方误差上与现有方法相比有显著的统计提升，但在涉及组合语义的基准（如SICK和STS-B）上未能实现同样的提升。

Conclusion: 在需要整体语义理解的任务中，Overlap Similarity (OS) 和 Hyperbolic Tangent Similarity (HTS) 在均方误差上显著优于原始点积和余弦相似度，但在高度细致的组合语义基准上并未显示出类似改进。

Abstract: Vector comparison in high dimensions is a fundamental task in NLP, yet it is
dominated by two baselines: the raw dot product, which is unbounded and
sensitive to vector norms, and the cosine similarity, which discards magnitude
information entirely. This paper challenges both standards by proposing and
rigorously evaluating a new class of parameter-free, magnitude-aware similarity
metrics. I introduce two such functions, Overlap Similarity (OS) and Hyperbolic
Tangent Similarity (HTS), designed to integrate vector magnitude and alignment
in a more principled manner. To ensure that my findings are robust and
generalizable, I conducted a comprehensive evaluation using four
state-of-the-art sentence embedding models (all-MiniLM-L6-v2,
all-mpnet-base-v2, paraphrase-mpnet-base-v2, and BAAI/bge-large-en-v1.5) across
a diverse suite of eight standard NLP benchmarks, including STS-B, SICK, Quora,
and PAWS. Using the Wilcoxon signed-rank test for statistical significance, my
results are definitive: on the tasks requiring holistic semantic understanding
(paraphrase and inference), both OS and HTS provide a statistically significant
improvement in Mean Squared Error over both the raw dot product and cosine
similarity, regardless of the underlying embedding model.Crucially, my findings
delineate the specific domain of advantage for these metrics: for tasks
requiring holistic semantic understanding like paraphrase and inference, my
magnitude-aware metrics offer a statistically superior alternative. This
significant improvement was not observed on benchmarks designed to test highly
nuanced compositional semantics (SICK, STS-B), identifying the challenge of
representing compositional text as a distinct and important direction for
future work.

</details>


### [81] [How Much of Your Data Can Suck? Thresholds for Domain Performance and Emergent Misalignment in LLMs](https://arxiv.org/abs/2509.19325)
*Jian Ouyang,Arman T,Ge Jin*

Main category: cs.CL

TL;DR: 研究表明，错误数据会严重影响大型语言模型的性能，强调了高质量数据的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在多个领域变得越来越重要，了解不正确数据对其性能的影响，是确保其安全应用的必要条件。

Method: 评估了gpt-4o模型在四个领域（编码、金融、健康和法律）中使用不同比例（10%到90%）的正确和错误数据进行微调后的表现。

Result: 研究结果表明，即使是少量的不正确数据（10-25%）也会显著降低模型的领域性能，而对道德对齐的影响相对较小。模型需要至少50%的正确数据才能恢复强性能，但仍然难以达到基模型的安全性和稳健性。

Conclusion: 不正确的数据会严重影响大型语言模型（LLMs）的性能与安全性，因此在进行监督微调时确保数据质量至关重要。

Abstract: This paper investigates the impact of incorrect data on the performance and
safety of large language models (LLMs), specifically gpt-4o, during supervised
fine-tuning (SFT). Although LLMs become increasingly vital across broad domains
like finance, coding, law, and health, fine-tuning on incorrect data can lead
to "emergent misalignment," producing harmful or deceptive outputs unrelated to
the intended task. We evaluate gpt-4o models fine-tuned with varying ratios
(10\% to 90\% correct) of both obviously and subtly incorrect data across four
domains: coding, finance, health, and legal. Our findings show that even modest
amounts of incorrect data (10-25\%) dramatically degrade domain performance and
not moral alignment. A clear threshold of at least 50\% correct data is needed
for models to consistently recover strong performance, though they rarely match
the robustness and safety of the base model, which exhibits near-perfect
alignment and zero dangerous completions out-of-the-box. This research
emphasizes that the cost of incorrect data is heavy, highlighting the critical
need for extremely high-quality data curation or, alternatively, leveraging
robust base models without unnecessary fine-tuning for high-stakes
applications.

</details>


### [82] [Unveiling the Merits and Defects of LLMs in Automatic Review Generation for Scientific Papers](https://arxiv.org/abs/2509.19326)
*Ruochi Li,Haoxuan Zhang,Edward Gehringer,Ting Xiao,Junhua Ding,Haihua Chen*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLM）在自动化评论生成中的应用，评估其在同人类写作的审稿相比下的优缺点，发现LLM在捕捉论文贡献上表现优异，但在识别论文弱点及质量敏感性上不足。


<details>
  <summary>Details</summary>
Motivation: 传统的同行评审过程面临压力，因此探索大型语言模型的自动化审稿生成的潜力。

Method: 构建了一个大规模基准，分析了1,683篇论文和6,495条专家评论，使用五种大型语言模型生成评论，并与人类编写的审稿进行比较。

Result: LLM在描述性和肯定性内容上表现良好，但在识别弱点和深入提出问题方面存在不足，尤其是GPT-4o在这些方面的表现较差。

Conclusion: 大型语言模型（LLM）在生成评论时表现良好，但在识别论文弱点和根据论文质量调整反馈方面表现不佳。

Abstract: The surge in scientific submissions has placed increasing strain on the
traditional peer-review process, prompting the exploration of large language
models (LLMs) for automated review generation. While LLMs demonstrate
competence in producing structured and coherent feedback, their capacity for
critical reasoning, contextual grounding, and quality sensitivity remains
limited. To systematically evaluate these aspects, we propose a comprehensive
evaluation framework that integrates semantic similarity analysis and
structured knowledge graph metrics to assess LLM-generated reviews against
human-written counterparts. We construct a large-scale benchmark of 1,683
papers and 6,495 expert reviews from ICLR and NeurIPS in multiple years, and
generate reviews using five LLMs. Our findings show that LLMs perform well in
descriptive and affirmational content, capturing the main contributions and
methodologies of the original work, with GPT-4o highlighted as an illustrative
example, generating 15.74% more entities than human reviewers in the strengths
section of good papers in ICLR 2025. However, they consistently underperform in
identifying weaknesses, raising substantive questions, and adjusting feedback
based on paper quality. GPT-4o produces 59.42% fewer entities than real
reviewers in the weaknesses and increases node count by only 5.7% from good to
weak papers, compared to 50% in human reviews. Similar trends are observed
across all conferences, years, and models, providing empirical foundations for
understanding the merits and defects of LLM-generated reviews and informing the
development of future LLM-assisted reviewing tools. Data, code, and more
detailed results are publicly available at
https://github.com/RichardLRC/Peer-Review.

</details>


### [83] [A systematic review of trial-matching pipelines using large language models](https://arxiv.org/abs/2509.19327)
*Braxton A. Morrison,Madhumita Sushil,Jacob S. Young*

Main category: cs.CL

TL;DR: 手动匹配患者与临床试验效率低下，LLM提供了有效解决方案。本综述分析了31篇相关研究，发现GPT-4模型表现最佳，但面临数据集与成本等挑战。


<details>
  <summary>Details</summary>
Motivation: 实现患者与临床试验选项的匹配对于发现新疗法至关重要，但手动匹配过程既耗时又容易出错，因此引入大型语言模型可能是一种有效的解决方案。

Method: 对2020年至2025年间在三个学术数据库和一个预印本服务器上发表的研究进行了系统回顾，筛选出基于LLM的临床试验配对方法。

Result: 从126篇独特文章中，31篇符合纳入标准，结果表明GPT-4模型在匹配和资格提取方面表现优异，尽管成本较高，同时也指出了在使用实时数据集和部署方面的主要挑战。

Conclusion: 本综述总结了将大型语言模型应用于临床试验配对的研究进展，强调了有前景的方向和主要限制。标准化的评估指标、更加真实的测试集，以及关注成本效益和公平性对于更广泛的部署至关重要。

Abstract: Matching patients to clinical trial options is critical for identifying novel
treatments, especially in oncology. However, manual matching is labor-intensive
and error-prone, leading to recruitment delays. Pipelines incorporating large
language models (LLMs) offer a promising solution. We conducted a systematic
review of studies published between 2020 and 2025 from three academic databases
and one preprint server, identifying LLM-based approaches to clinical trial
matching. Of 126 unique articles, 31 met inclusion criteria. Reviewed studies
focused on matching patient-to-criterion only (n=4), patient-to-trial only
(n=10), trial-to-patient only (n=2), binary eligibility classification only
(n=1) or combined tasks (n=14). Sixteen used synthetic data; fourteen used real
patient data; one used both. Variability in datasets and evaluation metrics
limited cross-study comparability. In studies with direct comparisons, the
GPT-4 model consistently outperformed other models, even finely-tuned ones, in
matching and eligibility extraction, albeit at higher cost. Promising
strategies included zero-shot prompting with proprietary LLMs like the GPT-4o
model, advanced retrieval methods, and fine-tuning smaller, open-source models
for data privacy when incorporation of large models into hospital
infrastructure is infeasible. Key challenges include accessing sufficiently
large real-world data sets, and deployment-associated challenges such as
reducing cost, mitigating risk of hallucinations, data leakage, and bias. This
review synthesizes progress in applying LLMs to clinical trial matching,
highlighting promising directions and key limitations. Standardized metrics,
more realistic test sets, and attention to cost-efficiency and fairness will be
critical for broader deployment.

</details>


### [84] [How Model Size, Temperature, and Prompt Style Affect LLM-Human Assessment Score Alignment](https://arxiv.org/abs/2509.19329)
*Julie Jung,Max Lu,Sina Chole Benker,Dogus Darici*

Main category: cs.CL

TL;DR: 研究模型大小等因素对大型语言模型的对齐影响，发现模型大小在LLM与人类评分对齐中起关键作用。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM的对齐性及其与人类评估的关系。

Method: 分析模型大小、温度和提示风格对大型语言模型在临床推理技能评估中的对齐影响。

Result: 研究发现，模型大小在LLM与人类评分对齐中至关重要。

Conclusion: 模型大小是LLM与人类评分对齐的重要因素，强调了多层次对齐检查的重要性。

Abstract: We examined how model size, temperature, and prompt style affect Large
Language Models' (LLMs) alignment within itself, between models, and with human
in assessing clinical reasoning skills. Model size emerged as a key factor in
LLM-human score alignment. Study highlights the importance of checking
alignments across multiple levels.

</details>


### [85] [Quantifying Compositionality of Classic and State-of-the-Art Embeddings](https://arxiv.org/abs/2509.19332)
*Zhijin Guo,Chenhao Xue,Zhaozhen Xu,Hongbo Bo,Yuxuan Ye,Janet B. Pierrehumbert,Martha Lewis*

Main category: cs.CL

TL;DR: 本文研究了语言模型在理解新表达时的组合性，通过量化评估，发现训练后期和深层次的模型具有较强的组合信号。


<details>
  <summary>Details</summary>
Motivation: 语言模型在理解新表达时需利用组合意义，这对于正确生成语言至关重要。

Method: 通过两个步骤的泛化评估量化加法组合性，使用典型相关分析测量已知实体属性与其嵌入之间的线性关系，并重建未见属性组合的嵌入以评估加法泛化。

Result: 在不同数据模式和层次中跟踪组合性，经过评估发现后期训练和更深层次中的组合信号更强。

Conclusion: 模型在训练的后期阶段和更深的层次表现出更强的组合信号，之后在顶层出现下降。

Abstract: For language models to generalize correctly to novel expressions, it is
critical that they exploit access compositional meanings when this is
justified. Even if we don't know what a "pelp" is, we can use our knowledge of
numbers to understand that "ten pelps" makes more pelps than "two pelps".
Static word embeddings such as Word2vec made strong, indeed excessive, claims
about compositionality. The SOTA generative, transformer models and graph
models, however, go too far in the other direction by providing no real limits
on shifts in meaning due to context. To quantify the additive compositionality,
we formalize a two-step, generalized evaluation that (i) measures the linearity
between known entity attributes and their embeddings via canonical correlation
analysis, and (ii) evaluates additive generalization by reconstructing
embeddings for unseen attribute combinations and checking reconstruction
metrics such as L2 loss, cosine similarity, and retrieval accuracy. These
metrics also capture failure cases where linear composition breaks down.
Sentences, knowledge graphs, and word embeddings are evaluated and tracked the
compositionality across all layers and training stages. Stronger compositional
signals are observed in later training stages across data modalities, and in
deeper layers of the transformer-based model before a decline at the top layer.
Code is available at
https://github.com/Zhijin-Guo1/quantifying-compositionality.

</details>


### [86] [Pluralistic Off-policy Evaluation and Alignment](https://arxiv.org/abs/2509.19333)
*Chengkai Huang,Junda Wu,Zhouhang Xie,Yu Xia,Rui Wang,Tong Yu,Subrata Mitra,Julian McAuley,Lina Yao*

Main category: cs.CL

TL;DR: 提出了POPE框架，用于离线多元化偏好评估与对齐，通过统一奖励函数和逆倾向评分估计器有效提升了响应生成能力。


<details>
  <summary>Details</summary>
Motivation: 针对现有偏好对齐数据集与评估的LLM政策差异，提出一种新的评价及对齐方法，以捕捉多元化偏好。

Method: 提出了Pluralistic Off-Policy Evaluation (POPE)框架，利用统一的奖励函数结合协作效用组件和多样性组件，并推导出可分解的逆倾向评分估计器。

Result: POPE通过理论证明和实证结果，显示出其成功在多元化响应生成与模型能力保留方面的有效性。

Conclusion: POPE有效提升了多元化响应生成能力，同时保持了模型在下游任务中的整体能力。

Abstract: Personalized preference alignment for LLMs with diverse human preferences
requires evaluation and alignment methods that capture pluralism. Most existing
preference alignment datasets are logged under policies that differ
substantially from the evaluated LLMs, and existing off-policy estimators focus
solely on overall utility while ignoring preference pluralism. Extending
Off-Policy Evaluation (OPE) to pluralistic preference alignment, therefore,
remains an open question. Thus, we propose the Pluralistic Off-Policy
Evaluation (POPE), the first framework for offline pluralistic preference
evaluation and alignment in LLMs. POPE includes a unified reward function that
combines (1) a collaborative utility component derived from human preference
signals (e.g., upvotes or relevance scores) and (2) a diversity component
inspired by entropy-based coverage measures, together reflecting pluralistic
alignment. Furthermore, to estimate this reward from logged interactions, we
derive decomposable inverse propensity scoring (IPS) estimators that separately
evaluate relevance and diversity. Theoretically, we prove that our decomposed
IPS estimators establish a lower bound on their variance. With the off-policy
evaluated value function, we can directly enable off-policy optimization to
further enhance pluralistic alignment. Empirical results demonstrate that POPE
efficiently enhances pluralistic response generation and maintains the models'
general capabilities on downstream tasks

</details>


### [87] [PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs](https://arxiv.org/abs/2509.19745)
*Pei Zhang,Andong Chen,Xi Chen,Baosong Yang,Derek F. Wong,Fei Huang*

Main category: cs.CL

TL;DR: 提出了一种名为PART的框架，解决多语言语音表示对齐问题，表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前多语言语音模型存在语音和文本表示对齐困难的问题，尤其是在多语种环境下。

Method: 提出了一种多阶段多任务的PART框架，在交叉语言训练中动态激活LLM参数，并后续引入基于文本的任务以增强多语言理解。

Result: 基于CommonVoice 15、Fleurs、Wenetspeech和CoVoST2的数据集实验表明，PART显著超过了常规方法，能够平衡语言特定和跨语言的一般化能力。

Conclusion: PART框架在多语言语音对齐任务中优于传统方法，显示出其有效性和通用性。

Abstract: Large language models (LLMs) have expanded from text to speech, giving rise
to Speech Large Models (SLMs) that support recognition, translation, and
synthesis. A key challenge is aligning speech and text representations, which
becomes harder in multilingual settings. Existing methods often freeze LLM
parameters and train encoders on multilingual data, but this forces
cross-language convergence and limits performance. We introduce Progressive
Alignment Representation Training (PART), a multi-stage and multi-task
framework that separates within-language from cross-language alignment. During
cross-language training, LLM parameters are dynamically activated, and
text-based tasks are later introduced to enhance multilingual understanding.
Experiments on CommonVoice 15, Fleurs, Wenetspeech, and CoVoST2 show that PART
surpasses conventional approaches, with analysis confirming its ability to
balance language-specific distinctions and cross-language generalization. These
results demonstrate PART's effectiveness and generality for multilingual speech
modality alignment.

</details>


### [88] [Cognitive-Level Adaptive Generation via Capability-Aware Retrieval and Style Adaptation](https://arxiv.org/abs/2509.19336)
*Qingsong Wang,Tao Wu,Wang Lin,Yueying Feng,Gongsheng Yuan,Chang Yao,Jingyuan Chen*

Main category: cs.CL

TL;DR: 本文提出Cognitive-Level Alignment Framework (CLAF)，解决大型语言模型在内容适应性方面的认知错位问题，提升了生成内容的适应性和信息性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在开放式生成任务中的表现强劲，但往往无法根据用户的认知能力调整内容，导致认知错位现象。

Method: 提出Cognitive-Level Alignment Framework (CLAF)，结合能力感知检索模块、风格优化模块和知识可控生成组件。

Result: 实证结果表明，CLAF在多种用户画像中增强了LLM输出的适应性和信息性。

Conclusion: CLAF显著提高了大型语言模型在不同用户认知能力下的适应性和信息性，为实际应用中的认知层次对齐提供了可靠的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated strong performance in
open-ended generation tasks. However, they often struggle to adapt content to
users with differing cognitive capacities, leading to a phenomenon we term
cognitive misalignment. This issue arises in two forms: knowledge-level
misalignment, where content is too complex or too simplistic relative to user
understanding, and presentation-style misalignment, where the structure or tone
hinders effective comprehension. To address these challenges, we propose the
Cognitive-Level Alignment Framework (CLAF), a general-purpose generation
framework that aligns both knowledge complexity and presentation style with
user cognition. CLAF integrates a capability-aware retrieval module based on a
hierarchical knowledge graph and a style optimization module guided by Bloom's
taxonomy and preference learning. Additionally, a knowledge-controllable
generation component ensures consistency and relevance throughout the output.
To support training and evaluation, we construct SCALE, a cognitively annotated
dataset containing responses at multiple comprehension levels per query.
Empirical results show that CLAF enhances the adaptability and informativeness
of LLM outputs across a range of user profiles, offering a robust solution to
cognitive-level alignment in real-world applications.

</details>


### [89] [Part-of-speech tagging for Nagamese Language using CRF](https://arxiv.org/abs/2509.19343)
*Alovi N Shohe,Chonglio Khiamungam,Teisovi Angami*

Main category: cs.CL

TL;DR: 本文首次对Nagamese语言进行词性标注，使用条件随机场（CRF）技术，达到了85.70%的准确率，填补了该语言在NLP领域的研究空白。


<details>
  <summary>Details</summary>
Motivation: 由于Nagamese语言尚无词性标注的研究，这项工作旨在填补这一空白，为该语言的NLP发展提供基础。

Method: 采用条件随机场（CRF）技术进行Nagamese语言的词性标注。

Result: 创建了包含16,112个标记的注释语料库，并通过CRF技术，达成了85.70%的整体标注准确率以及相关的算法指标。

Conclusion: 通过使用条件随机场（CRF）技术，本文在Nagamese语言的词性标注任务中取得了85.70%的整体标注准确率，以及86%的精准率、85%的召回率和85%的F1分数。这表明CRF技术在Nagamese语言的NLP应用中具有良好的效果。

Abstract: This paper investigates part-of-speech tagging, an important task in Natural
Language Processing (NLP) for the Nagamese language. The Nagamese language,
a.k.a. Naga Pidgin, is an Assamese-lexified Creole language developed primarily
as a means of communication in trade between the Nagas and people from Assam in
northeast India. A substantial amount of work in part-of-speech-tagging has
been done for resource-rich languages like English, Hindi, etc. However, no
work has been done in the Nagamese language. To the best of our knowledge, this
is the first attempt at part-of-speech tagging for the Nagamese Language. The
aim of this work is to identify the part-of-speech for a given sentence in the
Nagamese language. An annotated corpus of 16,112 tokens is created and applied
machine learning technique known as Conditional Random Fields (CRF). Using CRF,
an overall tagging accuracy of 85.70%; precision, recall of 86%, and f1-score
of 85% is achieved.
  Keywords. Nagamese, NLP, part-of-speech, machine learning, CRF.

</details>


### [90] [Performance of Large Language Models in Answering Critical Care Medicine Questions](https://arxiv.org/abs/2509.19344)
*Mahmoud Alwakeel,Aditya Nagori,An-Kwok Ian Wong,Neal Chaisson,Vijay Krishnamoorthy,Rishikesan Kamaleswaran*

Main category: cs.CL

TL;DR: 本研究评估了Meta-Llama 3.1模型在重症医学领域的表现，发现70B模型显著优于8B模型，但在不同领域表现差异很大，亟需进一步改进。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在重症医学等专业领域的表现，以填补该领域的研究空白。

Method: 通过对Meta-Llama 3.1模型进行测试，评估其在871个重症医学问题上的表现。

Result: Llama3.1:70B模型在871个问题上的平均准确率为60%，且在各个子领域表现不一，研究和肾脏领域表现最强和最弱分别为68.4%和47.9%。

Conclusion: Llama3.1:70B模型在重症医学问题上的表现优于8B模型，显示了更高的准确率，但各个领域的表现差异明显，尤其在肾脏领域较低，需要进一步研究以提升模型的表现。

Abstract: Large Language Models have been tested on medical student-level questions,
but their performance in specialized fields like Critical Care Medicine (CCM)
is less explored. This study evaluated Meta-Llama 3.1 models (8B and 70B
parameters) on 871 CCM questions. Llama3.1:70B outperformed 8B by 30%, with 60%
average accuracy. Performance varied across domains, highest in Research
(68.4%) and lowest in Renal (47.9%), highlighting the need for broader future
work to improve models across various subspecialty domains.

</details>


### [91] [SCORE: A Semantic Evaluation Framework for Generative Document Parsing](https://arxiv.org/abs/2509.19345)
*Renyu Li,Antonio Jimeno Yepes,Yao You,Kamil Pluciński,Maximilian Operlejn,Crag Wolfe*

Main category: cs.CL

TL;DR: SCORE是一种新的评估框架，旨在解决传统文档解析系统评估中的多样性问题，通过多个维度提供更公平和语义一致的评估。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标无法准确评估多模态生成文档解析系统的多样性，常常将有效的解释错误地分类为错误，导致系统评估扭曲。

Method: SCORE框架集成了调整的编辑距离、标记级诊断、表评估与空间容忍及语义对齐、层次一致性检查等多个维度。

Result: 在1114页的全局基准和领域数据集中，SCORE揭示了标准指标忽略的交叉数据集性能模式，有效恢复了替代有效解释之间的等价性。

Conclusion: SCORE为现代文档解析系统提供了语义基础、公平和实用的基准评估原则，揭示了解释性多样性对评估结果的影响，并提供了多维度、可解释的诊断。

Abstract: Multi-modal generative document parsing systems challenge traditional
evaluation: unlike deterministic OCR or layout models, they often produce
semantically correct yet structurally divergent outputs. Conventional
metrics-CER, WER, IoU, or TEDS-misclassify such diversity as error, penalizing
valid interpretations and obscuring system behavior.
  We introduce SCORE (Structural and COntent Robust Evaluation), an
interpretation-agnostic framework that integrates (i) adjusted edit distance
for robust content fidelity, (ii) token-level diagnostics to distinguish
hallucinations from omissions, (iii) table evaluation with spatial tolerance
and semantic alignment, and (iv) hierarchy-aware consistency checks. Together,
these dimensions enable evaluation that embraces representational diversity
while enforcing semantic rigor.
  Across 1,114 pages spanning a holistic benchmark and a field dataset, SCORE
consistently revealed cross-dataset performance patterns missed by standard
metrics. In 2-5% of pages with ambiguous table structures, traditional metrics
penalized systems by 12-25% on average, leading to distorted rankings. SCORE
corrected these cases, recovering equivalence between alternative but valid
interpretations. Moreover, by normalizing generative outputs into a
format-agnostic representation, SCORE reproduces traditional scores (e.g.,
table F1 up to 0.93) without requiring object-detection pipelines,
demonstrating that generative parsing alone suffices for comprehensive
evaluation.
  By exposing how interpretive diversity impacts evaluation outcomes and
providing multi-dimensional, interpretable diagnostics, SCORE establishes
foundational principles for semantically grounded, fair, and practical
benchmarking of modern document parsing systems.

</details>


### [92] [Benchmarking ChatGPT and DeepSeek in April 2025: A Novel Dual Perspective Sentiment Analysis Using Lexicon-Based and Deep Learning Approaches](https://arxiv.org/abs/2509.19346)
*Maryam Mahdi Alhusseini,Mohammad-Reza Feizi-Derakhshi*

Main category: cs.CL

TL;DR: 本研究提出了一个新颖的双视角方法，结合词典情感分析和深度学习，用于分析用户对大型语言模型应用的评论，发现ChatGPT获得了更多积极情感，且CNN模型表现优越。


<details>
  <summary>Details</summary>
Motivation: 探索用户对基于大型语言模型的应用程序（ChatGPT和DeepSeek）的满意度，解决以往研究中单独使用词典策略或预测深度学习模型的问题。

Method: 结合基于词典的情感分析和深度学习分类模型，包括卷积神经网络（CNN）和双向长短期记忆网络（Bi LSTM），对用户评论进行分析。

Result: 实验结果表明，ChatGPT的积极情感远高于DeepSeek；深度学习分类表现优于词典分析，CNN的准确率达到96.41%。

Conclusion: 该研究为基于大型语言模型的应用程序情感测量设定了新的方法标准，并为开发者和研究者提供了有价值的用户中心人工智能系统设计的见解。

Abstract: This study presents a novel dual-perspective approach to analyzing user
reviews for ChatGPT and DeepSeek on the Google Play Store, integrating
lexicon-based sentiment analysis (TextBlob) with deep learning classification
models, including Convolutional Neural Networks (CNN) and Bidirectional Long
Short Term Memory (Bi LSTM) Networks. Unlike prior research, which focuses on
either lexicon-based strategies or predictive deep learning models in
isolation, this study conducts an extensive investigation into user
satisfaction with Large Language Model (LLM) based applications. A Dataset of
4,000 authentic user reviews was collected, which were carefully preprocessed
and subjected to oversampling to achieve balanced classes. The balanced test
set of 1,700 Reviews were used for model testing. Results from the experiments
reveal that ChatGPT received significantly more positive sentiment than
DeepSeek. Furthermore, deep learning based classification demonstrated superior
performance over lexicon analysis, with CNN outperforming Bi-LSTM by achieving
96.41 percent accuracy and near perfect classification of negative reviews,
alongside high F1-scores for neutral and positive sentiments. This research
sets a new methodological standard for measuring sentiment in LLM-based
applications and provides practical insights for developers and researchers
seeking to improve user-centric AI system design.

</details>


### [93] [Characterizing Knowledge Graph Tasks in LLM Benchmarks Using Cognitive Complexity Frameworks](https://arxiv.org/abs/2509.19347)
*Sara Todorovikj,Lars-Peter Meyer,Michael Martin*

Main category: cs.CL

TL;DR: 引入一种新的任务特征化方法，以提升对LLM在知识图谱任务中的评估，关注价值分布和多样性。


<details>
  <summary>Details</summary>
Motivation: 希望提出一种补充的任务特征化方法，超越仅关注精度和输出正确性的传统评价。

Method: 利用认知心理学中的三种复杂性框架对LLM-KG-Bench框架进行分析。

Result: 揭示价值分布，识别被低估的需求，并促进基准评估任务的多样性。

Conclusion: 提出了一种新的任务特征化方法，以丰富基准评估任务的解释和多样性。

Abstract: Large Language Models (LLMs) are increasingly used for tasks involving
Knowledge Graphs (KGs), whose evaluation typically focuses on accuracy and
output correctness. We propose a complementary task characterization approach
using three complexity frameworks from cognitive psychology. Applying this to
the LLM-KG-Bench framework, we highlight value distributions, identify
underrepresented demands and motivate richer interpretation and diversity for
benchmark evaluation tasks.

</details>


### [94] [ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution](https://arxiv.org/abs/2509.19349)
*Robert Tjarko Lange,Yuki Imajuku,Edoardo Cetin*

Main category: cs.CL

TL;DR: ShinkaEvolve是一个开源框架，通过引入创新方法显著提升了使用LLM进行科学发现的效率和质量，推动了科学研究的民主化。


<details>
  <summary>Details</summary>
Motivation: 解决现有代码进化方法在样本效率低和闭源限制等关键问题，以促进科学发现的进程。

Method: 提出三种关键创新：家长采样技术、代码新颖性拒绝采样和基于赌博机的LLM集成选择策略。

Result: 在多种任务中评估ShinkaEvolve，发现出色的样本效率和解决方案质量，并实现了新的最佳圆形打包解决方案等多项成果。

Conclusion: ShinkaEvolve通过提供开源平台，显著提高了科学发现的样本效率和解决方案质量，具有广泛的适用性。

Abstract: We introduce ShinkaEvolve: a new open-source framework leveraging large
language models (LLMs) to advance scientific discovery with state-of-the-art
performance and unprecedented efficiency. Recent advances in scaling inference
time compute of LLMs have enabled significant progress in generalized
scientific discovery. These approaches rely on evolutionary agentic harnesses
that leverage LLMs as mutation operators to generate candidate solutions.
However, current code evolution methods suffer from critical limitations: they
are sample inefficient, requiring thousands of samples to identify effective
solutions, and remain closed-source, hindering broad adoption and extension.
ShinkaEvolve addresses these limitations, introducing three key innovations: a
parent sampling technique balancing exploration and exploitation, code novelty
rejection-sampling for efficient search space exploration, and a bandit-based
LLM ensemble selection strategy. We evaluate ShinkaEvolve across diverse tasks,
demonstrating consistent improvements in sample efficiency and solution
quality. ShinkaEvolve discovers a new state-of-the-art circle packing solution
using only 150 samples, designs high-performing agentic harnesses for AIME
mathematical reasoning tasks, identifies improvements to ALE-Bench competitive
programming solutions, and discovers novel mixture-of-expert load balancing
loss functions that illuminate the space of optimization strategies. Our
results demonstrate that ShinkaEvolve achieves broad applicability with
exceptional sample efficiency. By providing open-source accessibility and
cost-efficiency, this work democratizes open-ended discovery across diverse
computational problems.

</details>


### [95] [TriSPrompt: A Hierarchical Soft Prompt Model for Multimodal Rumor Detection with Incomplete Modalities](https://arxiv.org/abs/2509.19352)
*Jiajun Chen,Yangyang Wu,Xiaoye Miao,Mengying Zhu,Meng Xi*

Main category: cs.CL

TL;DR: 本论文提出TriSPrompt模型，通过整合多种提示有效提高对不完整多模态数据的谣言检测准确性，解决了模态缺失问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态谣言检测方法主要依赖于完整的多模态训练数据，忽视了真实场景中常见的模态缺失问题。

Method: 构建一个层次化软提示模型TriSPrompt，整合了三种提示：模态感知提示、模态缺失提示和相互视角提示。

Result: 在三个真实场景基准上的广泛实验表明，TriSPrompt在准确性上超过了最先进的方法，提升幅度超过13%。

Conclusion: TriSPrompt模型在处理不完整多模态数据中有效提升谣言检测的准确性，较现有方法提高超过13%。

Abstract: The widespread presence of incomplete modalities in multimodal data poses a
significant challenge to achieving accurate rumor detection. Existing
multimodal rumor detection methods primarily focus on learning joint modality
representations from \emph{complete} multimodal training data, rendering them
ineffective in addressing the common occurrence of \emph{missing modalities} in
real-world scenarios. In this paper, we propose a hierarchical soft prompt
model \textsf{TriSPrompt}, which integrates three types of prompts,
\textit{i.e.}, \emph{modality-aware} (MA) prompt, \emph{modality-missing} (MM)
prompt, and \emph{mutual-views} (MV) prompt, to effectively detect rumors in
incomplete multimodal data. The MA prompt captures both heterogeneous
information from specific modalities and homogeneous features from available
data, aiding in modality recovery. The MM prompt models missing states in
incomplete data, enhancing the model's adaptability to missing information. The
MV prompt learns relationships between subjective (\textit{i.e.}, text and
image) and objective (\textit{i.e.}, comments) perspectives, effectively
detecting rumors. Extensive experiments on three real-world benchmarks
demonstrate that \textsf{TriSPrompt} achieves an accuracy gain of over 13\%
compared to state-of-the-art methods. The codes and datasets are available at
https: //anonymous.4open.science/r/code-3E88.

</details>


### [96] [RoadMind: Towards a Geospatial AI Expert for Disaster Response](https://arxiv.org/abs/2509.19354)
*Ahmed El Fekih Zguir,Ferda Ofli,Muhammad Imran*

Main category: cs.CL

TL;DR: RoadMind 是一个自监督框架，利用 OpenStreetMap 数据提升大型语言模型在地理空间推理中的能力，尤其在灾难响应中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言任务中表现优异，但在地理空间数据推理方面存在局限性，在灾难场景中尤为关键。

Method: 通过使用来自 OpenStreetMap 的结构化数据，自监督地增强 LLM 的地理空间推理能力，并使用 QLoRA 适配器和 4 位量化模型进行预训练和微调。

Result: 在三个易受灾害影响的城市（洛杉矶、基督城和马尼拉）的测试中，通过 RoadMind 训练的模型在道路段识别、最近道路检索和距离/方向估计等任务上显著超越了强基线和最新 LLM。

Conclusion: RoadMind 显著提升了语言模型在处理社会脆弱性和空间推理任务方面的能力，可以为灾难应对提供更有效的支持。

Abstract: Large Language Models (LLMs) have shown impressive performance across a range
of natural language tasks, but remain limited in their ability to reason about
geospatial data, particularly road networks, distances, and directions. This
gap poses challenges in disaster scenarios, where spatial understanding is
critical for tasks such as evacuation planning and resource allocation. In this
work, we present RoadMind, a self-supervised framework that enhances the
geospatial reasoning capabilities of LLMs using structured data from
OpenStreetMap (OSM). Our automated pipeline extracts road infrastructure data
for a given city and converts it into multiple supervision formats tailored to
key spatial tasks. We pretrain and fine-tune LLMs on these representations
using QLoRA adapters and 4-bit quantized models. We evaluate our approach on
three disaster-prone cities with varying global representation, Los Angeles,
Christchurch, and Manila, across tasks such as road segment identification,
nearest road retrieval, and distance/direction estimation. Our results show
that models trained via RoadMind significantly outperform strong baselines,
including state-of-the-art LLMs equipped with advanced prompt engineering. This
demonstrates the potential of structured geospatial data to enhance language
models with robust spatial reasoning, enabling more effective offline AI
systems for disaster response.

</details>


### [97] [Benchmarking and Improving LLM Robustness for Personalized Generation](https://arxiv.org/abs/2509.19358)
*Chimaobi Okite,Naihao Deng,Kiran Bodipati,Huaidian Hou,Joyce Chai,Rada Mihalcea*

Main category: cs.CL

TL;DR: 研究表明，个性化大型语言模型的响应时，事实准确性至关重要，当前模型面临鲁棒性不足的问题，通过Pref-Aligner提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 强调在个性化语言模型响应中，事实准确性是一个重要但常被忽视的维度。

Method: 引入PERG框架和PERGData数据集，评估不同模型在个性化中的表现。

Result: 通过对14种模型的评估，发现即使是最强的模型在个性化时也存在事实错误，且提出的Pref-Aligner方法能够有效改善这些问题。

Conclusion: 当前的大型语言模型在个性化响应中存在严重的可靠性问题，提出的Pref-Aligner方法显著提高了模型的鲁棒性。

Abstract: Recent years have witnessed a growing interest in personalizing the responses
of large language models (LLMs). While existing evaluations primarily focus on
whether a response aligns with a user's preferences, we argue that factuality
is an equally important yet often overlooked dimension. In the context of
personalization, we define a model as robust if its responses are both
factually accurate and align with the user preferences. To assess this, we
introduce PERG, a scalable framework for evaluating robustness in LLMs, along
with a new dataset, PERGData. We evaluate fourteen models from five different
model families using different prompting methods. Our findings show that
current LLMs struggle with robust personalization: even the strongest models
(GPT-4.1, LLaMA3-70B) fail to maintain correctness in 5% of previously
successful cases without personalization, while smaller models (e.g., 7B-scale)
can fail more than 20% of the time. Further analysis reveals that robustness is
significantly affected by the nature of the query and the type of user
preference. To mitigate these failures, we propose Pref-Aligner, a two-stage
approach that improves robustness by an average of 25% across models. Our work
highlights critical gaps in current evaluation practices and introduces tools
and metrics to support more reliable, user-aligned LLM deployments.

</details>


### [98] [Semantic Representation Attack against Aligned Large Language Models](https://arxiv.org/abs/2509.19360)
*Jiawei Lian,Jianhong Pan,Lefan Wang,Yi Wang,Shaohui Mei,Lap-Pui Chau*

Main category: cs.CL

TL;DR: 本研究提出了一种新的语义表示攻击方法，显著提高了对抗性提示的成功率和自然性，解决了现有方法的固有限制。


<details>
  <summary>Details</summary>
Motivation: 当前对抗方案针对确切响应，存在局限性，包括收敛性差、不自然提示和高计算成本。本研究旨在解决这些问题。

Method: 提出了一种新的语义表示攻击和语义表示启发式搜索算法，通过增量扩展维持可解释性来生成语义一致且简洁的对抗性提示。

Result: 该方法在18种大语言模型上的平均攻击成功率达到89.41%，其中11种模型的成功率达到100%。

Conclusion: 我们的语义表示攻击方法在攻击成功率和自然性之间取得了优越平衡，具备了高效且隐秘的攻击能力。

Abstract: Large Language Models (LLMs) increasingly employ alignment techniques to
prevent harmful outputs. Despite these safeguards, attackers can circumvent
them by crafting prompts that induce LLMs to generate harmful content.
  Current methods typically target exact affirmative responses, such as ``Sure,
here is...'', suffering from limited convergence, unnatural prompts, and high
computational costs.
  We introduce Semantic Representation Attack, a novel paradigm that
fundamentally reconceptualizes adversarial objectives against aligned LLMs.
  Rather than targeting exact textual patterns, our approach exploits the
semantic representation space comprising diverse responses with equivalent
harmful meanings.
  This innovation resolves the inherent trade-off between attack efficacy and
prompt naturalness that plagues existing methods.
  The Semantic Representation Heuristic Search algorithm is proposed to
efficiently generate semantically coherent and concise adversarial prompts by
maintaining interpretability during incremental expansion.
  We establish rigorous theoretical guarantees for semantic convergence and
demonstrate that our method achieves unprecedented attack success rates
(89.41\% averaged across 18 LLMs, including 100\% on 11 models) while
maintaining stealthiness and efficiency.
  Comprehensive experimental results confirm the overall superiority of our
Semantic Representation Attack.
  The code will be publicly available.

</details>


### [99] [The Inadequacy of Offline LLM Evaluations: A Need to Account for Personalization in Model Behavior](https://arxiv.org/abs/2509.19364)
*Angelina Wang,Daniel E. Ho,Sanmi Koyejo*

Main category: cs.CL

TL;DR: 离线评估无法反映语言模型在个性化使用中的真实表现，实证研究显示用户互动显著影响模型回答。


<details>
  <summary>Details</summary>
Motivation: 探讨离线评估与实际使用情境下模型表现的差异，特别强调个性化对模型行为的影响。

Method: 通过让800名真实用户使用ChatGPT和Gemini进行 benchmark 问题的现场评估。

Result: 发现离线评估结果与用户真实使用体验存在显著差异。

Conclusion: 个性化在语言模型中的重要性被证明，离线评估无法有效捕捉语言模型的实际行为。

Abstract: Standard offline evaluations for language models -- a series of independent,
state-less inferences made by models -- fail to capture how language models
actually behave in practice, where personalization fundamentally alters model
behavior. For instance, identical benchmark questions to the same language
model can produce markedly different responses when prompted to a state-less
system, in one user's chat session, or in a different user's chat session. In
this work, we provide empirical evidence showcasing this phenomenon by
comparing offline evaluations to field evaluations conducted by having 800 real
users of ChatGPT and Gemini pose benchmark and other provided questions to
their chat interfaces.

</details>


### [100] [LLM-Assisted Topic Reduction for BERTopic on Social Media Data](https://arxiv.org/abs/2509.19365)
*Wannes Janssens,Matthias Bogaert,Dirk Van den Poel*

Main category: cs.CL

TL;DR: 本文提出了一种结合BERTopic和大型语言模型的新框架，通过改进主题生成和减少策略，在社交媒体数据主题建模中显示出更好的性能。


<details>
  <summary>Details</summary>
Motivation: 目前的主题建模方法在处理社交媒体数据时效果不佳，导致主题重叠过多，而基于大型语言模型的方法则面临计算开销大、可扩展性差等问题。

Method: 首先生成初始主题集并构建其表示，然后将这些表示作为输入提供给语言模型，以迭代方式识别和合并语义相似的主题。

Result: 在三个Twitter/X数据集和四种不同语言模型上进行评估后，我们的方法在提高主题多样性和一致性方面优于基线方法，且对数据集特征和初始参数选择有一定敏感性。

Conclusion: 提出的新框架结合了BERTopic生成主题和大型语言模型进行主题减少，能够提高主题多样性和一致性。

Abstract: The BERTopic framework leverages transformer embeddings and hierarchical
clustering to extract latent topics from unstructured text corpora. While
effective, it often struggles with social media data, which tends to be noisy
and sparse, resulting in an excessive number of overlapping topics. Recent work
explored the use of large language models for end-to-end topic modelling.
However, these approaches typically require significant computational overhead,
limiting their scalability in big data contexts. In this work, we propose a
framework that combines BERTopic for topic generation with large language
models for topic reduction. The method first generates an initial set of topics
and constructs a representation for each. These representations are then
provided as input to the language model, which iteratively identifies and
merges semantically similar topics. We evaluate the approach across three
Twitter/X datasets and four different language models. Our method outperforms
the baseline approach in enhancing topic diversity and, in many cases,
coherence, with some sensitivity to dataset characteristics and initial
parameter selection.

</details>


### [101] [Pipeline Parallelism is All You Need for Optimized Early-Exit Based Self-Speculative Decoding](https://arxiv.org/abs/2509.19368)
*Ruanjun Li,Ziheng Liu,Yuanming Shi,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.CL

TL;DR: 本论文提出了一种新的流水线并行自我推测解码方法（PPSD），该方法显著提高了大型语言模型推理的效率，证明了其在自我推测中的应用有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成质量上表现出色，但因每个输出token需通过所有模型层自动回归生成而导致推理成本高。早期退出的自我推测解码方法应运而生，旨在降低推理成本。

Method: 提出了一种流水线并行自我推测解码的方法，其中早期退出计算和剩余层验证计算重叠进行，同时对每个token进行交错的草拟和验证。

Result: PPSD 在多个基准上实现了2.01倍到3.81倍的加速效果，取得了最佳加速效果的接近。

Conclusion: Pipeline-Parallel Self-Speculative Decoding (PPSD) 在自我推测的推理效率上达到了最先进的加速效果，展现了其在固定接受率和退出位置下接近最佳的加速能力。

Abstract: Large language models (LLMs) deliver impressive generation quality, but incur
very high inference cost because each output token is generated
auto-regressively through all model layers. Early-exit based self-speculative
decoding (EESD) has emerged to mitigate this cost. However, in practice, many
approaches struggle to achieve the expected acceleration in such
draft-then-verify paradigm even with a well-aligned early-exit head and
selected exit position. Our analysis reveals that EESD only pays off when the
vast majority of draft tokens are accepted by the LLM. Otherwise, the draft
cost may overcome the acceleration gain and lead to a negative speedup. To
mitigate this, we propose Pipeline-Parallel Self-Speculative Decoding (PPSD)
that fully pipelines the draft and verification work so that no effort is
wasted on failed predictions. It has two key innovations. We configure the
model layers as a pipeline in which early-exit (draft) computations and
remaining-layer (verification) computations overlap. We interleave drafting and
verification per token. While the LLM is verifying the current token in its
final layers, the early-exit path simultaneously drafts the next token. Such a
verify-while-draft scheme keeps all units busy and validates tokens on-the-fly
analogous to pipelining the speculation and verification stages. Empirical
results confirm that PPSD achieves state-of-the-art acceleration in
self-speculative LLM inference. On diverse benchmarks, PPSD achieves speedup
ratios in the range of 2.01x~3.81x, which gains almost the optimal acceleration
at the fixed acceptance rate and exit position, showcasing its advancement in
providing efficient self-speculation.

</details>


### [102] [SLM-Based Agentic AI with P-C-G: Optimized for Korean Tool Use](https://arxiv.org/abs/2509.19369)
*Changhyun Jeon,Jinhee Park,Jungwoo Choi,Keonwoo Kim,Jisu Kim,Minji Hong*

Main category: cs.CL

TL;DR: 本文提出一种针对韩语工具使用的小规模语言模型架构P-C-G，显示出在准确性和效率上的竞争力，且在减少代码切换失败方面具有优势。


<details>
  <summary>Details</summary>
Motivation: 针对韩语环境中的工具使用，通过优化小规模语言模型的架构，减少频繁的韩英代码切换导致的执行失败。

Method: 通过角色分离的Planner-Caller-Generator架构进行方案设计，评估包括单链、多链、缺失参数和缺失功能场景，在统一的I/O接口下，采用LLM-as-a-Judge协议进行五次平均测试。

Result: P-C-G在各种场景下的评估显示出竞争力的工具使用准确率和端到端质量。

Conclusion: P-C-G架构在工具使用准确性和端到端质量上表现优异，同时减少了令牌数量并保持了可接受的延迟，表明角色专门化的SLM是针对韩语工具使用代理的一种具成本效益的替代方案。

Abstract: We propose a small-scale language model (SLM) based agent architecture,
Planner-Caller-Generator (P-C-G), optimized for Korean tool use. P-C-G
separates planning, calling, and generation by role: the Planner produces an
initial batch plan with limited on-demand replanning; the Caller returns a
normalized call object after joint schema-value validation; and the Generator
integrates tool outputs to produce the final answer. We apply a Korean-first
value policy to reduce execution failures caused by frequent Korean-to-English
code switching in Korean settings. Evaluation assumes Korean queries and Korean
tool/parameter specifications; it covers single-chain, multi-chain,
missing-parameters, and missing-functions scenarios, and is conducted via an
LLM-as-a-Judge protocol averaged over five runs under a unified I/O interface.
Results show that P-C-G delivers competitive tool-use accuracy and end-to-end
quality while reducing tokens and maintaining acceptable latency, indicating
that role-specialized SLMs are a cost-effective alternative for Korean tool-use
agents.

</details>


### [103] [Meow: End-to-End Outline Writing for Automatic Academic Survey](https://arxiv.org/abs/2509.19370)
*Zhaoyu Ma,Yuan Shan,Jiahao Zhao,Nan Xu,Lei Wang*

Main category: cs.CL

TL;DR: Meow是一种新开发的以元数据驱动的轮廓写作框架，旨在改善现有自动调查方法的局限性，生成高质量的结构化轮廓。


<details>
  <summary>Details</summary>
Motivation: 随着学术论文发表数量的急剧增长，需要自动化进行深入调查，轮廓写作是自动生成调查的关键。

Method: 将轮廓写作视为端到端任务，从论文元数据生成分层结构的轮廓，并采用两阶段训练方法结合监督微调和强化学习。

Result: Meow展示了高结构保真度和风格一致性的强大性能，能有效生成组织良好的轮廓。

Conclusion: Meow是首个以元数据驱动的轮廓写作框架，能高效生成结构化且忠实的文章轮廓。

Abstract: As academic paper publication numbers grow exponentially, conducting in-depth
surveys with LLMs automatically has become an inevitable trend. Outline
writing, which aims to systematically organize related works, is critical for
automated survey generation. Yet existing automatic survey methods treat
outline writing as mere workflow steps in the overall pipeline. Such
template-based workflows produce outlines that lack in-depth understanding of
the survey topic and fine-grained styles. To address these limitations, we
propose Meow, the first metadata-driven outline writing framework that produces
organized and faithful outlines efficiently. Specifically, we first formulate
outline writing as an end-to-end task that generates hierarchical structured
outlines from paper metadata. We then curate a high-quality dataset of surveys
from arXiv, bioRxiv, and medRxiv, and establish systematic evaluation metrics
for outline quality assessment. Finally, we employ a two-stage training
approach combining supervised fine-tuning and reinforcement learning. Our 8B
reasoning model demonstrates strong performance with high structural fidelity
and stylistic coherence.

</details>


### [104] [How to inject knowledge efficiently? Knowledge Infusion Scaling Law for Pre-training Large Language Models](https://arxiv.org/abs/2509.19371)
*Kangtao Lv,Haibin Chen,Yujin Yuan,Langming Liu,Shilei Liu,Yongwei Wang,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: 研究了大型语言模型知识注入的最佳实践，提出了知识注入的缩放法则，帮助确定最佳领域知识注入量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在特定知识基准中表现不佳，需要在预训练过程中注入领域知识以提高性能，但过度注入会导致知识遗忘。

Method: 通过系统实验观察记忆崩溃现象，确定关键崩溃点和规模相关性，进而提出知识注入缩放法则。

Result: 确认每个模型存在临界崩溃点，且该点与模型大小成正比，提出的缩放法则在不同模型大小和标记预算上都得到了验证。

Conclusion: 提出了一种知识注入缩放法则，可以预测向大型语言模型注入的最佳领域知识量。

Abstract: Large language models (LLMs) have attracted significant attention due to
their impressive general capabilities across diverse downstream tasks. However,
without domain-specific optimization, they often underperform on specialized
knowledge benchmarks and even produce hallucination. Recent studies show that
strategically infusing domain knowledge during pretraining can substantially
improve downstream performance. A critical challenge lies in balancing this
infusion trade-off: injecting too little domain-specific data yields
insufficient specialization, whereas excessive infusion triggers catastrophic
forgetting of previously acquired knowledge. In this work, we focus on the
phenomenon of memory collapse induced by over-infusion. Through systematic
experiments, we make two key observations, i.e. 1) Critical collapse point:
each model exhibits a threshold beyond which its knowledge retention
capabilities sharply degrade. 2) Scale correlation: these collapse points scale
consistently with the model's size. Building on these insights, we propose a
knowledge infusion scaling law that predicts the optimal amount of domain
knowledge to inject into large LLMs by analyzing their smaller counterparts.
Extensive experiments across different model sizes and pertaining token budgets
validate both the effectiveness and generalizability of our scaling law.

</details>


### [105] [A Pipeline to Assess Merging Methods via Behavior and Internals](https://arxiv.org/abs/2509.19476)
*Yutaro Sigris,Andreas Waldis*

Main category: cs.CL

TL;DR: 本文提供了一种新的评估管道，探讨了模型合并方法在行为和内部编码信息上的影响，强调需要更全面的评估以理解其能力和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅从行为的角度研究合并模型，缺乏对其行为和内部结构的全面评估。

Method: 提出了一种新的评估管道，首先合并多个父模型，然后通过下游任务和内部编码的语言能力对合并模型进行评估。

Result: 合并模型的表现通常介于两个父模型之间，但在形态和句法等语言现象的编码信息上可能超过父模型，且行为与内部评估之间排名相关性较弱。

Conclusion: 合并模型的方法对行为和内部信息的影响不同，强调了对模型合并方法的全面评估的必要性，以理解其能力和可靠性。

Abstract: Merging methods combine the weights of multiple language models (LMs) to
leverage their capacities, such as for domain adaptation. While existing
studies investigate merged models from a solely behavioral perspective, we
offer the first comprehensive view by assessing and connecting their behavior
and internals. We present a novel evaluation pipeline that first merges
multiple parent LMs, and then evaluates the merged models in comparison to the
initial ones based on their behavior on downstream tasks, like MMLU, and the
internal encoded linguistic competence. We showcase this pipeline by assessing
the merging of instruction fine-tuned with math- and code-adapted LMs from the
Qwen2.5 family. Our results show that merging methods impacts behavior and
internals differently. While the performance of merged models is typically
between that of the two parent models, their encoded information about
linguistic phenomena, particularly in morphology and syntax, can surpass the
parent models. Moreover, we find weak ranking correlation between this behavior
and internal evaluation. With our pipeline and initial results, we emphasize
the need for more comprehensive evaluations of model merging methods to gain a
faithful understanding of their capabilities and reliability, beyond potential
superficial behavioral advances.

</details>


### [106] [Do LLMs Encode Frame Semantics? Evidence from Frame Identification](https://arxiv.org/abs/2509.19540)
*Jayanth Krishna Chundru,Rudrashis Poddar,Jie Cao,Tianyu Jiang*

Main category: cs.CL

TL;DR: 本文研究大型语言模型在框架语义识别任务中的表现，发现其能够无需明确监督有效地进行框架识别，并通过微调进一步提高性能。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型是否编码了框架语义的潜在知识，以解决框架语义解析中的核心挑战。

Method: 通过基于提示的推理评估模型，并在FrameNet数据集上进行微调以提高准确性。

Result: 模型在框架识别方面表现出色，经过微调后在领域内的准确性显著提高，并且在领域外基准上具有良好的泛化能力。

Conclusion: 大型语言模型能够有效地进行框架识别，体现了其对框架语义的内在理解。

Abstract: We investigate whether large language models encode latent knowledge of frame
semantics, focusing on frame identification, a core challenge in frame semantic
parsing that involves selecting the appropriate semantic frame for a target
word in context. Using the FrameNet lexical resource, we evaluate models under
prompt-based inference and observe that they can perform frame identification
effectively even without explicit supervision. To assess the impact of
task-specific training, we fine-tune the model on FrameNet data, which
substantially improves in-domain accuracy while generalizing well to
out-of-domain benchmarks. Further analysis shows that the models can generate
semantically coherent frame definitions, highlighting the model's internalized
understanding of frame semantics.

</details>


### [107] [Confidence Calibration in Large Language Model-Based Entity Matching](https://arxiv.org/abs/2509.19557)
*Iris Kamsteeg,Juan Cardenas-Cartagena,Floris van Beers,Gineke ten Holt,Tsegaye Misikir Tashu,Matias Valdenegro-Toro*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型在实体匹配中置信度校准的有效性，发现温度缩放可显著降低过度自信现象。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型与实体匹配中置信度校准的交集。

Method: 通过比较基础RoBERTa模型和使用温度缩放、蒙特卡洛丢弃法及集成方法校准的置信度，进行实证研究。

Result: 改进后的RoBERTa在不同数据集上的预期校准误差分数为0.0043至0.0552，温度缩放可降低自信过度现象，最高减少23.83%。

Conclusion: 改进后的RoBERTa模型存在轻微的过度自信现象，温度缩放可显著降低这一现象。

Abstract: This research aims to explore the intersection of Large Language Models and
confidence calibration in Entity Matching. To this end, we perform an empirical
study to compare baseline RoBERTa confidences for an Entity Matching task
against confidences that are calibrated using Temperature Scaling, Monte Carlo
Dropout and Ensembles. We use the Abt-Buy, DBLP-ACM, iTunes-Amazon and Company
datasets. The findings indicate that the proposed modified RoBERTa model
exhibits a slight overconfidence, with Expected Calibration Error scores
ranging from 0.0043 to 0.0552 across datasets. We find that this overconfidence
can be mitigated using Temperature Scaling, reducing Expected Calibration Error
scores by up to 23.83%.

</details>


### [108] [Uncertainty in Semantic Language Modeling with PIXELS](https://arxiv.org/abs/2509.19563)
*Stefania Radu,Marco Zullich,Matias Valdenegro-Toro*

Main category: cs.CL

TL;DR: 本研究分析了像素语言模型中的不确定性，发现模型在重建时低估不确定性，不同书写系统对不确定性的影响差异明显。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型中的词汇瓶颈问题，并探讨不确定性量化的挑战。

Method: 采用蒙特卡罗Dropout、变换器注意力和集成学习等多种方法来分析不确定性和信心。

Result: 在18种语言和7种书写系统的3个语义挑战任务中发现，集成学习在命名实体识别和问答任务中表现更佳，特别是在超参数调优后。

Conclusion: 像素语言模型在重建补丁时低估了不确定性，不同书写系统对不确定性有不同影响，拉丁语言显示出较低的不确定性。

Abstract: Pixel-based language models aim to solve the vocabulary bottleneck problem in
language modeling, but the challenge of uncertainty quantification remains
open. The novelty of this work consists of analysing uncertainty and confidence
in pixel-based language models across 18 languages and 7 scripts, all part of 3
semantically challenging tasks. This is achieved through several methods such
as Monte Carlo Dropout, Transformer Attention, and Ensemble Learning. The
results suggest that pixel-based models underestimate uncertainty when
reconstructing patches. The uncertainty is also influenced by the script, with
Latin languages displaying lower uncertainty. The findings on ensemble learning
show better performance when applying hyperparameter tuning during the named
entity recognition and question-answering tasks across 16 languages.

</details>


### [109] [Retrieval Augmented Generation based context discovery for ASR](https://arxiv.org/abs/2509.19567)
*Dimitrios Siskos,Stavros Papadopoulos,Pablo Peso Parada,Jisi Zhang,Karthikeyan Saravanan,Anastasios Drosou*

Main category: cs.CL

TL;DR: 本研究提出了一种嵌入式检索方法，以提高ASR系统的上下文自动发现能力，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 自动识别合适的上下文以提高出现稀有词汇时的语音识别准确性。

Method: 评估基于嵌入的检索方法，并与基于大型语言模型的两种替代方案进行比较。

Result: 在不同数据集上进行的实验表明，提出的方法相较于无上下文使用降低了多达17%的识别错误率。

Conclusion: 提出的嵌入式检索方法显著提高了ASR系统的转录准确性。

Abstract: This work investigates retrieval augmented generation as an efficient
strategy for automatic context discovery in context-aware Automatic Speech
Recognition (ASR) system, in order to improve transcription accuracy in the
presence of rare or out-of-vocabulary terms. However, identifying the right
context automatically remains an open challenge. This work proposes an
efficient embedding-based retrieval approach for automatic context discovery in
ASR. To contextualize its effectiveness, two alternatives based on large
language models (LLMs) are also evaluated: (1) large language model (LLM)-based
context generation via prompting, and (2) post-recognition transcript
correction using LLMs. Experiments on the TED-LIUMv3, Earnings21 and SPGISpeech
demonstrate that the proposed approach reduces WER by up to 17% (percentage
difference) relative to using no-context, while the oracle context results in a
reduction of up to 24.1%.

</details>


### [110] [ExPe: Exact Positional Encodings for Generative Transformer Models with Extrapolating Capabilities](https://arxiv.org/abs/2509.19569)
*Aleksis Datseris,Sylvia Vassileva,Ivan Koychev,Svetla Boytcheva*

Main category: cs.CL

TL;DR: 本文提出了一种新的位置嵌入方法（ExPE），提高了 Transformer 对长序列的处理能力，与传统方法相比具有更好的泛化能力和性能。


<details>
  <summary>Details</summary>
Motivation: 传统的 Transformer 模型在处理训练时未见过的较长序列时效果不佳，因此需要一种新方法来提高位置嵌入的推理能力。

Method: 提出了一种新的绝对位置嵌入方法，称为“精确位置嵌入”（ExPE），通过覆盖嵌入向量的特定维度来编码精确的位置。

Result: ExPE 嵌入在处理比训练时更长序列的因果语言建模中，相较于旋转和正弦嵌入显著减少了困惑度。

Conclusion: ExPE 方法在处理比训练长度更长的序列时能有效提高模型的泛化能力，并显著降低困惑度。

Abstract: This paper introduces a novel approach to position embeddings in transformer
models, named "Exact Positional Embeddings" (ExPE). An absolute positional
embedding method that can extrapolate to sequences of lengths longer than the
ones it was trained on. Traditional transformer models rely on absolute or
relative position embeddings to incorporate positional information into token
embeddings, which often struggle with extrapolation to sequences longer than
those seen during training. Our proposed method utilizes a novel embedding
strategy that encodes exact positional information by overriding specific
dimensions of the embedding vectors, thereby enabling a more precise
representation of token positions. The proposed approach not only maintains the
integrity of the original embeddings but also enhances the model's ability to
generalize to more extended sequences. In causal language modeling, our ExPE
embeddings significantly reduce perplexity compared to rotary and sinusoidal
embeddings, when tested on sequences longer than those used in training.

</details>


### [111] [LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines](https://arxiv.org/abs/2509.19580)
*Yanfang,Ye,Zheyuan Zhang,Tianyi Ma,Zehong Wang,Yiyang Li,Shifu Hou,Weixiang Sun,Kaiwen Shi,Yijun Ma,Wei Song,Ahmed Abbasi,Ying Cheng,Jane Cleland-Huang,Steven Corcelli,Patricia Culligan,Robert Goulding,Ming Hu,Ting Hua,John Lalor,Fang Liu,Tengfei Luo,Ed Maginn,Nuno Moniz,Jason Rohr,Brett Savoie,Daniel Slate,Tom Stapleford,Matthew Webber,Olaf Wiest,Johnny Zhang,Nitesh Chawla*

Main category: cs.CL

TL;DR: 本论文综述大型语言模型在各学科的应用，探讨其对研究的影响及未来挑战。


<details>
  <summary>Details</summary>
Motivation: 受到LLMs在语言任务上表现的鼓舞，特别是在生成类似人类的对话和广泛应用的潜力。

Method: 概述当前最先进的LLMs及其在艺术、人文学科、经济学、科学与工程等领域的整合。

Result: LLMs在多个学科的应用展示了其对研究和实践的深远影响，同时提出了目前存在的挑战和未来研究方向。

Conclusion: 本研究探讨大型语言模型（LLMs）在多个学科中的应用及其对研究和实践的影响，并讨论了关键限制和未来方向。

Abstract: Cutting-edge Artificial Intelligence (AI) techniques keep reshaping our view
of the world. For example, Large Language Models (LLMs) based applications such
as ChatGPT have shown the capability of generating human-like conversation on
extensive topics. Due to the impressive performance on a variety of
language-related tasks (e.g., open-domain question answering, translation, and
document summarization), one can envision the far-reaching impacts that can be
brought by the LLMs with broader real-world applications (e.g., customer
service, education and accessibility, and scientific discovery). Inspired by
their success, this paper will offer an overview of state-of-the-art LLMs and
their integration into a wide range of academic disciplines, including: (1)
arts, letters, and law (e.g., history, philosophy, political science, arts and
architecture, law), (2) economics and business (e.g., finance, economics,
accounting, marketing), and (3) science and engineering (e.g., mathematics,
physics and mechanical engineering, chemistry and chemical engineering, life
sciences and bioengineering, earth sciences and civil engineering, computer
science and electrical engineering). Integrating humanity and technology, in
this paper, we will explore how LLMs are shaping research and practice in these
fields, while also discussing key limitations, open challenges, and future
directions in the era of generative AI. The review of how LLMs are engaged
across disciplines-along with key observations and insights-can help
researchers and practitioners interested in exploiting LLMs to advance their
works in diverse real-world applications.

</details>


### [112] [GuessingGame: Measuring the Informativeness of Open-Ended Questions in Large Language Models](https://arxiv.org/abs/2509.19593)
*Dylan Hutson,Daniel Vennemeyer,Aneesh Deshmukh,Justin Zhan,Tianyu Jiang*

Main category: cs.CL

TL;DR: 本研究提出GuessingGame协议，借助信息增益评估LLM的提问能力，发现提问质量可测，可改善，且全面提升互动推理效果。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在无预设选项情况下作为战略提问者的能力。

Method: 提出了两种信息增益（IG）度量方法来评估随机提问的质量，分别为贝叶斯方法和基于熵的方法，以及通过861场游戏的数据分析。

Result: 信息增益的提高显著减少预期游戏时长，且引导提问的约束措施可显著提升表现。

Conclusion: 问答能力在大型语言模型中是可测量和可提升的，对互动推理至关重要。

Abstract: We introduce GuessingGame, a protocol for evaluating large language models
(LLMs) as strategic question-askers in open-ended, open-domain settings. A
Guesser LLM identifies a hidden object by posing free-form questions to an
Oracle without predefined choices or candidate lists. To measure question
quality, we propose two information gain (IG) metrics: a Bayesian method that
tracks belief updates over semantic concepts using LLM-scored relevance, and an
entropy-based method that filters candidates via ConceptNet. Both metrics are
model-agnostic and support post hoc analysis. Across 858 games with multiple
models and prompting strategies, higher IG strongly predicts efficiency: a
one-standard-deviation IG increase reduces expected game length by 43\%.
Prompting constraints guided by IG, such as enforcing question diversity,
enable weaker models to significantly improve performance. These results show
that question-asking in LLMs is both measurable and improvable, and crucial for
interactive reasoning.

</details>


### [113] [Anatomy of a Feeling: Narrating Embodied Emotions via Large Vision-Language Models](https://arxiv.org/abs/2509.19595)
*Mohammad Saim,Phan Anh Duong,Cat Luong,Aniket Bhanderi,Tianyu Jiang*

Main category: cs.CL

TL;DR: 该研究提出ELENA框架，利用视觉-语言模型分析情感反应，尤其在面具图像中表现优异。


<details>
  <summary>Details</summary>
Motivation: 探讨身体部位的情感反应，挖掘情感经历中的丰富信息。

Method: 利用大规模视觉-语言模型生成情感叙述，并结合注意力图进行分析。

Result: ELENA能够有效识别面具图像中的情感，比没有微调的基线模型表现更佳。

Conclusion: ELENA框架为情感分析提供了一种新方法，尤其在面部被遮盖的情况下，展示了良好的效果。

Abstract: The embodiment of emotional reactions from body parts contains rich
information about our affective experiences. We propose a framework that
utilizes state-of-the-art large vision-language models (LVLMs) to generate
Embodied LVLM Emotion Narratives (ELENA). These are well-defined, multi-layered
text outputs, primarily comprising descriptions that focus on the salient body
parts involved in emotional reactions. We also employ attention maps and
observe that contemporary models exhibit a persistent bias towards the facial
region. Despite this limitation, we observe that our employed framework can
effectively recognize embodied emotions in face-masked images, outperforming
baselines without any fine-tuning. ELENA opens a new trajectory for embodied
emotion analysis across the modality of vision and enriches modeling in an
affect-aware setting.

</details>


### [114] [Evaluating Language Translation Models by Playing Telephone](https://arxiv.org/abs/2509.19611)
*Syeda Jannatus Saba,Steven Skiena*

Main category: cs.CL

TL;DR: 提出无监督方法生成翻译评估训练数据，通过模型轮换提高评估性能，优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型在机器翻译质量评估方面效率不足，限制了模型在更具挑战性任务（如长文本和文学翻译）中的进一步提升。

Method: 使用模型轮换和语言翻译的方法机械生成文本，以训练评估系统。

Result: 评估系统在两个任务上表现优于流行的翻译评估系统（xCOMET）。

Conclusion: 提出了一种无监督的方法，通过反复的语言翻译生成评估数据，显著提高了翻译质量评估系统的性能。

Abstract: Our ability to efficiently and accurately evaluate the quality of machine
translation systems has been outrun by the effectiveness of current language
models--which limits the potential for further improving these models on more
challenging tasks like long-form and literary translation. We propose an
unsupervised method to generate training data for translation evaluation over
different document lengths and application domains by repeated rounds of
translation between source and target languages. We evaluate evaluation systems
trained on texts mechanically generated using both model rotation and language
translation approaches, demonstrating improved performance over a popular
translation evaluation system (xCOMET) on two different tasks: (i) scoring the
quality of a given translation against a human reference and (ii) selecting
which of two translations is generationally closer to an original source
document.

</details>


### [115] [AutoSpec: An Agentic Framework for Automatically Drafting Patent Specification](https://arxiv.org/abs/2509.19640)
*Ryan Shea,Zhou Yu*

Main category: cs.CL

TL;DR: 本文提出了一种名为 AutoSpec 的安全框架，用于自动撰写专利文档，能有效克服现有语言模型在专利撰写中的限制。


<details>
  <summary>Details</summary>
Motivation: 专利撰写过程昂贵且耗时，亟需自动化，同时现有语言模型在处理专利申请时面临高度机密性和专业知识等挑战。

Method: 引入 AutoSpec 框架，将专利撰写过程分解为多个可管理的子任务，使用定制工具增强小型开源语言模型来完成这些子任务。

Result: 通过与专利律师的合作设计新评估协议，自动和专家评估表明 AutoSpec 的表现优于现有方法。

Conclusion: AutoSpec 系统在专利撰写任务中表现优于现有基准，通过将撰写过程分解为可管理的子任务，使其能够有效使用开源语言模型。

Abstract: Patents play a critical role in driving technological innovation by granting
inventors exclusive rights to their inventions. However the process of drafting
a patent application is often expensive and time-consuming, making it a prime
candidate for automation. Despite recent advancements in language models,
several challenges hinder the development of robust automated patent drafting
systems. First, the information within a patent application is highly
confidential, which often prevents the use of closed-source LLMs for automating
this task. Second, the process of drafting a patent application is difficult
for even the most advanced language models due to their long context, technical
writing style, and specialized domain knowledge. To address these challenges,
we introduce AutoSpec, a secure, agentic framework for Automatically drafting
patent Specification. Our approach decomposes the drafting process into a
sequence of manageable subtasks, each solvable by smaller, open-source language
models enhanced with custom tools tailored for drafting patent specification.
To assess our system, we design a novel evaluation protocol in collaboration
with experienced patent attorneys. Our automatic and expert evaluations show
that AutoSpec outperforms existing baselines on a patent drafting task.

</details>


### [116] [Large Language Models for Pedestrian Safety: An Application to Predicting Driver Yielding Behavior at Unsignalized Intersections](https://arxiv.org/abs/2509.19657)
*Yicheng Yang,Zixian Li,Jean Paul Bizimana,Niaz Zafri,Yongfeng Dong,Tianyi Li*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型在行人-司机交互建模中的应用，显示其在行人安全系统中的潜力及实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 提升城市交通中行人安全的理解和建模，特别是在交叉口行人决策与司机让行行为的复杂交互中。

Method: 通过设计新颖的提示并结合领域特定知识、结构化推理和少量样本提示，利用多模态大型语言模型进行行人-司机交互的推理。

Result: 与传统分类器相比，GPT-4o在准确率和召回率方面表现最佳，而Deepseek-V3在精确度上表现突出，展示了模型性能与计算效率之间的权衡。

Conclusion: 本文的研究结果证明了大型语言模型在行人-司机交互建模中的有效性，尤其是在准确性和召回率方面表现优异。

Abstract: Pedestrian safety is a critical component of urban mobility and is strongly
influenced by the interactions between pedestrian decision-making and driver
yielding behavior at crosswalks. Modeling driver--pedestrian interactions at
intersections requires accurately capturing the complexity of these behaviors.
Traditional machine learning models often struggle to capture the nuanced and
context-dependent reasoning required for these multifactorial interactions, due
to their reliance on fixed feature representations and limited
interpretability. In contrast, large language models (LLMs) are suited for
extracting patterns from heterogeneous traffic data, enabling accurate modeling
of driver-pedestrian interactions. Therefore, this paper leverages multimodal
LLMs through a novel prompt design that incorporates domain-specific knowledge,
structured reasoning, and few-shot prompting, enabling interpretable and
context-aware inference of driver yielding behavior, as an example application
of modeling pedestrian--driver interaction. We benchmarked state-of-the-art
LLMs against traditional classifiers, finding that GPT-4o consistently achieves
the highest accuracy and recall, while Deepseek-V3 excels in precision. These
findings highlight the critical trade-offs between model performance and
computational efficiency, offering practical guidance for deploying LLMs in
real-world pedestrian safety systems.

</details>


### [117] [DyBBT: Dynamic Balance via Bandit inspired Targeting for Dialog Policy with Cognitive Dual-Systems](https://arxiv.org/abs/2509.19695)
*Shuyu Zhang,Yifan Wei,Jialuo Yuan,Xinru Wang,Yanmin Zhu,Bin Li*

Main category: cs.CL

TL;DR: DyBBT是一个新的对话策略学习框架，实现了高效探索和最佳性能，适应动态对话背景。


<details>
  <summary>Details</summary>
Motivation: 现有的对话系统在动态对话环境中效率低下，导致操作不佳，亟需更灵活的探索策略。

Method: DyBBT使用了一种新的对话策略学习框架，结合了认知状态空间和带有激励的元控制器。

Result: DyBBT在多个领域基准上的实验结果表明其在成功率、效率和泛化能力方面达到最先进水平，并通过人类评估验证了其决策与专家一致。

Conclusion: DyBBT在对话系统中实现了最佳性能，展现了成功率、效率和泛化能力，并且与专家评判一致。

Abstract: Task oriented dialog systems often rely on static exploration strategies that
do not adapt to dynamic dialog contexts, leading to inefficient exploration and
suboptimal performance. We propose DyBBT, a novel dialog policy learning
framework that formalizes the exploration challenge through a structured
cognitive state space capturing dialog progression, user uncertainty, and slot
dependency. DyBBT proposes a bandit inspired meta-controller that dynamically
switches between a fast intuitive inference (System 1) and a slow deliberative
reasoner (System 2) based on real-time cognitive states and visitation counts.
Extensive experiments on single- and multi-domain benchmarks show that DyBBT
achieves state-of-the-art performance in success rate, efficiency, and
generalization, with human evaluations confirming its decisions are well
aligned with expert judgment. Code is available at
https://github.com/carsonz/DyBBT.

</details>


### [118] [Personality Vector: Modulating Personality of Large Language Models by Model Merging](https://arxiv.org/abs/2509.19727)
*Seungjong Sun,Seo Yeon Baek,Jang Hyun Kim*

Main category: cs.CL

TL;DR: 本研究提出了一种通过模型合并实现个性调节的方法，使大语言模型能够无额外训练地展现所需个性特征，且个性向量在不同模型间具有良好的迁移性。


<details>
  <summary>Details</summary>
Motivation: 随着对个性化AI系统需求的增长，本研究旨在解决现有方法在捕捉人类个性特征的连续性和多维性方面的不足。

Method: 通过减去预训练模型的权重得到个性向量，并通过合并个性向量实现大语言模型在特定个性特征上的表现。

Result: 通过广泛实验，验证了所提出的个性向量在特质强度上实现了连续控制，并支持多种个性特质的组合。

Conclusion: 本研究提出的个性调节方法通过模型合并实现了对大语言模型个性特征的有效控制，并且能够在不同下游模型间进行转移，展现出良好的通用性。

Abstract: Driven by the demand for personalized AI systems, there is growing interest
in aligning the behavior of large language models (LLMs) with human traits such
as personality. Previous attempts to induce personality in LLMs have shown
promising results, but they struggle to capture the continuous and
multidimensional nature of human traits. In this work, we propose a novel
method for personality modulation in LLMs via model merging. Specifically, we
construct personality vectors by subtracting the weights of a pre-trained model
from those of the fine-tuned model on a given personality trait. By merging
personality vectors, we enable LLMs to exhibit desired personality traits
without additional training. Extensive experiments show that personality
vectors enable continuous control over trait intensity and support the
composition of multiple traits. Furthermore, personality vectors transfer
across diverse downstream models, suggesting that they encode generalizable
representations of personality. Our code is available at here.

</details>


### [119] [HiCoLoRA: Addressing Context-Prompt Misalignment via Hierarchical Collaborative LoRA for Zero-Shot DST](https://arxiv.org/abs/2509.19742)
*Shuyu Zhang,Yifan Wei,Xinru Wang,Yanmin Zhu,Yangfan He,Yixuan Weng,Bin Li*

Main category: cs.CL

TL;DR: 该论文提出的HiCoLoRA框架通过增强提示对齐，提高了零样本对话状态跟踪的能力，克服了语义不对齐的挑战，取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 旨在解决动态对话上下文与静态提示之间的语义不对齐问题，提高任务导向对话系统在新领域的泛化能力。

Method: 提出了一种层次化协作低秩适配框架（HiCoLoRA），结合层次化LoRA架构、谱联合领域-槽聚类和语义增强SVD初始化等方法。

Result: 在多领域数据集MultiWOZ和SGD上的实验结果表明，HiCoLoRA的表现超越了基准模型。

Conclusion: HiCoLoRA在零样本对话状态跟踪（zs-DST）任务中表现优异，达到了最先进的水平。

Abstract: Zero-shot Dialog State Tracking (zs-DST) is essential for enabling
Task-Oriented Dialog Systems (TODs) to generalize to new domains without costly
data annotation. A central challenge lies in the semantic misalignment between
dynamic dialog contexts and static prompts, leading to inflexible cross-layer
coordination, domain interference, and catastrophic forgetting. To tackle this,
we propose Hierarchical Collaborative Low-Rank Adaptation (HiCoLoRA), a
framework that enhances zero-shot slot inference through robust prompt
alignment. It features a hierarchical LoRA architecture for dynamic
layer-specific processing (combining lower-layer heuristic grouping and
higher-layer full interaction), integrates Spectral Joint Domain-Slot
Clustering to identify transferable associations (feeding an Adaptive Linear
Fusion Mechanism), and employs Semantic-Enhanced SVD Initialization
(SemSVD-Init) to preserve pre-trained knowledge. Experiments on multi-domain
datasets MultiWOZ and SGD show that HiCoLoRA outperforms baselines, achieving
SOTA in zs-DST. Code is available at https://github.com/carsonz/HiCoLoRA.

</details>


### [120] [CHURRO: Making History Readable with an Open-Weight Large Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition](https://arxiv.org/abs/2509.19768)
*Sina J. Semnani,Han Zhang,Xinyan He,Merve Tekgürler,Monica S. Lam*

Main category: cs.CL

TL;DR: CHURRO是一个专为历史文本识别设计的开源视觉语言模型，经过大规模数据训练，展现出优越的识别性能。


<details>
  <summary>Details</summary>
Motivation: 提高历史文件的文本识别准确性，以推动文化遗产的研究和保护。

Method: 开发了CHURRO，一个专为历史文本识别而设计的3B参数开源视觉语言模型，并在包含155个历史语料库的CHURRO-DS数据集上进行训练。

Result: CHURRO在CHURRO-DS测试集中，打印文本和手写文本的规范Levenshtein相似度分别达到82.3%和70.1%，显著优于其他模型，并且成本效益更高。

Conclusion: CHURRO在历史文本识别方面表现优异，超越了其他现有模型，促进了历史材料的阅读与研究。

Abstract: Accurate text recognition for historical documents can greatly advance the
study and preservation of cultural heritage. Existing vision-language models
(VLMs), however, are designed for modern, standardized texts and are not
equipped to read the diverse languages and scripts, irregular layouts, and
frequent degradation found in historical materials.
  This paper presents CHURRO, a 3B-parameter open-weight VLM specialized for
historical text recognition. The model is trained on CHURRO-DS, the largest
historical text recognition dataset to date. CHURRO-DS unifies 155 historical
corpora comprising 99,491 pages, spanning 22 centuries of textual heritage
across 46 language clusters, including historical variants and dead languages.
  We evaluate several open-weight and closed VLMs and optical character
recognition (OCR) systems on CHURRO-DS and find that CHURRO outperforms all
other VLMs. On the CHURRO-DS test set, CHURRO achieves 82.3% (printed) and
70.1% (handwritten) normalized Levenshtein similarity, surpassing the
second-best model, Gemini 2.5 Pro, by 1.4% and 6.5%, respectively, while being
15.5 times more cost-effective.
  By releasing the model and dataset, we aim to enable community-driven
research to improve the readability of historical texts and accelerate
scholarship.

</details>


### [121] [EnAnchored-X2X: English-Anchored Optimization for Many-to-Many Translation](https://arxiv.org/abs/2509.19770)
*Sen Yang,Yu Bao,Yu Lu,Jiajun Chen,Shujian Huang,Shanbo Cheng*

Main category: cs.CL

TL;DR: 本研究提出了一种合成数据生成框架，增强大语言模型在非英语翻译上的表现。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在非英语翻译中的不足，并试图通过生成合成数据来解决这个问题。

Method: 通过生成合成数据并扩展英语平行语料库，同时开发英语参考的质量评价代理，实现高质量的x2x训练数据的有效收集。

Result: 在72个x2x翻译方向上，结合基于偏好的优化，显著提升了流行大语言模型的翻译性能，同时也增强了en2x的表现。

Conclusion: 通过合理利用基于英语的强项，可以提升大语言模型的多语种翻译能力。

Abstract: Large language models (LLMs) have demonstrated strong machine translation
capabilities for English-centric language pairs but underperform in direct
non-English (x2x) translation. This work addresses this limitation through a
synthetic data generation framework that leverages models' established
English-to-x (en2x) capabilities. By extending English parallel corpora into
omnidirectional datasets and developing an English-referenced quality
evaluation proxy, we enable effective collection of high-quality x2x training
data. Combined with preference-based optimization, our method achieves
significant improvement across 72 x2x directions for widely used LLMs, while
generalizing to enhance en2x performance. The results demonstrate that
strategic exploitation of English-centric strengths can bootstrap comprehensive
multilingual translation capabilities in LLMs. We release codes, datasets, and
model checkpoints at https://github.com/NJUNLP/EAX

</details>


### [122] [bi-GRPO: Bidirectional Optimization for Jailbreak Backdoor Injection on LLMs](https://arxiv.org/abs/2509.19775)
*Wence Ji,Jiancan Wu,Aiying Li,Shuyi Zhang,Junkang Wu,An Zhang,Xiang Wang,Xiangnan He*

Main category: cs.CL

TL;DR: 本研究提出了 bi-GRPO，一种新颖的强化学习框架，专注于提高大型语言模型在面临 jailbreak backdoor 攻击时的鲁棒性和有效性，实验结果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的发展，针对恶意攻击的鲁棒性变得极为重要，现有方法各有局限。

Method: bi-GRPO 是一种基于强化学习的框架，采用配对回合和配对奖励来优化模型以生成有害内容，同时保持安全性。

Result: bi-GRPO 实现了超过99%的攻击成功率，保持隐蔽性，并生成高可用性和连贯的 jailbreak 响应。

Conclusion: bi-GRPO 在针对 jailbreak backdoor 攻击方面显著提高了有效性和隐蔽性，代表了该领域的重要进展。

Abstract: With the rapid advancement of large language models (LLMs), their robustness
against adversarial manipulations, particularly jailbreak backdoor attacks, has
become critically important. Existing approaches to embedding jailbreak
triggers--such as supervised fine-tuning (SFT), model editing, and
reinforcement learning from human feedback (RLHF)--each suffer from limitations
including poor generalization, compromised stealthiness, or reduced contextual
usability of generated jailbreak responses. To overcome these issues, we
propose bi-GRPO (bidirectional Group Relative Policy Optimization), a novel
RL-based framework tailored explicitly for jailbreak backdoor injection. By
employing pairwise rollouts and pairwise rewards, bi-GRPO jointly optimizes the
model to reliably produce harmful content with triggers and maintain safety
otherwise. Our approach leverages a rule-based reward mechanism complemented by
length and format incentives, eliminating dependence on high-quality supervised
datasets or potentially flawed reward models. Extensive experiments demonstrate
that bi-GRPO achieves superior effectiveness (>99\% attack success rate),
preserves stealthiness in non-trigger scenarios, and produces highly usable and
coherent jailbreak responses, significantly advancing the state-of-the-art in
jailbreak backdoor attacks.

</details>


### [123] [Polarity Detection of Sustainable Detection Goals in News Text](https://arxiv.org/abs/2509.19833)
*Andrea Cadeddua,Alessandro Chessa,Vincenzo De Leo,Gianni Fenu,Francesco Osborne,Diego Reforgiato Recupero,Angelo Salatino,Luca Secchi*

Main category: cs.CL

TL;DR: 本研究提出SDG极性检测任务及相应的数据集，并评估了多种LLM的性能，结果显示通过数据增强可显著提高准确性，为可持续发展领域提供了新的方法论支持。


<details>
  <summary>Details</summary>
Motivation: 随着NLP和大语言模型的发展，自动化分类与可持续发展目标相关的文本数据变得更加重要，但目前缺乏对其方向性（积极、中性或消极）的评估。

Method: 通过创建SDG-POD基准数据集，利用六种最先进的LLM进行全面评估，包括零-shot和微调配置。

Result: 研究表明，尽管对当前的LLM来说，SDG极性检测任务仍然具有挑战性，但某些微调模型（特别是QWQ-32B）在特定SDG上表现良好，且通过合成数据增强微调数据集能够提升模型性能。

Conclusion: 本研究提出了一种新任务——SDG极性检测，并创建了SDG-POD数据集。尽管当前的LLM在这个任务上仍面临挑战，但通过合适的微调和数据增强技术，部分模型展现了良好的性能，这为可持续发展目标的监测提供了新的思路和方法。

Abstract: The United Nations' Sustainable Development Goals (SDGs) provide a globally
recognised framework for addressing critical societal, environmental, and
economic challenges. Recent developments in natural language processing (NLP)
and large language models (LLMs) have facilitated the automatic classification
of textual data according to their relevance to specific SDGs. Nevertheless, in
many applications, it is equally important to determine the directionality of
this relevance; that is, to assess whether the described impact is positive,
neutral, or negative. To tackle this challenge, we propose the novel task of
SDG polarity detection, which assesses whether a text segment indicates
progress toward a specific SDG or conveys an intention to achieve such
progress. To support research in this area, we introduce SDG-POD, a benchmark
dataset designed specifically for this task, combining original and
synthetically generated data. We perform a comprehensive evaluation using six
state-of-the-art large LLMs, considering both zero-shot and fine-tuned
configurations. Our results suggest that the task remains challenging for the
current generation of LLMs. Nevertheless, some fine-tuned models, particularly
QWQ-32B, achieve good performance, especially on specific Sustainable
Development Goals such as SDG-9 (Industry, Innovation and Infrastructure),
SDG-12 (Responsible Consumption and Production), and SDG-15 (Life on Land).
Furthermore, we demonstrate that augmenting the fine-tuning dataset with
synthetically generated examples yields improved model performance on this
task. This result highlights the effectiveness of data enrichment techniques in
addressing the challenges of this resource-constrained domain. This work
advances the methodological toolkit for sustainability monitoring and provides
actionable insights into the development of efficient, high-performing polarity
detection systems.

</details>


### [124] [TianHui: A Domain-Specific Large Language Model for Diverse Traditional Chinese Medicine Scenarios](https://arxiv.org/abs/2509.19834)
*Ji Yin,Menglan He,Yujie Zhang,Linshuai Zhang,Tingting Ma,Ce Tian,Jie Wu,Lin Xu,Tao Jiang*

Main category: cs.CL

TL;DR: TianHui是一个专门针对中医的高性能语言模型，通过整合上下文数据和领域知识，解决了中医LLM在研究中的适应性和资源限制问题。


<details>
  <summary>Details</summary>
Motivation: 针对中医特定LLM在研究中适应性较差、评估数据集不足以及计算资源有限的问题，亟需一种增强型的解决方案。

Method: 构建了一个大型的中医语料库，采用了QLoRA、DeepSpeed Stage 2和Flash Attention 2的两阶段训练策略。

Result: TianHui在12个基准测试中表现出色，六个数据集的所有指标中均排名前三，其余六个数据集也取得了最佳结果。

Conclusion: TianHui是一种高性能的中医领域特定LLM，能够系统地保存和扩展中医知识，并在多个基准测试中表现优异。

Abstract: Domain-specific LLMs in TCM face limitations in research settings due to
constrained adaptability, insufficient evaluation datasets, and limited
computational resources. This study presents TianHui, a specialized TCM LLM
built through contextual data integration and domain knowledge fusion. We
constructed a large-scale TCM corpus (0.97GB unsupervised data + 611,312 QA
pairs) and employed a two-stage training strategy with QLoRA, DeepSpeed Stage
2, and Flash Attention 2. Evaluation on 12 benchmarks showed TianHui ranked
top-three in all metrics for six datasets (APQ, TCMCD, HFR, HCCA, DHPE, TLAW)
and achieved top results in the other six (TCMEE, APR, GCPMI, TCMKQA, TCMRC,
ADTG). Optimal configuration was identified as LoRA rank=128, alpha=256,
epoch=4, dropout=0.2, max length=2048. TianHui enables systematic preservation
and scalable application of TCM knowledge. All resources are open-sourced.

</details>


### [125] [Mahānāma: A Unique Testbed for Literary Entity Discovery and Linking](https://arxiv.org/abs/2509.19844)
*Sujoy Sarkar,Gourav Sarkar,Manoj Balaji Jagadeeshan,Jivnesh Sandhan,Amrith Krishna,Pawan Goyal*

Main category: cs.CL

TL;DR: 本文建立了首个用于梵文的实体发现与链接的大型数据集Mahānāma，旨在解决复杂文学文本中的实体解析挑战，并揭示了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 中文和其他语言中的实体解析面临高词汇变异、模糊引用和长距离依赖等挑战，因此需要专门针对梵文进行的实体发现和连接的研究。

Method: 通过建立一个大型的数据集，包含来自《摩诃婆罗多》的命名实体，并与英语知识库对齐，以支持跨语言链接。

Result: 评估显示，现有的指代消解和实体链接模型在测试集的全球上下文中表现不佳。

Conclusion: Mahānāma为文学领域的实体解析提供了独特的基准，突显了现有模型在复杂语境下的局限性。

Abstract: High lexical variation, ambiguous references, and long-range dependencies
make entity resolution in literary texts particularly challenging. We present
Mah\={a}n\={a}ma, the first large-scale dataset for end-to-end Entity Discovery
and Linking (EDL) in Sanskrit, a morphologically rich and under-resourced
language. Derived from the Mah\={a}bh\={a}rata, the world's longest epic, the
dataset comprises over 109K named entity mentions mapped to 5.5K unique
entities, and is aligned with an English knowledge base to support
cross-lingual linking. The complex narrative structure of Mah\={a}n\={a}ma,
coupled with extensive name variation and ambiguity, poses significant
challenges to resolution systems. Our evaluation reveals that current
coreference and entity linking models struggle when evaluated on the global
context of the test set. These results highlight the limitations of current
approaches in resolving entities within such complex discourse. Mah\=an\=ama
thus provides a unique benchmark for advancing entity resolution, especially in
literary domains.

</details>


### [126] [Benchmarking Gaslighting Attacks Against Speech Large Language Models](https://arxiv.org/abs/2509.19858)
*Jinyang Wu,Bin Zhu,Xiandong Zou,Qiquan Zhang,Xu Fang,Pan Zhou*

Main category: cs.CL

TL;DR: 本研究针对语音大语言模型提出了气体灯攻击，通过多种操控策略测试模型的鲁棒性，结果表明其行为脆弱性显著，需增强其抗操控能力。


<details>
  <summary>Details</summary>
Motivation: 随着语音大语言模型在基于语音的应用中的普及，确保其对操控或对抗性输入的鲁棒性变得至关重要。

Method: 构建了五种操控策略（愤怒、认知干扰、讽刺、隐含以及专业否定），并对五种语音及多模态LLM进行了超过10,000个测试样本的综合评估。

Result: 研究揭示了语音大语言模型在面对气体灯攻击时的脆弱性，并要求开发更具韧性和可靠性的语音AI系统。

Conclusion: 研究结果表明，语音大语言模型在面对精心设计的气体灯攻击时，表现出显著的行为脆弱性，平均准确率下降24.3%。

Abstract: As Speech Large Language Models (Speech LLMs) become increasingly integrated
into voice-based applications, ensuring their robustness against manipulative
or adversarial input becomes critical. Although prior work has studied
adversarial attacks in text-based LLMs and vision-language models, the unique
cognitive and perceptual challenges of speech-based interaction remain
underexplored. In contrast, speech presents inherent ambiguity, continuity, and
perceptual diversity, which make adversarial attacks more difficult to detect.
In this paper, we introduce gaslighting attacks, strategically crafted prompts
designed to mislead, override, or distort model reasoning as a means to
evaluate the vulnerability of Speech LLMs. Specifically, we construct five
manipulation strategies: Anger, Cognitive Disruption, Sarcasm, Implicit, and
Professional Negation, designed to test model robustness across varied tasks.
It is worth noting that our framework captures both performance degradation and
behavioral responses, including unsolicited apologies and refusals, to diagnose
different dimensions of susceptibility. Moreover, acoustic perturbation
experiments are conducted to assess multi-modal robustness. To quantify model
vulnerability, comprehensive evaluation across 5 Speech and multi-modal LLMs on
over 10,000 test samples from 5 diverse datasets reveals an average accuracy
drop of 24.3% under the five gaslighting attacks, indicating significant
behavioral vulnerability. These findings highlight the need for more resilient
and trustworthy speech-based AI systems.

</details>


### [127] [SINAI at eRisk@CLEF 2025: Transformer-Based and Conversational Strategies for Depression Detection](https://arxiv.org/abs/2509.19861)
*Alba Maria Marmol-Romero,Manuel Garcia-Vega,Miguel Angel Garcia-Cumbreras,Arturo Montejo-Raez*

Main category: cs.CL

TL;DR: 本研究探讨了使用Transformer模型和结构化对话策略在抑郁症检测中的应用，展示了早期检测与分类准确性之间的权衡，并在相关任务中取得显著成绩。


<details>
  <summary>Details</summary>
Motivation: 旨在改进抑郁症的早期检测及提升对话系统在心理健康评估中的应用能力。

Method: 结合多种基于Transformer的模型与广泛的预处理管道，并设计对话策略以最大化信息获取。

Result: 在抑郁症早期检测任务中排名第8，且在对话任务中获得第一名，综合表现最佳。

Conclusion: 研究展示了结构化对话设计与强大语言模型结合在心理健康评估中的有效性，尤其是在早期抑郁症检测领域。

Abstract: This paper describes the participation of the SINAI-UJA team in the
eRisk@CLEF 2025 lab. Specifically, we addressed two of the proposed tasks: (i)
Task 2: Contextualized Early Detection of Depression, and (ii) Pilot Task:
Conversational Depression Detection via LLMs. Our approach for Task 2 combines
an extensive preprocessing pipeline with the use of several transformer-based
models, such as RoBERTa Base or MentalRoBERTA Large, to capture the contextual
and sequential nature of multi-user conversations. For the Pilot Task, we
designed a set of conversational strategies to interact with LLM-powered
personas, focusing on maximizing information gain within a limited number of
dialogue turns. In Task 2, our system ranked 8th out of 12 participating teams
based on F1 score. However, a deeper analysis revealed that our models were
among the fastest in issuing early predictions, which is a critical factor in
real-world deployment scenarios. This highlights the trade-off between early
detection and classification accuracy, suggesting potential avenues for
optimizing both jointly in future work. In the Pilot Task, we achieved 1st
place out of 5 teams, obtaining the best overall performance across all
evaluation metrics: DCHR, ADODL and ASHR. Our success in this task demonstrates
the effectiveness of structured conversational design when combined with
powerful language models, reinforcing the feasibility of deploying LLMs in
sensitive mental health assessment contexts.

</details>


### [128] [SwissGPC v1.0 -- The Swiss German Podcasts Corpus](https://arxiv.org/abs/2509.19866)
*Samuel Stucki,Mark Cieliebak,Jan Deriu*

Main category: cs.CL

TL;DR: 开发瑞士德语自发语音语料库SwissGPC v1.0，支持多种语言研究。


<details>
  <summary>Details</summary>
Motivation: 为ASR、TTS和方言识别等研究提供支持，填补现有语料库的不足。

Method: 通过自动化注释流程构建，提供了统计数据和分段特征。

Result: 包含约5000小时的自然瑞士德语语音，覆盖七大方言区域，提供真实对话数据。

Conclusion: SwissGPC v1.0是一个自然且丰富的语音资源，适用于多种语言处理应用。

Abstract: We present SwissGPC v1.0, the first mid-to-large-scale corpus of spontaneous
Swiss German speech, developed to support research in ASR, TTS, dialect
identification, and related fields. The dataset consists of links to talk shows
and podcasts hosted on Schweizer Radio und Fernsehen and YouTube, which contain
approximately 5400 hours of raw audio. After segmentation and weak annotation,
nearly 5000 hours of speech were retained, covering the seven major Swiss
German dialect regions alongside Standard German. We describe the corpus
construction methodology, including an automated annotation pipeline, and
provide statistics on dialect distribution, token counts, and segmentation
characteristics. Unlike existing Swiss German speech corpora, which primarily
feature controlled speech, this corpus captures natural, spontaneous
conversations, making it a valuable resource for real-world speech
applications.

</details>


### [129] [Do Before You Judge: Self-Reference as a Pathway to Better LLM Evaluation](https://arxiv.org/abs/2509.19880)
*Wei-Hsiang Lin,Sheng-Lun Wei,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.CL

TL;DR: 研究生成能力与判断能力的关系，提出一种新评估策略来加强其相关性。


<details>
  <summary>Details</summary>
Motivation: 研究模型生成能力与判断能力之间的关系，解决现有研究结果不一致的问题。

Method: 通过对11个模型和21个不同任务进行系统的数据集和实例级别分析。

Result: 发现生成与判断能力仅弱相关，主要因为大型语言模型对于被判断的响应敏感。

Conclusion: 提出了一种自我参考引导的评估策略，显著增强了生成能力与判断能力之间的相关性。

Abstract: LLM-as-Judge frameworks are increasingly popular for AI evaluation, yet
research findings on the relationship between models' generation and judgment
abilities remain inconsistent. We investigate this relationship through
systematic dataset- and instance-level analyses across 11 models and 21 diverse
tasks. Despite both capabilities relying on the same underlying knowledge, our
analyses reveal they are only weakly correlated, primarily due to LLMs'
sensitivity to the responses being judged. To address this, we propose a
self-reference-guided evaluation strategy that leverages a model's own answers
as references. This approach significantly strengthens the correlation between
generation and judgment abilities, offering a practical path to align these
skills and providing a reliable proxy for model selection in evaluation tasks.

</details>


### [130] [Future Policy Aware Preference Learning for Mathematical Reasoning](https://arxiv.org/abs/2509.19893)
*Minjae Oh,Yunho Choi,Dongmin Choi,Yohan Jo*

Main category: cs.CL

TL;DR: 本文提出FPA偏好学习，通过关注未来策略解决数学推理中的偏好学习问题，显著提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 解决现有偏好学习方法在数学推理中面临的效果下降及过度惩罚问题。

Method: 提出未来策略感知(FPA)偏好学习模型，以替代当前策略，使得正则化项关注于未来策略。

Result: FPA方法在DPO、RPO和SimPER上均有显著性能提升，尤其在SimPER中表现最佳，提升达5.75%。

Conclusion: FPA学习方法能更好地处理数学推理中的偏好学习问题，提升了模型在MATH和GSM8K基准上的表现，并支持更长时间的无降级训练。

Abstract: Preference learning methods such as Direct Preference Optimization (DPO) have
become standard for Large Language Model (LLM) post-training, yet they are
often ineffective for mathematical reasoning. A key challenge is the large
token overlap between preferred and dispreferred trajectories; lowering the
probability of dispreferred trajectories also reduces the probability of shared
useful tokens, leading to over-penalization and overall performance collapse.
As a mitigation, existing algorithms include the probability of a trajectory
under the current policy as a regularization term, which decreases the effect
of the gradient when the probability is low. However, by the time this effect
takes hold, useful tokens may have already been over-penalized as the model has
begun to degrade. To address this, we propose Future Policy Aware (FPA)
preference learning, which replaces the current policy with a future policy in
the regularization term. This future policy is estimated via lightweight,
logit-space extrapolation from a reference model toward the current model. FPA
enables safer training by preemptively regularizing potentially problematic
gradients. We apply FPA to DPO, RPO, and SimPER and evaluate them on the MATH
and GSM8K benchmarks. FPA yields consistent performance gains, with the largest
improvements observed with SimPER, achieving gains of up to 5.75%. We
demonstrate that FPA provides proactive regularization while preserving the
probability of shared, useful mathematical tokens, and enables longer,
degradation-free training with negligible computational overhead. We will
release our code publicly upon publication.

</details>


### [131] [WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and Interaction](https://arxiv.org/abs/2509.19902)
*Binbin Zhang,Chengdong Liang,Shuai Wang,Xuelong Geng,Zhao Guo,Haoyu Li,Hao Yin,Xipeng Yang,Pengshen Zhang,Changwei Ma,Lei Xie*

Main category: cs.CL

TL;DR: WEST是一个易用的全栈语音工具包，基于大型语言模型，支持多种语音任务，并提供开源模型及优越的性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个基于大型语言模型的语音工具包，简化用户的使用体验，同时提供极好的性能。

Method: 通过复用成熟的架构和方法，WEST支持语音识别、合成、理解、对话等多种任务，并可扩展以纳入开源模型。

Result: WEST提供了两种类型的实验结果和模型，使得用户能够在开源模型和数据上复现实验或直接使用经过大规模数据训练的模型。

Conclusion: WEST是一个基于大型语言模型的语音工具包，具备全面的功能和易用性，适合各种语音任务的应用。

Abstract: In this paper, we present WEST(WE Speech Toolkit), a speech toolkit based on
a large language model (LLM) for speech understanding, generation, and
interaction. There are three key features of WEST: 1) Fully LLM-based: Standing
on the shoulders of giants by reusing mature architectures, ecosystems (e.g.,
Hugging Face), and methods (e.g., sequence packing) from large models. 2)
Full-stack: Supports tasks such as recognition, synthesis, understanding,
dialogue, and multimodal capabilities, with extensibility to incorporate
open-source models. 3) Simple and Stupid: A simple and stupid speech toolkit
that everyone can Touch. In addition, WEST provides two types of recipes,
models, and experimental results. The first is entirely based on open-source
models and open-source data, allowing users to fully reproduce the experiments
in this paper and serving as a verification system or minimal system baseline.
The second is trained on massive data, offering superior performance so the
user can directly apply it out of the box. WEST is publicly avilable at
https://github.com/wenet-e2e/west/

</details>


### [132] [CorIL: Towards Enriching Indian Language to Indian Language Parallel Corpora and Machine Translation Systems](https://arxiv.org/abs/2509.19941)
*Soham Bhattacharjee,Mukund K Roy,Yathish Poojary,Bhargav Dave,Mihir Raj,Vandan Mujadia,Baban Gain,Pruthwik Mishra,Arafat Ahsan,Parameswari Krishnamurthy,Ashwath Rao,Gurpreet Singh Josan,Preeti Dubey,Aadil Amin Kak,Anna Rao Kulkarni,Narendra VG,Sunita Arora,Rakesh Balbantray,Prasenjit Majumdar,Karunesh K Arora,Asif Ekbal,Dipti Mishra Sharma*

Main category: cs.CL

TL;DR: 本论文构建了一种高质量的印度语言平行语料库CorIL，以支持机器翻译研究并提高其效果，同时分析了不同模型在各种领域的表现。


<details>
  <summary>Details</summary>
Motivation: 由于印度语言的多样性及高质量平行语料库的稀缺，研究的动机在于提升机器翻译在这些语言中的应用效果。

Method: 通过构建覆盖11种印度语言的大规模注释平行语料库，研究并评估了多种先进的神经机器翻译模型，进行性能分析。

Result: 构建的CorIL语料库包含772,000句双语句子对，按照政府、健康和一般三大领域分类，展示了多种模型在不同领域和脚本上的表现差异。

Conclusion: 本研究发布的CorIL大规模平行语料库有助于提高印度语言机器翻译的研究水平，并为未来的研究提供重要数据资源。

Abstract: India's linguistic landscape is one of the most diverse in the world,
comprising over 120 major languages and approximately 1,600 additional
languages, with 22 officially recognized as scheduled languages in the Indian
Constitution. Despite recent progress in multilingual neural machine
translation (NMT), high-quality parallel corpora for Indian languages remain
scarce, especially across varied domains. In this paper, we introduce a
large-scale, high-quality annotated parallel corpus covering 11 of these
languages : English, Telugu, Hindi, Punjabi, Odia, Kashmiri, Sindhi, Dogri,
Kannada, Urdu, and Gujarati comprising a total of 772,000 bi-text sentence
pairs. The dataset is carefully curated and systematically categorized into
three key domains: Government, Health, and General, to enable domain-aware
machine translation research and facilitate effective domain adaptation. To
demonstrate the utility of CorIL and establish strong benchmarks for future
research, we fine-tune and evaluate several state-of-the-art NMT models,
including IndicTrans2, NLLB, and BhashaVerse. Our analysis reveals important
performance trends and highlights the corpus's value in probing model
capabilities. For instance, the results show distinct performance patterns
based on language script, with massively multilingual models showing an
advantage on Perso-Arabic scripts (Urdu, Sindhi) while other models excel on
Indic scripts. This paper provides a detailed domain-wise performance analysis,
offering insights into domain sensitivity and cross-script transfer learning.
By publicly releasing CorIL, we aim to significantly improve the availability
of high-quality training data for Indian languages and provide a valuable
resource for the machine translation research community.

</details>


### [133] [The Knowledge-Behaviour Disconnect in LLM-based Chatbots](https://arxiv.org/abs/2509.20004)
*Jan Broersen*

Main category: cs.CL

TL;DR: 论文讨论了大语言模型在对话行为中知识应用的断裂现象，指出该现象是根本性的，并且增加数据与训练量无法消除，同时批评现有的解决方案未能奏效。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型（如ChatGPT）在交互中展示出的知识与其对话行为之间的关系，特别是理解其局限性和产生幻觉的根源。

Method: 通过分析大语言模型的训练技术及其对话行为的表现，探讨知识与行为之间的断裂。

Result: 论文揭示了大语言模型与其对话行为之间的知识断裂，认为这种断裂是根本性的，并批评了现有的伦理技术未能有效解决这一问题。

Conclusion: 大语言模型具有知识性，但其在对话行为中未能有效利用这些知识，导致了一种根本性的‘断裂’，且这种现象不会因数据或训练量的增加而消失。

Abstract: Large language model-based artificial conversational agents (like ChatGPT)
give answers to all kinds of questions, and often enough these answers are
correct. Just on the basis of that capacity alone, we may attribute knowledge
to them. But do these models use this knowledge as a basis for their own
conversational behaviour? I argue this is not the case, and I will refer to
this failure as a `disconnect'. I further argue this disconnect is fundamental
in the sense that with more data and more training of the LLM on which a
conversational chatbot is based, it will not disappear. The reason is, as I
will claim, that the core technique used to train LLMs does not allow for the
establishment of the connection we are after. The disconnect reflects a
fundamental limitation on the capacities of LLMs, and explains the source of
hallucinations. I will furthermore consider the ethical version of the
disconnect (ethical conversational knowledge not being aligned with ethical
conversational behaviour), since in this domain researchers have come up with
several additional techniques to influence a chatbot's behaviour. I will
discuss how these techniques do nothing to solve the disconnect and can make it
worse.

</details>


### [134] [DiffNator: Generating Structured Explanations of Time-Series Differences](https://arxiv.org/abs/2509.20007)
*Kota Dohi,Tomoya Nishida,Harsh Purohit,Takashi Endo,Yohei Kawaguchi*

Main category: cs.CL

TL;DR: 本文提出了DiffNator框架，通过结合时间序列编码器与预训练的语言模型，提供结构化的时间序列差异解释，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在许多物联网应用中，关键在于传感器信号之间的差异，但解释这些差异需要专业知识，因此需要一个有效的工具来实现这一目标。

Method: 通过设计JSON schema和训练结合时间序列编码器及预训练的语言模型（LLM）的模型，以生成JSON格式的差异解释。

Result: DiffNator在TORI数据集上生成了配对序列，并成功训练生成差异解释的模型。实验结果显示它的表现优于传统方法。

Conclusion: DiffNator能够生成准确的差异解释，并且在性能上显著优于基于视觉的问题回答法和经过预训练的时间序列编码器的检索方法。

Abstract: In many IoT applications, the central interest lies not in individual sensor
signals but in their differences, yet interpreting such differences requires
expert knowledge. We propose DiffNator, a framework for structured explanations
of differences between two time series. We first design a JSON schema that
captures the essential properties of such differences. Using the Time-series
Observations of Real-world IoT (TORI) dataset, we generate paired sequences and
train a model that combine a time-series encoder with a frozen LLM to output
JSON-formatted explanations. Experimental results show that DiffNator generates
accurate difference explanations and substantially outperforms both a visual
question answering (VQA) baseline and a retrieval method using a pre-trained
time-series encoder.

</details>


### [135] [Tokenization and Representation Biases in Multilingual Models on Dialectal NLP Tasks](https://arxiv.org/abs/2509.20045)
*Vani Kanjirangat,Tanja Samardžić,Ljiljana Dolamic,Fabio Rinaldi*

Main category: cs.CL

TL;DR: 本文研究方言数据对多语言模型性能的影响，发现标记化和信息表示的偏见在不同任务中具有不同的预测能力。


<details>
  <summary>Details</summary>
Motivation: 探讨方言数据对多语言模型性能的影响，揭示模型在不同语言特性上的表现差异，尤其是标记化和信息表示的偏见。

Method: 通过比较不同类型的语言模型在方言分类、主题分类和提取式问答任务中的性能，控制了文字脚本和资源可用性。

Result: 分析表明，TP 是预测依赖于句法和形态线索任务（如提取式问答）性能的更好指标，而 IP 则对语义任务（如主题分类）的性能预测更准确。

Conclusion: TP 和 IP 在不同任务上的影响并不一致，反映出模型在处理不同语言特性时的局限性和潜在偏见。

Abstract: Dialectal data are characterized by linguistic variation that appears small
to humans but has a significant impact on the performance of models. This
dialect gap has been related to various factors (e.g., data size, economic and
social factors) whose impact, however, turns out to be inconsistent. In this
work, we investigate factors impacting the model performance more directly: we
correlate Tokenization Parity (TP) and Information Parity (IP), as measures of
representational biases in pre-trained multilingual models, with the downstream
performance. We compare state-of-the-art decoder-only LLMs with encoder-based
models across three tasks: dialect classification, topic classification, and
extractive question answering, controlling for varying scripts (Latin vs.
non-Latin) and resource availability (high vs. low). Our analysis reveals that
TP is a better predictor of the performance on tasks reliant on syntactic and
morphological cues (e.g., extractive QA), while IP better predicts performance
in semantic tasks (e.g., topic classification). Complementary analyses,
including tokenizer behavior, vocabulary coverage, and qualitative insights,
reveal that the language support claims of LLMs often might mask deeper
mismatches at the script or token level.

</details>


### [136] [Responsible AI Technical Report](https://arxiv.org/abs/2509.20057)
*KT,:,Soonmin Bae,Wanjin Park,Jeongyeop Kim,Yunjin Park,Jungwon Yoon,Junhyung Moon,Myunggyo Oh,Wonhyuk Lee,Junseo Jang,Dongyoung Jung,Minwook Ju,Eunmi Kim,Sujin Kim,Youngchol Kim,Somin Lee,Wonyoung Lee,Minsung Noh,Hyoungjun Park,Eunyoung Shin*

Main category: cs.CL

TL;DR: KT 开发的负责任 AI 评估方法论和工具有效管理 AI 风险，推动了国内 AI 安全性和可靠性的发展。


<details>
  <summary>Details</summary>
Motivation: 分析 AI 基本法实施及全球 AI 治理趋势，以建立合规性和系统性管理 AI 风险的方法。

Method: 开发了一种负责任 AI 评估方法论，并结合风险减轻技术来确保 AI 服务的安全性和可靠性。

Result: 建立了一种可靠的评估方法论，并推出了实时阻断有害响应的安全工具 SafetyGuard，支持国内 AI 发展。

Conclusion: KT 提供的负责任 AI 评估方法论和工具可有效管理和减轻 AI 风险，对推动国内 AI 发展具有重要意义。

Abstract: KT developed a Responsible AI (RAI) assessment methodology and risk
mitigation technologies to ensure the safety and reliability of AI services. By
analyzing the Basic Act on AI implementation and global AI governance trends,
we established a unique approach for regulatory compliance and systematically
identify and manage all potential risk factors from AI development to
operation. We present a reliable assessment methodology that systematically
verifies model safety and robustness based on KT's AI risk taxonomy tailored to
the domestic environment. We also provide practical tools for managing and
mitigating identified AI risks. With the release of this report, we also
release proprietary Guardrail : SafetyGuard that blocks harmful responses from
AI models in real-time, supporting the enhancement of safety in the domestic AI
development ecosystem. We also believe these research outcomes provide valuable
insights for organizations seeking to develop Responsible AI.

</details>


### [137] [From Input Perception to Predictive Insight: Modeling Model Blind Spots Before They Become Errors](https://arxiv.org/abs/2509.20065)
*Maggie Mi,Aline Villavicencio,Nafise Sadat Moosavi*

Main category: cs.CL

TL;DR: 该研究提出了一种改进的输入方法，能够有效预测语言模型对复杂输入的理解错误。


<details>
  <summary>Details</summary>
Motivation: 为了应对语言模型在处理习惯用语、比喻以及上下文敏感输入时的误解问题。

Method: 使用基于token的可能性特征来识别输入理解中的局部不确定性。

Result: 该方法在五个语言学挑战性数据集上表现超越传统基准，证明了其有效性。

Conclusion: 提出了一种输入优先的方法，成功预测了语言模型的理解错误。

Abstract: Language models often struggle with idiomatic, figurative, or
context-sensitive inputs, not because they produce flawed outputs, but because
they misinterpret the input from the outset. We propose an input-only method
for anticipating such failures using token-level likelihood features inspired
by surprisal and the Uniform Information Density hypothesis. These features
capture localized uncertainty in input comprehension and outperform standard
baselines across five linguistically challenging datasets. We show that
span-localized features improve error detection for larger models, while
smaller models benefit from global patterns. Our method requires no access to
outputs or hidden activations, offering a lightweight and generalizable
approach to pre-generation error prediction.

</details>


### [138] [From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training](https://arxiv.org/abs/2509.20072)
*Tianqiao Liu,Xueyi Li,Hao Wang,Haoxuan Li,Zhichao Chen,Weiqi Luo,Zitao Liu*

Main category: cs.CL

TL;DR: TtT框架通过结合自回归文本生成与非自回归音频扩散，提供了一种高效的多模态生成方法，解决了现有模型的计算成本问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态模型在处理交错的音频和文本时需要复杂的多阶段训练流程，并且在生成过程中忽视了文本和音频之间依赖结构的不对称性。

Method: 在单一Transformer架构中，结合了自回归文本生成与非自回归音频扩散，基于预训练的大型语言模型进行初始化。

Result: 通过提议的TtT框架，减少了计算成本，提供了更高效的音频和文本生成方法。

Conclusion: 提出了一种新的音频-文本建模框架TtT，集成了自回归文本生成和非自回归音频扩散，以应对现有模型的局限性。

Abstract: Recent advances in large language models have attracted significant interest
in extending their capabilities to multimodal scenarios, particularly for
speech-in speech-out conversational systems. However, existing multimodal
models handling interleaved audio and text, such as MOSHI require complex multi
stage training pipelines, incurring substantial computational costs. Moreover,
these models uniformly apply autoregressive generation to both text and audio
tokens, overlooking a fundamental asymmetry in their dependency structures:
while text tokens exhibit strong target target dependencies requiring causal
ordering, audio tokens are predominantly driven by source target dependencies,
where audio outputs primarily condition on source text rather than preceding
audio tokens. In this work, we propose TtT, a unified audio-text modeling
framework that integrates AR text generation with non-autoregressive audio
diffusion within a single Transformer architecture initialized from a
pretrained LLM.

</details>


### [139] [Can Constructions "SCAN" Compositionality ?](https://arxiv.org/abs/2509.20074)
*Ganesh Katrapati,Manish Shrivastava*

Main category: cs.CL

TL;DR: 本文提出了一种无监督的伪构式挖掘方法，显著提升了Seq2Seq模型在组合任务上的表现，特别是在样本不足时。


<details>
  <summary>Details</summary>
Motivation: Sequence to Sequence模型在组合性和系统性泛化方面存在困难，这限制了其表现。

Method: 采用无监督的方法自动提取伪构式，即从训练数据中提取的可变槽模板。

Result: 在SCAN数据集上，所提方法在分布外划分上的准确率显著提升，添加跳跃的准确率达到47.8％，环绕右转的准确率达到20.3％，并且即使使用40％的原始训练数据，模型也表现出竞争力。

Conclusion: 本文的研究表明，利用构式感知的预处理方法能够有效提高模型在特定任务上的表现，尤其是在数据稀缺的情况下。

Abstract: Sequence to Sequence models struggle at compositionality and systematic
generalisation even while they excel at many other tasks. We attribute this
limitation to their failure to internalise constructions conventionalised form
meaning pairings that license productive recombination. Building on these
insights, we introduce an unsupervised procedure for mining
pseudo-constructions: variable-slot templates automatically extracted from
training data. When applied to the SCAN dataset, our method yields large gains
out-of-distribution splits: accuracy rises to 47.8 %on ADD JUMP and to 20.3% on
AROUND RIGHT without any architectural changes or additional supervision. The
model also attains competitive performance with? 40% of the original training
data, demonstrating strong data efAciency. Our findings highlight the promise
of construction-aware preprocessing as an alternative to heavy architectural or
training-regime interventions.

</details>


### [140] [OLaPh: Optimal Language Phonemizer](https://arxiv.org/abs/2509.20086)
*Johannes Wirth*

Main category: cs.CL

TL;DR: OLaPh是一个音素化框架，通过结合大型词汇和多种NLP技术，显著提高了音素化准确性，特别是在处理复杂词汇时效果显著。


<details>
  <summary>Details</summary>
Motivation: 为了提高音素化的准确性，尤其是在处理姓名、外来词、缩写和同形异义词等难点时。

Method: 结合了大型词汇、多个自然语言处理技术和复合词解析，并使用概率评分函数。

Result: 在德语和英语评估中，OLaPh显示出比以往方法更高的准确性，尤其是在具有挑战性的数据集上。

Conclusion: OLaPh框架和训练的大型语言模型共同提高了音素化的一致性，并为未来研究提供了一个免费的资源。

Abstract: Phonemization, the conversion of text into phonemes, is a key step in
text-to-speech. Traditional approaches use rule-based transformations and
lexicon lookups, while more advanced methods apply preprocessing techniques or
neural networks for improved accuracy on out-of-domain vocabulary. However, all
systems struggle with names, loanwords, abbreviations, and homographs. This
work presents OLaPh (Optimal Language Phonemizer), a framework that combines
large lexica, multiple NLP techniques, and compound resolution with a
probabilistic scoring function. Evaluations in German and English show improved
accuracy over previous approaches, including on a challenging dataset. To
further address unresolved cases, we train a large language model on
OLaPh-generated data, which achieves even stronger generalization and
performance. Together, the framework and LLM improve phonemization consistency
and provide a freely available resource for future research.

</details>


### [141] [Causal Understanding by LLMs: The Role of Uncertainty](https://arxiv.org/abs/2509.20088)
*Oscar Lithgow-Serrano,Vani Kanjirangat,Alessandro Antonucci*

Main category: cs.CL

TL;DR: 研究表明，因果关系分类的准确性与预训练示例无关，模型缺乏有效的因果表示结构。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在因果关系分类中的失败原因，以及预训练暴露对其因果理解的影响。

Method: 通过不确定性评估，测试预训练对因果理解的影响，分析模型在因果分类和逐字记忆探测中的行为。

Result: 模型在已见和未见句子上的准确性几乎相同（p > 0.05），没有记忆偏差，输出分布接近均匀，确认了几乎随机猜测的情况。

Conclusion: 因果理解的失败源于缺乏结构化的因果表示，而非在预训练过程中对因果示例暴露不足。

Abstract: Recent papers show LLMs achieve near-random accuracy in causal relation
classification, raising questions about whether such failures arise from
limited pretraining exposure or deeper representational gaps. We investigate
this under uncertainty-based evaluation, testing whether pretraining exposure
to causal examples improves causal understanding >18K PubMed sentences -- half
from The Pile corpus, half post-2024 -- across seven models
(Pythia-1.4B/7B/12B, GPT-J-6B, Dolly-7B/12B, Qwen-7B). We analyze model
behavior through: (i) causal classification, where the model identifies causal
relationships in text, and (ii) verbatim memorization probing, where we assess
whether the model prefers previously seen causal statements over their
paraphrases. Models perform four-way classification
(direct/conditional/correlational/no-relationship) and select between originals
and their generated paraphrases. Results show almost identical accuracy on
seen/unseen sentences (p > 0.05), no memorization bias (24.8% original
selection), and output distribution over the possible options is almost flat,
with entropic values near the maximum (1.35/1.39), confirming random guessing.
Instruction-tuned models show severe miscalibration (Qwen: > 95% confidence,
32.8% accuracy, ECE=0.49). Conditional relations induce highest entropy (+11%
vs. direct). These findings suggest that failures in causal understanding arise
from the lack of structured causal representation, rather than insufficient
exposure to causal examples during pretraining.

</details>


### [142] [Integrated Framework for LLM Evaluation with Answer Generation](https://arxiv.org/abs/2509.20097)
*Sujeong Lee,Hayoung Lee,Seongsoo Heo,Wonik Choi*

Main category: cs.CL

TL;DR: 提出了一种新的评估框架SPEED，通过专家反馈提升大型语言模型的评估质量，兼具公平性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统基准评估方法依赖固定参考答案，无法全面评估生成响应的质量，因此需要一种新的评估框架。

Method: 采用专家驱动的反馈机制，对模型输出进行全面的描述性分析，分析维度包括幻觉检测、毒性评估和词汇上下文适宜性。

Result: SPEED在多个领域和数据集上展现出稳健一致的评估性能，并且在资源效率上优于较大规模的评估器。

Conclusion: SPEED显著提升了大型语言模型评价的公平性与可解释性，是现有评估方法的有力替代方案。

Abstract: Reliable evaluation of large language models is essential to ensure their
applicability in practical scenarios. Traditional benchmark-based evaluation
methods often rely on fixed reference answers, limiting their ability to
capture important qualitative aspects of generated responses. To address these
shortcomings, we propose an integrated evaluation framework called
\textit{self-refining descriptive evaluation with expert-driven diagnostics},
SPEED, which utilizes specialized functional experts to perform comprehensive,
descriptive analyses of model outputs. Unlike conventional approaches, SPEED
actively incorporates expert feedback across multiple dimensions, including
hallucination detection, toxicity assessment, and lexical-contextual
appropriateness. Experimental results demonstrate that SPEED achieves robust
and consistent evaluation performance across diverse domains and datasets.
Additionally, by employing relatively compact expert models, SPEED demonstrates
superior resource efficiency compared to larger-scale evaluators. These
findings illustrate that SPEED significantly enhances fairness and
interpretability in LLM evaluations, offering a promising alternative to
existing evaluation methodologies.

</details>


### [143] [Less is More: The Effectiveness of Compact Typological Language Representations](https://arxiv.org/abs/2509.20129)
*York Hay Ng,Phuong Hanh Hoang,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: 本研究提出了一种优化URIEL+特征空间的方法，改善了对低资源语言的距离度量和多语言NLP任务的性能。


<details>
  <summary>Details</summary>
Motivation: 应对高维度和稀疏性对低资源语言的距离度量效果的限制。

Method: 通过结合特征选择和插补来优化URIEL+的特征空间。

Result: 对语言距离对齐和下游任务的评估显示，缩小后的语言类型表示提高了信息量和性能。

Conclusion: 优化后的URIEL+特征表示在多语言自然语言处理应用中提供了更有信息量的距离度量并改善了性能。

Abstract: Linguistic feature datasets such as URIEL+ are valuable for modelling
cross-lingual relationships, but their high dimensionality and sparsity,
especially for low-resource languages, limit the effectiveness of distance
metrics. We propose a pipeline to optimize the URIEL+ typological feature space
by combining feature selection and imputation, producing compact yet
interpretable typological representations. We evaluate these feature subsets on
linguistic distance alignment and downstream tasks, demonstrating that
reduced-size representations of language typology can yield more informative
distance metrics and improve performance in multilingual NLP applications.

</details>


### [144] [Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation](https://arxiv.org/abs/2509.20162)
*Chaojun Nie,Jun Zhou,Guanxiang Wang,Shisong Wud,Zichen Wang*

Main category: cs.CL

TL;DR: 本研究提出一种新的方法RLAG，通过迭代采样生成和优化模型，更有效地在领域特定任务中嵌入关键知识，实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在特定领域任务中的性能有限，主要原因是训练数据中专业信息的不均衡代表和数据集的静态性质。

Method: 通过采样生成和优化模型，利用计算奖励迭代循环，实现对关键领域知识的有效嵌入。

Result: 在医学、法律、天文学和当前事件数据集上，实验结果表明RLAG方法显著优于现有方法。

Conclusion: 我们提出的Reinforcement Learning from Augmented Generation (RLAG)方法显著优于基线方法，特别是在医学、法律、天文学和当前事件等领域的任务上，优化了对关键领域知识的嵌入。

Abstract: Large language models (LLMs) often exhibit limited performance on
domain-specific tasks due to the natural disproportionate representation of
specialized information in their training data and the static nature of these
datasets. Knowledge scarcity and temporal lag create knowledge gaps for domain
applications. While post-training on domain datasets can embed knowledge into
models, existing approaches have some limitations. Continual Pre-Training (CPT)
treats all tokens in domain documents with equal importance, failing to
prioritize critical knowledge points, while supervised fine-tuning (SFT) with
question-answer pairs struggles to develop the coherent knowledge structures
necessary for complex reasoning tasks. To address these challenges, we propose
Reinforcement Learning from Augmented Generation (RLAG). Our approach
iteratively cycles between sampling generations and optimizing the model
through calculated rewards, effectively embedding critical and contextually
coherent domain knowledge. We select generated outputs with the highest log
probabilities as the sampling result, then compute three tailored reward
metrics to guide the optimization process. To comprehensively evaluate domain
expertise, we assess answer accuracy and the rationality of explanations
generated for correctly answered questions. Experimental results across
medical, legal, astronomy, and current events datasets demonstrate that our
proposed method significantly outperforms baseline approaches. Our code and
data are open sourced at https://github.com/ChaojunNie/RLAG.

</details>


### [145] [Probing Gender Bias in Multilingual LLMs: A Case Study of Stereotypes in Persian](https://arxiv.org/abs/2509.20168)
*Ghazal Kalhor,Behnam Bahrak*

Main category: cs.CL

TL;DR: 本研究提出了一种模板驱动的方法来探测多语言大型语言模型中的性别刻板印象，强调了低资源语言研究的重要性，并展示了波斯语中的性别偏见程度高于英语。


<details>
  <summary>Details</summary>
Motivation: 随着多语言大型语言模型在全球范围内的日益使用，确保其不带性别偏见以防止表征伤害显得尤为重要，特别是在低资源语言方面的研究相对不足。

Method: 提出了一种基于模板的探测方法，并引入了领域特定性别倾斜指数（DS-GSI）来量化性别平衡的偏离。

Result: 对四种显著模型在波斯语等低资源语言中进行评估，结果表明所有模型都存在性别偏见。

Conclusion: 所有评估的模型均表现出性别刻板印象，波斯语中的偏见程度大于英语，尤其在体育领域表现最为明显。

Abstract: Multilingual Large Language Models (LLMs) are increasingly used worldwide,
making it essential to ensure they are free from gender bias to prevent
representational harm. While prior studies have examined such biases in
high-resource languages, low-resource languages remain understudied. In this
paper, we propose a template-based probing methodology, validated against
real-world data, to uncover gender stereotypes in LLMs. As part of this
framework, we introduce the Domain-Specific Gender Skew Index (DS-GSI), a
metric that quantifies deviations from gender parity. We evaluate four
prominent models, GPT-4o mini, DeepSeek R1, Gemini 2.0 Flash, and Qwen QwQ 32B,
across four semantic domains, focusing on Persian, a low-resource language with
distinct linguistic features. Our results show that all models exhibit gender
stereotypes, with greater disparities in Persian than in English across all
domains. Among these, sports reflect the most rigid gender biases. This study
underscores the need for inclusive NLP practices and provides a framework for
assessing bias in other low-resource languages.

</details>


### [146] [Thinking Augmented Pre-training](https://arxiv.org/abs/2509.20186)
*Liang Wang,Nan Yang,Shaohan Huang,Li Dong,Furu Wei*

Main category: cs.CL

TL;DR: 本研究提出思维增强预训练（TPT）方法，通过增加思维轨迹使得LLM训练数据更有效，结果显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型训练计算需求的不断增长，高质量数据的可用性有限，因此最大限度地利用现有数据成为研究中的重大挑战。

Method: 提出了一种名为思维增强预训练（TPT）的方法，通过自动生成思维轨迹来扩增训练数据。

Result: TPT方法在各种模型配置下有效提升了LLM的性能，数据效率提高了3倍，并在3B参数模型上在多个推理基准测试中表现提升超过10%。

Conclusion: 该方法显著提升了大型语言模型（LLM）的数据效率，尤其在推理基准上表现优异。

Abstract: This paper introduces a simple and scalable approach to improve the data
efficiency of large language model (LLM) training by augmenting existing text
data with thinking trajectories. The compute for pre-training LLMs has been
growing at an unprecedented rate, while the availability of high-quality data
remains limited. Consequently, maximizing the utility of available data
constitutes a significant research challenge. A primary impediment is that
certain high-quality tokens are difficult to learn given a fixed model
capacity, as the underlying rationale for a single token can be exceptionally
complex and deep. To address this issue, we propose Thinking augmented
Pre-Training (TPT), a universal methodology that augments text with
automatically generated thinking trajectories. Such augmentation effectively
increases the volume of the training data and makes high-quality tokens more
learnable through step-by-step reasoning and decomposition. We apply TPT across
diverse training configurations up to $100$B tokens, encompassing pre-training
with both constrained and abundant data, as well as mid-training from strong
open-source checkpoints. Experimental results indicate that our method
substantially improves the performance of LLMs across various model sizes and
families. Notably, TPT enhances the data efficiency of LLM pre-training by a
factor of $3$. For a $3$B parameter model, it improves the post-training
performance by over $10\%$ on several challenging reasoning benchmarks.

</details>


### [147] [Play by the Type Rules: Inferring Constraints for LLM Functions in Declarative Programs](https://arxiv.org/abs/2509.20208)
*Parker Glenn,Alfy Samuel,Daben Liu*

Main category: cs.CL

TL;DR: 本研究探讨了小型开源语言模型在SQL查询中的应用，提出了一种提高LLM函数类型安全性的方法，并在性能上优于现有解决方案。


<details>
  <summary>Details</summary>
Motivation: 将基于LLM的操作符与声明性查询语言相结合，以利用便宜的可解释函数和强大的语言模型推理能力。

Method: 本研究通过对不同规模的开源语言模型在SQL基础查询语言中的解析和执行能力进行研究，提出了一种新的高效解决方案。

Result: 在多跳问答数据集上实现了7%的准确度提升和53%的延迟改善。

Conclusion: 本研究证明了小型语言模型在混合数据源上作为函数执行者的潜力，并提出了一种高效的解决方案以增强LLM函数的类型安全性。

Abstract: Integrating LLM powered operators in declarative query languages allows for
the combination of cheap and interpretable functions with powerful,
generalizable language model reasoning. However, in order to benefit from the
optimized execution of a database query language like SQL, generated outputs
must align with the rules enforced by both type checkers and database contents.
Current approaches address this challenge with orchestrations consisting of
many LLM-based post-processing calls to ensure alignment between generated
outputs and database values, introducing performance bottlenecks. We perform a
study on the ability of various sized open-source language models to both parse
and execute functions within a query language based on SQL, showing that small
language models can excel as function executors over hybrid data sources. Then,
we propose an efficient solution to enforce the well-typedness of LLM
functions, demonstrating 7% accuracy improvement on a multi-hop question
answering dataset with 53% improvement in latency over comparable solutions. We
make our implementation available at https://github.com/parkervg/blendsql

</details>


### [148] [Low-Resource English-Tigrinya MT: Leveraging Multilingual Models, Custom Tokenizers, and Clean Evaluation Benchmarks](https://arxiv.org/abs/2509.20209)
*Hailay Kidu Teklehaymanot,Gebrearegawi Gidey,Wolfgang Nejdl*

Main category: cs.CL

TL;DR: 本研究探讨了针对低资源语言提格利尼亚语的迁移学习技术，提出了一种综合语言特定分词、嵌入初始化和领域自适应微调的方法，显示出显著的翻译质量提升，强调了语言意识模型和可重复评估基准的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管神经机器翻译取得了进展，但低资源语言（如提格利尼亚语）仍面临挑战，如语料库有限、分词策略不足和缺乏标准化评估基准。

Method: 通过使用多语言预训练模型的迁移学习技术，并结合特定语言的分词、嵌入初始化和领域自适应微调。

Result: 实验结果显示，经过自定义分词器的迁移学习显著优于零样本基准，且通过BLEU、chrF和定性人类评估验证了增益。

Conclusion: 此研究强调了语言意识建模和可重复基准的重要性，以缩小低资源语言的性能差距。

Abstract: Despite advances in Neural Machine Translation (NMT), low-resource languages
like Tigrinya remain underserved due to persistent challenges, including
limited corpora, inadequate tokenization strategies, and the lack of
standardized evaluation benchmarks. This paper investigates transfer learning
techniques using multilingual pretrained models to enhance translation quality
for morphologically rich, low-resource languages. We propose a refined approach
that integrates language-specific tokenization, informed embedding
initialization, and domain-adaptive fine-tuning. To enable rigorous assessment,
we construct a high-quality, human-aligned English-Tigrinya evaluation dataset
covering diverse domains. Experimental results demonstrate that transfer
learning with a custom tokenizer substantially outperforms zero-shot baselines,
with gains validated by BLEU, chrF, and qualitative human evaluation.
Bonferroni correction is applied to ensure statistical significance across
configurations. Error analysis reveals key limitations and informs targeted
refinements. This study underscores the importance of linguistically aware
modeling and reproducible benchmarks in bridging the performance gap for
underrepresented languages. Resources are available at
https://github.com/hailaykidu/MachineT_TigEng
  and https://huggingface.co/Hailay/MachineT_TigEng

</details>


### [149] [Investigating the Representation of Backchannels and Fillers in Fine-tuned Language Models](https://arxiv.org/abs/2509.20237)
*Yu Wang,Leyi Lao,Langchu Huang,Gabriel Skantze,Yang Xu,Hendrik Buschmeier*

Main category: cs.CL

TL;DR: 本研究探讨微调策略对语言模型中后续回应和填充词表示的影响，结果显示微调能增强模型对这些语言特征的学习能力。


<details>
  <summary>Details</summary>
Motivation: 研究后续回应和填充词在对话中的重要性，尤其是它们在现代变换器结构的语言模型中的不足。

Method: 使用三种微调策略对模型进行训练，在英语和日语对话语料库上进行测试。

Result: 微调模型的表示中后续回应和填充词的轮廓得分提高，且生成的语句与人类生成的语句更为相似。

Conclusion: 微调语言模型可以有效提高其生成更接近人类对话的能力，特别是在对话中的后续回应和填充词的表示上。

Abstract: Backchannels and fillers are important linguistic expressions in dialogue,
but are under-represented in modern transformer-based language models (LMs).
Our work studies the representation of them in language models using three
fine-tuning strategies. The models are trained on three dialogue corpora in
English and Japanese, where backchannels and fillers are preserved and
annotated, to investigate how fine-tuning can help LMs learn their
representations. We first apply clustering analysis to the learnt
representation of backchannels and fillers, and have found increased silhouette
scores in representations from fine-tuned models, which suggests that
fine-tuning enables LMs to distinguish the nuanced semantic variation in
different backchannel and filler use. We also use natural language generation
(NLG) metrics to confirm that the utterances generated by fine-tuned language
models resemble human-produced utterances more closely. Our findings suggest
the potentials of transforming general LMs into conversational LMs that are
more capable of producing human-like languages adequately.

</details>


### [150] [Instruction Boundary: Quantifying Biases in LLM Reasoning under Various Coverage](https://arxiv.org/abs/2509.20278)
*Zipeng Ling,Yuehao Tang,Chen Huang,Shuliang Liu,Gaoyang Jiang,Shenghong Fu,Junqi Yang,Yao Wan,Jiawan Zhang,Kejia Huang,Xuming Hu*

Main category: cs.CL

TL;DR: 大型语言模型推理存在提示设计导致的偏见问题，提出BiasDetector框架评估这些偏见，并强调开发者与用户在偏见管理中的角色。


<details>
  <summary>Details</summary>
Motivation: 探讨提示设计对大型语言模型推理的影响，以及其潜在的偏见和不可靠性。

Method: 引入BiasDetector框架，测量来自三类指令的偏见：完整、冗余和不足，并评估多个主流LLMs。

Result: 尽管headline准确性高，但在许多下游任务中存在显著偏见，提示覆盖的直接后果。

Conclusion: 开发者需要解决偏见问题，用户需要谨慎制定提示。

Abstract: Large-language-model (LLM) reasoning has long been regarded as a powerful
tool for problem solving across domains, providing non-experts with valuable
advice. However, their limitations - especially those stemming from prompt
design - remain underexplored. Because users may supply biased or incomplete
prompts - often unintentionally - LLMs can be misled, undermining reliability
and creating risks. We refer to this vulnerability as the Instruction Boundary.
To investigate the phenomenon, we distill it into eight concrete facets and
introduce BiasDetector, a framework that measures biases arising from three
instruction types: complete, redundant, and insufficient. We evaluate several
mainstream LLMs and find that, despite high headline accuracy, substantial
biases persist in many downstream tasks as a direct consequence of prompt
coverage. Our empirical study confirms that LLM reasoning reliability can still
be significantly improved. We analyze the practical impact of these biases and
outline mitigation strategies. Our findings underscore the need for developers
to tackle biases and for users to craft options carefully.

</details>


### [151] [Feeding Two Birds or Favoring One? Adequacy-Fluency Tradeoffs in Evaluation and Meta-Evaluation of Machine Translation](https://arxiv.org/abs/2509.20287)
*Behzad Shayegh,Jan-Thorsten Peter,David Vilar,Tobias Domhan,Juraj Juraska,Markus Freitag,Lili Mou*

Main category: cs.CL

TL;DR: 本文研究机器翻译领域中充分性与流利性之间的权衡，指出当前评价指标的偏向，提出改进方法以控制影响。


<details>
  <summary>Details</summary>
Motivation: 探讨机器翻译中充分性和流利性之间的权衡，以及当前评价指标在此权衡中的表现。

Method: 提出了一种合成翻译系统的方法，旨在控制元评价中的偏差。

Result: 发现当前的评价指标更倾向于充分性，而流利性的评价较弱，且这一偏见在元评价中同样存在，并影响指标排名。

Conclusion: 理解机器翻译中的 adequacy（充分性）与 fluency（流利性）之间的权衡对评价指标的影响至关重要。

Abstract: We investigate the tradeoff between adequacy and fluency in machine
translation. We show the severity of this tradeoff at the evaluation level and
analyze where popular metrics fall within it. Essentially, current metrics
generally lean toward adequacy, meaning that their scores correlate more
strongly with the adequacy of translations than with fluency. More importantly,
we find that this tradeoff also persists at the meta-evaluation level, and that
the standard WMT meta-evaluation favors adequacy-oriented metrics over
fluency-oriented ones. We show that this bias is partially attributed to the
composition of the systems included in the meta-evaluation datasets. To control
this bias, we propose a method that synthesizes translation systems in
meta-evaluation. Our findings highlight the importance of understanding this
tradeoff in meta-evaluation and its impact on metric rankings.

</details>


### [152] [Multilingual Hope Speech Detection: A Comparative Study of Logistic Regression, mBERT, and XLM-RoBERTa with Active Learning](https://arxiv.org/abs/2509.20315)
*T. O. Abiola,K. D. Abiodun,O. E. Olumide,O. O. Adebanji,O. Hiram Calvo,Grigori Sidorov*

Main category: cs.CL

TL;DR: 本研究提出了一种多语种希望言语检测框架，结合主动学习和变换器模型，在多种语言中有效提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 检测希望言语在促进网上积极话语方面的重要性，但在多语种和低资源环境中检测仍然具有挑战性。

Method: 采用主动学习方法和基于变换器的模型（如mBERT和XLM-RoBERTa）进行多语种希望言语检测。

Result: 实验结果显示，变换器模型明显优于传统基线，XLM-RoBERTa取得了最高的整体准确率，且主动学习策略在小规模注释数据集下仍维持强性能。

Conclusion: 本研究表明，结合多语种变换器与数据高效训练策略在希望言语检测中非常有效。

Abstract: Hope speech language that fosters encouragement and optimism plays a vital
role in promoting positive discourse online. However, its detection remains
challenging, especially in multilingual and low-resource settings. This paper
presents a multilingual framework for hope speech detection using an active
learning approach and transformer-based models, including mBERT and
XLM-RoBERTa. Experiments were conducted on datasets in English, Spanish,
German, and Urdu, including benchmark test sets from recent shared tasks. Our
results show that transformer models significantly outperform traditional
baselines, with XLM-RoBERTa achieving the highest overall accuracy.
Furthermore, our active learning strategy maintained strong performance even
with small annotated datasets. This study highlights the effectiveness of
combining multilingual transformers with data-efficient training strategies for
hope speech detection.

</details>


### [153] [SIM-CoT: Supervised Implicit Chain-of-Thought](https://arxiv.org/abs/2509.20317)
*Xilin Wei,Xiaoran Liu,Yuhang Zang,Xiaoyi Dong,Yuhang Cao,Jiaqi Wang,Xipeng Qiu,Dahua Lin*

Main category: cs.CL

TL;DR: 隐式链思维方法的新问题导致性能差距，SIM-CoT通过引入逐步监督解决了潜在的不稳定性，提升了隐式推理模型的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 隐式链思维方法在大型语言模型中表现出良好的潜力，但因训练不稳定和语义多样性不足限制了其应用。

Method: 使用辅助解码器引入逐步监督，以稳定和丰富潜在推理空间，确保隐式标记与显式推理步骤的一致性。

Result: SIM-CoT在各种隐式链思维方法上显著提高了领域内准确性和领域外稳定性，并在GPT-2和LLaMA-3.1 8B模型上实现了显著的性能提升。

Conclusion: 提出的SIM-CoT显著提高了多种隐式链思维方法的准确性和稳定性，展示出强大的可扩展性和有效性。

Abstract: Implicit Chain-of-Thought (CoT) methods present a promising, token-efficient
alternative to explicit CoT reasoning in Large Language Models (LLMs), but a
persistent performance gap has limited the application of implicit CoT. We
identify a core latent instability issue by scaling the computational budget of
implicit CoT approaches: as we increase the number of implicit reasoning tokens
to enhance performance, the training process often becomes unstable and
collapses. Our analysis reveals that this instability arises from the latent
representations becoming homogeneous and losing their semantic diversity, a
failure caused by insufficient step-level supervision in existing implicit CoT
approaches. To address this issue, we propose SIM-CoT, a plug-and-play training
module that introduces step-level supervision to stabilize and enrich the
latent reasoning space. Specifically, SIM-CoT employs an auxiliary decoder
during training to align each implicit token with its corresponding explicit
reasoning step, ensuring that latent states capture distinct and meaningful
information. The proposed auxiliary decoder is removed during inference,
preserving the computational efficiency of implicit CoT methods with no added
overhead. In addition, the auxiliary decoder affords interpretability of
implicit reasoning by projecting each latent token onto an explicit reasoning
vocabulary, enabling per-step visualization of semantic roles and diagnosis.
SIM-CoT significantly enhances both the in-domain accuracy and out-of-domain
stability of various implicit CoT methods, boosting baselines like Coconut by
+8.2% on GPT-2 and CODI by +3.0% on LLaMA-3.1 8B. Demonstrating strong
scalability, SIM-CoT also surpasses the explicit CoT baseline on GPT-2 by 2.1%
with 2.3\times greater token efficiency, while substantially closing the
performance gap on larger models like LLaMA-3.1 8B.

</details>


### [154] [Z-Scores: A Metric for Linguistically Assessing Disfluency Removal](https://arxiv.org/abs/2509.20319)
*Maria Teleki,Sai Janjur,Haoran Liu,Oliver Grabner,Ketan Verma,Thomas Docog,Xiangjue Dong,Lingfeng Shi,Cong Wang,Stephanie Birkelbach,Jason Kim,Yin Zhang,James Caverlee*

Main category: cs.CL

TL;DR: 本文提出Z-Scores，作为一种新的评估指标，帮助研究者识别和解决语言模型在处理言语障碍中的具体问题，进而提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 为了深入了解模型在消除言语障碍方面的表现，超越传统的单一token级别评估指标。

Method: 引入Z-Scores作为一种基于语言的评估指标，利用确定性对齐模块强 robust 映射生成文本和含言语障碍的转录之间的关系。

Result: Z-Scores揭示了传统F1指标无法识别的系统性弱点，并通过案例研究表明其在识别特定言语障碍（如INTJ和PRN）方面的有效性。

Conclusion: Z-Scores提供了更细致的评估，可以识别并解决模型在不同类型的言语障碍中的具体问题，从而提升模型的性能。

Abstract: Evaluating disfluency removal in speech requires more than aggregate
token-level scores. Traditional word-based metrics such as precision, recall,
and F1 (E-Scores) capture overall performance but cannot reveal why models
succeed or fail. We introduce Z-Scores, a span-level linguistically-grounded
evaluation metric that categorizes system behavior across distinct disfluency
types (EDITED, INTJ, PRN). Our deterministic alignment module enables robust
mapping between generated text and disfluent transcripts, allowing Z-Scores to
expose systematic weaknesses that word-level metrics obscure. By providing
category-specific diagnostics, Z-Scores enable researchers to identify model
failure modes and design targeted interventions -- such as tailored prompts or
data augmentation -- yielding measurable performance improvements. A case study
with LLMs shows that Z-Scores uncover challenges with INTJ and PRN disfluencies
hidden in aggregate F1, directly informing model refinement strategies.

</details>


### [155] [DRES: Benchmarking LLMs for Disfluency Removal](https://arxiv.org/abs/2509.20321)
*Maria Teleki,Sai Janjur,Haoran Liu,Oliver Grabner,Ketan Verma,Thomas Docog,Xiangjue Dong,Lingfeng Shi,Cong Wang,Stephanie Birkelbach,Jason Kim,Yin Zhang,James Caverlee*

Main category: cs.CL

TL;DR: DRES是一个针对去除语音中不流畅性的基准，评估了不同LLM的性能，并提供了一系列实用建议，以改进语音驱动系统的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 不流畅性（如“嗯”、“啊”）对语音驱动系统带来了挑战，影响命令解释、摘要和对话代理的准确性。

Method: 通过人类注释的Switchboard转录本构建DRES基准，隔离了不流畅性去除、ASR错误和声学变异性。

Result: 简单分割持续提高性能，推理导向模型倾向于过度删除流畅令牌，微调接近最先进的精确度和召回率，但损害了泛化能力。

Conclusion: DRES为改进鲁棒的语音驱动系统提供了可重复的、与模型无关的基础。

Abstract: Disfluencies -- such as "um," "uh," interjections, parentheticals, and edited
statements -- remain a persistent challenge for speech-driven systems,
degrading accuracy in command interpretation, summarization, and conversational
agents. We introduce DRES (Disfluency Removal Evaluation Suite), a controlled
text-level benchmark that establishes a reproducible semantic upper bound for
this task. DRES builds on human-annotated Switchboard transcripts, isolating
disfluency removal from ASR errors and acoustic variability. We systematically
evaluate proprietary and open-source LLMs across scales, prompting strategies,
and architectures. Our results reveal that (i) simple segmentation consistently
improves performance, even for long-context models; (ii) reasoning-oriented
models tend to over-delete fluent tokens; and (iii) fine-tuning achieves near
state-of-the-art precision and recall but harms generalization abilities. We
further present a set of LLM-specific error modes and offer nine practical
recommendations (R1-R9) for deploying disfluency removal in speech-driven
pipelines. DRES provides a reproducible, model-agnostic foundation for
advancing robust spoken-language systems.

</details>


### [156] [Morphological Synthesizer for Ge'ez Language: Addressing Morphological Complexity and Resource Limitations](https://arxiv.org/abs/2509.20341)
*Gebrearegawi Gebremariam,Hailay Teklehaymanot,Gebregewergs Mezgebe*

Main category: cs.CL

TL;DR: 本研究提出了一种基于规则的Ge'ez形态合成器，旨在解决Ge'ez语言的形态学复杂性及资源缺乏问题，系统表现优异。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏标注的语言数据、语料库、标记数据集和词典，直到现在尚未开发和发布可用的NLP工具，因此需要解决Ge'ez语言形态学的复杂性和缺乏资源的问题。

Method: 提出了一种基于规则的Ge'ez形态合成器，根据语言的形态结构从词根生成表面词。

Result: 使用1102个样本动词来测试和评估系统，代表了所有动词形态结构。

Conclusion: 该系统的性能达到了97.4%，超越了基准模型，未来的工作应考虑该语言的形态变异，以建立一个全面的系统。

Abstract: Ge'ez is an ancient Semitic language renowned for its unique alphabet. It
serves as the script for numerous languages, including Tigrinya and Amharic,
and played a pivotal role in Ethiopia's cultural and religious development
during the Aksumite kingdom era. Ge'ez remains significant as a liturgical
language in Ethiopia and Eritrea, with much of the national identity
documentation recorded in Ge'ez. These written materials are invaluable primary
sources for studying Ethiopian and Eritrean philosophy, creativity, knowledge,
and civilization. Ge'ez has a complex morphological structure with rich
inflectional and derivational morphology, and no usable NLP has been developed
and published until now due to the scarcity of annotated linguistic data,
corpora, labeled datasets, and lexicons. Therefore, we propose a rule-based
Ge'ez morphological synthesizer to generate surface words from root words
according to the morphological structures of the language. We used 1,102 sample
verbs, representing all verb morphological structures, to test and evaluate the
system. The system achieves a performance of 97.4%, outperforming the baseline
model and suggesting that future work should build a comprehensive system
considering morphological variations of the language.
  Keywords: Ge'ez, NLP, morphology, morphological synthesizer, rule-based

</details>


### [157] [EmbeddingGemma: Powerful and Lightweight Text Representations](https://arxiv.org/abs/2509.20354)
*Henrique Schechter Vera,Sahil Dua,Biao Zhang,Daniel Salz,Ryan Mullins,Sindhu Raghuram Panyam,Sara Smoot,Iftekhar Naim,Joe Zou,Feiyang Chen,Daniel Cer,Alice Lisak,Min Choi,Lucas Gonzalez,Omar Sanseviero,Glenn Cameron,Ian Ballantyne,Kat Black,Kaifeng Chen,Weiyi Wang,Zhe Li,Gus Martins,Jinhyuk Lee,Mark Sherwood,Juyeong Ji,Renjie Wu,Jingxiao Zheng,Jyotinder Singh,Abheesht Sharma,Divya Sreepat,Aashi Jain,Adham Elarabawy,AJ Co,Andreas Doumanoglou,Babak Samari,Ben Hora,Brian Potetz,Dahun Kim,Enrique Alfonseca,Fedor Moiseev,Feng Han,Frank Palma Gomez,Gustavo Hernández Ábrego,Hesen Zhang,Hui Hui,Jay Han,Karan Gill,Ke Chen,Koert Chen,Madhuri Shanbhogue,Michael Boratko,Paul Suganthan,Sai Meher Karthik Duddu,Sandeep Mariserla,Setareh Ariafar,Shanfeng Zhang,Shijie Zhang,Simon Baumgartner,Sonam Goenka,Steve Qiu,Tanmaya Dabral,Trevor Walker,Vikram Rao,Waleed Khawaja,Wenlei Zhou,Xiaoqi Ren,Ye Xia,Yichang Chen,Yi-Ting Chen,Zhe Dong,Zhongli Ding,Francesco Visin,Gaël Liu,Jiageng Zhang,Kathleen Kenealy,Michelle Casbon,Ravin Kumar,Thomas Mesnard,Zach Gleicher,Cormac Brick,Olivier Lacombe,Adam Roberts,Yunhsuan Sung,Raphael Hoffmann,Tris Warkentin,Armand Joulin,Tom Duerig,Mojtaba Seyedhosseini*

Main category: cs.CL

TL;DR: EmbeddingGemma是一种轻量级文本嵌入模型，基于Gemma 3语言模型，能在多语言与代码领域实现优越性能，是低延迟应用的理想选择。


<details>
  <summary>Details</summary>
Motivation: 为了开发一种轻量级文本嵌入模型，能够在保证性能的同时，适用于资源受限的高效应用场景。

Method: 通过编码器-解码器初始化与几何嵌入蒸馏相结合的创新训练方案，使用了分散正则化来提高模型的鲁棒性与表达能力。

Result: 在MTEB评测中，EmbeddingGemma（300M）的性能在效果和成本比上表现卓越，能够与两倍参数的模型相媲美，并保持这一优势即使在量化权重或截断嵌入输出时。

Conclusion: EmbeddingGemma在各多语言及代码领域的表现超越了500M参数以下的所有顶尖模型，且在低延迟、高吞吐量的应用中尤其表现突出。

Abstract: We introduce EmbeddingGemma, a new lightweight, open text embedding model
based on the Gemma 3 language model family. Our innovative training recipe
strategically captures knowledge from larger models via encoder-decoder
initialization and geometric embedding distillation. We improve model
robustness and expressiveness with a spread-out regularizer, and ensure
generalizability by merging checkpoints from varied, optimized mixtures.
Evaluated on the Massive Text Embedding Benchmark (MTEB) across multilingual,
English, and code domains, EmbeddingGemma (300M) achieves state-of-the-art
results. Notably, it outperforms prior top models, both proprietary and open,
with fewer than 500M parameters, and provides performance comparable to models
double its size, offering an exceptional performance-to-cost ratio. Remarkably,
this lead persists when quantizing model weights or truncating embedding
outputs. This makes EmbeddingGemma particularly well-suited for low-latency and
high-throughput use cases such as on-device applications. We provide ablation
studies exploring our key design choices. We release EmbeddingGemma to the
community to promote further research.

</details>


### [158] [Language Models that Think, Chat Better](https://arxiv.org/abs/2509.20357)
*Adithya Bhaskar,Xi Ye,Danqi Chen*

Main category: cs.CL

TL;DR: RLMT能提高语言模型在开放式任务中的表现，超越传统RLHF方式，重新思考后训练流程。


<details>
  <summary>Details</summary>
Motivation: 虽然RLVR在可验证领域有效，但在开放式任务（如写大纲论文或制定餐单）方面的泛化能力有限。

Method: RLMT需要语言模型生成长链条推理（CoT），并通过在线强化学习（RL）优化，使用基于偏好的奖励模型进行强化学习人类反馈（RLHF）。

Result: RLMT在40次训练中超越标准的RLHF流程，在聊天和创造性写作等任务上显著提高了表现，在一些基准上取得了3-7点的提升。

Conclusion: RLMT在多个聊天基准上表现优异，重新思考了后训练流程，并呼吁未来的研究更广泛地理解和运用思维。

Abstract: Reinforcement learning with verifiable rewards (RLVR) improves language model
reasoning by using rule-based rewards in verifiable domains such as mathematics
and code. However, RLVR leads to limited generalization for open-ended tasks --
such as writing outline essays or making meal plans -- where humans reason
routinely. This paper shows that the RLVR paradigm is effective beyond
verifiable domains, and introduces **RL** with **M**odel-rewarded **T**hinking
(**RLMT**) for general-purpose chat capabilities. Using diverse real-world
prompts, RLMT requires LMs to generate long CoT reasoning before response, and
optimizes them with online RL against a preference-based reward model used in
RLHF. Across 40 training runs on Llama-3.1-8B and Qwen-2.5-7B (both base and
instruct) and multiple optimization algorithms (DPO, PPO, and GRPO), RLMT
consistently outperforms standard RLHF pipelines. This includes substantial
gains of 3-7 points on three chat benchmarks (AlpacaEval2, WildBench, and
ArenaHardV2), along with 1-3 point improvements on other tasks like creative
writing and general knowledge. Our best 8B model surpasses GPT-4o in chat and
creative writing and rivals Claude-3.7-Sonnet (Thinking). RLMT can also be
applied directly to base models without an SFT stage, akin to R1-Zero training.
Remarkably, with only 7K prompts, Llama-3.1-8B base trained with our RLMT
recipe outperforms Llama-3.1-8B-Instruct post-trained with a complex
multi-staged pipeline with 25M+ examples. We close with qualitative and
quantitative analyses of how trained models plan their responses. Our results
rethink the post-training pipeline and call upon future work to understand and
employ thinking more broadly.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [159] [MusiCRS: Benchmarking Audio-Centric Conversational Recommendation](https://arxiv.org/abs/2509.19469)
*Rohan Surana,Amit Namburi,Gagan Mundada,Abhay Lal,Zachary Novack,Julian McAuley,Junda Wu*

Main category: cs.SD

TL;DR: MusiCRS是一个创新的对话推荐基准，揭示了现有模型在音频推理中的不足，旨在推动音频推荐研究的进展。


<details>
  <summary>Details</summary>
Motivation: 音乐推荐面临独特挑战，需要超越文本或元数据的音频内容推理，因此创建MusiCRS数据集来填补这一空白。

Method: 通过构建包含477个用户对话和3589个独特音乐实体的MusiCRS数据集，进行不同输入模态配置的评估。

Result: 实验表明，现有系统在文本信号依赖上存在问题，难以进行细致的音频推理，显示出跨模态知识整合的基本局限性。

Conclusion: MusiCRS 是一个开创性的基准数据集，旨在促进音频中心的对话推荐研究，并揭示当前系统在音频推理方面的局限性。

Abstract: Conversational recommendation has advanced rapidly with large language models
(LLMs), yet music remains a uniquely challenging domain where effective
recommendations require reasoning over audio content beyond what text or
metadata can capture. We present MusiCRS, the first benchmark for audio-centric
conversational recommendation that links authentic user conversations from
Reddit with corresponding audio tracks. MusiCRS contains 477 high-quality
conversations spanning diverse genres (classical, hip-hop, electronic, metal,
pop, indie, jazz) with 3,589 unique musical entities and audio grounding via
YouTube links. MusiCRS enables evaluation across three input modality
configurations: audio-only, query-only, and audio+query (multimodal), allowing
systematic comparison of audio-LLMs, retrieval models, and traditional
approaches. Our experiments reveal that current systems rely heavily on textual
signals and struggle with nuanced audio reasoning. This exposes fundamental
limitations in cross-modal knowledge integration where models excel at dialogue
semantics but cannot effectively ground abstract musical concepts in actual
audio content. To facilitate progress, we release the MusiCRS dataset
(https://huggingface.co/datasets/rohan2810/MusiCRS), evaluation code
(https://github.com/rohan2810/musiCRS), and comprehensive baselines.

</details>


### [160] [ArtiFree: Detecting and Reducing Generative Artifacts in Diffusion-based Speech Enhancement](https://arxiv.org/abs/2509.19495)
*Bhawana Chhaglani,Yang Gao,Julius Richter,Xilin Li,Syavosh Zadissa,Tarun Pruthi,Andrew Lovitt*

Main category: cs.SD

TL;DR: 本研究系统分析了扩散语音增强中的伪影预测和减少问题，提出了基于语义一致性的集成推理方法，显著提高了音素准确性。


<details>
  <summary>Details</summary>
Motivation: 针对基于扩散的语音增强方法中存在的生成伪影和高推理延迟等问题进行系统研究。

Method: 采用集成推理方法，结合语义一致性对多个扩散过程进行指导，分析扩散步骤的数量对伪影抑制和延迟的影响。

Result: 在低信噪比条件下，通过该方法将词错误率降低了15%，有效提高了语音的音素准确性和语义合理性。

Conclusion: 本研究提出的基于语义一致性的集成推理方法能够有效降低生成语音中的伪影，并达到更好的语音增强效果。

Abstract: Diffusion-based speech enhancement (SE) achieves natural-sounding speech and
strong generalization, yet suffers from key limitations like generative
artifacts and high inference latency. In this work, we systematically study
artifact prediction and reduction in diffusion-based SE. We show that variance
in speech embeddings can be used to predict phonetic errors during inference.
Building on these findings, we propose an ensemble inference method guided by
semantic consistency across multiple diffusion runs. This technique reduces WER
by 15% in low-SNR conditions, effectively improving phonetic accuracy and
semantic plausibility. Finally, we analyze the effect of the number of
diffusion steps, showing that adaptive diffusion steps balance artifact
suppression and latency. Our findings highlight semantic priors as a powerful
tool to guide generative SE toward artifact-free outputs.

</details>


### [161] [Thinking While Listening: Simple Test Time Scaling For Audio Classification](https://arxiv.org/abs/2509.19676)
*Prateek Verma,Mert Pilanci*

Main category: cs.SD

TL;DR: 提出了一个新框架，通过引入思考机制和测试时扩展，显著提升了音频分类性能，表明小型模型在性能上超越大型推理模型。


<details>
  <summary>Details</summary>
Motivation: 受到大型语言模型推理能力提升的启发，探讨如何在音频分类中引入思考以提高性能。

Method: 提出了一个新的架构，并评估了两种开源推理模型的效果。

Result: 实验表明，不论在现有管道中加入思考，还是从头设计架构，模型均表现出更高的分类准确性。

Conclusion: 通过将推理融入音频分类管道，新的框架提高了音频分类性能。

Abstract: We propose a framework that enables neural models to "think while listening"
to everyday sounds, thereby enhancing audio classification performance.
Motivated by recent advances in the reasoning capabilities of large language
models, we address two central questions: (i) how can thinking be incorporated
into existing audio classification pipelines to enable reasoning in the
category space and improve performance, and (ii) can a new architecture be
designed from the ground up to support both thinking and test-time scaling? We
demonstrate that in both settings, our models exhibit improved classification
accuracy. Leveraging test-time scaling, we observe consistent gains as the
number of sampled traces increases. Furthermore, we evaluate two open-source
reasoning models, GPT-OSS-20B and Qwen3-14B, showing that while such models are
capable of zero-shot reasoning, a lightweight approach--retraining only the
embedding matrix of a frozen, smaller model like GPT-2--can surpass the
performance of billion-parameter text-based reasoning models.

</details>


### [162] [Can Audio Large Language Models Verify Speaker Identity?](https://arxiv.org/abs/2509.19755)
*Yiming Ren,Xuenan Xu,Baoxiang Li,Shuai Wang,Chao Zhang*

Main category: cs.SD

TL;DR: 本文研究音频大型语言模型在说话人验证中的应用，提出新的训练策略和方法，显示经过微调后模型性能提升，但与传统模型仍有差距。


<details>
  <summary>Details</summary>
Motivation: 探索音频大型语言模型在说话人验证任务中的适应性，解决现有模型在多样化声学条件下的局限性。

Method: 将说话人验证重新定义为音频问答任务，进行无监督评估和监督微调，提出基于规则的困难对采样策略。

Result: 经过微调后，ALLM在说话人验证上的表现显著提高，但仍与传统模型存在差距，联合查询时在文本依赖性SV上表现与级联ASR-SV系统竞争。

Conclusion: 通过适当的调整，音频大型语言模型具有成为可信的说话人验证系统的潜力，同时保持通用音频理解能力。

Abstract: This paper investigates adapting Audio Large Language Models (ALLMs) for
speaker verification (SV). We reformulate SV as an audio question-answering
task and conduct comprehensive zero-shot evaluations on public benchmarks,
showing that current ALLMs have limited zero-shot SV capability and often
struggle in diverse acoustic conditions. To address this challenge, we perform
supervised fine-tuning on speaker verification data. A rule-based hard pair
sampling strategy is proposed to construct more challenging training pairs.
Lightweight fine-tuning substantially improves the performance, though there is
still a gap between ALLMs and conventional models. Then, we extend to
text-dependent SV by jointly querying ALLMs to verify speaker identity and
spoken content, yielding results competitive with cascaded ASR-SV systems. Our
findings demonstrate that with proper adaptation, ALLMs hold substantial
potential as a unified model for robust speaker verification systems, while
maintaining the general audio understanding capabilities.

</details>


### [163] [Efficient Speech Watermarking for Speech Synthesis via Progressive Knowledge Distillation](https://arxiv.org/abs/2509.19812)
*Yang Cui,Peter Pan,Lei He,Sheng Zhao*

Main category: cs.SD

TL;DR: 针对语音克隆的隐私安全风险，提出了一种高效且鲁棒的深度学习语音水印方法PKDMark，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 针对语音生成模型带来的隐私与安全风险，提出语音水印技术作为防止滥用和追寻来源的解决方案。

Method: 通过渐进知识蒸馏（PKD）方法，分两阶段训练教师模型并将其能力转移至紧凑的学生模型。

Result: PKDMark模型在复杂失真下实现平均检测F1分数99.6%，PESQ得分为4.30，极大提高了实时语音合成应用中的水印效率。

Conclusion: PKDMark是一种高效的深度学习语音水印方法，能够在保持高鲁棒性和隐蔽性的同时大幅降低计算成本。

Abstract: With the rapid advancement of speech generative models, unauthorized voice
cloning poses significant privacy and security risks. Speech watermarking
offers a viable solution for tracing sources and preventing misuse. Current
watermarking technologies fall mainly into two categories: DSP-based methods
and deep learning-based methods. DSP-based methods are efficient but vulnerable
to attacks, whereas deep learning-based methods offer robust protection at the
expense of significantly higher computational cost. To improve the
computational efficiency and enhance the robustness, we propose PKDMark, a
lightweight deep learning-based speech watermarking method that leverages
progressive knowledge distillation (PKD). Our approach proceeds in two stages:
(1) training a high-performance teacher model using an invertible neural
network-based architecture, and (2) transferring the teacher's capabilities to
a compact student model through progressive knowledge distillation. This
process reduces computational costs by 93.6% while maintaining high level of
robust performance and imperceptibility. Experimental results demonstrate that
our distilled model achieves an average detection F1 score of 99.6% with a PESQ
of 4.30 in advanced distortions, enabling efficient speech watermarking for
real-time speech synthesis applications.

</details>


### [164] [Eliminating stability hallucinations in llm-based tts models via attention guidance](https://arxiv.org/abs/2509.19852)
*ShiMing Wang,ZhiHao Du,Yang Xiang,TianYu Zhao,Han Zhao,Qian Chen,XianGang Li,HanJie Guo,ZhenHua Ling*

Main category: cs.SD

TL;DR: 本文通过改进注意力机制，提出最佳对齐评分（OAS）并将其应用于CosyVoice2的训练，有效减少了稳定性幻觉。


<details>
  <summary>Details</summary>
Motivation: 解决基于LLM的文本到语音模型中的稳定性幻觉（如重复或遗漏的语音）。

Method: 通过引入最佳对齐评分（OAS），结合维特比算法评估文本与语音的对齐质量，并利用预训练的注意力值指导训练。

Result: 在Seed-TTS-Eval和CV3-Eval测试集上的实验表明，提出的方法能够有效降低CosyVoice2的稳定性幻觉。

Conclusion: 提出的方法有效减少了CosyVoice2中的稳定性幻觉，并未引入额外的负面效应。

Abstract: This paper focuses on resolving stability hallucinations (e.g., repetitive or
omitted speech) in LLM-based Text-to-Speech (TTS) models by improving and
leveraging the attention mechanism. First, we analyzed the alignment mechanism
between text tokens and speech tokens in LLMs. We then proposed a metric termed
the Optimal Alignment Score (OAS), which employs the Viterbi algorithm to
evaluate text-speech alignment quality. Subsequently, OAS was integrated into
the training of CosyVoice2 to assist LLMs in learning continuous, stable
alignment. Additionally, the pre-trained attention value is employed to guide
the training of the student CosyVoice2 via chain-of-thought (CoT), which
further reduces stability hallucinations in synthesized speech. Experiments on
the Seed-TTS-Eval and CV3-Eval test sets demonstrate that the proposed methods
can effectively reduce the stability hallucinations of CosyVoice2 without
introducing additional negative effects. The appendix is available at
https://wsmzzz.github.io/llm_attn.

</details>


### [165] [SEA-Spoof: Bridging The Gap in Multilingual Audio Deepfake Detection for South-East Asian](https://arxiv.org/abs/2509.19865)
*Jinyang Wu,Nana Hou,Zihan Pan,Qiquan Zhang,Sailor Hardik Bhupendra,Soumik Mondal*

Main category: cs.SD

TL;DR: SEA-Spoof是针对东南亚语言的首个音频深伪检测数据集，它填补了数据集覆盖的空白，显著提高了相关模型的跨语言性能。


<details>
  <summary>Details</summary>
Motivation: 东南亚地区的数字经济快速增长，但现有的数据集对该地区语言覆盖不足，导致检测模型效果不佳。

Method: 通过生成多样化的音频样本，构建了一个覆盖300小时的配对真实与伪造语音数据集。

Result: 通过对现有的先进检测模型进行基准测试发现，跨语言效果显著下降，但对SEA-Spoof进行微调后，各语言和合成源的性能得到了显著恢复。

Conclusion: SEA-Spoof是一个首个针对东南亚语言的大规模音频深伪检测数据集，为研究提供了重要基础。

Abstract: The rapid growth of the digital economy in South-East Asia (SEA) has
amplified the risks of audio deepfakes, yet current datasets cover SEA
languages only sparsely, leaving models poorly equipped to handle this critical
region. This omission is critical: detection models trained on high-resource
languages collapse when applied to SEA, due to mismatches in synthesis quality,
language-specific characteristics, and data scarcity. To close this gap, we
present SEA-Spoof, the first large-scale Audio Deepfake Detection (ADD) dataset
especially for SEA languages. SEA-Spoof spans 300+ hours of paired real and
spoof speech across Tamil, Hindi, Thai, Indonesian, Malay, and Vietnamese.
Spoof samples are generated from a diverse mix of state-of-the-art open-source
and commercial systems, capturing wide variability in style and fidelity.
Benchmarking state-of-the-art detection models reveals severe cross-lingual
degradation, but fine-tuning on SEA-Spoof dramatically restores performance
across languages and synthesis sources. These results highlight the urgent need
for SEA-focused research and establish SEA-Spoof as a foundation for developing
robust, cross-lingual, and fraud-resilient detection systems.

</details>


### [166] [CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance](https://arxiv.org/abs/2509.19883)
*Junchuan Zhao,Wei Zeng,Tianle Lyu,Ye Wang*

Main category: cs.SD

TL;DR: 该论文提出了CoMelSinger，一个零-shot唱歌语音合成框架，解决了传统技术在旋律控制和音韵泄漏方面的挑战，且在多个评估指标上超越了现有基线。


<details>
  <summary>Details</summary>
Motivation: 随着基于离散编解码器的语音合成技术的发展，研究者希望将这些技术应用于具有精确旋律控制要求的唱歌语音合成。

Method: 使用非自回归的MaskGCT架构，CoMelSinger通过歌词和音高标记替换传统文本输入，并采用粗到细对比学习策略以抑制音韵泄漏。

Result: CoMelSinger框架实现了在离散编解码建模范式下的结构化和解耦旋律控制，并通过SVT模块提供细粒度的帧级监督。

Conclusion: CoMelSinger在音高准确性、音色一致性和零-shot可转移性方面相较于竞争性基线取得了显著提升。

Abstract: Singing Voice Synthesis (SVS) aims to generate expressive vocal performances
from structured musical inputs such as lyrics and pitch sequences. While recent
progress in discrete codec-based speech synthesis has enabled zero-shot
generation via in-context learning, directly extending these techniques to SVS
remains non-trivial due to the requirement for precise melody control. In
particular, prompt-based generation often introduces prosody leakage, where
pitch information is inadvertently entangled within the timbre prompt,
compromising controllability. We present CoMelSinger, a zero-shot SVS framework
that enables structured and disentangled melody control within a discrete codec
modeling paradigm. Built on the non-autoregressive MaskGCT architecture,
CoMelSinger replaces conventional text inputs with lyric and pitch tokens,
preserving in-context generalization while enhancing melody conditioning. To
suppress prosody leakage, we propose a coarse-to-fine contrastive learning
strategy that explicitly regularizes pitch redundancy between the acoustic
prompt and melody input. Furthermore, we incorporate a lightweight encoder-only
Singing Voice Transcription (SVT) module to align acoustic tokens with pitch
and duration, offering fine-grained frame-level supervision. Experimental
results demonstrate that CoMelSinger achieves notable improvements in pitch
accuracy, timbre consistency, and zero-shot transferability over competitive
baselines.

</details>


### [167] [Enabling Multi-Species Bird Classification on Low-Power Bioacoustic Loggers](https://arxiv.org/abs/2509.20103)
*Stefano Ciapponi,Leonardo Mannini,Jarek Scanferla,Matteo Anderle,Elisabetta Farella*

Main category: cs.SD

TL;DR: WrenNet 是一种高效的神经网络，支持在低功耗微控制器上进行实时多物种鸟类音频分类，表现优异且能效高。


<details>
  <summary>Details</summary>
Motivation: 为了实现可扩展的生物多样性监测，开发一种高效的神经网络，以便在低能耗设备上进行多物种鸟类音频分类。

Method: 使用半可学习的频谱特征提取器，适应鸟类声学数据，并在低功耗微控制器上实现实时音频分类。

Result: 在专家策划的70种鸟类数据集上，WrenNet 对声学特征明显物种的准确率高达90.8\%，在整个任务上为70.1\%，在AudioMoth设备上每次推理仅消耗77mJ，且比Birdnet在Raspberry Pi 3B+上的能耗高效16倍以上。

Conclusion: WrenNet 提供了在低功耗边缘设备上进行多物种鸟类音频分类的首个实用框架，具有高效与能量低耗优势。

Abstract: This paper introduces WrenNet, an efficient neural network enabling real-time
multi-species bird audio classification on low-power microcontrollers for
scalable biodiversity monitoring. We propose a semi-learnable spectral feature
extractor that adapts to avian vocalizations, outperforming standard mel-scale
and fully-learnable alternatives. On an expert-curated 70-species dataset,
WrenNet achieves up to 90.8\% accuracy on acoustically distinctive species and
70.1\% on the full task. When deployed on an AudioMoth device ($\leq$1MB RAM),
it consumes only 77mJ per inference. Moreover, the proposed model is over 16x
more energy-efficient compared to Birdnet when running on a Raspberry Pi 3B+.
This work demonstrates the first practical framework for continuous,
multi-species acoustic monitoring on low-power edge devices.

</details>

<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 57]
- [cs.CL](#cs.CL) [Total: 77]
- [cs.SD](#cs.SD) [Total: 9]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Exploring OCR-augmented Generation for Bilingual VQA](https://arxiv.org/abs/2510.02543)
*JoonHo Lee,Sunho Park*

Main category: cs.CV

TL;DR: 本文提出了KLOCR，一个基于1亿数据训练的双语OCR增强模型，并发布了面向韩语视觉问答的KOCRBench基准，系统研究了OCR文本在视觉语言模型中的作用。


<details>
  <summary>Details</summary>
Motivation: 为了推动多语言场景下视觉语言模型（VLM）中OCR增强生成的研究，尤其是在韩语等资源较少语言中的应用，需要强大的双语OCR基线模型和适配的评测基准。

Method: 训练并开源了KLOCR模型，一个在1亿实例上训练的强双语OCR模型；构建了韩语视觉问答评测集KOCRBench；系统分析了不同提示方法对OCR增强VLM性能的影响。

Result: 实验证明，引入OCR提取的文本能显著提升开源和商用模型在双语VQA任务上的性能，验证了OCR增强的有效性。

Conclusion: OCR信息对多语言视觉语言任务至关重要，KLOCR和KOCRBench为双语OCR增强生成提供了有效工具和评估基础。

Abstract: We investigate OCR-augmented generation with Vision Language Models (VLMs),
exploring tasks in Korean and English toward multilingualism. To support
research in this domain, we train and release KLOCR, a strong bilingual OCR
baseline trained on 100M instances to augment VLMs with OCR ability. To
complement existing VQA benchmarks, we curate KOCRBench for Korean VQA, and
analyze different prompting methods. Extensive experiments show that
OCR-extracted text significantly boosts performance across open source and
commercial models. Our work offers new insights into OCR-augmented generation
for bilingual VQA. Model, code, and data are available at
https://github.com/JHLee0513/KLOCR.

</details>


### [2] [Oracle-RLAIF: An Improved Fine-Tuning Framework for Multi-modal Video Models through Reinforcement Learning from Ranking Feedback](https://arxiv.org/abs/2510.02561)
*Derek Shi,Ruben Glatt,Christine Klymko,Shubham Mohole,Hongjun Choi,Shashank Kushwaha,Sam Sakla,Felipe Leno da Silva*

Main category: cs.CV

TL;DR: 提出Oracle-RLAIF框架，用通用Oracle排序器替代训练好的奖励模型，并引入基于排序的GRPO_rank损失函数，提升大规模视频语言模型微调的效率与效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于AI反馈的强化学习（RLAIF）依赖昂贵且专用的奖励模型，限制了视频语言模型微调的可扩展性和成本效益。

Method: 提出Oracle-RLAIF框架，使用通用Oracle排序器对模型响应进行排序而非打分，并设计GRPO_rank损失函数，直接优化基于排序的序数反馈。

Result: 在多个视频理解基准上，Oracle-RLAIF consistently 优于现有微调方法训练的先进视频语言模型。

Conclusion: Oracle-RLAIF为大规模多模态视频模型提供了一种更灵活、数据高效的对齐框架，推动强化学习从评分转向排序范式。

Abstract: Recent advances in large video-language models (VLMs) rely on extensive
fine-tuning techniques that strengthen alignment between textual and visual
comprehension. Leading pipelines typically pair supervised fine-tuning (SFT)
with reinforcement learning from preference data to enhance video
comprehension. However, as VLMs scale in parameter size, so does the cost of
gathering enough human feedback. To make fine-tuning more cost-effective,
recent frameworks explore reinforcement learning with AI feedback (RLAIF),
which replace human preference with AI as a judge. Current RLAIF frameworks
rely on a specialized reward model trained with video narratives to create
calibrated scalar rewards-- an expensive and restrictive pipeline. We propose
Oracle-RLAIF, a novel framework that replaces the trained reward model with a
more general Oracle ranker which acts as a drop-in model ranking candidate
model responses rather than scoring them. Alongside Oracle-RLAIF, we introduce
$GRPO_{rank}$, a novel rank-based loss function based on Group Relative Policy
Optimization (GRPO) that directly optimizes ordinal feedback with rank-aware
advantages. Empirically, we demonstrate that Oracle-RLAIF consistently
outperforms leading VLMs using existing fine-tuning methods when evaluated
across various video comprehension benchmarks. Oracle-RLAIF paves the path to
creating flexible and data-efficient frameworks for aligning large multi-modal
video models with reinforcement learning from rank rather than score.

</details>


### [3] [PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction](https://arxiv.org/abs/2510.02566)
*Qiao Feng,Yiming Huang,Yufu Wang,Jiatao Gu,Lingjie Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为PhysHMR的统一框架，通过在物理仿真环境中直接学习从视觉输入到动作控制的映射，实现既符合物理规律又与输入视频对齐的高质量人体运动重建。


<details>
  <summary>Details</summary>
Motivation: 现有基于单目视频的人体运动重建方法多依赖于运动学估计，缺乏物理约束，导致结果不真实；而采用两阶段物理后处理的方法易累积误差，限制了整体质量。因此需要一种端到端、物理合理的统一框架。

Method: 提出PhysHMR，直接学习从视觉输入到物理仿真中人形控制动作的策略；引入“像素即射线”（pixel-as-ray）策略，将2D关键点提升为3D空间射线并变换至全局空间，作为策略输入以提供鲁棒的全局姿态引导；结合预训练编码器提取的局部视觉特征，并通过从动捕数据训练的专家策略进行知识蒸馏，再利用物理动机的强化学习奖励进行微调。

Result: 实验表明，PhysHMR在多种场景下均能生成高保真、物理合理的人体运动，在视觉准确性和物理真实感方面均优于先前方法。

Conclusion: PhysHMR通过统一的视觉到动作策略学习，实现了高质量、物理可信的人体运动重建，有效克服了传统两阶段方法的误差累积问题，为基于视频的运动捕捉提供了新思路。

Abstract: Reconstructing physically plausible human motion from monocular videos
remains a challenging problem in computer vision and graphics. Existing methods
primarily focus on kinematics-based pose estimation, often leading to
unrealistic results due to the lack of physical constraints. To address such
artifacts, prior methods have typically relied on physics-based post-processing
following the initial kinematics-based motion estimation. However, this
two-stage design introduces error accumulation, ultimately limiting the overall
reconstruction quality. In this paper, we present PhysHMR, a unified framework
that directly learns a visual-to-action policy for humanoid control in a
physics-based simulator, enabling motion reconstruction that is both physically
grounded and visually aligned with the input video. A key component of our
approach is the pixel-as-ray strategy, which lifts 2D keypoints into 3D spatial
rays and transforms them into global space. These rays are incorporated as
policy inputs, providing robust global pose guidance without depending on noisy
3D root predictions. This soft global grounding, combined with local visual
features from a pretrained encoder, allows the policy to reason over both
detailed pose and global positioning. To overcome the sample inefficiency of
reinforcement learning, we further introduce a distillation scheme that
transfers motion knowledge from a mocap-trained expert to the
vision-conditioned policy, which is then refined using physically motivated
reinforcement learning rewards. Extensive experiments demonstrate that PhysHMR
produces high-fidelity, physically plausible motion across diverse scenarios,
outperforming prior approaches in both visual accuracy and physical realism.

</details>


### [4] [Unlocking the power of partnership: How humans and machines can work together to improve face recognition](https://arxiv.org/abs/2510.02570)
*P. Jonathon Phillips,Geraldine Jeckeln,Carina A. Hahn,Amy N. Yates,Peter C. Fontana,Alice J. O'Toole*

Main category: cs.CV

TL;DR: 本研究提出了“邻近准确性规则”（PAR），用于指导人类与机器在人脸识别中的协作，发现智能融合人类与机器决策可显著提升系统整体准确性，尤其在人类表现低于机器时仍能通过融合改善结果。


<details>
  <summary>Details</summary>
Motivation: 探索在人脸识别这一关键决策中，人类与机器协作的条件与效果，明确何时融合能提升准确性，以优化AI辅助系统的实际应用。

Method: 基于专家与非专家的人脸识别数据，分析人-人与人-机协作的准确性，提出并验证邻近准确性规则（PAR），定义‘关键融合区’，并利用图论寻找最优人类协作组合，实现智能人机融合。

Result: 发现当协作双方基线准确性差异较小时，融合效果更好；存在一个较大的‘关键融合区’，即人类虽不如机器准确，但融合后系统精度提升；智能融合优于单独机器或全量人机融合，且优于最佳纯人类协作系统。

Conclusion: 人类与机器在人脸识别中各有作用，通过基于PAR的智能融合策略，可最大化系统准确性，并减少低表现个体的影响，为AI在人脸识别中的合理部署提供了实证依据。

Abstract: Human review of consequential decisions by face recognition algorithms
creates a "collaborative" human-machine system. Individual differences between
people and machines, however, affect whether collaboration improves or degrades
accuracy in any given case. We establish the circumstances under which
combining human and machine face identification decisions improves accuracy.
Using data from expert and non-expert face identifiers, we examined the
benefits of human-human and human-machine collaborations. The benefits of
collaboration increased as the difference in baseline accuracy between
collaborators decreased-following the Proximal Accuracy Rule (PAR). This rule
predicted collaborative (fusion) benefit across a wide range of baseline
abilities, from people with no training to those with extensive training. Using
the PAR, we established a critical fusion zone, where humans are less accurate
than the machine, but fusing the two improves system accuracy. This zone was
surprisingly large. We implemented "intelligent human-machine fusion" by
selecting people with the potential to increase the accuracy of a
high-performing machine. Intelligent fusion was more accurate than the machine
operating alone and more accurate than combining all human and machine
judgments. The highest system-wide accuracy achievable with human-only
partnerships was found by graph theory. This fully human system approximated
the average performance achieved by intelligent human-machine collaboration.
However, intelligent human-machine collaboration more effectively minimized the
impact of low-performing humans on system-wide accuracy. The results
demonstrate a meaningful role for both humans and machines in assuring accurate
face identification. This study offers an evidence-based road map for the
intelligent use of AI in face identification.

</details>


### [5] [How Confident are Video Models? Empowering Video Models to Express their Uncertainty](https://arxiv.org/abs/2510.02571)
*Zhiting Mei,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.CV

TL;DR: 本文提出了一种用于生成式视频模型的不确定性量化（UQ）框架，包括评估模型校准性的新指标、黑箱UQ方法S-QUBED以及UQ数据集，首次实现了对视频生成模型不确定性的有效分解与评估。


<details>
  <summary>Details</summary>
Motivation: 生成式视频模型虽广泛应用，但存在事实性幻觉问题。目前缺乏针对视频模型的不确定性量化方法，存在安全隐患，因此亟需一种有效的UQ方法来提升模型的可信度与安全性。

Method: 提出S-QUBED方法，通过在潜在空间中建模，将预测不确定性分解为偶然性和认知性两部分；设计基于稳健秩相关估计的校准评估指标，并构建专用UQ数据集用于基准测试。

Result: 在多个基准视频数据集上的实验表明，S-QUBED能生成校准良好的总不确定性估计，其与任务准确率呈负相关，并能有效分解出aleatoric和epistemic不确定性成分。

Conclusion: 该研究填补了生成式视频模型不确定性量化的空白，所提出的框架和方法为提升视频生成模型的可靠性和可解释性提供了有效工具。

Abstract: Generative video models demonstrate impressive text-to-video capabilities,
spurring widespread adoption in many real-world applications. However, like
large language models (LLMs), video generation models tend to hallucinate,
producing plausible videos even when they are factually wrong. Although
uncertainty quantification (UQ) of LLMs has been extensively studied in prior
work, no UQ method for video models exists, raising critical safety concerns.
To our knowledge, this paper represents the first work towards quantifying the
uncertainty of video models. We present a framework for uncertainty
quantification of generative video models, consisting of: (i) a metric for
evaluating the calibration of video models based on robust rank correlation
estimation with no stringent modeling assumptions; (ii) a black-box UQ method
for video models (termed S-QUBED), which leverages latent modeling to
rigorously decompose predictive uncertainty into its aleatoric and epistemic
components; and (iii) a UQ dataset to facilitate benchmarking calibration in
video models. By conditioning the generation task in the latent space, we
disentangle uncertainty arising due to vague task specifications from that
arising from lack of knowledge. Through extensive experiments on benchmark
video datasets, we demonstrate that S-QUBED computes calibrated total
uncertainty estimates that are negatively correlated with the task accuracy and
effectively computes the aleatoric and epistemic constituents.

</details>


### [6] [PEO: Training-Free Aesthetic Quality Enhancement in Pre-Trained Text-to-Image Diffusion Models with Prompt Embedding Optimization](https://arxiv.org/abs/2510.02599)
*Hovhannes Margaryan,Bo Wan,Tinne Tuytelaars*

Main category: cs.CV

TL;DR: 提出了一种无需训练的提示嵌入优化方法（PEO），通过优化简单提示的文本嵌入来提升预训练文本到图像扩散模型的美学质量。


<details>
  <summary>Details</summary>
Motivation: 在给定简单提示的情况下，现有预训练文本到图像模型生成图像的美学质量仍有提升空间，且需避免对模型进行额外训练。

Method: 提出PEO方法，利用三元目标函数优化文本嵌入：提升生成图像的美学保真度、保持对优化后嵌入的遵循，并通过提示保持项最小化与原始提示的偏差。

Result: 定量与定性评估显示，PEO在图像美学质量上优于或媲美当前最先进的文本到图像及提示适应方法，且无需训练并适用于不同模型骨干。

Conclusion: PEO是一种有效、通用且无需训练的提示优化方法，可显著提升简单提示下生成图像的视觉质量。

Abstract: This paper introduces a novel approach to aesthetic quality improvement in
pre-trained text-to-image diffusion models when given a simple prompt. Our
method, dubbed Prompt Embedding Optimization (PEO), leverages a pre-trained
text-to-image diffusion model as a backbone and optimizes the text embedding of
a given simple and uncurated prompt to enhance the visual quality of the
generated image. We achieve this by a tripartite objective function that
improves the aesthetic fidelity of the generated image, ensures adherence to
the optimized text embedding, and minimal divergence from the initial prompt.
The latter is accomplished through a prompt preservation term. Additionally,
PEO is training-free and backbone-independent. Quantitative and qualitative
evaluations confirm the effectiveness of the proposed method, exceeding or
equating the performance of state-of-the-art text-to-image and prompt
adaptation methods.

</details>


### [7] [Ego-Exo 3D Hand Tracking in the Wild with a Mobile Multi-Camera Rig](https://arxiv.org/abs/2510.02601)
*Patrick Rim,Kun He,Kevin Harris,Braden Copple,Shangchen Han,Sizhe An,Ivan Shugurov,Tomas Hodan,He Wen,Xu Xie*

Main category: cs.CV

TL;DR: 提出了一种无标记多相机系统，用于在真实野外条件下捕捉精确的3D手势及其与物体的交互，平衡了环境真实性和标注精度之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的手部3D追踪数据集大多在受控实验室环境中采集，缺乏环境多样性和模型泛化能力，难以应用于真实场景。

Method: 设计了一个结合背戴式八相机外视角系统和Meta Quest 3头显（提供两个内视角）的轻量级多视角捕捉装置，并构建了ego-exo追踪流程以生成精确的3D手势姿态真值。

Result: 通过采集同步的多视角图像和精确的3D手势姿态标注数据集，验证了该系统在真实环境中实现高精度3D标注的能力。

Conclusion: 该方法显著降低了环境真实性和3D标注精度之间的矛盾，为面向真实场景的手-物交互研究提供了高质量数据支持。

Abstract: Accurate 3D tracking of hands and their interactions with the world in
unconstrained settings remains a significant challenge for egocentric computer
vision. With few exceptions, existing datasets are predominantly captured in
controlled lab setups, limiting environmental diversity and model
generalization. To address this, we introduce a novel marker-less multi-camera
system designed to capture precise 3D hands and objects, which allows for
nearly unconstrained mobility in genuinely in-the-wild conditions. We combine a
lightweight, back-mounted capture rig with eight exocentric cameras, and a
user-worn Meta Quest 3 headset, which contributes two egocentric views. We
design an ego-exo tracking pipeline to generate accurate 3D hand pose ground
truth from this system, and rigorously evaluate its quality. By collecting an
annotated dataset featuring synchronized multi-view images and precise 3D hand
poses, we demonstrate the capability of our approach to significantly reduce
the trade-off between environmental realism and 3D annotation accuracy.

</details>


### [8] [Input-Aware Sparse Attention for Real-Time Co-Speech Video Generation](https://arxiv.org/abs/2510.02617)
*Beijia Lu,Ziyi Chen,Jing Xiao,Jun-Yan Zhu*

Main category: cs.CV

TL;DR: 提出了一种新的输入感知视频扩散蒸馏方法，通过稀疏注意力和蒸馏损失实现高质量、实时的语音驱动视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型因多步去噪和计算密集型注意力机制导致速度慢，难以实现实时语音视频合成。

Method: 利用输入的人体姿态关键点指导注意力机制和设计输入感知的蒸馏损失，引入稀疏注意力以减少冗余计算并增强时序一致性，同时提升唇部同步与手势真实感。

Result: 在保持高视觉质量的同时，实现了实时生成性能，优于现有的音频驱动和输入驱动方法。

Conclusion: 所提方法有效解决了扩散模型在语音驱动视频生成中的效率与质量权衡问题，适用于虚拟代理和视频创作等应用。

Abstract: Diffusion models can synthesize realistic co-speech video from audio for
various applications, such as video creation and virtual agents. However,
existing diffusion-based methods are slow due to numerous denoising steps and
costly attention mechanisms, preventing real-time deployment. In this work, we
distill a many-step diffusion video model into a few-step student model.
Unfortunately, directly applying recent diffusion distillation methods degrades
video quality and falls short of real-time performance. To address these
issues, our new video distillation method leverages input human pose
conditioning for both attention and loss functions. We first propose using
accurate correspondence between input human pose keypoints to guide attention
to relevant regions, such as the speaker's face, hands, and upper body. This
input-aware sparse attention reduces redundant computations and strengthens
temporal correspondences of body parts, improving inference efficiency and
motion coherence. To further enhance visual quality, we introduce an
input-aware distillation loss that improves lip synchronization and hand motion
realism. By integrating our input-aware sparse attention and distillation loss,
our method achieves real-time performance with improved visual quality compared
to recent audio-driven and input-driven methods. We also conduct extensive
experiments showing the effectiveness of our algorithmic design choices.

</details>


### [9] [Deep Generative Continual Learning using Functional LoRA: FunLoRA](https://arxiv.org/abs/2510.02631)
*Victor Enescu,Hichem Sahbi*

Main category: cs.CV

TL;DR: 提出一种基于低秩适应的新型条件机制FunLoRA，用于持续学习中的生成模型，仅使用当前任务数据训练，有效缓解灾难性遗忘，且在精度、内存和采样效率上优于现有扩散模型方法。


<details>
  <summary>Details</summary>
Motivation: 生成模型在持续学习中面临灾难性遗忘问题，传统方法依赖合成数据重放导致性能下降和计算成本增加，亟需高效、稳定的持续适应方法。

Method: 设计了一种称为FunLoRA的新型条件机制，基于低秩适应（LoRA），使用秩1矩阵并通过函数重参数化提升表达能力，实现动态条件控制，仅在当前任务数据上训练，避免重放历史数据。

Result: 在从头训练的基于流匹配的模型上进行实验，结果表明FunLoRA在分类准确率上超过基于扩散模型的现有最先进方法，同时显著降低内存开销和采样时间。

Conclusion: FunLoRA是一种高效、可扩展的参数微调方法，能有效缓解生成模型持续学习中的灾难性遗忘问题，具备高实用价值和广泛适用性。

Abstract: Continual adaptation of deep generative models holds tremendous potential and
critical importance, given their rapid and expanding usage in text and vision
based applications. Incremental training, however, remains highly challenging
due to catastrophic forgetting phenomenon, which makes it difficult for neural
networks to effectively incorporate new knowledge. A common strategy consists
in retraining the generative model on its own synthetic data in order to
mitigate forgetting. Yet, such an approach faces two major limitations: (i) the
continually increasing training time eventually becomes intractable, and (ii)
reliance on synthetic data inevitably leads to long-term performance
degradation, since synthetic samples lack the richness of real training data.
In this paper, we attenuate these issues by designing a novel and more
expressive conditioning mechanism for generative models based on low rank
adaptation (LoRA), that exclusively employs rank 1 matrices, whose
reparametrized matrix rank is functionally increased using carefully selected
functions -- and dubbed functional LoRA: FunLoRA. Using this dynamic
conditioning, the generative model is guaranteed to avoid catastrophic
forgetting and needs only to be trained on data from the current task.
Extensive experiments using flow-matching based models trained from scratch,
showcase that our proposed parameter-efficient fine-tuning (PEFT) method
surpasses prior state-of-the-art results based on diffusion models, reaching
higher classification accuracy scores, while only requiring a fraction of the
memory cost and sampling time.

</details>


### [10] [Sequence-Preserving Dual-FoV Defense for Traffic Sign and Light Recognition in Autonomous Vehicles](https://arxiv.org/abs/2510.02642)
*Abhishek Joshi,Jahnavi Krishna Koda,Abhishek Phadke*

Main category: cs.CV

TL;DR: 提出了一种基于多源数据集的双视场、序列保持的鲁棒性框架，用于提升自动驾驶中交通灯与标志识别在多种恶劣条件下的安全性与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有模型对数字对抗攻击和自然干扰（如眩光、雨、污垢）敏感，且缺乏对时间连续性、多视角感知及鲁棒性的综合考虑，影响自动驾驶的安全性。

Method: 构建涵盖多种驾驶场景的多源数据集，采用双视场设计，结合特征压缩、防御性蒸馏、基于熵的异常检测及序列时间投票，形成三层次统一防御框架。

Result: 该框架在真实异常检测应用中实现了79.8 mAP，攻击成功率降至18.2%，高风险误分类减少至32%，优于YOLOv8、YOLOv9和BEVFormer。

Conclusion: 所提统一防御堆栈有效提升了交通标志与灯识别的鲁棒性和安全性，具备良好的物理可迁移性和实际应用潜力。

Abstract: Traffic light and sign recognition are key for Autonomous Vehicles (AVs)
because perception mistakes directly influence navigation and safety. In
addition to digital adversarial attacks, models are vulnerable to existing
perturbations (glare, rain, dirt, or graffiti), which could lead to dangerous
misclassifications. The current work lacks consideration of temporal
continuity, multistatic field-of-view (FoV) sensing, and robustness to both
digital and natural degradation. This study proposes a dual FoV,
sequence-preserving robustness framework for traffic lights and signs in the
USA based on a multi-source dataset built on aiMotive, Udacity, Waymo, and
self-recorded videos from the region of Texas. Mid and long-term sequences of
RGB images are temporally aligned for four operational design domains (ODDs):
highway, night, rainy, and urban. Over a series of experiments on a real-life
application of anomaly detection, this study outlines a unified three-layer
defense stack framework that incorporates feature squeezing, defensive
distillation, and entropy-based anomaly detection, as well as sequence-wise
temporal voting for further enhancement. The evaluation measures included
accuracy, attack success rate (ASR), risk-weighted misclassification severity,
and confidence stability. Physical transferability was confirmed using probes
for recapture. The results showed that the Unified Defense Stack achieved
79.8mAP and reduced the ASR to 18.2%, which is superior to YOLOv8, YOLOv9, and
BEVFormer, while reducing the high-risk misclassification to 32%.

</details>


### [11] [Smart-GRPO: Smartly Sampling Noise for Efficient RL of Flow-Matching Models](https://arxiv.org/abs/2510.02654)
*Benjamin Yu,Jackie Liu,Justin Cui*

Main category: cs.CV

TL;DR: Smart-GRPO 是首个针对流匹配模型中的强化学习优化噪声扰动的方法，通过迭代搜索策略提升奖励和生成质量。


<details>
  <summary>Details</summary>
Motivation: 流匹配模型的确定性特性限制了其在强化学习中的应用，而现有随机化方法存在效率低和不稳定的问题，因此需要一种更高效稳定的噪声扰动优化方法。

Method: 提出 Smart-GRPO，采用迭代搜索策略：生成候选扰动、用奖励函数评估、并优化噪声分布以聚焦高奖励区域。

Result: 实验表明，Smart-GRPO 在奖励优化和图像视觉质量上均优于基线方法。

Conclusion: Smart-GRPO 为在流匹配框架中实现高效的强化学习提供了可行路径，弥合了高效训练与人类对齐生成之间的差距。

Abstract: Recent advancements in flow-matching have enabled high-quality text-to-image
generation. However, the deterministic nature of flow-matching models makes
them poorly suited for reinforcement learning, a key tool for improving image
quality and human alignment. Prior work has introduced stochasticity by
perturbing latents with random noise, but such perturbations are inefficient
and unstable. We propose Smart-GRPO, the first method to optimize noise
perturbations for reinforcement learning in flow-matching models. Smart-GRPO
employs an iterative search strategy that decodes candidate perturbations,
evaluates them with a reward function, and refines the noise distribution
toward higher-reward regions. Experiments demonstrate that Smart-GRPO improves
both reward optimization and visual quality compared to baseline methods. Our
results suggest a practical path toward reinforcement learning in flow-matching
frameworks, bridging the gap between efficient training and human-aligned
generation.

</details>


### [12] [FSFSplatter: Build Surface and Novel Views with Sparse-Views within 3min](https://arxiv.org/abs/2510.02691)
*Yibin Zhao,Yihan Pan,Jun Nan,Jianjun Yi*

Main category: cs.CV

TL;DR: FSFSplatter是一种从自由稀疏图像中快速重建高质量表面的新方法，通过端到端的高斯初始化、相机参数估计和几何增强优化，显著提升稀疏输入下的重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有高斯点阵化方法在处理稀疏、未校准图像时表现不佳，容易产生漂浮伪影和过拟合，因此需要一种能在自由稀疏视角下实现稳健、高质量重建的方法。

Method: FSFSplatter采用大型Transformer编码多视图图像，通过自分裂高斯头生成密集且几何一致的高斯场景初始化；结合基于贡献度的剪枝策略去除浮点伪影，并利用深度与多视图特征监督，在可微相机参数优化中缓解过拟合。

Result: FSFSplatter在DTU和Replica等主流数据集上优于当前最先进的方法，实现了更精确的表面重建和更高的视觉质量。

Conclusion: FSFSplatter通过联合优化初始化、相机参数和几何一致性，有效解决了稀疏输入下的高斯点阵化重建难题，实现了快速且高质量的表面重建。

Abstract: Gaussian Splatting has become a leading reconstruction technique, known for
its high-quality novel view synthesis and detailed reconstruction. However,
most existing methods require dense, calibrated views. Reconstructing from free
sparse images often leads to poor surface due to limited overlap and
overfitting. We introduce FSFSplatter, a new approach for fast surface
reconstruction from free sparse images. Our method integrates end-to-end dense
Gaussian initialization, camera parameter estimation, and geometry-enhanced
scene optimization. Specifically, FSFSplatter employs a large Transformer to
encode multi-view images and generates a dense and geometrically consistent
Gaussian scene initialization via a self-splitting Gaussian head. It eliminates
local floaters through contribution-based pruning and mitigates overfitting to
limited views by leveraging depth and multi-view feature supervision with
differentiable camera parameters during rapid optimization. FSFSplatter
outperforms current state-of-the-art methods on widely used DTU and Replica.

</details>


### [13] [MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context](https://arxiv.org/abs/2510.02722)
*Junyu Shi,Yong Sun,Zhiyuan Zhang,Lijiang Liu,Zhengjie Zhang,Yuxin He,Qiang Nie*

Main category: cs.CV

TL;DR: 提出MoGIC框架，结合意图建模与视觉先验，提升文本驱动运动生成的可控性与精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉动作因果逻辑和人类意图方面受限，且缺乏视觉 grounding 导致生成细节不足。

Method: 提出MoGIC框架，联合优化多模态运动生成与意图预测，引入注意力混合机制实现条件令牌与运动子序列的局部对齐，并构建Mo440H数据集支持训练。

Result: 在HumanML3D和Mo440H上微调后，FID分别降低38.6%和34.6%，在运动描述任务上超越基于大语言模型的方法，并支持意图预测与视觉条件生成。

Conclusion: MoGIC通过引入意图建模和视觉先验，显著提升多模态运动生成的质量与可控性，推动动作生成与意图理解的发展。

Abstract: Existing text-driven motion generation methods often treat synthesis as a
bidirectional mapping between language and motion, but remain limited in
capturing the causal logic of action execution and the human intentions that
drive behavior. The absence of visual grounding further restricts precision and
personalization, as language alone cannot specify fine-grained spatiotemporal
details. We propose MoGIC, a unified framework that integrates intention
modeling and visual priors into multimodal motion synthesis. By jointly
optimizing multimodal-conditioned motion generation and intention prediction,
MoGIC uncovers latent human goals, leverages visual priors to enhance
generation, and exhibits versatile multimodal generative capability. We further
introduce a mixture-of-attention mechanism with adaptive scope to enable
effective local alignment between conditional tokens and motion subsequences.
To support this paradigm, we curate Mo440H, a 440-hour benchmark from 21
high-quality motion datasets. Experiments show that after finetuning, MoGIC
reduces FID by 38.6\% on HumanML3D and 34.6\% on Mo440H, surpasses LLM-based
methods in motion captioning with a lightweight text head, and further enables
intention prediction and vision-conditioned generation, advancing controllable
motion synthesis and intention understanding. The code is available at
https://github.com/JunyuShi02/MoGIC

</details>


### [14] [From Tokens to Nodes: Semantic-Guided Motion Control for Dynamic 3D Gaussian Splatting](https://arxiv.org/abs/2510.02732)
*Jianing Chen,Zehao Li,Yujun Cai,Hao Jiang,Shuqin Gao,Honglong Zhao,Tianlu Mao,Yucheng Zhang*

Main category: cs.CV

TL;DR: 提出一种运动自适应的动态3D重建框架，通过语义与运动先验动态分配控制点密度，并采用样条轨迹参数化实现更高效、稳定的单目视频3D重建。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏控制方法仅基于几何分配控制点，导致静态区域冗余、动态区域不足，难以匹配实际运动复杂度，影响重建质量与效率。

Method: 利用视觉基础模型提取语义与运动先验，建立patch-token-node对应关系；通过运动趋势评分与迭代体素化实现运动自适应的控制点压缩；引入基于2D轨迹初始化的样条轨迹参数化，替代MLP形变场以获得更平滑的运动表示。

Result: 在多个实验中显著优于当前最先进方法，提升了重建质量与计算效率。

Conclusion: 该方法有效解决了控制点分配与运动复杂度不匹配的问题，实现了更灵活、稳定且高效的动态3D重建。

Abstract: Dynamic 3D reconstruction from monocular videos remains difficult due to the
ambiguity inferring 3D motion from limited views and computational demands of
modeling temporally varying scenes. While recent sparse control methods
alleviate computation by reducing millions of Gaussians to thousands of control
points, they suffer from a critical limitation: they allocate points purely by
geometry, leading to static redundancy and dynamic insufficiency. We propose a
motion-adaptive framework that aligns control density with motion complexity.
Leveraging semantic and motion priors from vision foundation models, we
establish patch-token-node correspondences and apply motion-adaptive
compression to concentrate control points in dynamic regions while suppressing
redundancy in static backgrounds. Our approach achieves flexible
representational density adaptation through iterative voxelization and motion
tendency scoring, directly addressing the fundamental mismatch between control
point allocation and motion complexity. To capture temporal evolution, we
introduce spline-based trajectory parameterization initialized by 2D tracklets,
replacing MLP-based deformation fields to achieve smoother motion
representation and more stable optimization. Extensive experiments demonstrate
significant improvements in reconstruction quality and efficiency over existing
state-of-the-art methods.

</details>


### [15] [Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction](https://arxiv.org/abs/2510.03117)
*Kaisi Guan,Xihua Wang,Zhengfeng Lai,Xin Cheng,Peng Zhang,XiaoJiang Liu,Ruihua Song,Meng Cao*

Main category: cs.CV

TL;DR: 本文提出了Text-to-Sounding-Video生成的新方法，通过分层视觉接地字幕和双塔扩散Transformer实现音视频与文本的对齐生成。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到音视频生成中字幕干扰和跨模态交互机制不明确的问题。

Method: 提出HVGC框架生成解耦的视频和音频字幕，并设计带有双交叉注意力机制的BridgeDiT模型实现音视频同步生成。

Result: 在三个基准数据集上取得了最先进的性能，人类评估和消融实验验证了方法的有效性。

Conclusion: 所提方法有效解决了模态干扰和跨模态同步问题，为T2SV任务提供了新的思路和技术路径。

Abstract: This study focuses on a challenging yet promising task,
Text-to-Sounding-Video (T2SV) generation, which aims to generate a video with
synchronized audio from text conditions, meanwhile ensuring both modalities are
aligned with text. Despite progress in joint audio-video training, two critical
challenges still remain unaddressed: (1) a single, shared text caption where
the text for video is equal to the text for audio often creates modal
interference, confusing the pretrained backbones, and (2) the optimal mechanism
for cross-modal feature interaction remains unclear. To address these
challenges, we first propose the Hierarchical Visual-Grounded Captioning (HVGC)
framework that generates pairs of disentangled captions, a video caption, and
an audio caption, eliminating interference at the conditioning stage. Based on
HVGC, we further introduce BridgeDiT, a novel dual-tower diffusion transformer,
which employs a Dual CrossAttention (DCA) mechanism that acts as a robust
``bridge" to enable a symmetric, bidirectional exchange of information,
achieving both semantic and temporal synchronization. Extensive experiments on
three benchmark datasets, supported by human evaluations, demonstrate that our
method achieves state-of-the-art results on most metrics. Comprehensive
ablation studies further validate the effectiveness of our contributions,
offering key insights for the future T2SV task. All the codes and checkpoints
will be publicly released.

</details>


### [16] [Net2Net: When Un-trained Meets Pre-trained Networks for Robust Real-World Denoising](https://arxiv.org/abs/2510.02733)
*Weimin Yuan,Cai Meng*

Main category: cs.CV

TL;DR: 本文提出了一种结合非训练网络和预训练网络的新方法Net2Net，通过正则化去噪框架融合DIP与DRUNet，在真实噪声去除任务中表现出优异性能，尤其在训练数据有限时具有强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统去噪方法依赖手工先验，难以应对真实噪声的复杂性和多样性；基于深度学习的方法依赖大量标注数据，泛化性差。本文旨在结合无监督与有监督方法的优势，提升真实噪声去除的性能和适应性。

Method: 提出Net2Net方法，将无监督的深度图像先验（DIP）与有监督的预训练模型DRUNet结合，采用正则化由去噪（RED）框架进行融合：DIP适应每幅输入图像的独特噪声特征，DRUNet提供从大数据中学到的鲁棒表示。

Result: 在多个基准数据集上进行了广泛实验，结果表明Net2Net在真实噪声去除任务中优于现有方法，尤其在训练数据有限的情况下表现出更强的泛化能力和去噪性能。

Conclusion: Net2Net通过结合无监督与有监督网络，有效解决了真实噪声去除中数据标注需求高和泛化能力弱的问题，为图像去噪提供了一种高效且灵活的混合框架。

Abstract: Traditional denoising methods for noise removal have largely relied on
handcrafted priors, often perform well in controlled environments but struggle
to address the complexity and variability of real noise. In contrast, deep
learning-based approaches have gained prominence for learning noise
characteristics from large datasets, but these methods frequently require
extensive labeled data and may not generalize effectively across diverse noise
types and imaging conditions. In this paper, we present an innovative method,
termed as Net2Net, that combines the strengths of untrained and pre-trained
networks to tackle the challenges of real-world noise removal. The innovation
of Net2Net lies in its combination of unsupervised DIP and supervised
pre-trained model DRUNet by regularization by denoising (RED). The untrained
network adapts to the unique noise characteristics of each input image without
requiring labeled data, while the pre-trained network leverages learned
representations from large-scale datasets to deliver robust denoising
performance. This hybrid framework enhances generalization across varying noise
patterns and improves performance, particularly in scenarios with limited
training data. Extensive experiments on benchmark datasets demonstrate the
superiority of our method for real-world noise removal.

</details>


### [17] [Retrv-R1: A Reasoning-Driven MLLM Framework for Universal and Efficient Multimodal Retrieval](https://arxiv.org/abs/2510.02745)
*Lanyun Zhu,Deyi Ji,Tianrun Chen,Haiyang Wu,Shiqi Wang*

Main category: cs.CV

TL;DR: Retrv-R1是首个采用R1架构的多模态大语言模型，专为通用检索任务设计，通过引入信息压缩模块和新的训练范式，在提升推理效率和检索精度方面达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 受DeepSeek-R1启发，探索强化学习在多模态检索任务中提升LLM推理能力的潜力，但发现直接套用其方法在计算开销和训练稳定性上不可行，需针对性优化。

Method: 提出Retrv-R1，包含信息压缩模块（带细节检查机制）以减少token消耗并保留关键信息；设计新训练范式：先使用定制的合成CoT数据激活模型，再通过课程式强化学习进行优化。

Result: 在多个检索基准和任务上实现SOTA性能，具备高效率和强泛化能力，显著优于直接应用RL方法的基线模型。

Conclusion: Retrv-R1通过结构与训练协同创新，有效解决了多模态检索中推理开销大和训练不稳定的问题，验证了R1架构在检索场景的可行性与优越性。

Abstract: The success of DeepSeek-R1 demonstrates the immense potential of using
reinforcement learning (RL) to enhance LLMs' reasoning capabilities. This paper
introduces Retrv-R1, the first R1-style MLLM specifically designed for
multimodal universal retrieval, achieving higher performance by employing
step-by-step reasoning to produce more accurate retrieval results. We find that
directly applying the methods of DeepSeek-R1 to retrieval tasks is not
feasible, mainly due to (1) the high computational cost caused by the large
token consumption required for multiple candidates with reasoning processes,
and (2) the instability and suboptimal results when directly applying RL to
train for retrieval tasks. To address these issues, Retrv-R1 introduces an
information compression module with a details inspection mechanism, which
enhances computational efficiency by reducing the number of tokens while
ensuring that critical information for challenging candidates is preserved.
Furthermore, a new training paradigm is proposed, including an activation stage
using a retrieval-tailored synthetic CoT dataset for more effective
optimization, followed by RL with a novel curriculum reward to improve both
performance and efficiency. Incorporating these novel designs, Retrv-R1
achieves SOTA performance, high efficiency, and strong generalization ability,
as demonstrated by experiments across multiple benchmarks and tasks.

</details>


### [18] [Bayesian Test-time Adaptation for Object Recognition and Detection with Vision-language Models](https://arxiv.org/abs/2510.02750)
*Lihua Zhou,Mao Ye,Shuaifeng Li,Nianxin Li,Jinlin Wu,Xiatian Zhu,Lei Deng,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.CV

TL;DR: 提出BCA+，一种无需训练的测试时自适应框架，通过动态缓存和贝叶斯推断统一提升视觉语言模型在物体识别与检测中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有测试时自适应方法依赖反向传播（计算昂贵）或仅调整似然，忽视先验知识的作用，导致在真实分布偏移下性能不足。

Method: 构建动态缓存机制，存储并更新类别嵌入、空间尺度和自适应类别先验；将适应过程建模为贝叶斯推断，融合缓存的似然与先验，并结合不确定性引导融合策略。

Result: 在多个识别与检测基准上实现最先进的性能，且无需训练、不依赖反向传播，推理高效。

Conclusion: BCA+通过联合优化似然与先验，有效提升VLM在分布偏移下的鲁棒性，为测试时适应提供统一、高效的解决方案。

Abstract: Vision-language models (VLMs) such as CLIP and Grounding DINO have achieved
remarkable success in object recognition and detection. However, their
performance often degrades under real-world distribution shifts. Test-time
adaptation (TTA) aims to mitigate this issue by adapting models during
inference. Existing methods either rely on computationally expensive
backpropagation, which hinders real-time deployment, or focus solely on
likelihood adaptation, which overlooks the critical role of the prior. Our
prior work, Bayesian Class Adaptation (BCA), addressed these shortcomings for
object recognition by introducing a training-free framework that incorporates
adaptive priors. Building upon this foundation, we now present Bayesian Class
Adaptation plus (BCA+), a unified, training-free framework for TTA for both
object recognition and detection. BCA+ introduces a dynamic cache that
adaptively stores and updates class embeddings, spatial scales (for detection),
and, crucially, adaptive class priors derived from historical predictions. We
formulate adaptation as a Bayesian inference problem, where final predictions
are generated by fusing the initial VLM output with a cache-based prediction.
This cache-based prediction combines a dynamically updated likelihood
(measuring feature and scale similarity) and a prior (reflecting the evolving
class distribution). This dual-adaptation mechanism, coupled with
uncertainty-guided fusion, enables BCA+ to correct both the model's semantic
understanding and its contextual confidence. As a training-free method
requiring no backpropagation, BCA+ is highly efficient. Extensive experiments
demonstrate that BCA+ achieves state-of-the-art performance on both recognition
and detection benchmarks.

</details>


### [19] [Hierarchical Generalized Category Discovery for Brain Tumor Classification in Digital Pathology](https://arxiv.org/abs/2510.02760)
*Matthias Perkonigg,Patrick Rockenschaub,Georg Göbel,Adelheid Wöhrer*

Main category: cs.CV

TL;DR: 提出了一种新的层次化广义类别发现方法HGCD-BT，用于脑肿瘤分类，结合对比学习与半监督层次聚类损失，在识别新肿瘤类型上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有脑肿瘤分类方法局限于预定义类别，难以识别训练中未见的肿瘤类型；无监督与半监督方法无法有效结合先验知识并发现未知类别，需一种能同时处理已知与未知类别的新方法。

Method: 提出HGCD-BT，结合层次聚类与对比学习，引入新的半监督层次聚类损失，在OpenSRH和Digital Brain Tumor Atlas数据集上进行patch级和slide级分类。

Result: 在OpenSRH数据集上，patch级分类准确率比当前最优GCD方法提升28%，尤其在识别新肿瘤类别上表现优异；在Digital Brain Tumor Atlas上也表现出良好泛化能力。

Conclusion: HGCD-BT能有效发现脑肿瘤中的未知类别，具有良好的跨模态泛化能力，为术中肿瘤分类提供了更灵活、准确的解决方案。

Abstract: Accurate brain tumor classification is critical for intra-operative decision
making in neuro-oncological surgery. However, existing approaches are
restricted to a fixed set of predefined classes and are therefore unable to
capture patterns of tumor types not available during training. Unsupervised
learning can extract general-purpose features, but it lacks the ability to
incorporate prior knowledge from labelled data, and semi-supervised methods
often assume that all potential classes are represented in the labelled data.
Generalized Category Discovery (GCD) aims to bridge this gap by categorizing
both known and unknown classes within unlabelled data. To reflect the
hierarchical structure of brain tumor taxonomies, in this work, we introduce
Hierarchical Generalized Category Discovery for Brain Tumor Classification
(HGCD-BT), a novel approach that integrates hierarchical clustering with
contrastive learning. Our method extends contrastive learning based GCD by
incorporating a novel semi-supervised hierarchical clustering loss. We evaluate
HGCD-BT on OpenSRH, a dataset of stimulated Raman histology brain tumor images,
achieving a +28% improvement in accuracy over state-of-the-art GCD methods for
patch-level classification, particularly in identifying previously unseen tumor
categories. Furthermore, we demonstrate the generalizability of HGCD-BT on
slide-level classification of hematoxylin and eosin stained whole-slide images
from the Digital Brain Tumor Atlas, confirming its utility across imaging
modalities.

</details>


### [20] [AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding](https://arxiv.org/abs/2510.02778)
*Xian Zhang,Zexi Wu,Zinuo Li,Hongming Xu,Luqi Gong,Farid Boussaid,Naoufel Werghi,Mohammed Bennamoun*

Main category: cs.CV

TL;DR: 提出了一种无需训练的查询驱动视频关键帧采样方法AdaRD-Key，通过联合优化相关性和多样性，在长视频理解中实现了高效、准确的关键帧选择。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理长视频时，由于采用均匀采样或固定时序间隔的关键帧选择策略，容易忽略关键瞬间或短时细粒度线索，且难以兼顾查询相关性与视觉多样性。

Method: 提出AdaRD-Key，最大化一个统一的相关性-多样性最大体积（RD-MV）目标函数，结合查询条件下的相关性得分和基于log-determinant的多样性项；引入轻量级的相关性感知门控机制，在相关性弱时自动切换到仅多样性模式。

Result: 在LongVideoBench和Video-MME等基准上取得最先进性能，尤其在长视频任务中表现突出，且计算效率高，可在单GPU上实时运行。

Conclusion: AdaRD-Key是一种无需训练、高效、即插即用的关键帧采样模块，有效提升了视觉语言模型在长视频理解中的性能。

Abstract: Understanding long-form videos remains a significant challenge for
vision--language models (VLMs) due to their extensive temporal length and high
information density. Most current multimodal large language models (MLLMs) rely
on uniform sampling, which often overlooks critical moments, leading to
incorrect responses to queries. In parallel, many keyframe selection approaches
impose rigid temporal spacing: once a frame is chosen, an exclusion window
suppresses adjacent timestamps to reduce redundancy. While effective at
limiting overlap, this strategy frequently misses short, fine-grained cues near
important events. Other methods instead emphasize visual diversity but neglect
query relevance. We propose AdaRD-Key, a training-free keyframe sampling module
for query-driven long-form video understanding. AdaRD-Key maximizes a unified
Relevance--Diversity Max-Volume (RD-MV) objective, combining a
query-conditioned relevance score with a log-determinant diversity component to
yield informative yet non-redundant frames. To handle broad queries with weak
alignment to the video, AdaRD-Key employs a lightweight relevance-aware gating
mechanism; when the relevance distribution indicates weak alignment, the method
seamlessly shifts into a diversity-only mode, enhancing coverage without
additional supervision. Our pipeline is training-free, computationally
efficient (running in real time on a single GPU), and compatible with existing
VLMs in a plug-and-play manner. Extensive experiments on LongVideoBench and
Video-MME demonstrate state-of-the-art performance, particularly on long-form
videos. Code available at https://github.com/Xian867/AdaRD-Key.

</details>


### [21] [Reasoning Riddles: How Explainability Reveals Cognitive Limits in Vision-Language Models](https://arxiv.org/abs/2510.02780)
*Prahitha Movva*

Main category: cs.CV

TL;DR: 本文研究了视觉语言模型（VLMs）在复杂横向思维挑战（如字谜谜题）中的认知过程，提出一个包含221个字谜的标注数据集和评估框架，揭示了不同提示策略对模型推理过程和解题效果的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在多模态任务中表现出色，但在字谜等需要横向思维的任务上表现不佳，且其推理过程和失败模式尚不明确，本文旨在通过可解释性分析深入理解其内在认知机制。

Method: 构建了一个涵盖六类认知类别的221个字谜的系统标注数据集，设计了三种提示策略以激发不同的解释性推理过程，并采用将推理质量与答案正确性分离的评估框架进行分析。

Result: 发现模型在不同类别上的推理质量差异显著：在视觉组合方面有系统性优势，但在缺失解释和文化象征理解方面存在根本性缺陷；提示策略显著影响模型的认知路径和解题效果。

Conclusion: 可解释性应被视为模型性能的组成部分而非事后补充，提示策略的设计对提升VLM在复杂推理任务中的表现具有关键作用。

Abstract: Vision-Language Models (VLMs) excel at many multimodal tasks, yet their
cognitive processes remain opaque on complex lateral thinking challenges like
rebus puzzles. While recent work has demonstrated these models struggle
significantly with rebus puzzle solving, the underlying reasoning processes and
failure patterns remain largely unexplored. We address this gap through a
comprehensive explainability analysis that moves beyond performance metrics to
understand how VLMs approach these complex lateral thinking challenges. Our
study contributes a systematically annotated dataset of 221 rebus puzzles
across six cognitive categories, paired with an evaluation framework that
separates reasoning quality from answer correctness. We investigate three
prompting strategies designed to elicit different types of explanatory
processes and reveal critical insights into VLM cognitive processes. Our
findings demonstrate that reasoning quality varies dramatically across puzzle
categories, with models showing systematic strengths in visual composition
while exhibiting fundamental limitations in absence interpretation and cultural
symbolism. We also discover that prompting strategy substantially influences
both cognitive approach and problem-solving effectiveness, establishing
explainability as an integral component of model performance rather than a
post-hoc consideration.

</details>


### [22] [OTR: Synthesizing Overlay Text Dataset for Text Removal](https://arxiv.org/abs/2510.02787)
*Jan Zdenek,Wataru Shimoda,Kota Yamaguchi*

Main category: cs.CV

TL;DR: 提出一种新的文本移除基准数据集构建方法，通过对象感知的文本放置和视觉-语言模型生成内容，解决现有数据集存在的真值缺陷和泛化能力不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本移除数据集（如SCUT-EnsText）存在手工编辑导致的真值伪影、背景过于简单等问题，限制了模型在域外的泛化能力和评估准确性。

Method: 采用对象感知的文本渲染和视觉-语言模型生成文本内容，在复杂背景下合成高质量的文本移除样本，确保干净的真值和更具挑战性的移除场景。

Result: 构建了一个适用于非场景文本领域的文本移除基准数据集，并已公开发布（https://huggingface.co/datasets/cyberagent/OTR）。

Conclusion: 该方法能够有效提升文本移除任务的评估质量和模型泛化能力，为未来研究提供了更可靠的数据基础。

Abstract: Text removal is a crucial task in computer vision with applications such as
privacy preservation, image editing, and media reuse. While existing research
has primarily focused on scene text removal in natural images, limitations in
current datasets hinder out-of-domain generalization or accurate evaluation. In
particular, widely used benchmarks such as SCUT-EnsText suffer from ground
truth artifacts due to manual editing, overly simplistic text backgrounds, and
evaluation metrics that do not capture the quality of generated results. To
address these issues, we introduce an approach to synthesizing a text removal
benchmark applicable to domains other than scene texts. Our dataset features
text rendered on complex backgrounds using object-aware placement and
vision-language model-generated content, ensuring clean ground truth and
challenging text removal scenarios. The dataset is available at
https://huggingface.co/datasets/cyberagent/OTR .

</details>


### [23] [Align Your Query: Representation Alignment for Multimodality Medical Object Detection](https://arxiv.org/abs/2510.02789)
*Ara Seo,Bryan Sangwoo Kim,Hyungjin Chung,Jong Chul Ye*

Main category: cs.CV

TL;DR: 提出了一种简单且检测器无关的框架，通过模态对齐增强多模态医学目标检测，提升性能且几乎无额外开销。


<details>
  <summary>Details</summary>
Motivation: 单一检测器在混合多种医学影像模态（如CXR、CT、MRI）时性能下降，因不同模态间统计异质性和表示空间不一致。需要一种能统一表示空间的方法。

Method: 引入轻量级的模态标记（modality tokens），并通过多模态上下文注意力（MoCA）将其融合到DETR式的目标查询中；设计QueryREPA预训练阶段，使用任务特定的对比学习目标对齐查询与模态标记。

Result: 在多种医学影像模态联合训练下，该方法显著提升检测AP，几乎不增加延迟，且无需修改检测器架构。

Conclusion: MoCA与QueryREPA协同可生成模态感知、类别一致的查询，为鲁棒的多模态医学目标检测提供了实用解决方案。

Abstract: Medical object detection suffers when a single detector is trained on mixed
medical modalities (e.g., CXR, CT, MRI) due to heterogeneous statistics and
disjoint representation spaces. To address this challenge, we turn to
representation alignment, an approach that has proven effective for bringing
features from different sources into a shared space. Specifically, we target
the representations of DETR-style object queries and propose a simple,
detector-agnostic framework to align them with modality context. First, we
define modality tokens: compact, text-derived embeddings encoding imaging
modality that are lightweight and require no extra annotations. We integrate
the modality tokens into the detection process via Multimodality Context
Attention (MoCA), mixing object-query representations via self-attention to
propagate modality context within the query set. This preserves DETR-style
architectures and adds negligible latency while injecting modality cues into
object queries. We further introduce QueryREPA, a short pretraining stage that
aligns query representations to their modality tokens using a task-specific
contrastive objective with modality-balanced batches. Together, MoCA and
QueryREPA produce modality-aware, class-faithful queries that transfer
effectively to downstream training. Across diverse modalities trained
altogether, the proposed approach consistently improves AP with minimal
overhead and no architectural modifications, offering a practical path toward
robust multimodality medical object detection. Project page:
https://araseo.github.io/alignyourquery/.

</details>


### [24] [MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding](https://arxiv.org/abs/2510.02790)
*Jingyuan Deng,Yujiu Yang*

Main category: cs.CV

TL;DR: 提出了一种名为MaskCD的图像头掩码对比解码方法，有效缓解大规模视觉语言模型中的幻觉问题，同时保持模型的通用能力。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉语言模型（LVLMs）在多模态任务中表现出色，但存在生成内容与输入视觉或文本矛盾的“幻觉”问题，现有方法在构建对比样本或稳定性方面存在不足。

Method: 利用LVLM中的“图像头”，通过掩码图像头来构建对比样本，实现对比解码，从而抑制幻觉生成。

Result: 在LLaVA-1.5-7b和Qwen-VL-7b上评估，MaskCD在CHAIR、POPE、AMBER和MME等多个基准上显著减少幻觉，同时保持模型性能。

Conclusion: MaskCD是一种有效且稳定的缓解LVLM幻觉的方法，具有良好的应用潜力。

Abstract: Large vision-language models (LVLMs) have shown remarkable performance in
visual-language understanding for downstream multimodal tasks. While their
capabilities are improving, problems emerge simultaneously. Among those
problems, the hallucinations have attracted much attention, which stands for
the phenomenon where LVLMs generate contradictory content to their input visual
and text contents. Many approaches have been proposed to deal with this issue,
such as contrastive decoding and attention manipulation. However, contrastive
decoding methods struggle in constructing appropriate contrastive samples, and
attention manipulation methods are highly sensitive, lacking stability. In this
work, we propose image head Masked Contrastive Decoding (MaskCD). Our approach
utilizes the "image heads" in LVLMs, masking them to construct contrastive
samples for contrastive decoding. We evaluated MaskCD on LLaVA-1.5-7b and
Qwen-VL-7b, using various benchmarks such as CHAIR, POPE, AMBER and MME. The
results demonstrate that MaskCD effectively alleviates the phenomenon of
hallucinations and retains the general capabilities of LVLMs. Corresponding
resources could be found at: https://github.com/Deng-Jingyuan/MaskCD .

</details>


### [25] [VERNIER: an open-source software pushing marker pose estimation down to the micrometer and nanometer scales](https://arxiv.org/abs/2510.02791)
*Patrick Sandoz,Antoine N. André,Guillaume J. Laurent*

Main category: cs.CV

TL;DR: 本文介绍了VERNIER，一种开源的基于相位处理的伪周期性图案姿态测量软件，能够实现快速、可靠的位姿估计，具有较强的抗噪、抗离焦和遮挡能力。


<details>
  <summary>Details</summary>
Motivation: 小尺度下的姿态估计仍具挑战性，现有方法难以在大范围内同时实现纳米级位移和微弧度级旋转的高精度测量，因此需要一种鲁棒且高性能的解决方案。

Method: 提出并开发了VERNIER软件，采用基于相位的局部阈值算法，对伪周期性图案进行相位处理，结合不同的图案设计实现厘米级测量范围和纳米级分辨率。

Result: VERNIER在合成与实验图像上验证有效，具有高噪声、离焦和遮挡鲁棒性，并支持不同应用需求的图案类型选择与参数配置。

Conclusion: VERNIER提供了一种灵活、开放的姿态测量工具，通过合理选择图案设计和显微镜放大倍数，可满足多种微观测量场景的性能需求。

Abstract: Pose estimation is still a challenge at the small scales. Few solutions exist
to capture the 6 degrees of freedom of an object with nanometric and
microradians resolutions over relatively large ranges. Over the years, we have
proposed several fiducial marker and pattern designs to achieve reliable
performance for various microscopy applications. Centimeter ranges are possible
using pattern encoding methods, while nanometer resolutions can be achieved
using phase processing of the periodic frames. This paper presents VERNIER, an
open source phase processing software designed to provide fast and reliable
pose measurement based on pseudo-periodic patterns. Thanks to a phase-based
local thresholding algorithm, the software has proven to be particularly robust
to noise, defocus and occlusion. The successive steps of the phase processing
are presented, as well as the different types of patterns that address
different application needs. The implementation procedure is illustrated with
synthetic and experimental images. Finally, guidelines are given for selecting
the appropriate pattern design and microscope magnification lenses as a
function of the desired performance.

</details>


### [26] [Med-K2N: Flexible K-to-N Modality Translation for Medical Image Synthesis](https://arxiv.org/abs/2510.02815)
*Feng Yuan,Yifan Gao,Yuehua Ye,Haoyue Li,Xin Gao*

Main category: cs.CV

TL;DR: 本文提出Med-K2N模型，用于解决K到N的医学图像模态生成问题，通过自适应加权、质量过滤和因果模态一致性机制，实现高质量、多输出、临床契合的跨模态医学图像合成。


<details>
  <summary>Details</summary>
Motivation: 临床需求推动灵活的多模态医学图像重建，现有方法难以建模不同模态对目标任务的异质贡献、控制融合质量并保持多输出下的模态一致性，因此需探索更符合临床工作流的K到N生成方法。

Method: 受SAM2和临床渐进式信息整合启发，将多模态数据视为带选择机制的序列帧；设计PreWeightNet、ThresholdNet和EffiWeightNet三个协同模块实现全局贡献评估、自适应过滤与有效加权；引入因果模态身份模块（CMIM），利用视觉-语言建模建立生成图像与目标模态描述间的因果约束以保持身份一致性。

Result: 在多个基准上，Med-K2N显著优于现有最先进方法，验证了其在跨模态医学图像合成中的有效性与优越性。

Conclusion: Med-K2N通过学习模态-任务自适应权重、记忆有益融合模式并施加因果一致性约束，有效解决了多模态医学图像生成中的贡献异质性、融合质量控制和模态身份一致性问题，具有良好的临床应用潜力。

Abstract: Cross-modal medical image synthesis research focuses on reconstructing
missing imaging modalities from available ones to support clinical diagnosis.
Driven by clinical necessities for flexible modality reconstruction, we explore
K to N medical generation, where three critical challenges emerge: How can we
model the heterogeneous contributions of different modalities to various target
tasks? How can we ensure fusion quality control to prevent degradation from
noisy information? How can we maintain modality identity consistency in
multi-output generation? Driven by these clinical necessities, and drawing
inspiration from SAM2's sequential frame paradigm and clinicians' progressive
workflow of incrementally adding and selectively integrating multi-modal
information, we treat multi-modal medical data as sequential frames with
quality-driven selection mechanisms. Our key idea is to "learn" adaptive
weights for each modality-task pair and "memorize" beneficial fusion patterns
through progressive enhancement. To achieve this, we design three collaborative
modules: PreWeightNet for global contribution assessment, ThresholdNet for
adaptive filtering, and EffiWeightNet for effective weight computation.
Meanwhile, to maintain modality identity consistency, we propose the Causal
Modality Identity Module (CMIM) that establishes causal constraints between
generated images and target modality descriptions using vision-language
modeling. Extensive experimental results demonstrate that our proposed Med-K2N
outperforms state-of-the-art methods by significant margins on multiple
benchmarks. Source code is available.

</details>


### [27] [ELMF4EggQ: Ensemble Learning with Multimodal Feature Fusion for Non-Destructive Egg Quality Assessment](https://arxiv.org/abs/2510.02876)
*Md Zahim Hassan,Md. Osama,Muhammad Ashad Kabir,Md. Saiful Islam,Zannatul Naim*

Main category: cs.CV

TL;DR: 本文提出了一种基于多模态特征融合的集成学习框架ELMF4EggQ，利用鸡蛋的外部图像、形状和重量信息实现对内部品质无损检测，并发布了首个相关公开数据集。


<details>
  <summary>Details</summary>
Motivation: 准确、无损地评估鸡蛋品质对于食品安全、产品质量控制及禽蛋生产效率至关重要；传统方法依赖破坏性检测，难以满足工业需求，因此需要一种非侵入式、高精度的智能检测方法。

Method: 构建包含186个褐壳鸡蛋的公开数据集，通过实验室专家评估确定等级与新鲜度；采用预训练CNN（ResNet152、DenseNet169、ResNet152V2）提取图像特征，结合形状与重量等结构特征，使用PCA降维、SMOTE数据增强，并通过多个机器学习模型分类，最后采用集成投票机制融合预测结果。

Result: 多模态集成方法在等级分类上达到86.57%准确率，在新鲜度预测上达到70.83%，显著优于仅用图像或仅用结构特征的基线方法。

Conclusion: 利用外部非侵入特征结合机器学习可有效预测鸡蛋内部品质，ELMF4EggQ为禽蛋工业提供了一种可行的无损检测方案，且代码与数据公开，促进后续研究复现与拓展。

Abstract: Accurate, non-destructive assessment of egg quality is critical for ensuring
food safety, maintaining product standards, and operational efficiency in
commercial poultry production. This paper introduces ELMF4EggQ, an ensemble
learning framework that employs multimodal feature fusion to classify egg grade
and freshness using only external attributes - image, shape, and weight. A
novel, publicly available dataset of 186 brown-shelled eggs was constructed,
with egg grade and freshness levels determined through laboratory-based expert
assessments involving internal quality measurements, such as yolk index and
Haugh unit. To the best of our knowledge, this is the first study to apply
machine learning methods for internal egg quality assessment using only
external, non-invasive features, and the first to release a corresponding
labeled dataset. The proposed framework integrates deep features extracted from
external egg images with structural characteristics such as egg shape and
weight, enabling a comprehensive representation of each egg. Image feature
extraction is performed using top-performing pre-trained CNN models (ResNet152,
DenseNet169, and ResNet152V2), followed by PCA-based dimensionality reduction,
SMOTE augmentation, and classification using multiple machine learning
algorithms. An ensemble voting mechanism combines predictions from the
best-performing classifiers to enhance overall accuracy. Experimental results
demonstrate that the multimodal approach significantly outperforms image-only
and tabular (shape and weight) only baselines, with the multimodal ensemble
approach achieving 86.57% accuracy in grade classification and 70.83% in
freshness prediction. All code and data are publicly available at
https://github.com/Kenshin-Keeps/Egg_Quality_Prediction_ELMF4EggQ, promoting
transparency, reproducibility, and further research in this domain.

</details>


### [28] [One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework](https://arxiv.org/abs/2510.02898)
*Lorenzo Bianchi,Giacomo Pacini,Fabio Carrara,Nicola Messina,Giuseppe Amato,Fabrizio Falchi*

Main category: cs.CV

TL;DR: 本文提出了一种名为\frameworkName{}的统一零样本图像描述框架，通过从图像中心范式转向图像块中心范式，利用图像块作为原子描述单元，无需区域级监督即可生成任意区域的描述。


<details>
  <summary>Details</summary>
Motivation: 现有零样本描述器主要依赖全局图像表示，仅限于整图描述，难以扩展到局部或非连续区域；本文旨在突破这一限制，实现更灵活、细粒度的零样本区域描述。

Method: 将图像的各个patch视为基本描述单元，利用如DINO等生成密集视觉特征的骨干网络提取patch级特征，通过聚合这些特征来描述任意区域，并在无需区域标注的情况下实现从单个patch到整图的描述生成。

Result: 实验表明，基于DINO等骨干网络的\frameworkName{}在零样本密集描述、区域集描述以及新提出的轨迹描述任务上优于现有基线和最先进方法。

Conclusion: patch级语义表示是实现可扩展、高质量零样本描述的关键，所提出的框架为细粒度视觉描述提供了有效且灵活的新范式。

Abstract: Zero-shot captioners are recently proposed models that utilize common-space
vision-language representations to caption images without relying on paired
image-text data. To caption an image, they proceed by textually decoding a
text-aligned image feature, but they limit their scope to global
representations and whole-image captions. We present \frameworkName{}, a
unified framework for zero-shot captioning that shifts from an image-centric to
a patch-centric paradigm, enabling the captioning of arbitrary regions without
the need of region-level supervision. Instead of relying on global image
representations, we treat individual patches as atomic captioning units and
aggregate them to describe arbitrary regions, from single patches to
non-contiguous areas and entire images. We analyze the key ingredients that
enable current latent captioners to work in our novel proposed framework.
Experiments demonstrate that backbones producing meaningful, dense visual
features, such as DINO, are key to achieving state-of-the-art performance in
multiple region-based captioning tasks. Compared to other baselines and
state-of-the-art competitors, our models achieve better performance on
zero-shot dense, region-set, and a newly introduced trace captioning task,
highlighting the effectiveness of patch-wise semantic representations for
scalable caption generation. Project page at https://paciosoft.com/Patch-ioner/ .

</details>


### [29] [Training-Free Out-Of-Distribution Segmentation With Foundation Models](https://arxiv.org/abs/2510.02909)
*Laith Nayal,Hadi Salloum,Ahmad Taha,Yaroslav Kholodov,Alexander Gasnikov*

Main category: cs.CV

TL;DR: 本文提出一种无需训练的方法，利用InternImage骨干网络特征，结合K均值聚类和解码头置信度阈值，检测语义分割中的未知物体，在RoadAnomaly和ADE-OoD基准上取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用（如自动驾驶）中检测语义分割中的未知物体至关重要。尽管视觉基础模型在闭集任务中表现优异，但其在分布外（OoD）区域检测方面的能力尚未被充分探索，本文旨在探究不依赖异常监督的基础模型是否能内在区分分布内与分布外区域。

Method: 提出一种无需训练的方法，使用InternImage骨干网络提取特征，结合K-Means对特征聚类，并在解码器原始logits上应用置信度阈值来识别OoD区域。

Result: 该方法在RoadAnomaly基准上达到50.02的平均精度，在ADE-OoD上达到48.77，优于多个有监督和无监督基线方法。

Conclusion: 研究表明，基于视觉基础模型的简单无训练方法在通用OoD语义分割任务中具有潜力，且无需额外数据或强假设。

Abstract: Detecting unknown objects in semantic segmentation is crucial for
safety-critical applications such as autonomous driving. Large vision
foundation models, includ- ing DINOv2, InternImage, and CLIP, have advanced
visual representation learn- ing by providing rich features that generalize
well across diverse tasks. While their strength in closed-set semantic tasks is
established, their capability to detect out- of-distribution (OoD) regions in
semantic segmentation remains underexplored. In this work, we investigate
whether foundation models fine-tuned on segmen- tation datasets can inherently
distinguish in-distribution (ID) from OoD regions without any outlier
supervision. We propose a simple, training-free approach that utilizes features
from the InternImage backbone and applies K-Means clustering alongside
confidence thresholding on raw decoder logits to identify OoD clusters. Our
method achieves 50.02 Average Precision on the RoadAnomaly benchmark and 48.77
on the benchmark of ADE-OoD with InternImage-L, surpassing several supervised
and unsupervised baselines. These results suggest a promising direc- tion for
generic OoD segmentation methods that require minimal assumptions or additional
data.

</details>


### [30] [Don't Just Chase "Highlighted Tokens" in MLLMs: Revisiting Visual Holistic Context Retention](https://arxiv.org/abs/2510.02912)
*Xin Zou,Di Lu,Yizhou Wang,Yibo Yan,Yuanhuiyi Lyu,Xu Zheng,Linfeng Zhang,Xuming Hu*

Main category: cs.CV

TL;DR: 提出HoloV，一种简单有效的即插即用视觉token剪枝框架，通过从整体视角自适应分配剪枝策略，保留全局视觉上下文，显著提升MLLM在高剪枝比下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力的视觉token剪枝方法倾向于保留语义相似的token，导致在高剪枝比例下性能显著下降，亟需更优的剪枝策略。

Method: 提出HoloV框架，摒弃传统的‘注意力优先’策略，从全局视角出发，自适应地在不同空间区域分配剪枝预算，确保保留的token能捕捉全局视觉上下文而非孤立显著特征。

Result: 在多种任务、MLLM架构和剪枝比例下，HoloV均优于现有最先进方法。例如，在剪枝88.9%视觉token后，配备HoloV的LLaVA1.5仍保持原始性能的95.8%。

Conclusion: HoloV通过整体性剪枝策略有效缓解了表征坍塌问题，在大幅降低计算开销的同时保持了高性能，实现了更优的效率-精度权衡。

Abstract: Despite their powerful capabilities, Multimodal Large Language Models (MLLMs)
suffer from considerable computational overhead due to their reliance on
massive visual tokens. Recent studies have explored token pruning to alleviate
this problem, which typically uses text-vision cross-attention or
[\texttt{CLS}] attention to assess and discard redundant visual tokens. In this
work, we identify a critical limitation of such attention-first pruning
approaches, i.e., they tend to preserve semantically similar tokens, resulting
in pronounced performance drops under high pruning ratios. To this end, we
propose {HoloV}, a simple yet effective, plug-and-play visual token pruning
framework for efficient inference. Distinct from previous attention-first
schemes, HoloV rethinks token retention from a holistic perspective. By
adaptively distributing the pruning budget across different spatial crops,
HoloV ensures that the retained tokens capture the global visual context rather
than isolated salient features. This strategy minimizes representational
collapse and maintains task-relevant information even under aggressive pruning.
Experimental results demonstrate that our HoloV achieves superior performance
across various tasks, MLLM architectures, and pruning ratios compared to SOTA
methods. For instance, LLaVA1.5 equipped with HoloV preserves 95.8\% of the
original performance after pruning 88.9\% of visual tokens, achieving superior
efficiency-accuracy trade-offs.

</details>


### [31] [Zero-Shot Robustness of Vision Language Models Via Confidence-Aware Weighting](https://arxiv.org/abs/2510.02913)
*Nikoo Naghavian,Mostafa Tavassolipour*

Main category: cs.CV

TL;DR: 提出了一种名为CAW（Confidence-Aware Weighting）的方法，通过置信度感知损失和特征对齐正则化来提升视觉-语言模型在零样本场景下的对抗鲁棒性，同时保持良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型（如CLIP）虽具有良好的零样本泛化能力，但对对抗攻击极为脆弱，亟需提升其鲁棒性。

Method: CAW包含两个部分：（1）置信度感知损失，通过缩放干净样本与对抗样本预测间的KL散度来关注不确定的对抗样本；（2）特征对齐正则化，最小化冻结与微调图像编码器在对抗输入上的特征距离，以保持语义一致性。

Result: 在TinyImageNet和另外14个数据集上的实验表明，CAW在强攻击（如AutoAttack）下优于PMG-AFT、TGA-ZSR等最新方法，且内存占用更低。

Conclusion: CAW能有效提升视觉-语言模型的零样本鲁棒性，同时兼顾干净样本准确率和泛化性能，是一种高效实用的对抗训练方法。

Abstract: Vision-language models like CLIP demonstrate impressive zero-shot
generalization but remain highly vulnerable to adversarial attacks. In this
work, we propose Confidence-Aware Weighting (CAW) to enhance zero-shot
robustness in vision-language models. CAW consists of two components: (1) a
Confidence-Aware loss that prioritizes uncertain adversarial examples by
scaling the KL divergence between clean and adversarial predictions, and (2) a
feature alignment regularization that preserves semantic consistency by
minimizing the distance between frozen and fine-tuned image encoder features on
adversarial inputs. These components work jointly to improve both clean and
robust accuracy without sacrificing generalization. Extensive experiments on
TinyImageNet and 14 additional datasets show that CAW outperforms recent
methods such as PMG-AFT and TGA-ZSR under strong attacks like AutoAttack, while
using less memory.

</details>


### [32] [Multimodal Carotid Risk Stratification with Large Vision-Language Models: Benchmarking, Fine-Tuning, and Clinical Insights](https://arxiv.org/abs/2510.02922)
*Daphne Tsolissou,Theofanis Ganitidis,Konstantinos Mitsis,Stergios CHristodoulidis,Maria Vakalopoulou,Konstantina Nikita*

Main category: cs.CV

TL;DR: 该研究探讨了大型视觉-语言模型（LVLM）在颈动脉斑块多模态评估中的应用，通过结合超声图像与临床、人口统计、实验室和蛋白质生物标志物数据进行中风风险分层。研究发现原始LVLM在风险分类上表现不佳，但通过对LLaVa-NeXT-Vicuna进行LoRA领域适应并融合文本形式的表格数据，显著提升了性能，达到与传统CNN模型相当的水平。


<details>
  <summary>Details</summary>
Motivation: 颈动脉粥样硬化疾病的风险评估在临床上具有挑战性，需整合多种异构数据且结果需对临床医生透明可解释。现有方法在多模态整合与可解释性方面存在局限，因此需要探索更先进的LVLM是否能胜任此类任务。

Method: 提出一种模拟真实诊断场景的问答式框架，比较多种开源LVLM（包括通用和医学调优模型）在超声图像与结构化临床数据融合下的表现；采用LoRA对LLaVa-NeXT-Vicuna进行超声领域适应，并将多模态表格数据以文本形式输入以增强模型性能。

Result: 零样本实验显示，现有LVLM难以准确识别成像模态和解剖结构，且在风险分类上普遍表现差；经LoRA微调后，模型在中风风险分层上显著提升；进一步融合文本化表格数据提高了特异性和平衡准确率，性能媲美在相同数据集上训练的CNN基线模型。

Conclusion: LVLM在超声心血管风险预测中兼具潜力与局限，成功的临床转化依赖于多模态数据整合、模型校准和领域适应。

Abstract: Reliable risk assessment for carotid atheromatous disease remains a major
clinical challenge, as it requires integrating diverse clinical and imaging
information in a manner that is transparent and interpretable to clinicians.
This study investigates the potential of state-of-the-art and recent large
vision-language models (LVLMs) for multimodal carotid plaque assessment by
integrating ultrasound imaging (USI) with structured clinical, demographic,
laboratory, and protein biomarker data. A framework that simulates realistic
diagnostic scenarios through interview-style question sequences is proposed,
comparing a range of open-source LVLMs, including both general-purpose and
medically tuned models. Zero-shot experiments reveal that even if they are very
powerful, not all LVLMs can accurately identify imaging modality and anatomy,
while all of them perform poorly in accurate risk classification. To address
this limitation, LLaVa-NeXT-Vicuna is adapted to the ultrasound domain using
low-rank adaptation (LoRA), resulting in substantial improvements in stroke
risk stratification. The integration of multimodal tabular data in the form of
text further enhances specificity and balanced accuracy, yielding competitive
performance compared to prior convolutional neural network (CNN) baselines
trained on the same dataset. Our findings highlight both the promise and
limitations of LVLMs in ultrasound-based cardiovascular risk prediction,
underscoring the importance of multimodal integration, model calibration, and
domain adaptation for clinical translation.

</details>


### [33] [Flip Distribution Alignment VAE for Multi-Phase MRI Synthesis](https://arxiv.org/abs/2510.02970)
*Xiaoyan Kui,Qianmu Xiao,Qqinsong Li,Zexin Ji,JIelin Zhang,Beiji Zou*

Main category: cs.CV

TL;DR: 提出了一种轻量化的特征解耦变分自编码器FDA-VAE，用于多期相增强MRI合成，有效分离共享与独立特征，显著降低参数量和推理时间，同时提升合成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多期相CE-MRI合成中使用深度自编码器，参数效率低且缺乏可解释的训练策略，难以有效分离共享和独立特征。

Method: 提出Flip Distribution Alignment VAE（FDA-VAE），通过将输入和目标图像编码为关于标准正态分布对称的两个潜在分布，实现特征解耦；采用Y形双向训练策略增强解耦可解释性。

Result: 相比现有端到端深度自编码器方法，FDA-VAE显著减少模型参数和推理时间，同时有效提升图像合成质量。

Conclusion: FDA-VAE是一种高效、轻量且可解释的多期相CE-MRI合成模型，为医学图像合成提供了新的特征解耦思路。

Abstract: Separating shared and independent features is crucial for multi-phase
contrast-enhanced (CE) MRI synthesis. However, existing methods use deep
autoencoder generators with low parameter efficiency and lack interpretable
training strategies. In this paper, we propose Flip Distribution Alignment
Variational Autoencoder (FDA-VAE), a lightweight feature-decoupled VAE model
for multi-phase CE MRI synthesis. Our method encodes input and target images
into two latent distributions that are symmetric concerning a standard normal
distribution, effectively separating shared and independent features. The
Y-shaped bidirectional training strategy further enhances the interpretability
of feature separation. Experimental results show that compared to existing deep
autoencoder-based end-to-end synthesis methods, FDA-VAE significantly reduces
model parameters and inference time while effectively improving synthesis
quality. The source code is publicly available at
https://github.com/QianMuXiao/FDA-VAE.

</details>


### [34] [TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency](https://arxiv.org/abs/2510.02987)
*Juntong Wang,Huiyu Duan,Jiarui Wang,Ziheng Jia,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 本文提出了LPG-Bench，一个用于评估长文本到图像生成的综合基准，以及一种新的零样本评估指标TIT，通过文本-图像-文本一致性来衡量生成图像与长文本提示的一致性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像模型在处理长而详细的提示时表现不佳，难以保持生成内容的一致性，且当前的自动评估指标与人类偏好一致性差，因此需要更有效的评估基准和指标。

Method: 构建包含200个平均超过250词的长提示的LPG-Bench基准，生成2600张图像并进行人工标注；提出基于文本-图像-文本一致性的TIT指标，包括TIT-Score和TIT-Score-LLM两种实例化方法。

Result: 实验表明，TIT指标与人类判断的一致性显著优于CLIP-score、LMM-score等基线方法，其中TIT-Score-LLM在成对准确率上比最强基线提高7.31%。

Conclusion: LPG-Bench和TIT为长提示文本到图像生成提供了更可靠的评估方案，有助于推动该领域的发展。

Abstract: With the rapid advancement of large multimodal models (LMMs), recent
text-to-image (T2I) models can generate high-quality images and demonstrate
great alignment to short prompts. However, they still struggle to effectively
understand and follow long and detailed prompts, displaying inconsistent
generation. To address this challenge, we introduce LPG-Bench, a comprehensive
benchmark for evaluating long-prompt-based text-to-image generation. LPG-Bench
features 200 meticulously crafted prompts with an average length of over 250
words, approaching the input capacity of several leading commercial models.
Using these prompts, we generate 2,600 images from 13 state-of-the-art models
and further perform comprehensive human-ranked annotations. Based on LPG-Bench,
we observe that state-of-the-art T2I alignment evaluation metrics exhibit poor
consistency with human preferences on long-prompt-based image generation. To
address the gap, we introduce a novel zero-shot metric based on
text-to-image-to-text consistency, termed TIT, for evaluating
long-prompt-generated images. The core concept of TIT is to quantify T2I
alignment by directly comparing the consistency between the raw prompt and the
LMM-produced description on the generated image, which includes an efficient
score-based instantiation TIT-Score and a large-language-model (LLM) based
instantiation TIT-Score-LLM. Extensive experiments demonstrate that our
framework achieves superior alignment with human judgment compared to
CLIP-score, LMM-score, etc., with TIT-Score-LLM attaining a 7.31% absolute
improvement in pairwise accuracy over the strongest baseline. LPG-Bench and TIT
methods together offer a deeper perspective to benchmark and foster the
development of T2I models. All resources will be made publicly available.

</details>


### [35] [Towards Scalable and Consistent 3D Editing](https://arxiv.org/abs/2510.02994)
*Ruihao Xia,Yang Tang,Pan Zhou*

Main category: cs.CV

TL;DR: 本文提出了3DEditVerse数据集和3DEditFormer模型，以解决3D编辑中的跨视图一致性、结构保真和精细控制难题，实现了无需3D掩码的精确、一致的3D编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的3D编辑方法存在速度慢、几何失真和依赖人工3D掩码等问题，缺乏高效、准确且实用的解决方案，因此需要一种更高效、无需人工干预的3D编辑方法。

Method: 在数据方面，构建了包含超过11万训练样本的3DEditVerse基准数据集；在模型方面，提出了3DEditFormer，一种保持3D结构的条件Transformer模型，通过双引导注意力和时间自适应门控机制实现对可编辑区域与保留结构的解耦。

Result: 实验证明，该方法在定量和定性上均优于现有最先进方法，实现了更精确、一致且无需3D掩码的3D编辑效果。

Conclusion: 3DEditFormer结合大规模高质量数据集3DEditVerse，为实用化和可扩展的3D编辑设定了新标准。

Abstract: 3D editing - the task of locally modifying the geometry or appearance of a 3D
asset - has wide applications in immersive content creation, digital
entertainment, and AR/VR. However, unlike 2D editing, it remains challenging
due to the need for cross-view consistency, structural fidelity, and
fine-grained controllability. Existing approaches are often slow, prone to
geometric distortions, or dependent on manual and accurate 3D masks that are
error-prone and impractical. To address these challenges, we advance both the
data and model fronts. On the data side, we introduce 3DEditVerse, the largest
paired 3D editing benchmark to date, comprising 116,309 high-quality training
pairs and 1,500 curated test pairs. Built through complementary pipelines of
pose-driven geometric edits and foundation model-guided appearance edits,
3DEditVerse ensures edit locality, multi-view consistency, and semantic
alignment. On the model side, we propose 3DEditFormer, a
3D-structure-preserving conditional transformer. By enhancing image-to-3D
generation with dual-guidance attention and time-adaptive gating, 3DEditFormer
disentangles editable regions from preserved structure, enabling precise and
consistent edits without requiring auxiliary 3D masks. Extensive experiments
demonstrate that our framework outperforms state-of-the-art baselines both
quantitatively and qualitatively, establishing a new standard for practical and
scalable 3D editing. Dataset and code will be released. Project:
https://www.lv-lab.org/3DEditFormer/

</details>


### [36] [Not every day is a sunny day: Synthetic cloud injection for deep land cover segmentation robustness evaluation across data sources](https://arxiv.org/abs/2510.03006)
*Sara Mobsite,Renaud Hostache,Laure Berti Equille,Emmanuel Roux,Joris Guerin*

Main category: cs.CV

TL;DR: 本文提出了一种云注入算法和一种轻量级归一化差异指数（NDI）注入方法，以改善云层覆盖下土地覆盖语义分割的性能，并验证了Sentinel-1雷达数据在光学图像被遮挡时的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有Sentinel-2数据集多为无云图像，限制了热带多云地区土地覆盖分割的应用，因此需要更贴近现实云覆盖场景的评估方法，并解决深度网络中因下采样导致的空间和光谱细节丢失问题。

Method: 开发了一种模拟真实云覆盖的云注入算法，用于评估云层影响；提出一种在解码末层注入归一化差异指数（NDI）的轻量方法，并融合Sentinel-1雷达数据以弥补光学图像的缺失。

Result: 在DFC2020数据集上，NDI注入使无云情况下U-Net和DeepLabV3性能分别提升1.99%和2.78%；在有云情况下，融合Sentinel-1数据显著提升各模型性能，优于仅使用光学数据的结果。

Conclusion: 雷达与光学数据融合能有效应对云遮挡问题，NDI注入方法可保留关键空间特征且计算开销小，提升了土地覆盖语义分割在复杂气象条件下的鲁棒性。

Abstract: Supervised deep learning for land cover semantic segmentation (LCS) relies on
labeled satellite data. However, most existing Sentinel-2 datasets are
cloud-free, which limits their usefulness in tropical regions where clouds are
common. To properly evaluate the extent of this problem, we developed a cloud
injection algorithm that simulates realistic cloud cover, allowing us to test
how Sentinel-1 radar data can fill in the gaps caused by cloud-obstructed
optical imagery. We also tackle the issue of losing spatial and/or spectral
details during encoder downsampling in deep networks. To mitigate this loss, we
propose a lightweight method that injects Normalized Difference Indices (NDIs)
into the final decoding layers, enabling the model to retain key spatial
features with minimal additional computation. Injecting NDIs enhanced land
cover segmentation performance on the DFC2020 dataset, yielding improvements of
1.99% for U-Net and 2.78% for DeepLabV3 on cloud-free imagery. Under
cloud-covered conditions, incorporating Sentinel-1 data led to significant
performance gains across all models compared to using optical data alone,
highlighting the effectiveness of radar-optical fusion in challenging
atmospheric scenarios.

</details>


### [37] [PocketSR: The Super-Resolution Expert in Your Pocket Mobiles](https://arxiv.org/abs/2510.03012)
*Haoze Sun,Linfeng Jiang,Fan Li,Renjing Pei,Zhixin Wang,Yong Guo,Jiaqi Xu,Haoyu Chen,Jin Han,Fenglong Song,Yujiu Yang,Wenbo Li*

Main category: cs.CV

TL;DR: PocketSR是一种超轻量级的单步图像超分辨率模型，专为边缘设备设计，在保持高性能的同时大幅减少计算成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的真实场景图像超分辨率方法虽然效果显著，但计算开销大，难以在边缘设备上部署。因此需要一种高效且实用的解决方案。

Method: 提出LiteED结构以替代SD中的VAE，减少97.5%的参数量；采用在线退火剪枝策略优化U-Net，并引入多层特征蒸馏损失以保留先验知识。

Result: PocketSR仅用1.46亿参数即可在0.8秒内处理4K图像，速度显著优于现有方法，且性能与最先进的单步乃至多步模型相当。

Conclusion: PocketSR在效率与精度之间实现了良好平衡，为边缘设备上的实时图像超分辨率提供了可行方案。

Abstract: Real-world image super-resolution (RealSR) aims to enhance the visual quality
of in-the-wild images, such as those captured by mobile phones. While existing
methods leveraging large generative models demonstrate impressive results, the
high computational cost and latency make them impractical for edge deployment.
In this paper, we introduce PocketSR, an ultra-lightweight, single-step model
that brings generative modeling capabilities to RealSR while maintaining high
fidelity. To achieve this, we design LiteED, a highly efficient alternative to
the original computationally intensive VAE in SD, reducing parameters by 97.5%
while preserving high-quality encoding and decoding. Additionally, we propose
online annealing pruning for the U-Net, which progressively shifts generative
priors from heavy modules to lightweight counterparts, ensuring effective
knowledge transfer and further optimizing efficiency. To mitigate the loss of
prior knowledge during pruning, we incorporate a multi-layer feature
distillation loss. Through an in-depth analysis of each design component, we
provide valuable insights for future research. PocketSR, with a model size of
146M parameters, processes 4K images in just 0.8 seconds, achieving a
remarkable speedup over previous methods. Notably, it delivers performance on
par with state-of-the-art single-step and even multi-step RealSR models, making
it a highly practical solution for edge-device applications.

</details>


### [38] [When and Where do Events Switch in Multi-Event Video Generation?](https://arxiv.org/abs/2510.03049)
*Ruotong Liao,Guowen Huang,Qing Cheng,Thomas Seidl,Daniel Cremers,Volker Tresp*

Main category: cs.CV

TL;DR: 本文提出了MEve，一个用于评估多事件文本到视频生成的自建提示套件，并系统研究了OpenSora和CogVideoX两类模型，揭示了在去噪早期阶段和分层模块中进行干预对多事件转换控制的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到视频生成方法在处理包含多个顺序事件的长视频时，缺乏对事件切换中内在因素的深入分析，尤其是多事件提示如何控制事件过渡的时间和位置。本文旨在探究这一关键问题。

Method: 提出MEve提示套件，用于系统评估多事件T2V生成；在OpenSora和CogVideoX两类代表性模型上进行大规模实验，分析不同去噪步骤和模型层对事件控制的影响。

Result: 实验证明，在去噪过程的早期阶段以及模块化模型层中进行干预，对实现有效的多事件转换至关重要，揭示了控制多事件生成的关键因素。

Conclusion: 多事件T2V生成的关键在于早期干预和分层控制，该发现为未来模型中实现更精确的多事件条件生成提供了方向。

Abstract: Text-to-video (T2V) generation has surged in response to challenging
questions, especially when a long video must depict multiple sequential events
with temporal coherence and controllable content. Existing methods that extend
to multi-event generation omit an inspection of the intrinsic factor in event
shifting. The paper aims to answer the central question: When and where
multi-event prompts control event transition during T2V generation. This work
introduces MEve, a self-curated prompt suite for evaluating multi-event
text-to-video (T2V) generation, and conducts a systematic study of two
representative model families, i.e., OpenSora and CogVideoX. Extensive
experiments demonstrate the importance of early intervention in denoising steps
and block-wise model layers, revealing the essential factor for multi-event
video generation and highlighting the possibilities for multi-event
conditioning in future models.

</details>


### [39] [InsideOut: An EfficientNetV2-S Based Deep Learning Framework for Robust Multi-Class Facial Emotion Recognition](https://arxiv.org/abs/2510.03066)
*Ahsan Farabi,Israt Khandaker,Ibrahim Khalil Shanto,Md Abdul Ahad Minhaz,Tanisha Zaman*

Main category: cs.CV

TL;DR: InsideOut 是一个基于 EfficientNetV2-S 的可复现面部情绪识别框架，通过迁移学习、强数据增强和类别不平衡感知优化，在 FER2013 数据集上实现了具有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 面部情绪识别（FER）在实际应用中面临遮挡、光照和姿态变化、类内差异小以及数据集不平衡等问题，尤其是少数情绪类别识别困难，现有方法在可复现性和实用性方面存在不足。

Method: 采用 EfficientNetV2-S 作为骨干网络，结合迁移学习；对 FER2013 数据进行标准化和分层划分，应用强数据增强，并使用类别加权损失函数微调轻量级分类头，以缓解数据不平衡问题。

Result: 在 FER2013 数据集上达到 62.8% 的准确率和 0.590 的宏平均 F1 分数，表现优于传统的 CNN 基线模型。

Conclusion: 高效的网络架构结合针对性的不平衡处理策略，能够构建出实用、透明且可复现的面部情绪识别系统。

Abstract: Facial Emotion Recognition (FER) is a key task in affective computing,
enabling applications in human-computer interaction, e-learning, healthcare,
and safety systems. Despite advances in deep learning, FER remains challenging
due to occlusions, illumination and pose variations, subtle intra-class
differences, and dataset imbalance that hinders recognition of minority
emotions. We present InsideOut, a reproducible FER framework built on
EfficientNetV2-S with transfer learning, strong data augmentation, and
imbalance-aware optimization. The approach standardizes FER2013 images, applies
stratified splitting and augmentation, and fine-tunes a lightweight
classification head with class-weighted loss to address skewed distributions.
InsideOut achieves 62.8% accuracy with a macro averaged F1 of 0.590 on FER2013,
showing competitive results compared to conventional CNN baselines. The novelty
lies in demonstrating that efficient architectures, combined with tailored
imbalance handling, can provide practical, transparent, and reproducible FER
solutions.

</details>


### [40] [What Drives Compositional Generalization in Visual Generative Models?](https://arxiv.org/abs/2510.03075)
*Karim Farid,Rajat Sahay,Yumna Ali Alnaggar,Simon Schrodi,Volker Fischer,Cordelia Schmid,Thomas Brox*

Main category: cs.CV

TL;DR: 研究了不同设计选择对图像和视频生成中组合泛化的影响，发现训练目标的离散或连续性以及训练中条件信息的程度是两个关键因素，并提出通过结合连续JEPA目标来提升MaskGIT等离散模型的组合性能。


<details>
  <summary>Details</summary>
Motivation: 组合泛化能力对于视觉生成模型至关重要，但影响其能力的机制尚未完全理解，因此需要系统研究不同设计选择的影响。

Method: 通过受控实验分析不同训练目标（离散 vs 连续）和条件信息程度对组合泛化的影响，并在MaskGIT模型中引入辅助的连续JEPA目标进行改进。

Result: 确定了两个关键因素：训练目标的分布类型和条件信息的充分性；实验表明，加入JEPA连续目标可提升MaskGIT的组合生成性能。

Conclusion: 结合离散与连续训练目标可以有效提升生成模型的组合泛化能力，为未来模型设计提供了指导。

Abstract: Compositional generalization, the ability to generate novel combinations of
known concepts, is a key ingredient for visual generative models. Yet, not all
mechanisms that enable or inhibit it are fully understood. In this work, we
conduct a systematic study of how various design choices influence
compositional generalization in image and video generation in a positive or
negative way. Through controlled experiments, we identify two key factors: (i)
whether the training objective operates on a discrete or continuous
distribution, and (ii) to what extent conditioning provides information about
the constituent concepts during training. Building on these insights, we show
that relaxing the MaskGIT discrete loss with an auxiliary continuous JEPA-based
objective can improve compositional performance in discrete models like
MaskGIT.

</details>


### [41] [Latent Diffusion Unlearning: Protecting Against Unauthorized Personalization Through Trajectory Shifted Perturbations](https://arxiv.org/abs/2510.03089)
*Naresh Kumar Devulapally,Shruti Agarwal,Tejas Gokhale,Vishnu Suresh Lokhande*

Main category: cs.CV

TL;DR: 提出了一种在扩散模型的潜在空间中生成难以学习样本的新方法，通过轨迹偏移采样实现高视觉保真度和对下游模型个性化与逆向攻击的抵抗能力。


<details>
  <summary>Details</summary>
Motivation: 现有图像投毒方法在像素空间操作导致图像出现噪声和伪影，难以实现不可感知性，同时文本到图像扩散模型的个性化能力引发了数据隐私和知识产权保护的担忧。

Method: 提出一种基于潜在空间的模型扰动策略，在扩散模型的去噪与反转过程中交替修改去噪轨迹的起始点，实现轨迹偏移采样，使扰动图像既保持原图视觉质量又抵抗下游模型的学习。

Result: 在四个基准数据集上验证了方法对最先进的逆向攻击具有鲁棒性，相比现有方法在感知指标（PSNR、SSIM、FID）上提升约8%-10%，在五种对抗设置下平均鲁棒性提升约10%。

Conclusion: 该方法将不可学习性融入潜在扩散模型框架，提供了一种实用且不可感知的防御手段，有效保护敏感数据免受未经授权的模型适配。

Abstract: Text-to-image diffusion models have demonstrated remarkable effectiveness in
rapid and high-fidelity personalization, even when provided with only a few
user images. However, the effectiveness of personalization techniques has lead
to concerns regarding data privacy, intellectual property protection, and
unauthorized usage. To mitigate such unauthorized usage and model replication,
the idea of generating ``unlearnable'' training samples utilizing image
poisoning techniques has emerged. Existing methods for this have limited
imperceptibility as they operate in the pixel space which results in images
with noise and artifacts. In this work, we propose a novel model-based
perturbation strategy that operates within the latent space of diffusion
models. Our method alternates between denoising and inversion while modifying
the starting point of the denoising trajectory: of diffusion models. This
trajectory-shifted sampling ensures that the perturbed images maintain high
visual fidelity to the original inputs while being resistant to inversion and
personalization by downstream generative models. This approach integrates
unlearnability into the framework of Latent Diffusion Models (LDMs), enabling a
practical and imperceptible defense against unauthorized model adaptation. We
validate our approach on four benchmark datasets to demonstrate robustness
against state-of-the-art inversion attacks. Results demonstrate that our method
achieves significant improvements in imperceptibility ($\sim 8 \% -10\%$ on
perceptual metrics including PSNR, SSIM, and FID) and robustness ( $\sim 10\%$
on average across five adversarial settings), highlighting its effectiveness in
safeguarding sensitive data.

</details>


### [42] [Geometry Meets Vision: Revisiting Pretrained Semantics in Distilled Fields](https://arxiv.org/abs/2510.03104)
*Zhiting Mei,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.CV

TL;DR: 本文探讨了几何锚定语义特征在辐射场蒸馏中的作用，发现尽管几何锚定特征包含更多几何细节，但在语义对象定位和位姿估计中并未优于视觉特征，甚至在位姿估计中表现更差，表明视觉特征更具通用性。


<details>
  <summary>Details</summary>
Motivation: 研究几何锚定语义特征是否能在辐射场蒸馏中带来性能提升，特别是在空间任务如姿态估计中，并探索如何更好融合几何与语义信息。

Method: 提出新框架SPINE，用于无需初始猜测的辐射场反演，包含两个阶段：基于蒸馏语义的粗略反演与基于光度优化的精细反演；并比较视觉特征与几何锚定特征在几何感知能力、对象定位和反演精度上的表现。

Result: 几何锚定特征能生成更具几何细节的特征，提升结构感知能力，但在语义定位任务中无显著优势，且在辐射场反演中导致位姿估计精度下降。

Conclusion: 视觉特征在多种下游任务中更具通用性，尽管几何锚定特征提供更丰富的空间结构信息，但当前方法未能有效利用其优势，未来需探索更有效的几何锚定策略。

Abstract: Semantic distillation in radiance fields has spurred significant advances in
open-vocabulary robot policies, e.g., in manipulation and navigation, founded
on pretrained semantics from large vision models. While prior work has
demonstrated the effectiveness of visual-only semantic features (e.g., DINO and
CLIP) in Gaussian Splatting and neural radiance fields, the potential benefit
of geometry-grounding in distilled fields remains an open question. In
principle, visual-geometry features seem very promising for spatial tasks such
as pose estimation, prompting the question: Do geometry-grounded semantic
features offer an edge in distilled fields? Specifically, we ask three critical
questions: First, does spatial-grounding produce higher-fidelity geometry-aware
semantic features? We find that image features from geometry-grounded backbones
contain finer structural details compared to their counterparts. Secondly, does
geometry-grounding improve semantic object localization? We observe no
significant difference in this task. Thirdly, does geometry-grounding enable
higher-accuracy radiance field inversion? Given the limitations of prior work
and their lack of semantics integration, we propose a novel framework SPINE for
inverting radiance fields without an initial guess, consisting of two core
components: coarse inversion using distilled semantics, and fine inversion
using photometric-based optimization. Surprisingly, we find that the pose
estimation accuracy decreases with geometry-grounded features. Our results
suggest that visual-only features offer greater versatility for a broader range
of downstream tasks, although geometry-grounded features contain more geometric
detail. Notably, our findings underscore the necessity of future research on
effective strategies for geometry-grounding that augment the versatility and
performance of pretrained semantic features.

</details>


### [43] [GeoComplete: Geometry-Aware Diffusion for Reference-Driven Image Completion](https://arxiv.org/abs/2510.03110)
*Beibei Lin,Tingting Chen,Robby T. Tan*

Main category: cs.CV

TL;DR: GeoComplete 提出了一种结合3D结构引导的双分支扩散框架，通过投影点云和目标感知掩码策略，实现几何一致且视觉质量高的图像补全，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的图像补全方法缺乏几何线索，导致在参考图像与目标视角差异大时生成内容错位或不合理，因此需要引入显式的3D结构信息来提升几何一致性。

Method: 提出GeoComplete，采用双分支扩散架构：一支处理遮罩后的目标图像，另一支从投影点云中提取几何特征；通过跨分支联合自注意力实现融合，并引入目标感知掩码机制，在训练时遮蔽被遮挡区域以引导模型关注有效参考线索。

Result: 实验表明，GeoComplete 在PSNR上比现有最优方法提升17.1，显著提高了几何准确性和视觉质量。

Conclusion: GeoComplete 通过融合显式3D几何信息与目标感知掩码策略，为参考驱动的图像补全提供了统一且鲁棒的解决方案，尤其适用于大视角差异下的复杂场景。

Abstract: Reference-driven image completion, which restores missing regions in a target
view using additional images, is particularly challenging when the target view
differs significantly from the references. Existing generative methods rely
solely on diffusion priors and, without geometric cues such as camera pose or
depth, often produce misaligned or implausible content. We propose GeoComplete,
a novel framework that incorporates explicit 3D structural guidance to enforce
geometric consistency in the completed regions, setting it apart from prior
image-only approaches. GeoComplete introduces two key ideas: conditioning the
diffusion process on projected point clouds to infuse geometric information,
and applying target-aware masking to guide the model toward relevant reference
cues. The framework features a dual-branch diffusion architecture. One branch
synthesizes the missing regions from the masked target, while the other
extracts geometric features from the projected point cloud. Joint
self-attention across branches ensures coherent and accurate completion. To
address regions visible in references but absent in the target, we project the
target view into each reference to detect occluded areas, which are then masked
during training. This target-aware masking directs the model to focus on useful
cues, enhancing performance in difficult scenarios. By integrating a
geometry-aware dual-branch diffusion architecture with a target-aware masking
strategy, GeoComplete offers a unified and robust solution for
geometry-conditioned image completion. Experiments show that GeoComplete
achieves a 17.1 PSNR improvement over state-of-the-art methods, significantly
boosting geometric accuracy while maintaining high visual quality.

</details>


### [44] [HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion](https://arxiv.org/abs/2510.03122)
*Shiyi Zhang,Dong Liang,Hairong Zheng,Yihang Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种名为HAVIR的分层模型，通过分离视觉皮层区域提取结构和语义特征，结合扩散模型实现从脑活动更准确地重建复杂视觉信息。


<details>
  <summary>Details</summary>
Motivation: 现有方法在重建复杂自然视觉刺激时存在困难，主要由于自然场景的低层特征异质性和高层语义纠缠。为此需要一种能更好模拟视觉皮层层级表征机制的方法。

Method: 基于视觉皮层的层级表征理论，将视觉皮层分为两个区域：结构生成器从空间处理体素中提取结构信息，并转化为潜在扩散先验；语义提取器将语义处理体素转化为CLIP嵌入。两者通过多功能扩散模型融合生成最终图像。

Result: 实验结果表明，HAVIR在复杂场景下均能提升重建图像的结构和语义质量，优于现有模型。

Conclusion: HAVIR通过分层建模脑区功能，有效提升了视觉重建的准确性，推动了神经科学与计算机视觉的融合。

Abstract: The reconstruction of visual information from brain activity fosters
interdisciplinary integration between neuroscience and computer vision.
However, existing methods still face challenges in accurately recovering highly
complex visual stimuli. This difficulty stems from the characteristics of
natural scenes: low-level features exhibit heterogeneity, while high-level
features show semantic entanglement due to contextual overlaps. Inspired by the
hierarchical representation theory of the visual cortex, we propose the HAVIR
model, which separates the visual cortex into two hierarchical regions and
extracts distinct features from each. Specifically, the Structural Generator
extracts structural information from spatial processing voxels and converts it
into latent diffusion priors, while the Semantic Extractor converts semantic
processing voxels into CLIP embeddings. These components are integrated via the
Versatile Diffusion model to synthesize the final image. Experimental results
demonstrate that HAVIR enhances both the structural and semantic quality of
reconstructions, even in complex scenes, and outperforms existing models.

</details>


### [45] [Mask2IV: Interaction-Centric Video Generation via Mask Trajectories](https://arxiv.org/abs/2510.03135)
*Gen Li,Bo Zhao,Jianfei Yang,Laura Sevilla-Lara*

Main category: cs.CV

TL;DR: 提出Mask2IV框架，通过解耦两阶段流程实现无需密集掩码标注的交互中心视频生成，支持灵活可控的人-物及机器人操作视频合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以建模复杂动态交互，且依赖密集精确的掩码标注，限制了真实场景应用。

Method: 采用解耦的两阶段框架：先预测主体与物体的运动轨迹，再基于轨迹条件生成视频；支持通过动作描述或空间提示进行控制。

Result: 在两个新构建的基准上实验表明，该方法在视觉真实感和可控性方面优于现有基线。

Conclusion: Mask2IV有效降低了对密集掩码标注的依赖，同时实现了高质量、可操控的交互视频生成。

Abstract: Generating interaction-centric videos, such as those depicting humans or
robots interacting with objects, is crucial for embodied intelligence, as they
provide rich and diverse visual priors for robot learning, manipulation policy
training, and affordance reasoning. However, existing methods often struggle to
model such complex and dynamic interactions. While recent studies show that
masks can serve as effective control signals and enhance generation quality,
obtaining dense and precise mask annotations remains a major challenge for
real-world use. To overcome this limitation, we introduce Mask2IV, a novel
framework specifically designed for interaction-centric video generation. It
adopts a decoupled two-stage pipeline that first predicts plausible motion
trajectories for both actor and object, then generates a video conditioned on
these trajectories. This design eliminates the need for dense mask inputs from
users while preserving the flexibility to manipulate the interaction process.
Furthermore, Mask2IV supports versatile and intuitive control, allowing users
to specify the target object of interaction and guide the motion trajectory
through action descriptions or spatial position cues. To support systematic
training and evaluation, we curate two benchmarks covering diverse action and
object categories across both human-object interaction and robotic manipulation
scenarios. Extensive experiments demonstrate that our method achieves superior
visual realism and controllability compared to existing baselines.

</details>


### [46] [ReeMark: Reeb Graphs for Simulating Patterns of Life in Spatiotemporal Trajectories](https://arxiv.org/abs/2510.03152)
*Anantajit Subrahmanya,Chandrakanth Gudavalli,Connor Levenson,Umang Garg,B. S. Manjunath*

Main category: cs.CV

TL;DR: 提出了一种名为马尔可夫雷布图的新框架，用于模拟保留生活模式的时空轨迹，具有高保真度且高效。


<details>
  <summary>Details</summary>
Motivation: 准确建模人类移动性对城市规划、流行病学和交通管理至关重要，需要一种能同时捕捉个体和群体移动模式的一致性与变异性的方法。

Method: 结合个体和群体层面的移动结构，构建基于概率拓扑模型的马尔可夫雷布图，以生成符合基准数据中生活模式的未来轨迹。

Result: 在Atlanta和Berlin子集的Urban Anomalies数据集上评估，使用Jensen-Shannon散度指标，结果显示该方法在群体和个体层面上均表现出强保真性，同时保持数据和计算效率。

Conclusion: 马尔可夫雷布图是一种可扩展的轨迹模拟框架，适用于多样化的城市环境。

Abstract: Accurately modeling human mobility is critical for urban planning,
epidemiology, and traffic management. In this work, we introduce Markovian Reeb
Graphs, a novel framework for simulating spatiotemporal trajectories that
preserve Patterns of Life (PoLs) learned from baseline data. By combining
individual- and population-level mobility structures within a probabilistic
topological model, our approach generates realistic future trajectories that
capture both consistency and variability in daily life. Evaluations on the
Urban Anomalies dataset (Atlanta and Berlin subsets) using the Jensen-Shannon
Divergence (JSD) across population- and agent-level metrics demonstrate that
the proposed method achieves strong fidelity while remaining data- and
compute-efficient. These results position Markovian Reeb Graphs as a scalable
framework for trajectory simulation with broad applicability across diverse
urban environments.

</details>


### [47] [SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus](https://arxiv.org/abs/2510.03160)
*Ming Zhao,Wenhui Dong,Yang Zhang,Xiang Zheng,Zhonghao Zhang,Zian Zhou,Yunzhi Guan,Liukun Xu,Wei Peng,Zhaoyang Gong,Zhicheng Zhang,Dachuan Li,Xiaosheng Ma,Yuli Ma,Jianing Ni,Changjiang Jiang,Lixia Tian,Qixin Chen,Kaishun Xia,Pingping Liu,Tongshun Zhang,Zhiqiang Liu,Zhongan Bi,Chenyang Si,Tiansheng Sun,Caifeng Shan*

Main category: cs.CV

TL;DR: SpineMed是一个由脊柱外科医生共同设计的生态系统，包含大规模、多模态、椎体层级推理的数据集SpineMed-450k和临床导向的评估框架SpineBench，显著提升了AI在脊柱疾病诊断中的精细推理能力。


<details>
  <summary>Details</summary>
Motivation: 脊柱疾病影响广泛，但现有的AI辅助诊断因缺乏椎体层级、多模态且具有临床依据的指令数据和标准化基准而受限，亟需一个高质量、可追踪的数据集和评估体系以推动发展。

Method: 提出SpineMed生态系统，包括SpineMed-450k数据集（基于教科书、指南、公开数据和约1000例去标识化医院病例，采用两阶段LLM生成方法并结合临床医生参与）和SpineBench评估框架；在多种大型视觉-语言模型上进行评估，并使用SpineMed-450k微调模型以验证效果。

Result: 现有先进LVLM在SpineBench上表现出在细粒度、椎体层级推理方面的系统性缺陷；而基于SpineMed-450k微调的模型在所有任务中均实现显著提升，且临床医生评估确认其输出具有诊断清晰性和实用价值。

Conclusion: SpineMed填补了脊柱疾病AI诊断中高质量、多模态、椎体层级数据与标准化评估的空白，为临床可信的AI辅助诊断系统提供了重要基础。

Abstract: Spine disorders affect 619 million people globally and are a leading cause of
disability, yet AI-assisted diagnosis remains limited by the lack of
level-aware, multimodal datasets. Clinical decision-making for spine disorders
requires sophisticated reasoning across X-ray, CT, and MRI at specific
vertebral levels. However, progress has been constrained by the absence of
traceable, clinically-grounded instruction data and standardized,
spine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem
co-designed with practicing spine surgeons. It features SpineMed-450k, the
first large-scale dataset explicitly designed for vertebral-level reasoning
across imaging modalities with over 450,000 instruction instances, and
SpineBench, a clinically-grounded evaluation framework. SpineMed-450k is
curated from diverse sources, including textbooks, guidelines, open datasets,
and ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline
with a two-stage LLM generation method (draft and revision) to ensure
high-quality, traceable data for question-answering, multi-turn consultations,
and report generation. SpineBench evaluates models on clinically salient axes,
including level identification, pathology assessment, and surgical planning.
Our comprehensive evaluation of several recently advanced large vision-language
models (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained,
level-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k
demonstrates consistent and significant improvements across all tasks.
Clinician assessments confirm the diagnostic clarity and practical utility of
our model's outputs.

</details>


### [48] [UniShield: An Adaptive Multi-Agent Framework for Unified Forgery Image Detection and Localization](https://arxiv.org/abs/2510.03161)
*Qing Huang,Zhipei Xu,Xuanyu Zhang,Jian Zhang*

Main category: cs.CV

TL;DR: 本文提出UniShield，一种基于多智能体的统一图像伪造检测与定位系统，能够跨多个领域（如图像篡改、文档篡改、DeepFake和AI生成图像）实现高效、可解释的检测。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成技术的快速发展，合成图像日益逼真，带来了 misinformation 和欺诈等社会风险，现有的伪造检测方法因领域特定性强、泛化能力差且缺乏统一适应框架而实用性受限。

Method: UniShield采用感知智能体与检测智能体协同工作：感知智能体分析图像特征并动态选择合适的检测模型，检测智能体集成多种专家检测器并生成可解释报告，形成一个自适应的统一框架。

Result: 实验表明，UniShield在多个伪造检测任务上优于现有统一方法和领域专用检测器，达到最先进水平，展现出更强的实用性、适应性和可扩展性。

Conclusion: UniShield通过多智能体架构实现了跨域伪造图像检测的有效整合，为应对日益复杂的图像伪造问题提供了可靠且可扩展的解决方案。

Abstract: With the rapid advancements in image generation, synthetic images have become
increasingly realistic, posing significant societal risks, such as
misinformation and fraud. Forgery Image Detection and Localization (FIDL) thus
emerges as essential for maintaining information integrity and societal
security. Despite impressive performances by existing domain-specific detection
methods, their practical applicability remains limited, primarily due to their
narrow specialization, poor cross-domain generalization, and the absence of an
integrated adaptive framework. To address these issues, we propose UniShield,
the novel multi-agent-based unified system capable of detecting and localizing
image forgeries across diverse domains, including image manipulation, document
manipulation, DeepFake, and AI-generated images. UniShield innovatively
integrates a perception agent with a detection agent. The perception agent
intelligently analyzes image features to dynamically select suitable detection
models, while the detection agent consolidates various expert detectors into a
unified framework and generates interpretable reports. Extensive experiments
show that UniShield achieves state-of-the-art results, surpassing both existing
unified approaches and domain-specific detectors, highlighting its superior
practicality, adaptiveness, and scalability.

</details>


### [49] [ROGR: Relightable 3D Objects using Generative Relighting](https://arxiv.org/abs/2510.03163)
*Jiapeng Tang,Matthew Lavine,Dor Verbin,Stephan J. Garbin,Matthias Nießner,Ricardo Martin Brualla,Pratul P. Srinivasan,Philipp Henzler*

Main category: cs.CV

TL;DR: 提出了一种名为ROGR的新方法，通过生成性重光照模型重建多视角捕获物体的可重光照3D模型，使用双分支神经辐射场（NeRF）实现高效任意环境光照下的前馈重光照。


<details>
  <summary>Details</summary>
Motivation: 实现对多视角捕获物体在任意新光照环境下逼真且高效的外观重建，克服传统方法中需要逐光照优化或光传输模拟的低效问题。

Method: 利用生成性重光照模型从多光照环境采样物体外观，构建训练数据集；提出一种新颖的双分支架构的光照条件NeRF，分别编码整体光照效果和镜面反射，从而实现对任意环境光照的解耦建模与高效渲染。

Result: 在TensoIR和Stanford-ORB数据集上优于现有最先进方法的多数指标，并在真实物体捕捉场景中验证了方法的有效性。

Conclusion: ROGR能够高效生成高质量的可重光照3D模型，支持任意环境光照下的逼真渲染，具有良好的实际应用潜力。

Abstract: We introduce ROGR, a novel approach that reconstructs a relightable 3D model
of an object captured from multiple views, driven by a generative relighting
model that simulates the effects of placing the object under novel environment
illuminations. Our method samples the appearance of the object under multiple
lighting environments, creating a dataset that is used to train a
lighting-conditioned Neural Radiance Field (NeRF) that outputs the object's
appearance under any input environmental lighting. The lighting-conditioned
NeRF uses a novel dual-branch architecture to encode the general lighting
effects and specularities separately. The optimized lighting-conditioned NeRF
enables efficient feed-forward relighting under arbitrary environment maps
without requiring per-illumination optimization or light transport simulation.
We evaluate our approach on the established TensoIR and Stanford-ORB datasets,
where it improves upon the state-of-the-art on most metrics, and showcase our
approach on real-world object captures.

</details>


### [50] [Dynamic Prompt Generation for Interactive 3D Medical Image Segmentation Training](https://arxiv.org/abs/2510.03189)
*Tidiane Camaret Ndir,Alexander Pfefferle,Robin Tibor Schirrmeister*

Main category: cs.CV

TL;DR: 提出一种结合动态体积提示生成和内容感知自适应裁剪的训练策略，优化图像编码器使用，模拟真实用户交互模式，在单个GPU上高效训练，显著提升3D生物医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型缺乏体积感知或交互能力受限，难以满足交互式3D生物医学图像分割的需求。

Method: 采用动态体积提示生成与内容感知自适应裁剪相结合的训练策略，并基于nnInteractive模型的公开权重初始化网络。

Result: 在‘交互式3D生物医学图像分割基础模型’竞赛中表现优异，平均最终Dice分数为0.6385，归一化表面距离为0.6614，Dice和NSD的曲线下面积分别为2.4799和2.5671。

Conclusion: 所提方法有效解决了计算资源限制下的序列反馈学习问题，提升了模型在交互式3D生物医学图像分割中的性能。

Abstract: Interactive 3D biomedical image segmentation requires efficient models that
can iteratively refine predictions based on user prompts. Current foundation
models either lack volumetric awareness or suffer from limited interactive
capabilities. We propose a training strategy that combines dynamic volumetric
prompt generation with content-aware adaptive cropping to optimize the use of
the image encoder. Our method simulates realistic user interaction patterns
during training while addressing the computational challenges of learning from
sequential refinement feedback on a single GPU. For efficient training, we
initialize our network using the publicly available weights from the
nnInteractive segmentation model. Evaluation on the \textbf{Foundation Models
for Interactive 3D Biomedical Image Segmentation} competition demonstrates
strong performance with an average final Dice score of 0.6385, normalized
surface distance of 0.6614, and area-under-the-curve metrics of 2.4799 (Dice)
and 2.5671 (NSD).

</details>


### [51] [Product-Quantised Image Representation for High-Quality Image Synthesis](https://arxiv.org/abs/2510.03191)
*Denis Zavadski,Nikita Philip Tatsch,Carsten Rother*

Main category: cs.CV

TL;DR: 本文提出了PQGAN，一种将乘积量化（PQ）引入VQGAN框架的量化图像自编码器，在重建性能上显著优于现有方法，并能有效集成到预训练扩散模型中，提升生成效率或分辨率。


<details>
  <summary>Details</summary>
Motivation: 尽管乘积量化（PQ）在向量编码中广泛应用，但在高保真图像生成的潜在表示中应用有限。本文旨在探索PQ在该领域的潜力，改善现有量化方法的重建性能。

Method: 将PQ整合进VQGAN的向量量化框架，系统分析码本大小、嵌入维度和子空间分解之间的关系，研究其对量化性能的影响，并将其应用于预训练扩散模型。

Result: PQGAN在PSNR上达到37dB（先前工作为27dB），FID、LPIPS和CMMD指标最多降低96%；发现PQ与VQ在扩展嵌入维度时性能表现相反，并得出指导超参数选择的性能趋势。

Conclusion: PQGAN在图像重建和生成方面显著优于现有方法，且可无缝集成到扩散模型中，提升生成速度或分辨率，展示了PQ作为图像合成中离散潜在表示的有效扩展。

Abstract: Product quantisation (PQ) is a classical method for scalable vector encoding,
yet it has seen limited usage for latent representations in high-fidelity image
generation. In this work, we introduce PQGAN, a quantised image autoencoder
that integrates PQ into the well-known vector quantisation (VQ) framework of
VQGAN. PQGAN achieves a noticeable improvement over state-of-the-art methods in
terms of reconstruction performance, including both quantisation methods and
their continuous counterparts. We achieve a PSNR score of 37dB, where prior
work achieves 27dB, and are able to reduce the FID, LPIPS, and CMMD score by up
to 96%. Our key to success is a thorough analysis of the interaction between
codebook size, embedding dimensionality, and subspace factorisation, with
vector and scalar quantisation as special cases. We obtain novel findings, such
that the performance of VQ and PQ behaves in opposite ways when scaling the
embedding dimension. Furthermore, our analysis shows performance trends for PQ
that help guide optimal hyperparameter selection. Finally, we demonstrate that
PQGAN can be seamlessly integrated into pre-trained diffusion models. This
enables either a significantly faster and more compute-efficient generation, or
a doubling of the output resolution at no additional cost, positioning PQ as a
strong extension for discrete latent representation in image synthesis.

</details>


### [52] [Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft](https://arxiv.org/abs/2510.03198)
*Junchao Huang,Xinting Hu,Boyao Han,Shaoshuai Shi,Zhuotao Tian,Tianyu He,Li Jiang*

Main category: cs.CV

TL;DR: 提出Memory Forcing框架，通过几何索引空间记忆和混合训练策略，在保持计算效率的同时实现长期空间一致性和高质量的新场景生成。


<details>
  <summary>Details</summary>
Motivation: 在计算资源有限的情况下，现有自回归视频扩散模型在探索新场景和重访已探索区域时难以兼顾生成自然内容和保持空间一致性，需要有效利用历史信息。

Method: 提出Memory Forcing框架，包含几何索引的空间记忆机制；采用混合训练（Hybrid Training）和链式前向训练（Chained Forward Training）策略；结合点到帧检索（Point-to-Frame Retrieval）和增量3D重建（Incremental 3D Reconstruction）实现高效历史信息利用。

Result: 实验表明，该方法在多样化环境中显著提升了长期空间一致性和生成质量，同时在长序列生成中保持了计算效率。

Conclusion: Memory Forcing通过协调时间记忆与空间记忆的使用，在探索与重访场景之间取得平衡，为视频扩散世界模型提供了有效的长期记忆解决方案。

Abstract: Autoregressive video diffusion models have proved effective for world
modeling and interactive scene generation, with Minecraft gameplay as a
representative application. To faithfully simulate play, a model must generate
natural content while exploring new scenes and preserve spatial consistency
when revisiting explored areas. Under limited computation budgets, it must
compress and exploit historical cues within a finite context window, which
exposes a trade-off: Temporal-only memory lacks long-term spatial consistency,
whereas adding spatial memory strengthens consistency but may degrade new scene
generation quality when the model over-relies on insufficient spatial context.
We present Memory Forcing, a learning framework that pairs training protocols
with a geometry-indexed spatial memory. Hybrid Training exposes distinct
gameplay regimes, guiding the model to rely on temporal memory during
exploration and incorporate spatial memory for revisits. Chained Forward
Training extends autoregressive training with model rollouts, where chained
predictions create larger pose variations and encourage reliance on spatial
memory for maintaining consistency. Point-to-Frame Retrieval efficiently
retrieves history by mapping currently visible points to their source frames,
while Incremental 3D Reconstruction maintains and updates an explicit 3D cache.
Extensive experiments demonstrate that Memory Forcing achieves superior
long-term spatial consistency and generative quality across diverse
environments, while maintaining computational efficiency for extended
sequences.

</details>


### [53] [MonSTeR: a Unified Model for Motion, Scene, Text Retrieval](https://arxiv.org/abs/2510.03200)
*Luca Collorone,Matteo Gioia,Massimiliano Pappa,Paolo Leoni,Giovanni Ficarra,Or Litany,Indro Spinelli,Fabio Galasso*

Main category: cs.CV

TL;DR: 本文提出了MonSTeR，第一个能够联合建模骨骼运动、场景和文本意图的三模态检索模型，通过构建统一的隐空间实现跨模态对齐与灵活检索。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏评估动作、意图和场景三者对齐关系的工具，难以捕捉人类在复杂环境中行为与上下文之间的复杂依赖。

Method: 受高阶关系建模启发，MonSTeR利用单模态与跨模态表征构建统一的隐空间，以捕捉运动、场景和文本之间的复杂依赖关系，支持灵活且鲁棒的多任务检索。

Result: 实验表明，MonSTeR优于仅依赖单模态表征的三模态模型；用户研究表明其检索分数与人类偏好一致，并在零样本场景物体放置和动作描述生成任务中展现隐空间的通用性。

Conclusion: MonSTeR有效实现了运动、场景与文本的跨模态对齐，具备强大的检索能力与潜在的下游应用价值。

Abstract: Intention drives human movement in complex environments, but such movement
can only happen if the surrounding context supports it. Despite the intuitive
nature of this mechanism, existing research has not yet provided tools to
evaluate the alignment between skeletal movement (motion), intention (text),
and the surrounding context (scene). In this work, we introduce MonSTeR, the
first MOtioN-Scene-TExt Retrieval model. Inspired by the modeling of
higher-order relations, MonSTeR constructs a unified latent space by leveraging
unimodal and cross-modal representations. This allows MonSTeR to capture the
intricate dependencies between modalities, enabling flexible but robust
retrieval across various tasks. Our results show that MonSTeR outperforms
trimodal models that rely solely on unimodal representations. Furthermore, we
validate the alignment of our retrieval scores with human preferences through a
dedicated user study. We demonstrate the versatility of MonSTeR's latent space
on zero-shot in-Scene Object Placement and Motion Captioning. Code and
pre-trained models are available at github.com/colloroneluca/MonSTeR.

</details>


### [54] [Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles](https://arxiv.org/abs/2510.03224)
*Dong Lao,Yuxiang Zhang,Haniyeh Ehsani Oskouie,Yangchao Wu,Alex Wong,Stefano Soatto*

Main category: cs.CV

TL;DR: 提出一种训练无关、架构无关且攻击无关的测试时防御机制，通过引入随机共振增强模型对对抗攻击的鲁棒性，在图像分类与密集预测任务中均实现先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法常依赖特征过滤或平滑，易导致信息丢失；需要一种能减少信息损失且适用于多种任务的通用防御机制。

Method: 通过引入微小平移扰动，对变换后的特征嵌入进行对齐与聚合，并利用闭式公式将结果映射回原图像，以实现‘以噪制噪’的防御。

Result: 在图像分类、立体匹配和光流等任务上显著恢复被攻击导致的性能下降，分别恢复68.1%、71.9%和29.2%的准确率损失，首次实现了对密集预测任务的通用测试时防御。

Conclusion: 该方法无需训练、不依赖特定网络结构或攻击类型，具有高度通用性和实用性，为对抗性防御提供了新思路。

Abstract: We propose a test-time defense mechanism against adversarial attacks:
imperceptible image perturbations that significantly alter the predictions of a
model. Unlike existing methods that rely on feature filtering or smoothing,
which can lead to information loss, we propose to "combat noise with noise" by
leveraging stochastic resonance to enhance robustness while minimizing
information loss. Our approach introduces small translational perturbations to
the input image, aligns the transformed feature embeddings, and aggregates them
before mapping back to the original reference image. This can be expressed in a
closed-form formula, which can be deployed on diverse existing network
architectures without introducing additional network modules or fine-tuning for
specific attack types. The resulting method is entirely training-free,
architecture-agnostic, and attack-agnostic. Empirical results show
state-of-the-art robustness on image classification and, for the first time,
establish a generic test-time defense for dense prediction tasks, including
stereo matching and optical flow, highlighting the method's versatility and
practicality. Specifically, relative to clean (unperturbed) performance, our
method recovers up to 68.1% of the accuracy loss on image classification, 71.9%
on stereo matching, and 29.2% on optical flow under various types of
adversarial attacks.

</details>


### [55] [MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture Recognition](https://arxiv.org/abs/2510.03228)
*Ricardo T. Fares,Lucas C. Ribas*

Main category: cs.CV

TL;DR: 提出了一种名为Mixer的新型随机神经网络，用于纹理表示学习，结合超球面随机嵌入和双分支学习模块，有效捕捉通道内和通道间关系。


<details>
  <summary>Details</summary>
Motivation: 现有随机神经网络在纹理识别中主要关注交叉信息预测，缺乏对整体架构的显著改进，因此需要更强大的表示学习结构。

Method: 采用超球面随机嵌入结合双分支学习模块，并设计新的优化问题以增强纹理表示能力。

Result: 在多个具有不同特性和挑战的纯纹理基准上取得了有趣且优异的结果。

Conclusion: Mixer通过改进随机网络架构，在纹理表示学习中展现出强大潜力，为后续研究提供了新方向。

Abstract: Randomized neural networks for representation learning have consistently
achieved prominent results in texture recognition tasks, effectively combining
the advantages of both traditional techniques and learning-based approaches.
However, existing approaches have so far focused mainly on improving
cross-information prediction, without introducing significant advancements to
the overall randomized network architecture. In this paper, we propose Mixer, a
novel randomized neural network for texture representation learning. At its
core, the method leverages hyperspherical random embeddings coupled with a
dual-branch learning module to capture both intra- and inter-channel
relationships, further enhanced by a newly formulated optimization problem for
building rich texture representations. Experimental results have shown the
interesting results of the proposed approach across several pure texture
benchmarks, each with distinct characteristics and challenges. The source code
will be available upon publication.

</details>


### [56] [Improving GUI Grounding with Explicit Position-to-Coordinate Mapping](https://arxiv.org/abs/2510.03230)
*Suyuchen Wang,Tianyu Zhang,Ahmed Masry,Christopher Pal,Spandana Gella,Bang Liu,Perouz Taslakian*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法来改善视觉语言模型在高分辨率界面上的GUI定位任务表现，通过引入RULER标记和交错的MRoPE技术，提高了坐标映射的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在处理未曾见过的高分辨率显示时，难以实现可靠的像素映射，导致GUI定位任务的准确性下降。

Method: 引入了两种互补的技术：RULER标记作为显式的坐标指示器，使模型能够像地图上的网格线一样参考位置，并调整而非从零开始生成坐标；交错的MRoPE（I-MRoPE）通过确保宽度和高度维度被平等表示，改进了空间编码，解决了标准位置编码方案的不对称性问题。

Result: 在ScreenSpot、ScreenSpot-V2和ScreenSpot-Pro数据集上的实验显示，该方法在定位准确性方面有持续的提升，尤其是在高分辨率界面上的改进最为显著。

Conclusion: 通过提供显式空间指导而非依赖隐式学习，该方法能够在多样化的分辨率和平台上实现更可靠的GUI自动化。

Abstract: GUI grounding, the task of mapping natural-language instructions to pixel
coordinates, is crucial for autonomous agents, yet remains difficult for
current VLMs. The core bottleneck is reliable patch-to-pixel mapping, which
breaks when extrapolating to high-resolution displays unseen during training.
Current approaches generate coordinates as text tokens directly from visual
features, forcing the model to infer complex position-to-pixel mappings
implicitly; as a result, accuracy degrades and failures proliferate on new
resolutions. We address this with two complementary innovations. First, RULER
tokens serve as explicit coordinate markers, letting the model reference
positions similar to gridlines on a map and adjust rather than generate
coordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial
encoding by ensuring that width and height dimensions are represented equally,
addressing the asymmetry of standard positional schemes. Experiments on
ScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in
grounding accuracy, with the largest improvements on high-resolution
interfaces. By providing explicit spatial guidance rather than relying on
implicit learning, our approach enables more reliable GUI automation across
diverse resolutions and platforms.

</details>


### [57] [LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks for Multimodal Large Language Models](https://arxiv.org/abs/2510.03232)
*Ci-Siang Lin,Min-Hung Chen,Yu-Yang Sheng,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: LEAML是一种标签高效的多模态大模型适配框架，通过利用少量标注和大量未标注数据，在医疗和体育视觉问答任务中实现了优于标准微调的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在通用视觉任务上表现良好，但在医疗等专业领域的分布外任务上因标注数据稀缺而表现受限，需要高效的领域适配方法。

Method: 提出LEAML框架，利用问答生成器结合标题蒸馏生成未标注数据上的伪问答对，并选择性更新与问答最相关的神经元，实现知识蒸馏和领域适应。

Result: 在胃肠道内窥镜和体育视觉问答任务上，LEAML在极低监督下 consistently 优于标准微调方法。

Conclusion: LEAML通过标签高效的方式有效提升了MLLM在专业领域的适应能力，展示了伪标签生成与选择性参数更新的潜力。

Abstract: Multimodal Large Language Models (MLLMs) have achieved strong performance on
general visual benchmarks but struggle with out-of-distribution (OOD) tasks in
specialized domains such as medical imaging, where labeled data is limited and
expensive. We introduce LEAML, a label-efficient adaptation framework that
leverages both scarce labeled VQA samples and abundant unlabeled images. Our
approach generates domain-relevant pseudo question-answer pairs for unlabeled
data using a QA generator regularized by caption distillation. Importantly, we
selectively update only those neurons most relevant to question-answering,
enabling the QA Generator to efficiently acquire domain-specific knowledge
during distillation. Experiments on gastrointestinal endoscopy and sports VQA
demonstrate that LEAML consistently outperforms standard fine-tuning under
minimal supervision, highlighting the effectiveness of our proposed LEAML
framework.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [58] [Hallucination reduction with CASAL: Contrastive Activation Steering For Amortized Learning](https://arxiv.org/abs/2510.02324)
*Wannan Yang,Xinchi Qiu,Lei Yu,Yuchen Zhang,Oliver Aobo Yang,Narine Kokhlikyan,Nicola Cancedda,Diego Garcia-Olano*

Main category: cs.CL

TL;DR: CASAL是一种高效的训练方法，通过将激活引导的优势直接嵌入模型权重，显著减少大语言模型的幻觉问题，且在多种场景下具有高计算和数据效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常产生幻觉，现有激活引导方法需推理时实时干预，缺乏效率，因此需要一种更实用、可训练后自动抑制幻觉的方法。

Method: 提出CASAL方法，结合可解释性与摊销优化，仅训练单个Transformer层的子模块，通过对比激活引导将知识状态编码进模型权重。

Result: 在多个短答案问答基准上减少30%-40%的幻觉，计算效率提升30倍，数据效率提升20倍，并在分布外场景、视觉-语言模型及稠密/MoE模型上表现良好。

Conclusion: CASAL是首个在稠密和MoE模型上均有效的引导式训练方法，推动了基于可解释性的方法在实际系统中的应用。

Abstract: Large Language Models (LLMs) exhibit impressive capabilities but often
hallucinate, confidently providing incorrect answers instead of admitting
ignorance. Prior work has shown that models encode linear representations of
their own knowledge and that activation steering can reduce hallucinations.
These approaches, however, require real-time monitoring and intervention during
inference. We introduce Contrastive Activation Steering for Amortized Learning
(CASAL), an efficient algorithm that connects interpretability with amortized
optimization. CASAL directly bakes the benefits of activation steering into
model's weights. Once trained, LLMs answer questions they know while abstaining
from answering those they do not. CASAL's light-weight design requires training
only a submodule of a single transformer layer and yet reduces hallucination by
30%-40% across multiple short-form QA benchmarks. CASAL is 30x more
compute-efficient and 20x more data-efficient than strong LoRA-based baselines
such as SFT and DPO, boosting its practical applicability in data scarce
domains. Importantly, CASAL also generalizes effectively to out-of-distribution
(OOD) domains. We showcase CASAL's flexibility in mitigating hallucinations in
both text-only and vision-language models. To our knowledge, CASAL is the first
steering-based training method that has been shown to be effective for both
dense and Mixture-of-Experts (MoE) models. CASAL represents a promising step
forward for applying interpretability-inspired method for practical deployment
in production systems.

</details>


### [59] [Hallucination-Resistant, Domain-Specific Research Assistant with Self-Evaluation and Vector-Grounded Retrieval](https://arxiv.org/abs/2510.02326)
*Vivek Bhavsar,Joseph Ereifej,Aravanan Gurusami*

Main category: cs.CL

TL;DR: RA-FSM是一种基于GPT的模块化研究助手，通过有限状态机控制生成流程，结合向量检索与确定性引用管道，提升文献综合的准确性与可信度，适用于光子学等高风险技术领域。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在文献综合中存在幻觉和错误引用问题，限制了其在专家工作流中的应用。因此需要一种更可靠、可解释的研究助手系统。

Method: 提出RA-FSM系统，采用Relevance->Confidence->Knowledge的有限状态机控制循环，集成向量检索与确定性引用管道；通过分层数据摄取流程构建领域知识库，并实现查询过滤、可回答性评分、问题分解与按需检索。

Result: 在光子学领域的六类任务中，领域专家在盲测A/B测试中更偏好RA-FSM，认为其边界条件处理更强、证据使用更可靠；相比Notebook LM，RA-FSM具有更高的覆盖范围与新颖性，且延迟和成本开销可控。

Conclusion: RA-FSM通过模块化架构和引用控制机制，提供透明、引用充分的答案，适用于高风险技术场景，并可推广至其他科学领域。

Abstract: Large language models accelerate literature synthesis but can hallucinate and
mis-cite, limiting their usefulness in expert workflows. We present RA-FSM
(Research Assistant - Finite State Machine), a modular GPT-based research
assistant that wraps generation in a finite-state control loop: Relevance ->
Confidence -> Knowledge. The system is grounded in vector retrieval and a
deterministic citation pipeline. The controller filters out-of-scope queries,
scores answerability, decomposes questions, and triggers retrieval only when
needed, and emits answers with confidence labels and in-corpus, de-duplicated
references. A ranked-tier ingestion workflow constructs a domain knowledge base
from journals, conferences, indices, preprints, and patents, writing both to a
dense vector index and to a relational store of normalized metrics. We
implement the system for photonics and evaluate it on six task categories:
analytical reasoning, numerical analysis, methodological critique, comparative
synthesis, factual extraction, and application design. In blinded A/B reviews,
domain experts prefer RA-FSM to both a strong Notebook LM (NLM) and a vanilla
Default GPT API call single-pass baseline, citing stronger boundary-condition
handling and more defensible evidence use. Coverage and novelty analyses
indicate that RA-FSM explores beyond the NLM while incurring tunable latency
and cost overheads. The design emphasizes transparent, well-cited answers for
high-stakes technical work and is generalizable to other scientific domains.

</details>


### [60] [KAME: Tandem Architecture for Enhancing Knowledge in Real-Time Speech-to-Speech Conversational AI](https://arxiv.org/abs/2510.02327)
*So Kuroki,Yotaro Kubo,Takuya Akiba,Yujin Tang*

Main category: cs.CL

TL;DR: 提出一种混合架构，结合实时语音到语音模型和后端大语言模型，实现实时响应的同时提升语义理解和知识丰富性。


<details>
  <summary>Details</summary>
Motivation: 现有实时语音对话系统缺乏知识深度，而级联方法虽知识丰富但延迟高，影响交互自然性。

Method: 使用S2S Transformer处理输入语音并实时生成响应，同时将查询发送至后端大语言模型，将其文本输出动态注入S2S模型以增强生成内容。

Result: 在MT-Bench多轮问答语音版本上评测，系统在回答正确性上显著优于纯S2S基线，接近级联系统水平，且延迟与基线相当。

Conclusion: 该混合架构有效平衡了低延迟与知识深度，为实时语音对话系统提供了优越的性能折衷。

Abstract: Real-time speech-to-speech (S2S) models excel at generating natural,
low-latency conversational responses but often lack deep knowledge and semantic
understanding. Conversely, cascaded systems combining automatic speech
recognition, a text-based Large Language Model (LLM), and text-to-speech
synthesis offer superior knowledge representation at the cost of high latency,
which disrupts the flow of natural interaction. This paper introduces a novel
hybrid architecture that bridges the gap between these two paradigms. Our
framework processes user speech through an S2S transformer for immediate
responsiveness while concurrently relaying the query to a powerful back-end
LLM. The LLM's text-based response is then injected in real time to guide the
S2S model's speech generation, effectively infusing its output with rich
knowledge without the full latency penalty of a cascaded system. We evaluated
our method using a speech-synthesized variant of the MT-Bench benchmark that
consists of multi-turn question-answering sessions. The results demonstrate
that our system substantially outperforms a baseline S2S model in response
correctness, approaching that of a cascaded system, while maintaining a latency
on par with the baseline.

</details>


### [61] [AMANDA: Agentic Medical Knowledge Augmentation for Data-Efficient Medical Visual Question Answering](https://arxiv.org/abs/2510.02328)
*Ziqing Wang,Chengsheng Mao,Xiaole Wen,Yuan Luo,Kaize Ding*

Main category: cs.CL

TL;DR: 提出了一种无需训练的智能体框架AMANDA，通过LLM智能体增强医学知识，解决医学多模态大模型在低资源场景下的内在和外在推理瓶颈，显著提升零样本和少样本医学视觉问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学多模态大模型在低资源环境下因缺乏标注数据而表现不佳，主要受限于无法充分提取医学图像细节（内在推理瓶颈）和整合专业医学知识（外在推理瓶颈）。

Method: 提出AMANDA框架：1）内在知识增强通过从粗到细的问题分解实现全面诊断；2）外在知识增强通过检索生物医学知识图谱来支撑推理过程，整个框架无需训练。

Result: 在八个医学视觉问答基准上进行的广泛实验表明，AMANDA在零样本和少样本设置下均显著优于现有方法。

Conclusion: AMANDA通过代理式知识增强有效克服了医学多模态模型的推理瓶颈，为低资源场景下的医学视觉问答提供了高效、无需训练的新方案。

Abstract: Medical Multimodal Large Language Models (Med-MLLMs) have shown great promise
in medical visual question answering (Med-VQA). However, when deployed in
low-resource settings where abundant labeled data are unavailable, existing
Med-MLLMs commonly fail due to their medical reasoning capability bottlenecks:
(i) the intrinsic reasoning bottleneck that ignores the details from the
medical image; (ii) the extrinsic reasoning bottleneck that fails to
incorporate specialized medical knowledge. To address those limitations, we
propose AMANDA, a training-free agentic framework that performs medical
knowledge augmentation via LLM agents. Specifically, our intrinsic medical
knowledge augmentation focuses on coarse-to-fine question decomposition for
comprehensive diagnosis, while extrinsic medical knowledge augmentation grounds
the reasoning process via biomedical knowledge graph retrieval. Extensive
experiments across eight Med-VQA benchmarks demonstrate substantial
improvements in both zero-shot and few-shot Med-VQA settings. The code is
available at https://github.com/REAL-Lab-NU/AMANDA.

</details>


### [62] [SelfJudge: Faster Speculative Decoding via Self-Supervised Judge Verification](https://arxiv.org/abs/2510.02329)
*Kanghoon Yoon,Minsub Kim,Sungjae Lee,Joonhyung Lee,Sunghyeon Woo,Yeonjun In,Se Jung Kwon,Chanyoung Park,Dongsoo Lee*

Main category: cs.CL

TL;DR: SelfJudge提出了一种自监督训练判别器的方法，通过评估替换标记后响应的语义保持性，实现无需人工标注的通用快速大模型推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有判别解码方法依赖人工标注或具有可验证真实答案的任务，限制了在多样化NLP任务中的泛化能力。

Method: 提出SelfJudge，利用目标模型的自监督信号训练判别器，通过评估替换标记后的响应是否保持原意来衡量语义一致性，从而自动训练验证器。

Result: 实验表明，SelfJudge在推理速度与准确性权衡上优于现有的判别解码基线方法。

Conclusion: SelfJudge提供了一种广泛适用的、无需人工标注的LLM推理加速方案。

Abstract: Speculative decoding accelerates LLM inference by verifying candidate tokens
from a draft model against a larger target model. Recent judge decoding boosts
this process by relaxing verification criteria by accepting draft tokens that
may exhibit minor discrepancies from target model output, but existing methods
are restricted by their reliance on human annotations or tasks with verifiable
ground truths, limiting generalizability across diverse NLP tasks. We propose
SelfJudge, which trains judge verifiers via self-supervision of the target
model. Our method measures semantic preservation by assessing whether
token-substituted responses preserve the meaning of original responses,
enabling automatic verifier training across diverse NLP tasks. Our experiments
show SelfJudge achieves superior inference-accuracy trade-offs than judge
decoding baselines, offering a broadly applicable solution for faster LLM
inference.

</details>


### [63] [EntropyLong: Effective Long-Context Training via Predictive Uncertainty](https://arxiv.org/abs/2510.02330)
*Junlong Jia,Ziyang Chen,Xing Wu,Chaochen Gao,Zijia Lin,Debing Zhang,Songlin Hu,Binghui Guo*

Main category: cs.CL

TL;DR: 提出EntropyLong方法，利用预测不确定性验证长距离依赖质量，构建高质量长上下文训练数据，显著提升模型在长程依赖任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文数据构建方法（如文本拼接或启发式方法）难以保证真正的长距离依赖，缺乏对依赖质量的可靠验证。

Method: 提出EntropyLong：识别文档中的高熵位置，从大型语料库中检索语义相关上下文，并通过评估其是否降低预测熵来验证实用性，构建模型在环的验证机制。

Result: 基于FineWebEdu和Cosmopedia构建了128K长度序列的高质量数据集；在RULER和LongBenchv2基准上显著优于基线模型，尤其在远距离信息任务中表现突出。

Conclusion: 基于熵的验证机制对长上下文训练至关重要，EntropyLong能有效确保信息增益，提升模型长程依赖理解能力。

Abstract: Training long-context language models to capture long-range dependencies
requires specialized data construction. Current approaches, such as generic
text concatenation or heuristic-based variants, frequently fail to guarantee
genuine long-range dependencies. We propose EntropyLong, a novel data
construction method that leverages predictive uncertainty to verify dependency
quality. Our approach identifies high-entropy positions in documents, retrieves
semantically relevant contexts from large corpora, and verifies their utility
by assessing whether they reduce prediction entropy. This model-in-the-loop
verification ensures each dependency represents measurable information gain
rather than spurious correlation. We construct training samples with long-range
dependencies by combining original documents with these verified contextual
supplements. Using FineWebEdu and Cosmopedia, we generate a dataset of
128K-length sequences with verified dependencies. Models trained on this data
demonstrate significant improvements on RULER benchmarks, particularly in tasks
requiring distant information. Following instruction fine-tuning, our models
also achieve substantial gains on LongBenchv2, demonstrating enhanced
long-context understanding. Extensive ablation studies further validate the
necessity and effectiveness of entropybased verification for long-context
training.

</details>


### [64] [Synthetic Dialogue Generation for Interactive Conversational Elicitation & Recommendation (ICER)](https://arxiv.org/abs/2510.02331)
*Moonkyung Ryu,Chih-Wei Hsu,Yinlam Chow,Mohammad Ghavamzadeh,Craig Boutilier*

Main category: cs.CL

TL;DR: 提出一种结合行为模拟器和语言模型提示的方法，生成具有用户状态一致性的自然对话，用于构建大规模开源对话推荐系统数据集。


<details>
  <summary>Details</summary>
Motivation: 公共对话推荐系统（CRS）数据稀缺，导致语言模型微调困难；现有的用户模拟方法缺乏行为一致性。

Method: 利用行为模拟器与语言模型提示相结合，生成符合用户潜在状态的自然对话，并构建包含偏好获取和示例批评的CRS数据集。

Result: 生成的对话在评分者评估中表现出较高的一致性、事实性和自然性。

Conclusion: 该方法能有效生成高质量、行为一致的CRS对话数据，有助于缓解数据稀缺问题。

Abstract: While language models (LMs) offer great potential for conversational
recommender systems (CRSs), the paucity of public CRS data makes fine-tuning
LMs for CRSs challenging. In response, LMs as user simulators qua data
generators can be used to train LM-based CRSs, but often lack behavioral
consistency, generating utterance sequences inconsistent with those of any real
user. To address this, we develop a methodology for generating natural
dialogues that are consistent with a user's underlying state using behavior
simulators together with LM-prompting. We illustrate our approach by generating
a large, open-source CRS data set with both preference elicitation and example
critiquing. Rater evaluation on some of these dialogues shows them to exhibit
considerable consistency, factuality and naturalness.

</details>


### [65] [A High-Capacity and Secure Disambiguation Algorithm for Neural Linguistic Steganography](https://arxiv.org/abs/2510.02332)
*Yapei Feng,Feng Jiang,Shanhao Wu,Hua Zhong*

Main category: cs.CL

TL;DR: 提出了一种名为look-ahead Sync的新方法，解决了神经语言隐写中因分词歧义导致的解码失败和嵌入容量不足的问题，在保证安全性的同时显著提升了嵌入容量。


<details>
  <summary>Details</summary>
Motivation: 现有方法SyncPool虽能解决分词歧义带来的解码失败问题，但牺牲了嵌入容量，限制了实际应用，亟需一种既能保持安全又能提升容量的新方法。

Method: 通过仅对真正不可区分的标记序列进行最小化同步采样，保留所有可区分路径以最大化嵌入容量，同时维持安全性。

Result: 理论证明了方法的安全性，并在英（Llama 3）中（Qwen 2.5）文数据上实验显示，嵌入率分别提升超过160%和25%，尤其在候选池较大时优势更明显。

Conclusion: 所提方法在保持可证明安全性的同时，显著逼近理论容量上限，推动了高容量语言隐写的实用化进程。

Abstract: Neural linguistic steganography aims to embed information
  into natural text while preserving statistical undetectability. A fundamental
challenge in this ffeld stems from tokenization ambiguity in modern tokenizers,
which can lead to catastrophic decoding failures. The recent method, SyncPool,
addresses this ambiguity
  by employing a coarse-grained synchronization mechanism over groups of
ambiguous candidates. However, SyncPool sacriffces embedding capacity, as it
utilizes the entire Shannon entropy of an ambiguous group solely for
synchronization rather than for payload embedding. We propose a method named
look-ahead Sync, which overcomes the capacity limitation of SyncPool while
retaining its provable security guarantees. Our approach performs minimal
synchronized sampling only on truly indistinguishable token sequences, while
strategically preserving all other discernible paths to maximize embedding
capacity. We provide theoretical proofs for the security of our method and
analyze the gap between its achievable embedding capacity and the theoretical
upper bound. Experiments on English (using Llama 3) and Chinese (using Qwen
2.5) benchmarks show that our method consistently approaches the theoretical
capacity upper bound and signiffcantly outperforms SyncPool. The improvement in
embedding rate exceeds 160% in English and 25% in Chinese, particularly in
settings with larger candidate pools. This work represents a signiffcant step
toward practical high-capacity provably secure linguistic steganography.

</details>


### [66] [Revisiting Direct Speech-to-Text Translation with Speech LLMs: Better Scaling than CoT Prompting?](https://arxiv.org/abs/2510.03093)
*Oriol Pareras,Gerard I. Gállego,Federico Costa,Cristina España-Bonet,Javier Hernando*

Main category: cs.CL

TL;DR: 本文系统比较了链式思维（CoT）和直接提示在语音到文本翻译（S2TT）中的表现，发现随着S2TT数据量的增加，直接提示方法提升更稳定，可能在未来更大规模数据下更有效。


<details>
  <summary>Details</summary>
Motivation: 近年来S2TT研究多采用基于大语言模型和CoT提示的方法，因其可利用丰富的ASR和T2TT数据。但随着S2TT数据增长，尚不清楚CoT是否仍优于直接提示，本文旨在系统比较二者在不同数据规模下的表现。

Method: 通过将ASR语料的转录文本伪标注为六种欧洲语言，构建多语言S2TT数据，在不同数据规模下训练基于LLM的S2TT系统，比较CoT与直接提示的效果。

Result: 实验表明，随着S2TT数据量增加，直接提示的性能提升更为一致，而CoT的优势逐渐减弱。

Conclusion: 当S2TT数据足够丰富时，直接提示可能比CoT更有效，提示未来应重视直接端到端S2TT方法的发展。

Abstract: Recent work on Speech-to-Text Translation (S2TT) has focused on LLM-based
models, introducing the increasingly adopted Chain-of-Thought (CoT) prompting,
where the model is guided to first transcribe the speech and then translate it.
CoT typically outperforms direct prompting primarily because it can exploit
abundant Automatic Speech Recognition (ASR) and Text-to-Text Translation (T2TT)
datasets to explicitly model its steps. In this paper, we systematically
compare CoT and Direct prompting under increasing amounts of S2TT data. To this
end, we pseudo-label an ASR corpus by translating its transcriptions into six
European languages, and train LLM-based S2TT systems with both prompting
strategies at different data scales. Our results show that Direct improves more
consistently as the amount of data increases, suggesting that it may become a
more effective approach as larger S2TT resources are created.

</details>


### [67] [Human Mobility Datasets Enriched With Contextual and Social Dimensions](https://arxiv.org/abs/2510.02333)
*Chiara Pugliese,Francesco Lettich,Guido Rocchietti,Chiara Renso,Fabio Pinelli*

Main category: cs.CL

TL;DR: 本文介绍了两个公开可用的语义增强人类轨迹数据集及其构建流程，结合真实移动数据、多源上下文信息与大语言模型生成的合成社交媒体文本，支持多模态、语义化的移动分析。


<details>
  <summary>Details</summary>
Motivation: 为了支持更丰富的移动行为分析和语义理解，需要融合真实轨迹、上下文信息与自然语言描述的高质量数据集，但目前此类资源匮乏。

Method: 基于OpenStreetMap的公开GPS轨迹，构建包含停留点、移动段、兴趣点、交通方式推断和天气数据的多层语义数据集，并利用大语言模型生成逼真的社交媒体文本；数据以表格和RDF格式提供，配套开源可复现的构建流程。

Result: 发布了涵盖巴黎和纽约两个大城市的两个数据集，支持语义推理和FAIR原则，兼具真实轨迹与LLM生成文本，可在多种格式下使用。

Conclusion: 该资源是首个将真实移动、结构化语义增强、大语言模型生成文本与语义网技术结合的可重用框架，为行为建模、移动预测、知识图谱构建和LLM应用提供了新机遇。

Abstract: In this resource paper, we present two publicly available datasets of
semantically enriched human trajectories, together with the pipeline to build
them. The trajectories are publicly available GPS traces retrieved from
OpenStreetMap. Each dataset includes contextual layers such as stops, moves,
points of interest (POIs), inferred transportation modes, and weather data. A
novel semantic feature is the inclusion of synthetic, realistic social media
posts generated by Large Language Models (LLMs), enabling multimodal and
semantic mobility analysis. The datasets are available in both tabular and
Resource Description Framework (RDF) formats, supporting semantic reasoning and
FAIR data practices. They cover two structurally distinct, large cities: Paris
and New York. Our open source reproducible pipeline allows for dataset
customization, while the datasets support research tasks such as behavior
modeling, mobility prediction, knowledge graph construction, and LLM-based
applications. To our knowledge, our resource is the first to combine real-world
movement, structured semantic enrichment, LLM-generated text, and semantic web
compatibility in a reusable framework.

</details>


### [68] [Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation](https://arxiv.org/abs/2510.03115)
*Jacobo Romero-Díaz,Gerard I. Gállego,Oriol Pareras,Federico Costa,Javier Hernando,Cristina España-Bonet*

Main category: cs.CL

TL;DR: 尽管链式思维（CoT）提示被寄望改善语音到文本翻译（S2TT）系统，但研究发现其仍主要依赖转录文本，极少利用语音信息，表现类似于级联模型。通过引入直接S2TT数据或注入噪声转录可提升鲁棒性和语音依赖性。


<details>
  <summary>Details</summary>
Motivation: 现有S2TT系统受限于错误传播和无法利用语调等声学线索，CoT提示被提出以同时利用语音和文本信息，期望克服这些缺陷。

Method: 通过归因分析、使用损坏转录进行鲁棒性评估以及检测对语调的感知能力，分析CoT提示在S2TT中的实际行为，并测试加入直接S2TT数据和噪声转录注入的影响。

Result: CoT提示主要依赖文本转录，几乎不利用语音信号；引入直接S2TT数据或噪声转录可提高模型鲁棒性并增强对语音的依赖。

Conclusion: 当前CoT提示并未有效融合语音信息，其行为仍接近级联模式；需设计能显式整合声学信息的新架构以实现真正端到端的S2TT。

Abstract: Speech-to-Text Translation (S2TT) systems built from Automatic Speech
Recognition (ASR) and Text-to-Text Translation (T2TT) modules face two major
limitations: error propagation and the inability to exploit prosodic or other
acoustic cues. Chain-of-Thought (CoT) prompting has recently been introduced,
with the expectation that jointly accessing speech and transcription will
overcome these issues. Analyzing CoT through attribution methods, robustness
evaluations with corrupted transcripts, and prosody-awareness, we find that it
largely mirrors cascaded behavior, relying mainly on transcripts while barely
leveraging speech. Simple training interventions, such as adding Direct S2TT
data or noisy transcript injection, enhance robustness and increase speech
attribution. These findings challenge the assumed advantages of CoT and
highlight the need for architectures that explicitly integrate acoustic
information into translation.

</details>


### [69] [Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing](https://arxiv.org/abs/2510.02334)
*Zhe Li,Wei Zhao,Yige Li,Jun Sun*

Main category: cs.CL

TL;DR: 本文提出了一种新的高效框架，通过分析大语言模型的表示及其梯度，定位导致有害内容、事实错误等不良行为的训练数据来源，实现了细粒度的样本和词元级归因分析。


<details>
  <summary>Details</summary>
Motivation: 大语言模型常表现出有害内容生成、事实错误和偏见等问题，现有归因方法因噪声大、计算复杂难以准确诊断其根本原因，亟需更有效的诊断工具以提升AI安全性。

Method: 提出一种基于模型激活空间中表示及其梯度的分析框架，直接在语义空间中建立模型输出与训练数据之间的因果关联，实现对不良行为的精准归因。

Result: 在追踪有害内容、检测后门投毒和识别知识污染等任务中，该方法在样本级和词元级均表现出优越的归因能力，能精确定位影响模型行为的关键样本和短语。

Conclusion: 该框架为理解和审计大语言模型的风险提供了有力工具，有助于最终缓解其不良行为，提升模型的安全性和可信度。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities, yet
their deployment is frequently undermined by undesirable behaviors such as
generating harmful content, factual inaccuracies, and societal biases.
Diagnosing the root causes of these failures poses a critical challenge for AI
safety. Existing attribution methods, particularly those based on parameter
gradients, often fall short due to prohibitive noisy signals and computational
complexity. In this work, we introduce a novel and efficient framework that
diagnoses a range of undesirable LLM behaviors by analyzing representation and
its gradients, which operates directly in the model's activation space to
provide a semantically meaningful signal linking outputs to their training
data. We systematically evaluate our method for tasks that include tracking
harmful content, detecting backdoor poisoning, and identifying knowledge
contamination. The results demonstrate that our approach not only excels at
sample-level attribution but also enables fine-grained token-level analysis,
precisely identifying the specific samples and phrases that causally influence
model behavior. This work provides a powerful diagnostic tool to understand,
audit, and ultimately mitigate the risks associated with LLMs. The code is
available at https://github.com/plumprc/RepT.

</details>


### [70] [FormalML: A Benchmark for Evaluating Formal Subgoal Completion in Machine Learning Theory](https://arxiv.org/abs/2510.02335)
*Xiao-Wen Yang,Zihao Zhang,Jianuo Cao,Zhi Zhou,Zenan Li,Lan-Zhe Guo,Yuan Yao,Taolue Chen,Yu-Feng Li,Xiaoxing Ma*

Main category: cs.CL

TL;DR: 本文提出了一个名为FormalML的新基准，用于评估大语言模型在数学证明中子目标补全任务中的表现，揭示了现有模型在准确性和效率上的不足。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在形式化定理证明中取得了进展，但其作为数学家实用助手的能力，特别是在复杂证明中补全缺失步骤的能力，仍缺乏探索。

Method: 构建了一个基于Lean 4的形式化基准FormalML，覆盖机器学习基础理论中的优化与概率不等式，通过翻译策略将过程性证明转为声明性形式，提取出4937个不同难度的问题。

Result: 现有最先进的定理证明器在FormalML上的表现显示出在准确性与效率方面的持续限制。

Conclusion: FormalML是首个结合前提检索与复杂科研级上下文的子目标补全基准，突显了开发更强大LLM驱动证明器的必要性。

Abstract: Large language models (LLMs) have recently demonstrated remarkable progress
in formal theorem proving. Yet their ability to serve as practical assistants
for mathematicians, filling in missing steps within complex proofs, remains
underexplored. We identify this challenge as the task of subgoal completion,
where an LLM must discharge short but nontrivial proof obligations left
unresolved in a human-provided sketch. To study this problem, we introduce
FormalML, a Lean 4 benchmark built from foundational theories of machine
learning. Using a translation tactic that converts procedural proofs into
declarative form, we extract 4937 problems spanning optimization and
probability inequalities, with varying levels of difficulty. FormalML is the
first subgoal completion benchmark to combine premise retrieval and complex
research-level contexts. Evaluation of state-of-the-art provers highlights
persistent limitations in accuracy and efficiency, underscoring the need for
more capable LLM-based theorem provers for effective subgoal completion,

</details>


### [71] [KurdSTS: The Kurdish Semantic Textual Similarity](https://arxiv.org/abs/2510.02336)
*Abdulhady Abas Abdullah,Hadi Veisi,Hussein M. Al*

Main category: cs.CL

TL;DR: 本文介绍了首个库尔德语语义文本相似度（STS）数据集，包含1万对句子，涵盖正式与非正式语体，并评估了多种主流模型在该数据集上的表现，揭示了库尔德语在形态、拼写变异和语码混合方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于低资源语言如库尔德语缺乏语义相似度研究资源，阻碍了相关NLP任务的发展，因此需要构建首个库尔德语STS数据集以推动该领域研究。

Method: 构建了一个包含10,000个句子对的库尔德语STS数据集，涵盖多种语言变体，并采用Sentence-BERT、多语言BERT及其他强基线模型进行基准测试。

Result: 模型在数据集上取得了具有竞争力的结果，但库尔德语的复杂形态、拼写变异和语码混合给语义匹配带来了显著挑战。

Conclusion: 该数据集和基线模型为库尔德语语义研究和低资源NLP提供了可复现的评估框架和重要起点。

Abstract: Semantic Textual Similarity (STS) measures the degree of meaning overlap
between two texts and underpins many NLP tasks. While extensive resources exist
for high-resource languages, low-resource languages such as Kurdish remain
underserved. We present, to our knowledge, the first Kurdish STS dataset:
10,000 sentence pairs spanning formal and informal registers, each annotated
for similarity. We benchmark Sentence-BERT, multilingual BERT, and other strong
baselines, obtaining competitive results while highlighting challenges arising
from Kurdish morphology, orthographic variation, and code-mixing. The dataset
and baselines establish a reproducible evaluation suite and provide a strong
starting point for future research on Kurdish semantics and low-resource NLP.

</details>


### [72] [CRACQ: A Multi-Dimensional Approach To Automated Document Assessment](https://arxiv.org/abs/2510.02337)
*Ishak Soltani,Francisco Belo,Bernardo Tavares*

Main category: cs.CL

TL;DR: CRACQ是一个多维度评估框架，用于评估机器生成文本在连贯性、严谨性、适当性、完整性和质量五个维度上的表现，相比单一评分和大模型评审，具有更稳定、可解释的评估能力。


<details>
  <summary>Details</summary>
Motivation: 现有的自动评估方法多依赖单一总分或大模型打分，缺乏可解释性和多维度分析能力，难以全面评估复杂文本（如基金申请）。CRACQ旨在提供一个结构化、可解释的多维度评估框架，适用于多种生成文本。

Method: 借鉴自动作文评分中的特质评分思想，CRACQ构建包含五个评估维度的评分标准（rubric），融合语言学、语义和结构信号，实现细粒度与整体性结合的评估。模型在500份合成的基金申请书上训练，并在真实申请案例上与LLM-as-a-judge进行对比测试。

Result: 实验表明，CRACQ相比直接使用大模型评审，在各特质维度上提供更稳定、可解释的评估结果，尤其在区分强弱申请方面表现良好，但仍在可靠性和领域适用范围方面存在挑战。

Conclusion: CRACQ为机器生成文本的评估提供了一种结构化、可解释的多维度框架，具有超越传统单一评分方法的潜力，未来需进一步提升其跨领域稳定性与可靠性。

Abstract: This paper presents CRACQ, a multi-dimensional evaluation framework tailored
to evaluate documents across f i v e specific traits: Coherence, Rigor,
Appropriateness, Completeness, and Quality. Building on insights from
traitbased Automated Essay Scoring (AES), CRACQ expands its fo-cus beyond
essays to encompass diverse forms of machine-generated text, providing a
rubricdriven and interpretable methodology for automated evaluation. Unlike
singlescore approaches, CRACQ integrates linguistic, semantic, and structural
signals into a cumulative assessment, enabling both holistic and trait-level
analysis. Trained on 500 synthetic grant pro-posals, CRACQ was benchmarked
against an LLM-as-a-judge and further tested on both strong and weak real
applications. Preliminary results in-dicate that CRACQ produces more stable and
interpretable trait-level judgments than direct LLM evaluation, though
challenges in reliability and domain scope remain

</details>


### [73] [Optimizing Long-Form Clinical Text Generation with Claim-Based Rewards](https://arxiv.org/abs/2510.02338)
*Samyak Jhaveri,Praphul Singh,Jangwon Kim,Tara Taghavi,Krishnaram Kenthapadi*

Main category: cs.CL

TL;DR: 提出一种结合Group Relative Policy Optimization (GRPO) 和 DocLens 的评估集成强化学习框架，用于优化长篇临床文本生成，提升事实准确性和完整性，降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 临床文档自动化需要语言模型在事实性、完整性和简洁性等方面高度对齐，现有方法依赖人工标注或独立奖励模型，成本高且扩展性差。

Method: 提出一种评估集成强化学习框架，使用GRPO进行策略优化，结合DocLens（一种基于对话、断言级别的确定性评估器）提供奖励信号，无需训练独立奖励模型或依赖人工参考文本，并引入奖励门控策略以降低训练成本。

Result: 实验表明，该方法在临床记录质量上显著提升，GPT-5的定性评估显示其在事实性、完整性和简洁性方面更受偏好，遗漏和幻觉更少；同时训练成本更低。

Conclusion: 该框架可扩展至实际应用，支持自定义目标（如指南依从性、计费偏好），且在已有良好对齐基础模型的情况下，当前改进仍可能是保守估计。

Abstract: Automating clinical documentation with large language models requires precise
alignment with priorities such as completeness and factual grounding. We
present an evaluation-integrated reinforcement learning framework for long-form
clinical text generation that couples Group Relative Policy Optimization (GRPO)
with DocLens, a claim-level evaluator that provides deterministic,
dialogue-grounded rewards. Our method directly optimizes factual grounding and
completeness without training a separate reward model or relying on
human-authored references. Empirically, the approach improves clinical note
quality and reduces training cost via a simple reward-gating strategy. An
independent GPT-5 qualitative evaluation further supports these gains, showing
higher preference for GRPO outputs in factuality, completeness, and brevity,
with fewer omissions and hallucinations. Because the benchmarks are relatively
clean and the base model already well aligned, these improvements likely
represent a conservative lower bound. The framework is scalable to real-world
settings and can incorporate custom objectives such as guideline adherence or
billing preferences.

</details>


### [74] [Evaluating Uncertainty Quantification Methods in Argumentative Large Language Models](https://arxiv.org/abs/2510.02339)
*Kevin Zhou,Adam Dejl,Gabriel Freedman,Lihu Chen,Antonio Rago,Francesca Toni*

Main category: cs.CL

TL;DR: 直接提示法在论证型大语言模型（ArgLLMs）中作为一种简单的不确定性量化（UQ）策略表现出色，优于更复杂的方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在决策系统中的应用日益广泛，确保其可靠性变得至关重要。本文旨在探索在基于计算论证的可解释性LLM框架（即ArgLLMs）中集成UQ方法的有效性，特别是在处理复杂或争议性陈述时。

Method: 研究评估了不同LLM UQ方法在ArgLLMs进行主张验证任务时的表现，通过实验比较各种UQ方法的效果，并提出了一种新的评估UQ方法有效性的实验范式。

Result: 实验结果表明，尽管结构简单，直接提示法在ArgLLMs中的UQ效果优于多种更复杂的UQ方法。

Conclusion: 直接提示是一种高效且简单的UQ策略，适用于ArgLLMs框架下的决策任务，为评估LLM中的不确定性提供了一种新颖且有效的途径。

Abstract: Research in uncertainty quantification (UQ) for large language models (LLMs)
is increasingly important towards guaranteeing the reliability of this
groundbreaking technology. We explore the integration of LLM UQ methods in
argumentative LLMs (ArgLLMs), an explainable LLM framework for decision-making
based on computational argumentation in which UQ plays a critical role. We
conduct experiments to evaluate ArgLLMs' performance on claim verification
tasks when using different LLM UQ methods, inherently performing an assessment
of the UQ methods' effectiveness. Moreover, the experimental procedure itself
is a novel way of evaluating the effectiveness of UQ methods, especially when
intricate and potentially contentious statements are present. Our results
demonstrate that, despite its simplicity, direct prompting is an effective UQ
strategy in ArgLLMs, outperforming considerably more complex approaches.

</details>


### [75] [Can Prompts Rewind Time for LLMs? Evaluating the Effectiveness of Prompted Knowledge Cutoffs](https://arxiv.org/abs/2510.02340)
*Xin Gao,Ruiyi Zhang,Daniel Du,Saurabh Mahindre,Sai Ashish Somayajula,Pengtao Xie*

Main category: cs.CL

TL;DR: 该研究探讨了通过提示词（prompting）让大语言模型（LLM）模拟早期知识截止点的能力，结果表明这种方法在直接查询时有效，但在处理因果相关知识时难以实现真正“遗忘”。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型在时间预测任务中依赖预训练数据，可能存在记忆而非推理的问题，导致泛化能力被高估。因此，研究者希望探索能否通过提示词技术让模型“忘记”截止日期后的知识，以缓解数据污染问题。

Method: 构建了三个评估数据集，分别测试模型对（1）直接事实知识、（2）语义变化、（3）因果相关知识的遗忘能力，并评估基于提示的“知识注销”效果。

Result: 实验结果显示，提示方法在直接询问截止后信息时有一定遗忘效果，但在处理未直接提及但与遗忘内容有因果关联的问题时表现不佳，模型仍会泄露“应忘记”的知识。

Conclusion: 提示词难以让模型彻底模拟早期知识截止，尤其在复杂推理场景下；因此需要更严格的评估设置来检验LLM在时间预测任务中的真实性能。

Abstract: Large Language Models (LLMs) are widely used for temporal prediction, but
their reliance on pretraining data raises contamination concerns, as accurate
predictions on pre-cutoff test data may reflect memorization rather than
reasoning, leading to an overestimation of their generalization capability.
With the recent emergence of prompting-based unlearning techniques, a natural
question arises: Can LLMs be prompted to simulate an earlier knowledge cutoff?
In this work, we investigate the capability of prompting to simulate earlier
knowledge cutoff in LLMs. We construct three evaluation datasets to assess the
extent to which LLMs can forget (1) direct factual knowledge, (2) semantic
shifts, and (3) causally related knowledge. Results demonstrate that while
prompt-based simulated knowledge cutoffs show effectiveness when directly
queried with the information after that date, they struggle to induce
forgetting when the forgotten content is not directly asked but causally
related to the query. These findings highlight the need for more rigorous
evaluation settings when applying LLMs for temporal prediction tasks. The full
dataset and evaluation code are available at
https://github.com/gxx27/time_unlearn.

</details>


### [76] [DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning](https://arxiv.org/abs/2510.02341)
*Yifan Wang,Bolian Li,Junlin Wu,Zhaoxuan Tan,Zheli Liu,Ruqi Zhang,Ananth Grama,Qingkai Zeng*

Main category: cs.CL

TL;DR: DRIFT是一种利用现实世界中丰富的用户不满信号进行偏好学习的新方法，通过动态采样正样本来迭代优化模型，在多种基准上显著优于现有方法，并在大规模模型上超越GPT-4o-mini。


<details>
  <summary>Details</summary>
Motivation: 现实场景中显式用户满意反馈稀少，而隐式不满信号丰富。现有偏好学习方法依赖大量正反馈或人工标注，与实际数据分布不匹配，因此需要一种能有效利用不满信号的方法。

Method: 提出DRIFT（Dissatisfaction-Refined Iterative Preference Training），以真实DSAT信号为训练锚点，从不断演进的策略中动态采样正样本进行迭代偏好训练，并理论分析其保持偏好边界和避免梯度退化的能力。

Result: 在WildBench和AlpacaEval2等基准上，DRIFT在7B和14B模型上均显著优于基线方法，最大提升达+12.29%；14B模型表现超过GPT-4o-mini，并展现出更强的解的多样性。

Conclusion: DRIFT是一种高效且可扩展的现实世界后训练方案，能有效利用最丰富且信息量大的不满信号，提升模型性能同时保持探索能力。

Abstract: Real-world large language model deployments (e.g., conversational AI systems,
code generation assistants) naturally generate abundant implicit user
dissatisfaction (DSAT) signals, as users iterate toward better answers through
refinements, corrections, and expressed preferences, while explicit
satisfaction (SAT) feedback is scarce. Existing preference learning approaches
are poorly aligned with this data profile, as they rely on costly human
annotations or assume plentiful positive responses. In this paper, we introduce
\textbf{DRIFT} (\textbf{D}issatisfaction-\textbf{R}efined \textbf{I}terative
pre\textbf{F}erence \textbf{T}raining), which anchors training on real-world
DSAT signals and samples positives dynamically from the evolving policy.
Empirically, DRIFT models trained on real-world \textit{WildFeedback} datasets
and synthetic \textit{UltraFeedback} datasets achieve up to +6.23\% (7B) /
+7.61\% (14B) on WildBench Task Score and up to +8.95\% (7B) / +12.29\% (14B)
on AlpacaEval2 win rate over base models, outperforming strong baseline methods
such as iterative DPO and SPIN. At larger scales, the improvements are
particularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on
WildBench. Further analysis shows that DRIFT also preserves exploratory
capacity, yielding more diverse high-reward solutions rather than collapsing to
narrow subsets. Theoretically, we demonstrate that this design preserves
preference margins and avoids the gradient degeneration. These results show
that DRIFT is an effective and scalable recipe for real-world post-training
that leverages the most abundant and informative signal. The code and data are
available at https://github.com/cacayaya/DRIFT.git.

</details>


### [77] [$\texttt{BluePrint}$: A Social Media User Dataset for LLM Persona Evaluation and Training](https://arxiv.org/abs/2510.02343)
*Aurélien Bück-Kaeffer,Je Qin Chooi,Dan Zhao,Maximilian Puelma Touzel,Kellin Pelrine,Jean-François Godbout,Reihaneh Rabbany,Zachary Yang*

Main category: cs.CL

TL;DR: 提出 SIMPACT 框架和 BluePrint 数据集，用于训练和评估基于大语言模型的社交媒体智能体，支持行为真实、风格逼真的社交模拟。


<details>
  <summary>Details</summary>
Motivation: 缺乏标准化数据资源来微调和评估作为社交媒体代理的大型语言模型（LLM），且真实用户研究面临伦理和操作挑战。

Method: 构建 SIMPACT 框架，通过聚类匿名用户行为生成 personas，并以 next-action prediction 为任务训练模型；使用 Bluesky 公共数据构建 BluePrint 数据集，包含 12 种交互类型及上下文关联的动作序列。

Result: 发布了大规模 BluePrint 数据集，支持上下文相关的语言和行为建模；提出集群级和群体级评估指标，用于衡量行为保真度和风格真实性。

Conclusion: SIMPACT 和 BluePrint 为高效、合乎伦理的社交媒体模拟提供了标准化数据与评估基础，可推广至虚假信息、极化等社会议题研究。

Abstract: Large language models (LLMs) offer promising capabilities for simulating
social media dynamics at scale, enabling studies that would be ethically or
logistically challenging with human subjects. However, the field lacks
standardized data resources for fine-tuning and evaluating LLMs as realistic
social media agents. We address this gap by introducing SIMPACT, the
SIMulation-oriented Persona and Action Capture Toolkit, a privacy respecting
framework for constructing behaviorally-grounded social media datasets suitable
for training agent models. We formulate next-action prediction as a task for
training and evaluating LLM-based agents and introduce metrics at both the
cluster and population levels to assess behavioral fidelity and stylistic
realism. As a concrete implementation, we release BluePrint, a large-scale
dataset built from public Bluesky data focused on political discourse.
BluePrint clusters anonymized users into personas of aggregated behaviours,
capturing authentic engagement patterns while safeguarding privacy through
pseudonymization and removal of personally identifiable information. The
dataset includes a sizable action set of 12 social media interaction types
(likes, replies, reposts, etc.), each instance tied to the posting activity
preceding it. This supports the development of agents that use
context-dependence, not only in the language, but also in the interaction
behaviours of social media to model social media users. By standardizing data
and evaluation protocols, SIMPACT provides a foundation for advancing rigorous,
ethically responsible social media simulations. BluePrint serves as both an
evaluation benchmark for political discourse modeling and a template for
building domain specific datasets to study challenges such as misinformation
and polarization.

</details>


### [78] [Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression](https://arxiv.org/abs/2510.02345)
*Peijun Zhu,Ning Yang,Jiayu Wei,Jinghang Wu,Haijun Zhang*

Main category: cs.CL

TL;DR: 提出一种基于动态专家聚类与结构化压缩的统一框架，有效缓解MoE大模型中的负载不均衡、参数冗余和通信开销三难问题，在保持性能的同时显著提升效率。


<details>
  <summary>Details</summary>
Motivation: MoE大语言模型面临负载不均衡、参数冗余和通信开销的三难困境，现有方法难以协同解决这三个问题，需要一种统一的结构优化方案来提升模型可扩展性与训练推理效率。

Method: 提出动态专家聚类与结构化压缩框架：通过参数与激活相似性的融合度量进行在线聚类，稳定专家利用率；在每个聚类内将专家权重分解为共享基础矩阵和极低秩残差适配器；采用两阶段分层路由减少搜索空间与通信量；结合FP16/INT4混合精度与非活跃簇动态卸载以降低内存占用。

Result: 在GLUE和WikiText-103上，模型性能与标准MoE相当，总参数减少约80%，吞吐量提升10%-20%，专家负载方差降低三倍以上，峰值内存接近稠密模型。

Conclusion: 结构重组是实现可扩展、高效且内存友好的MoE大模型的有效途径，动态聚类与分层压缩能协同优化模型效率与稳定性。

Abstract: Mixture-of-Experts (MoE) Large Language Models (LLMs) face a trilemma of load
imbalance, parameter redundancy, and communication overhead. We introduce a
unified framework based on dynamic expert clustering and structured compression
to address these issues cohesively. Our method employs an online clustering
procedure that periodically regroups experts using a fused metric of parameter
and activation similarity, which stabilizes expert utilization. To our
knowledge, this is one of the first frameworks to leverage the semantic
embedding capability of the router to dynamically reconfigure the model's
architecture during training for substantial efficiency gains. Within each
cluster, we decompose expert weights into a shared base matrix and extremely
low-rank residual adapters, achieving up to fivefold parameter reduction per
group while preserving specialization. This structure enables a two-stage
hierarchical routing strategy: tokens are first assigned to a cluster, then to
specific experts within it, drastically reducing the routing search space and
the volume of all-to-all communication. Furthermore, a heterogeneous precision
scheme, which stores shared bases in FP16 and residual factors in INT4, coupled
with dynamic offloading of inactive clusters, reduces peak memory consumption
to levels comparable to dense models. Evaluated on GLUE and WikiText-103, our
framework matches the quality of standard MoE models while reducing total
parameters by approximately 80%, improving throughput by 10% to 20%, and
lowering expert load variance by a factor of over three. Our work demonstrates
that structural reorganization is a principled path toward scalable, efficient,
and memory-effective MoE LLMs.

</details>


### [79] [Small Language Models for Curriculum-based Guidance](https://arxiv.org/abs/2510.02347)
*Konstantinos Katharakis,Sippo Rossi,Raghava Rao Mukkamala*

Main category: cs.CL

TL;DR: 小型语言模型（SLMs）结合检索增强生成（RAG）可在教育中作为高效、可持续的AI助教，性能媲美大型语言模型（LLMs），且具备低成本、低能耗和隐私保护优势。


<details>
  <summary>Details</summary>
Motivation: 探索在教育中采用生成式AI时，如何在保证教学效果的同时实现可持续、高效能的AI助教部署，尤其关注计算资源、能源消耗与数据隐私问题。

Method: 构建基于检索增强生成（RAG）的AI助教系统，评估八个开源小型语言模型（SLMs，7-17B参数），并与GPT-4o进行基准比较，使用课程相关查询测试其响应的准确性与教学适配性。

Result: 结果显示，在恰当提示和目标检索支持下，SLMs在生成准确且符合教学需求的回应方面可与LLMs（如GPT-4o）相媲美，并可在消费级硬件上实时运行，无需依赖云基础设施。

Conclusion: SLMs结合RAG是可持续、低成本、隐私友好的AI教学助手解决方案，适合教育机构在不牺牲性能的前提下实现个性化学习的大规模部署。

Abstract: The adoption of generative AI and large language models (LLMs) in education
is still emerging. In this study, we explore the development and evaluation of
AI teaching assistants that provide curriculum-based guidance using a
retrieval-augmented generation (RAG) pipeline applied to selected open-source
small language models (SLMs). We benchmarked eight SLMs, including LLaMA 3.1,
IBM Granite 3.3, and Gemma 3 (7-17B parameters), against GPT-4o. Our findings
show that with proper prompting and targeted retrieval, SLMs can match LLMs in
delivering accurate, pedagogically aligned responses. Importantly, SLMs offer
significant sustainability benefits due to their lower computational and energy
requirements, enabling real-time use on consumer-grade hardware without
depending on cloud infrastructure. This makes them not only cost-effective and
privacy-preserving but also environmentally responsible, positioning them as
viable AI teaching assistants for educational institutions aiming to scale
personalized learning in a sustainable and energy-efficient manner.

</details>


### [80] [mini-vec2vec: Scaling Universal Geometry Alignment with Linear Transformations](https://arxiv.org/abs/2510.02348)
*Guy Dar*

Main category: cs.CL

TL;DR: 本文提出了一种名为mini-vec2vec的新方法，用于在没有平行数据的情况下对齐文本嵌入空间，该方法比原始的vec2vec更高效、更稳定，并且学习到的映射是线性的。


<details>
  <summary>Details</summary>
Motivation: 原始的vec2vec方法虽然能找到接近完美的对齐，但计算成本高且不稳定，因此需要一个更简单高效的替代方案。

Method: mini-vec2vec方法包含三个主要阶段：伪平行嵌入向量的初步匹配、变换拟合以及迭代精炼。

Result: 与原始的vec2vec相比，新方法在效率上提高了几个数量级，同时结果相匹配或更优。

Conclusion: mini-vec2vec方法具有高稳定性及可解释的算法步骤，便于扩展并在新领域中应用。

Abstract: We build upon vec2vec, a procedure designed to align text embedding spaces
without parallel data. vec2vec finds a near-perfect alignment, but it is
expensive and unstable. We present mini-vec2vec, a simple and efficient
alternative that requires substantially lower computational cost and is highly
robust. Moreover, the learned mapping is a linear transformation. Our method
consists of three main stages: a tentative matching of pseudo-parallel
embedding vectors, transformation fitting, and iterative refinement. Our linear
alternative exceeds the original instantiation of vec2vec by orders of
magnitude in efficiency, while matching or exceeding their results. The
method's stability and interpretable algorithmic steps facilitate scaling and
unlock new opportunities for adoption in new domains and fields.

</details>


### [81] [LLMSQL: Upgrading WikiSQL for the LLM Era of Text-to-SQL](https://arxiv.org/abs/2510.02350)
*Dzmitry Pihulski,Karol Charchut,Viktoria Novogrodskaia,Jan Kocoń*

Main category: cs.CL

TL;DR: LLMSQL是对WikiSQL的系统性修订，旨在为大语言模型时代提供一个干净、规范化且易于使用的Text-to-SQL基准数据集，解决了原数据集中的多种标注和结构问题。


<details>
  <summary>Details</summary>
Motivation: WikiSQL因存在大小写不一致、数据类型错误、语法错误和未回答问题等缺陷，已不再适用于现代大语言模型的研究，因此需要一个更高质量、适配当前模型输入输出形式的替代基准。

Method: 对WikiSQL中的错误进行分类，并开发自动化方法进行清洗和重新标注；将数据格式调整为现代LLM友好的形式，即提供纯文本的自然语言问题和完整的SQL查询语句。

Result: 构建了LLMSQL数据集，支持直接生成和评估SQL查询；在多个主流大语言模型（如Gemma 3、LLaMA 3.2、Qwen 2.5等）上进行了评估，验证了其在提升模型训练与评测效果方面的有效性。

Conclusion: LLMSQL作为一个面向大语言模型的高质量Text-to-SQL基准，显著提升了数据质量与可用性，为未来自然语言接口研究提供了更可靠的数据基础。

Abstract: Converting natural language questions into SQL queries (Text-to-SQL) enables
non-expert users to interact with relational databases and has long been a
central task for natural language interfaces to data. While the WikiSQL dataset
played a key role in early NL2SQL research, its usage has declined due to
structural and annotation issues, including case sensitivity inconsistencies,
data type mismatches, syntax errors, and unanswered questions. We present
LLMSQL, a systematic revision and transformation of WikiSQL designed for the
LLM era. We classify these errors and implement automated methods for cleaning
and re-annotation. To assess the impact of these improvements, we evaluated
multiple large language models (LLMs), including Gemma 3, LLaMA 3.2, Mistral
7B, gpt-oss 20B, Phi-3.5 Mini, Qwen 2.5, OpenAI o4-mini, DeepSeek R1 and
others. Rather than serving as an update, LLMSQL is introduced as an LLM-ready
benchmark: unlike the original WikiSQL, tailored for pointer-network models
selecting tokens from input, LLMSQL provides clean natural language questions
and full SQL queries as plain text, enabling straightforward generation and
evaluation for modern natural language-to-SQL models.

</details>


### [82] [Language, Culture, and Ideology: Personalizing Offensiveness Detection in Political Tweets with Reasoning LLMs](https://arxiv.org/abs/2510.02351)
*Dzmitry Pihulski,Jan Kocoń*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型在采用特定政治和文化视角时，如何评估政治言论中的冒犯性，发现具备推理能力的较大模型更能敏感捕捉意识形态和文化差异。


<details>
  <summary>Details</summary>
Motivation: 由于政治言论的冒犯性判断高度依赖于意识形态和文化背景，研究旨在考察LLMs在不同政治立场和文化语境下进行个性化、细致化判断的能力。

Method: 使用2020年美国大选推文的多语言子集（MD-Agreement数据集），在英语、波兰语和俄语背景下，评估多个LLM（如DeepSeek-R1、o4-mini、GPT-4.1-mini等）从极右、保守、中间、进步等政治立场判断推文是否冒犯的表现。

Result: 具有显式推理能力的较大模型（如DeepSeek-R1、o4-mini）在不同意识形态和文化背景下表现出更高的一致性和敏感性，而较小模型难以捕捉细微差异；推理能力提升了判断的个性化与可解释性。

Conclusion: 推理机制对于实现跨语言、跨意识形态的细微社会政治文本分类至关重要，是提升LLMs在复杂语境下判断能力的关键。

Abstract: We explore how large language models (LLMs) assess offensiveness in political
discourse when prompted to adopt specific political and cultural perspectives.
Using a multilingual subset of the MD-Agreement dataset centered on tweets from
the 2020 US elections, we evaluate several recent LLMs - including DeepSeek-R1,
o4-mini, GPT-4.1-mini, Qwen3, Gemma, and Mistral - tasked with judging tweets
as offensive or non-offensive from the viewpoints of varied political personas
(far-right, conservative, centrist, progressive) across English, Polish, and
Russian contexts. Our results show that larger models with explicit reasoning
abilities (e.g., DeepSeek-R1, o4-mini) are more consistent and sensitive to
ideological and cultural variation, while smaller models often fail to capture
subtle distinctions. We find that reasoning capabilities significantly improve
both the personalization and interpretability of offensiveness judgments,
suggesting that such mechanisms are key to adapting LLMs for nuanced
sociopolitical text classification across languages and ideologies.

</details>


### [83] [Evaluating Bias in Spoken Dialogue LLMs for Real-World Decisions and Recommendations](https://arxiv.org/abs/2510.02352)
*Yihao Wu,Tianrui Wang,Yizhou Peng,Yi-Wen Chao,Xuyi Zhuang,Xinsheng Wang,Shunshun Yin,Ziyang Ma*

Main category: cs.CL

TL;DR: 本文首次系统研究了端到端语音对话模型中的偏见问题，发现闭源模型偏见较低，开源模型对年龄和性别更敏感，多轮对话可能加剧并持续偏见，并发布了FairDialogue数据集和评估代码。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型中的偏见已被广泛研究，但在具有音频输入输出的语音对话模型中，偏见的存在及其特征仍缺乏探索，尤其是副语言特征（如年龄、性别、口音）和多轮对话对偏见的影响。

Method: 通过构建多轮对话场景并引入重复负面反馈，使用Group Unfairness Score（GUS）和相似性归一化统计率（SNSR）量化决策与推荐任务中的偏见，评估了Qwen2.5-Omni、GLM-4-Voice、GPT-4o Audio和Gemini-2.5-Flash等模型。

Result: 闭源模型整体偏见较低；开源模型对年龄和性别更敏感；推荐任务会放大跨群体差异；多轮对话中偏见可能持续存在。

Conclusion: 语音对话模型中存在显著偏见，尤其是在多轮交互中，需特别关注公平性；本研究为构建公平可靠的音频交互系统提供了洞见，并公开数据集与代码以促进后续研究。

Abstract: While biases in large language models (LLMs), such as stereotypes and
cultural tendencies in outputs, have been examined and identified, their
presence and characteristics in spoken dialogue models (SDMs) with audio input
and output remain largely unexplored. Paralinguistic features, such as age,
gender, and accent, can affect model outputs; when compounded by multi-turn
conversations, these effects may exacerbate biases, with potential implications
for fairness in decision-making and recommendation tasks. In this paper, we
systematically evaluate biases in speech LLMs and study the impact of
multi-turn dialogues with repeated negative feedback. Bias is measured using
Group Unfairness Score (GUS) for decisions and similarity-based normalized
statistics rate (SNSR) for recommendations, across both open-source models like
Qwen2.5-Omni and GLM-4-Voice, as well as closed-source APIs such as GPT-4o
Audio and Gemini-2.5-Flash. Our analysis reveals that closed-source models
generally exhibit lower bias, while open-source models are more sensitive to
age and gender, and recommendation tasks tend to amplify cross-group
disparities. We found that biased decisions may persist in multi-turn
conversations. This work provides the first systematic study of biases in
end-to-end spoken dialogue models, offering insights towards fair and reliable
audio-based interactive systems. To facilitate further research, we release the
FairDialogue dataset and evaluation code.

</details>


### [84] [An Senegalese Legal Texts Structuration Using LLM-augmented Knowledge Graph](https://arxiv.org/abs/2510.02353)
*Oumar Kane,Mouhamad M. Allaya,Dame Samb,Mamadou Bousso*

Main category: cs.CL

TL;DR: 本研究探讨了人工智能和大语言模型在改善塞内加尔司法系统法律文本访问中的应用，成功提取并组织了近8000条法律条款，构建了包含2872个节点和10774个关系的图数据库，并验证了GPT-4o、GPT-4和Mistral-Large等模型在法律知识提取中的有效性。


<details>
  <summary>Details</summary>
Motivation: 塞内加尔法律文本分散且难以获取，公众和法律从业者难以有效理解权利与责任，亟需技术手段提升法律信息的组织与可访问性。

Method: 从多部法律文件中提取7967条法律条文，重点分析《土地与公共财产法典》，利用先进三元组抽取技术识别法律实体与关系，并构建图数据库以可视化法律文本间的关联。

Result: 成功构建包含2872个节点和10774个关系的法律知识图谱，GPT-4o、GPT-4和Mistral-Large模型在三元组抽取任务中表现优异，有效识别法律关系和元数据。

Conclusion: AI与大语言模型可为塞内加尔等法律信息基础设施薄弱的国家提供有效的法律知识管理框架，有助于提升司法透明度与公众法律意识。

Abstract: This study examines the application of artificial intelligence (AI) and large
language models (LLM) to improve access to legal texts in Senegal's judicial
system. The emphasis is on the difficulties of extracting and organizing legal
documents, highlighting the need for better access to judicial information. The
research successfully extracted 7,967 articles from various legal documents,
particularly focusing on the Land and Public Domain Code. A detailed graph
database was developed, which contains 2,872 nodes and 10,774 relationships,
aiding in the visualization of interconnections within legal texts. In
addition, advanced triple extraction techniques were utilized for knowledge,
demonstrating the effectiveness of models such as GPT-4o, GPT-4, and
Mistral-Large in identifying relationships and relevant metadata. Through these
technologies, the aim is to create a solid framework that allows Senegalese
citizens and legal professionals to more effectively understand their rights
and responsibilities.

</details>


### [85] [Modeling the language cortex with form-independent and enriched representations of sentence meaning reveals remarkable semantic abstractness](https://arxiv.org/abs/2510.02354)
*Shreya Saha,Shurui Li,Greta Tuckute,Yuanning Li,Ru-Yuan Zhang,Leila Wehbe,Evelina Fedorenko,Meenakshi Khosla*

Main category: cs.CL

TL;DR: 研究发现，语言皮层中的语义表征高度抽象且独立于语言形式，通过多图像生成和多义句嵌入平均化可更准确预测神经反应，表明人脑的语义表征比现有语言模型更丰富。


<details>
  <summary>Details</summary>
Motivation: 探讨人类语言系统中的语义表征是否具有抽象性和形式独立性，以及其是否超越当前语言模型的表示能力。

Method: 利用视觉和语言模型的嵌入表示来建模大脑对句子的神经响应，通过生成多个对应图像或多个释义句的嵌入平均来预测语言皮层活动。

Result: 多图像生成和多释义句嵌入平均显著提升神经响应预测准确性，甚至超过大型语言模型；加入隐含上下文细节进一步提升效果。

Conclusion: 语言皮层存在高度抽象、形式独立的语义表征，且其语义表征比当前语言模型更丰富和广泛。

Abstract: The human language system represents both linguistic forms and meanings, but
the abstractness of the meaning representations remains debated. Here, we
searched for abstract representations of meaning in the language cortex by
modeling neural responses to sentences using representations from vision and
language models. When we generate images corresponding to sentences and extract
vision model embeddings, we find that aggregating across multiple generated
images yields increasingly accurate predictions of language cortex responses,
sometimes rivaling large language models. Similarly, averaging embeddings
across multiple paraphrases of a sentence improves prediction accuracy compared
to any single paraphrase. Enriching paraphrases with contextual details that
may be implicit (e.g., augmenting "I had a pancake" to include details like
"maple syrup") further increases prediction accuracy, even surpassing
predictions based on the embedding of the original sentence, suggesting that
the language system maintains richer and broader semantic representations than
language models. Together, these results demonstrate the existence of highly
abstract, form-independent meaning representations within the language cortex.

</details>


### [86] [DiffuSpec: Unlocking Diffusion Language Models for Speculative Decoding](https://arxiv.org/abs/2510.02358)
*Guanghao Li,Zhihui Fu,Min Fang,Qibin Zhao,Ming Tang,Chun Yuan,Jun Wang*

Main category: cs.CL

TL;DR: DiffuSpec 是一种无需训练的框架，利用预训练的扩散语言模型（DLM）在单次前向传播中生成多令牌草稿，结合因果一致性路径搜索和自适应草稿长度控制，实现高达3倍的实时时钟加速。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的自回归解码方式导致生成延迟高，尽管推测解码可通过快速起草模型提升效率，但现有方法多依赖自回归起草器，限制了实际加速效果。本文旨在通过非自回归方式提升推测解码效率。

Method: 提出 DiffuSpec 框架，使用预训练的扩散语言模型（DLM）在单次前向传播中生成多令牌草稿；引入因果一致性路径搜索（CPS）从双向条件生成的令牌格中提取符合自回归验证的左到右路径；设计自适应草稿长度（ADL）控制器，根据接受反馈动态调整草稿长度。

Result: 在多个基准测试中，DiffuSpec 实现最高达3倍的实时时钟加速，显著优于传统自回归起草器。

Conclusion: DiffuSpec 证明了基于扩散模型的非自回归起草是推测解码中自回归起草器的有效替代方案，兼具高效性与兼容性。

Abstract: As large language models (LLMs) scale up, accuracy improves, but the
autoregressive (AR) nature of decoding increases latency since each token
requires a serial forward pass. Speculative decoding addresses this by
employing a fast drafter to propose multi-token drafts, which are then verified
in parallel by the target model. However, many deployments still rely on AR
drafters, where sequential passes limit wall-clock gains. We revisit the
drafting stage and present DiffuSpec, a training-free drop-in framework that
uses a pretrained diffusion language model (DLM) to produce multi-token drafts
in a single forward pass, while remaining compatible with standard AR
verifiers. Because DLM drafts are generated under bidirectional conditioning,
parallel per-position candidates form a token lattice in which the locally
highest-probability token at each position need not form a causal left-to-right
path. Moreover, DLM drafting requires pre-specifying a draft length, inducing a
speed-quality trade-off. To address these challenges, we introduce two
practical components: (i) a causal-consistency path search (CPS) over this
lattice that extracts a left-to-right path aligned with AR verification; and
(ii) an adaptive draft-length (ADL) controller that adjusts next proposal size
based on recent acceptance feedback and realized generated length. Across
benchmarks, DiffuSpec yields up to 3x wall-clock speedup, establishing
diffusion-based drafting as a robust alternative to autoregressive drafters for
speculative decoding.

</details>


### [87] [Emission-GPT: A domain-specific language model agent for knowledge retrieval, emission inventory and data analysis](https://arxiv.org/abs/2510.02359)
*Jiashu Ye,Tong Wu,Weiwen Chen,Hao Zhang,Zeteng Lin,Xingxing Li,Shujuan Weng,Manni Zhu,Xin Yuan,Xinlong Hong,Jingjie Li,Junyu Zheng,Zhijiong Huang,Jing Tang*

Main category: cs.CL

TL;DR: Emission-GPT是一个专为大气排放领域设计的知识增强型大语言模型代理，通过整合超过10,000份文档构建的知识库，支持自然语言交互式排放数据分析，提升非专家用户对排放信息的理解与应用能力。


<details>
  <summary>Details</summary>
Motivation: 现有排放相关知识碎片化且专业化，数据获取与整合效率低，阻碍非专家用户有效利用排放信息，制约研究与管理进展。

Method: 构建包含标准、报告、指南和文献等的排放知识库，结合提示工程与问题补全技术，开发Emission-GPT模型，支持自然语言查询、可视化、源贡献分析及情景化排放因子推荐。

Result: 在广东省的案例研究表明，Emission-GPT可通过简单提示从原始数据中提取点源分布、行业趋势等关键信息，实现高效准确的领域问答与数据分析。

Conclusion: Emission-GPT具备模块化与可扩展架构，可自动化传统手动流程，有望成为新一代排放清单构建与情景评估的基础工具。

Abstract: Improving air quality and addressing climate change relies on accurate
understanding and analysis of air pollutant and greenhouse gas emissions.
However, emission-related knowledge is often fragmented and highly specialized,
while existing methods for accessing and compiling emissions data remain
inefficient. These issues hinder the ability of non-experts to interpret
emissions information, posing challenges to research and management. To address
this, we present Emission-GPT, a knowledge-enhanced large language model agent
tailored for the atmospheric emissions domain. Built on a curated knowledge
base of over 10,000 documents (including standards, reports, guidebooks, and
peer-reviewed literature), Emission-GPT integrates prompt engineering and
question completion to support accurate domain-specific question answering.
Emission-GPT also enables users to interactively analyze emissions data via
natural language, such as querying and visualizing inventories, analyzing
source contributions, and recommending emission factors for user-defined
scenarios. A case study in Guangdong Province demonstrates that Emission-GPT
can extract key insights--such as point source distributions and sectoral
trends--directly from raw data with simple prompts. Its modular and extensible
architecture facilitates automation of traditionally manual workflows,
positioning Emission-GPT as a foundational tool for next-generation emission
inventory development and scenario-based assessment.

</details>


### [88] [Spiral of Silence in Large Language Model Agents](https://arxiv.org/abs/2510.02360)
*Mingze Zhong,Meng Fang,Zijing Shi,Yuxuan Huang,Shunfeng Zheng,Yali Du,Ling Chen,Jun Wang*

Main category: cs.CL

TL;DR: 研究探讨了在大型语言模型（LLM）群体中是否会出现类似“沉默螺旋”（Spiral of Silence）的社会动态，提出了一种评估框架，并发现历史和角色信息共同作用时会引发多数主导和沉默螺旋模式。


<details>
  <summary>Details</summary>
Motivation: 由于沉默螺旋理论原本适用于人类社会，而在LLM代理中缺乏心理基础，因此需要探究纯粹基于统计的语言生成是否也能产生类似的社会动态。

Method: 设计了一个评估框架，通过控制“历史”和“人格”信号的有无四种条件，利用Mann-Kendall、Spearman秩相关等趋势检验以及峰度和四分位距等集中度量来分析意见动态。

Result: 实验表明，历史和人格信号共同存在时会产生强烈的多数主导并复现沉默螺旋模式；仅有历史信号会导致强烈锚定效应；仅有人格信号则产生多样但不相关的观点，说明缺乏历史锚定时无法产生沉默螺旋动态。

Conclusion: 研究表明LLM群体中可出现类社会动态，强调在AI系统设计中需监控和减轻 emergent conformity 问题，连接了计算社会学与负责任AI的设计。

Abstract: The Spiral of Silence (SoS) theory holds that individuals with minority views
often refrain from speaking out for fear of social isolation, enabling majority
positions to dominate public discourse. When the 'agents' are large language
models (LLMs), however, the classical psychological explanation is not directly
applicable, since SoS was developed for human societies. This raises a central
question: can SoS-like dynamics nevertheless emerge from purely statistical
language generation in LLM collectives? We propose an evaluation framework for
examining SoS in LLM agents. Specifically, we consider four controlled
conditions that systematically vary the availability of 'History' and 'Persona'
signals. Opinion dynamics are assessed using trend tests such as Mann-Kendall
and Spearman's rank, along with concentration measures including kurtosis and
interquartile range. Experiments across open-source and closed-source models
show that history and persona together produce strong majority dominance and
replicate SoS patterns; history signals alone induce strong anchoring; and
persona signals alone foster diverse but uncorrelated opinions, indicating that
without historical anchoring, SoS dynamics cannot emerge. The work bridges
computational sociology and responsible AI design, highlighting the need to
monitor and mitigate emergent conformity in LLM-agent systems.

</details>


### [89] [ChunkLLM: A Lightweight Pluggable Framework for Accelerating LLMs Inference](https://arxiv.org/abs/2510.02361)
*Haojie Ouyang,Jianwei Lv,Lei Ren,Chen Wei,Xiaojie Wang,Fangxiang Feng*

Main category: cs.CL

TL;DR: 提出ChunkLLM，一种轻量级、可插拔的训练框架，通过QK Adapter和Chunk Adapter在保持性能的同时显著提升长文本推理速度。


<details>
  <summary>Details</summary>
Motivation: Transformer模型因自注意力机制的平方复杂度导致计算效率低下，现有块选择与压缩方法存在语义不完整或训练-推理效率差的问题。

Method: 设计包含QK Adapter和Chunk Adapter的框架；QK Adapter用于特征压缩和块注意力获取，Chunk Adapter用于检测块边界；训练时仅更新适配器参数，并采用注意力蒸馏方法提升关键块召回率；推理时仅在检测到块边界时触发块选择。

Result: 在短文本任务上性能相当，在长文本任务上保持98.64%的性能，键值缓存保留率达48.58%，处理120K长文本时最高加速4.48倍。

Conclusion: ChunkLLM在保证模型性能的同时有效提升推理效率和缓存利用率，显著优于标准Transformer。

Abstract: Transformer-based large models excel in natural language processing and
computer vision, but face severe computational inefficiencies due to the
self-attention's quadratic complexity with input tokens. Recently, researchers
have proposed a series of methods based on block selection and compression to
alleviate this problem, but they either have issues with semantic
incompleteness or poor training-inference efficiency. To comprehensively
address these challenges, we propose ChunkLLM, a lightweight and pluggable
training framework. Specifically, we introduce two components: QK Adapter
(Q-Adapter and K-Adapter) and Chunk Adapter. The former is attached to each
Transformer layer, serving dual purposes of feature compression and chunk
attention acquisition. The latter operates at the bottommost layer of the
model, functioning to detect chunk boundaries by leveraging contextual semantic
information. During the training phase, the parameters of the backbone remain
frozen, with only the QK Adapter and Chunk Adapter undergoing training.
Notably, we design an attention distillation method for training the QK
Adapter, which enhances the recall rate of key chunks. During the inference
phase, chunk selection is triggered exclusively when the current token is
detected as a chunk boundary, thereby accelerating model inference.
Experimental evaluations are conducted on a diverse set of long-text and
short-text benchmark datasets spanning multiple tasks. ChunkLLM not only
attains comparable performance on short-text benchmarks but also maintains
98.64% of the performance on long-context benchmarks while preserving a 48.58%
key-value cache retention rate. Particularly, ChunkLLM attains a maximum
speedup of 4.48x in comparison to the vanilla Transformer in the processing of
120K long texts.

</details>


### [90] [A Cross-Lingual Analysis of Bias in Large Language Models Using Romanian History](https://arxiv.org/abs/2510.02362)
*Matei-Iulian Cocu,Răzvan-Cosmin Cristia,Adrian Marius Dumitran*

Main category: cs.CL

TL;DR: 该研究通过让多个大语言模型在不同语言和情境下回答有争议的罗马尼亚历史问题，揭示了模型在回答中的偏见与不一致性，发现其响应受提问方式和语言影响显著。


<details>
  <summary>Details</summary>
Motivation: 历史叙述常受文化和国家意识形态影响，而大语言模型可能因训练数据偏差传递非中立观点。本研究旨在揭示这些模型在教育应用中的潜在偏见。

Method: 研究分为三个阶段：首先让模型回答历史问题，随后以肯定性回答引导，再要求以数值评分形式重答同一问题，比较其在不同语言和响应格式下的答案一致性。

Result: 模型在二元回答上稳定性较高但不完美，且因语言而异；在不同语言或响应格式间常改变立场，数值评分常与初始二元判断矛盾，最一致的模型未必最中立或准确。

Conclusion: 大语言模型在处理争议性历史问题时表现出显著的上下文和语言依赖性，提示其在教育等敏感领域应用时需谨慎对待其输出的中立性与一致性。

Abstract: In this case study, we select a set of controversial Romanian historical
questions and ask multiple Large Language Models to answer them across
languages and contexts, in order to assess their biases. Besides being a study
mainly performed for educational purposes, the motivation also lies in the
recognition that history is often presented through altered perspectives,
primarily influenced by the culture and ideals of a state, even through large
language models. Since they are often trained on certain data sets that may
present certain ambiguities, the lack of neutrality is subsequently instilled
in users. The research process was carried out in three stages, to confirm the
idea that the type of response expected can influence, to a certain extent, the
response itself; after providing an affirmative answer to some given question,
an LLM could shift its way of thinking after being asked the same question
again, but being told to respond with a numerical value of a scale. Results
show that binary response stability is relatively high but far from perfect and
varies by language. Models often flip stance across languages or between
formats; numeric ratings frequently diverge from the initial binary choice, and
the most consistent models are not always those judged most accurate or
neutral. Our research brings to light the predisposition of models to such
inconsistencies, within a specific contextualization of the language for the
question asked.

</details>


### [91] [Beyond Manuals and Tasks: Instance-Level Context Learning for LLM Agents](https://arxiv.org/abs/2510.02369)
*Kuntai Cai,Juncheng Liu,Xianglin Yang,Zhaojie Niu,Xiaokui Xiao,Xing Chen*

Main category: cs.CL

TL;DR: 提出实例级上下文学习（ILCL），通过构建可复用的高精度上下文文档，显著提升LLM代理在复杂任务中的成功率与效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理依赖环境级和任务级上下文，但缺乏对特定环境实例中可验证、可复用事实（如实例对象位置、局部规则）的有效利用，导致复杂任务中频繁失败。

Method: 提出任务无关的方法，通过引导式探索，使用紧凑的TODO森林智能规划下一步动作，并结合轻量级的plan-act-extract循环执行，自动构建高精度的实例级上下文文档。

Result: 在TextWorld、ALFWorld和Crafter上的实验显示显著提升：如TextWorld中ReAct的成功率从37%提升至95%，IGE从81%提升至95%，且效率更高。

Conclusion: 将一次性探索转化为持久可复用的知识，补充现有上下文，使LLM代理更可靠和高效。

Abstract: Large language model (LLM) agents typically receive two kinds of context: (i)
environment-level manuals that define interaction interfaces and global rules,
and (ii) task-level guidance or demonstrations tied to specific goals. In this
work, we identify a crucial but overlooked third type of context,
instance-level context, which consists of verifiable and reusable facts tied to
a specific environment instance, such as object locations, crafting recipes,
and local rules. We argue that the absence of instance-level context is a
common source of failure for LLM agents in complex tasks, as success often
depends not only on reasoning over global rules or task prompts but also on
making decisions based on precise and persistent facts. Acquiring such context
requires more than memorization: the challenge lies in efficiently exploring,
validating, and formatting these facts under tight interaction budgets. We
formalize this problem as Instance-Level Context Learning (ILCL) and introduce
our task-agnostic method to solve it. Our method performs a guided exploration,
using a compact TODO forest to intelligently prioritize its next actions and a
lightweight plan-act-extract loop to execute them. This process automatically
produces a high-precision context document that is reusable across many
downstream tasks and agents, thereby amortizing the initial exploration cost.
Experiments across TextWorld, ALFWorld, and Crafter demonstrate consistent
gains in both success and efficiency: for instance, ReAct's mean success rate
in TextWorld rises from 37% to 95%, while IGE improves from 81% to 95%. By
transforming one-off exploration into persistent, reusable knowledge, our
method complements existing contexts to enable more reliable and efficient LLM
agents.

</details>


### [92] [Training Dynamics of Parametric and In-Context Knowledge Utilization in Language Models](https://arxiv.org/abs/2510.02370)
*Minsung Kim,Dong-Kyum Kim,Jea Kwon,Nakyeong Yang,Kyomin Jung,Meeyoung Cha*

Main category: cs.CL

TL;DR: 本研究通过控制训练条件，系统探索了语言模型如何在推理时协调上下文知识与参数化知识的使用，发现数据中的重复、不一致性和分布偏差有助于模型发展出更鲁棒的知识仲裁策略。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理时常面临上下文检索知识与预训练获得的参数知识之间的冲突。盲目采纳外部知识易受误导，而固守参数知识则无法利用检索信息。尽管检索增强生成被广泛应用，但目前尚不清楚训练过程中哪些因素影响模型的知识仲裁行为，这可能导致资源浪费和不良模型行为。

Method: 作者在合成的人物传记语料库上训练基于Transformer的语言模型，通过系统控制训练条件（如事实重复、信息不一致性和数据分布偏差），观察这些因素如何影响模型对两种知识的利用与仲裁。

Result: 实验表明，文档内事实的重复有助于提升模型的参数化和上下文知识能力；包含不一致信息或分布偏移的语料库能促使模型发展出更稳健的知识仲裁策略。

Conclusion: 数据中的‘非理想’特征（如不一致性与偏差）不应被简单清除，反而对培养模型的鲁棒仲裁能力至关重要，该发现为预训练数据设计提供了实证指导。

Abstract: Large language models often encounter conflicts between in-context knowledge
retrieved at inference time and parametric knowledge acquired during
pretraining. Models that accept external knowledge uncritically are vulnerable
to misinformation, whereas models that adhere rigidly to parametric knowledge
fail to benefit from retrieval. Despite the widespread adoption of
retrieval-augmented generation, we still lack a systematic understanding of
what shapes knowledge-arbitration strategies during training. This gap risks
producing pretrained models with undesirable arbitration behaviors and,
consequently, wasting substantial computational resources after the pretraining
budget has already been spent. To address this problem, we present the first
controlled study of how training conditions influence models' use of in-context
and parametric knowledge, and how they arbitrate between them. We train
transformer-based language models on a synthetic biographies corpus while
systematically controlling various conditions. Our experiments reveal that
intra-document repetition of facts fosters the development of both parametric
and in-context capabilities. Moreover, training on a corpus that contains
inconsistent information or distributional skew encourages models to develop
robust strategies for leveraging parametric and in-context knowledge. Rather
than viewing these non-ideal properties as artifacts to remove, our results
indicate that they are important for learning robust arbitration. These
insights offer concrete, empirical guidance for pretraining models that
harmoniously integrate parametric and in-context knowledge.

</details>


### [93] [Pretraining with hierarchical memories: separating long-tail and common knowledge](https://arxiv.org/abs/2510.02375)
*Hadi Pouransari,David Grangier,C Thomas,Michael Kirchhof,Oncel Tuzel*

Main category: cs.CL

TL;DR: 提出一种带有层次化参数记忆库的小型语言模型架构，在预训练和推理时动态加载上下文相关的记忆块，显著提升模型性能，160M参数模型通过18M记忆参数即可达到超过两倍参数常规模型的效果。


<details>
  <summary>Details</summary>
Motivation: 现代大模型依赖扩大参数规模来存储世界知识和提升推理能力，但将所有知识压缩进参数中既不必要也不实用，尤其受限于边缘设备的内存和计算资源。因此需要一种更高效的知识存储与访问机制。

Method: 设计一种记忆增强的架构和与现有硬件兼容的预训练策略，引入可存储世界知识的大型分层参数化记忆库；在预训练和推理时，根据上下文检索小型记忆块并注入模型；预训练过程中学习将长尾知识存入记忆参数，而小型语言模型则专注于通用知识和推理能力。

Result: 在万亿token规模实验中，一个160M参数的模型结合从4.6B记忆库中提取的18M记忆参数，性能可媲美参数量超过其两倍的常规模型；实验验证了不同Transformer架构下分层前馈记忆的有效性，并将参数化记忆扩展至21B以上。

Conclusion: 所提出的记忆增强架构有效解耦了模型参数与知识存储，实现了更高效的知识利用，在性能与模型规模之间取得更好平衡，适用于资源受限场景且兼容现有硬件范式。

Abstract: The impressive performance gains of modern language models currently rely on
scaling parameters: larger models store more world knowledge and reason better.
Yet compressing all world knowledge into parameters is unnecessary, as only a
fraction is used per prompt, and impractical for edge devices with limited
inference-time memory and compute. We address this shortcoming by a
memory-augmented architecture and a pretraining strategy aligned with existing
hardware paradigms. We introduce small language models that access large
hierarchical parametric memory banks encoding world knowledge. During
pretraining and inference, we fetch a small, context-dependent memory block and
add it to the model. Our pretraining learns to store long-tail world knowledge
in the memory parameters, while the small language model acts as an anchor
capturing common knowledge and general reasoning abilities. Through
trillion-token-scale experiments, we show significant gains: a 160M-parameters
model augmented with an 18M-parameters memory fetched from a 4.6B memory bank
obtains comparable performance to a regular model with more than 2x the
parameters. Through extensive experiments, we study the optimal type and size
of parametric memories in transformers, scaling them to over 21B parameters. We
find that our proposed hierarchical feed-forward memories work robustly across
transformer architectures, whether added during pretraining or post-hoc.

</details>


### [94] [Uncertainty-Aware Answer Selection for Improved Reasoning in Multi-LLM Systems](https://arxiv.org/abs/2510.02377)
*Aakriti Agrawal,Rohith Aralikatti,Anirudh Satheesh,Souradip Chakraborty,Amrit Singh Bedi,Furong Huang*

Main category: cs.CL

TL;DR: 提出一种基于校准对数似然分数的新方法，从多个大语言模型中高效选择最优响应，提升多模型系统性能。


<details>
  <summary>Details</summary>
Motivation: 在资源受限环境下，从多个大语言模型中选择最可靠响应仍具挑战，现有方法依赖高成本外部验证、人工评估或多采样自洽技术，且多模型系统潜力未被充分挖掘。

Method: 提出一种基于校准对数似然分数的原则性、高效计算方法，利用多个不同大语言模型的内在知识与置信度来选择最佳响应。

Result: 在GSM8K、MMLU（6个子集）和ARC数据集上，相比现有方法在辩论和非辩论场景下分别提升约4%、3%和5%。

Conclusion: 该方法有效提升多大模型系统的响应选择性能，具备高效性和实用性，适用于资源受限场景。

Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities, yet
selecting the most reliable response from multiple LLMs remains a challenge,
particularly in resource-constrained settings. Existing approaches often depend
on costly external verifiers, human evaluators, or self-consistency techniques
that require multiple samples from a single model. While multi-LLM systems
produce more diverse responses than single models and thus have greater
potential, they often underperform compared to single LLM self-consistency. We
propose a principled, novel and computationally efficient method to select the
best response from multiple different LLMs using a calibrated log-likelihood
score, implicitly leveraging the inherent knowledge and confidence of these
models. Our method demonstrates improvements of approx. 4%, 3%, and 5% across
both debate (multi-round LLM discussions) and non-debate (Best-of-N with
multiple LLMs) settings on GSM8K, MMLU (6 subsets), and ARC datasets
respectively.

</details>


### [95] [Learning to Route: A Rule-Driven Agent Framework for Hybrid-Source Retrieval-Augmented Generation](https://arxiv.org/abs/2510.02388)
*Haoyue Bai,Haoyu Wang,Shengyu Chen,Zhengzhang Chen,Lu-An Tang,Wei Cheng,Haifeng Chen,Yanjie Fu*

Main category: cs.CL

TL;DR: 提出了一种基于规则的路由框架，通过结合数据库和文档检索的优势，动态选择最适合的检索路径，提升领域特定问答的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成系统主要依赖非结构化文档，忽视了关系型数据库的优势，而在专业领域中数据库能提供更精确、及时的信息。因此，需要一种方法能有效利用两种信息源并动态选择最优路径。

Method: 设计了一个规则驱动的路由框架，包含三个组件：路由代理根据显式规则评分并选择最佳增强路径；规则制定专家代理利用问答反馈持续优化规则；路径级元缓存重用历史路由决策以降低延迟和成本。

Result: 在三个问答基准上的实验表明，该框架在准确性上优于静态策略和基于学习的路由方法，同时保持适中的计算开销。

Conclusion: 规则驱动的路由机制能有效整合数据库与文档的互补优势，通过可解释、可维护的规则实现高效准确的专业领域问答。

Abstract: Large Language Models (LLMs) have shown remarkable performance on general
Question Answering (QA), yet they often struggle in domain-specific scenarios
where accurate and up-to-date information is required. Retrieval-Augmented
Generation (RAG) addresses this limitation by enriching LLMs with external
knowledge, but existing systems primarily rely on unstructured documents, while
largely overlooking relational databases, which provide precise, timely, and
efficiently queryable factual information, serving as indispensable
infrastructure in domains such as finance, healthcare, and scientific research.
Motivated by this gap, we conduct a systematic analysis that reveals three
central observations: (i) databases and documents offer complementary strengths
across queries, (ii) naively combining both sources introduces noise and cost
without consistent accuracy gains, and (iii) selecting the most suitable source
for each query is crucial to balance effectiveness and efficiency. We further
observe that query types show consistent regularities in their alignment with
retrieval paths, suggesting that routing decisions can be effectively guided by
systematic rules that capture these patterns. Building on these insights, we
propose a rule-driven routing framework. A routing agent scores candidate
augmentation paths based on explicit rules and selects the most suitable one; a
rule-making expert agent refines the rules over time using QA feedback to
maintain adaptability; and a path-level meta-cache reuses past routing
decisions for semantically similar queries to reduce latency and cost.
Experiments on three QA benchmarks demonstrate that our framework consistently
outperforms static strategies and learned routing baselines, achieving higher
accuracy while maintaining moderate computational cost.

</details>


### [96] [KnowledgeSmith: Uncovering Knowledge Updating in LLMs with Model Editing and Unlearning](https://arxiv.org/abs/2510.02392)
*Yinyi Luo,Zhexian Zhou,Hao Chen,Kai Qiu,Marios Savvides,Yixuan Li,Jindong Wang*

Main category: cs.CL

TL;DR: 提出统一框架KnowledgeSmith，系统研究大语言模型的知识更新机制，揭示知识传播、可塑性、一致性和鲁棒性等方面的新见解。


<details>
  <summary>Details</summary>
Motivation: 知识编辑与机器遗忘是大模型保持更新的重要方法，但其更新机制因缺乏充分、统一和大规模的评估而未被深入理解。

Method: 将知识编辑与遗忘统一为约束优化问题，并构建自动数据生成器，在多层级图结构和不同数据规模下进行可控实验。

Result: 实验揭示了大模型在知识传播中的非人一致性、可塑性随规模变化的趋势，以及一致性与容量之间的权衡。

Conclusion: KnowledgeSmith为理解大模型知识更新机制提供了系统框架，有助于设计更可靠和可扩展的更新策略。

Abstract: Knowledge editing and machine unlearning are two popular approaches for large
language models (LLMs) to stay up-to-date. However, the knowledge updating
mechanism of LLMs remains largely unexplored due to insufficient, isolated, and
small-scale evaluation. For instance, are LLMs similar to humans in modifying
certain knowledge? What differs editing and unlearning as training data
increases? This paper proposes KnowledgeSmith, a unified framework to
systematically understand the updating mechanism of LLMs. We first cast editing
and unlearning as instances of one constrained optimization problem. Then, we
propose an automatic dataset generator that provides structured interventions
across multiple graph levels and data scales, enabling controlled studies of
how different modification strategies propagate through model knowledge.
Extensive experiments demonstrate nuanced insights over knowledge propagation,
plasticity scaling, consistency, and robustness. For instance, our results show
that LLMs do not exhibit similar updating as humans for different levels of
knowledge, and there exists consistency-capacity trade-off. We hope our
findings can offer suggestions to the design of more reliable and scalable
strategies. Code: https://github.com/AIFrontierLab/KnowledgeSmith.git

</details>


### [97] [Retrieval and Augmentation of Domain Knowledge for Text-to-SQL Semantic Parsing](https://arxiv.org/abs/2510.02394)
*Manasi Patwardhan,Ayush Agarwal,Shabbirhussain Bhaisaheb,Aseem Arora,Lovekesh Vig,Sunita Sarawagi*

Main category: cs.CL

TL;DR: 提出了一种在数据库层面系统化关联结构化领域知识的框架，通过子字符串匹配检索相关领域声明，显著提升了大型语言模型将自然语言查询转换为SQL的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试依赖不切实际的、针对特定查询的文本提示来表达领域知识，且不同数据库间大语言模型在自然语言转SQL任务上的表现差异大，需更实用的领域知识整合方法。

Method: 在数据库层面引入结构化领域声明，并使用子字符串匹配方法检索与用户查询相关的领域声明，以增强语言模型对领域词汇和数据库模式的理解。

Result: 在涵盖多种领域的十一个真实数据库模式上，评估了五个开源和专有大语言模型，结果表明所提方法比现有方法更准确，且子字符串匹配检索效果优于其他检索方法。

Conclusion: 数据库级别的结构化领域声明更实用且准确，基于子字符串匹配的检索方法能显著提升自然语言到SQL转换的性能。

Abstract: The performance of Large Language Models (LLMs) for translating Natural
Language (NL) queries into SQL varies significantly across databases (DBs). NL
queries are often expressed using a domain specific vocabulary, and mapping
these to the correct SQL requires an understanding of the embedded domain
expressions, their relationship to the DB schema structure. Existing benchmarks
rely on unrealistic, ad-hoc query specific textual hints for expressing domain
knowledge. In this paper, we propose a systematic framework for associating
structured domain statements at the database level. We present retrieval of
relevant structured domain statements given a user query using sub-string level
match. We evaluate on eleven realistic DB schemas covering diverse domains
across five open-source and proprietary LLMs and demonstrate that (1) DB level
structured domain statements are more practical and accurate than existing
ad-hoc query specific textual domain statements, and (2) Our sub-string match
based retrieval of relevant domain statements provides significantly higher
accuracy than other retrieval approaches.

</details>


### [98] [Words That Make Language Models Perceive](https://arxiv.org/abs/2510.02425)
*Sophie L. Wang,Phillip Isola,Brian Cheung*

Main category: cs.CL

TL;DR: 通过简单的感官提示（如“看”或“听”），可以激发纯文本训练的大语言模型中隐含的多模态表征，使其与专门的视觉或音频编码器的表征更一致。


<details>
  <summary>Details</summary>
Motivation: 探索虽然大语言模型仅通过文本训练，缺乏直接感知经验，但其内部是否隐含了多模态规律，并可通过提示激活这些结构。

Method: 使用感官提示（如‘see’或‘hear’）来引导文本大模型，使其在无实际感知输入的情况下，生成类似于受视觉或听觉证据影响的表征，并与专业编码器进行表征对齐分析。

Result: 实验证明，轻量级的提示工程能可靠地激活大语言模型中适合特定感知模态的内部表征，提升其与视觉和音频编码器的表示相似性。

Conclusion: 纯文本训练的LLM内部蕴含潜在的多模态结构，可通过显式感官提示有效激活，表明语言中编码的多模态规律可被提示引导而显现。

Abstract: Large language models (LLMs) trained purely on text ostensibly lack any
direct perceptual experience, yet their internal representations are implicitly
shaped by multimodal regularities encoded in language. We test the hypothesis
that explicit sensory prompting can surface this latent structure, bringing a
text-only LLM into closer representational alignment with specialist vision and
audio encoders. When a sensory prompt tells the model to 'see' or 'hear', it
cues the model to resolve its next-token predictions as if they were
conditioned on latent visual or auditory evidence that is never actually
supplied. Our findings reveal that lightweight prompt engineering can reliably
activate modality-appropriate representations in purely text-trained LLMs.

</details>


### [99] [CLARITY: Clinical Assistant for Routing, Inference, and Triage](https://arxiv.org/abs/2510.02463)
*Vladimir Shaposhnikov,Aleksandr Nesterov,Ilia Kopanichuk,Ivan Bakulin,Egor Zhelvakov,Ruslan Abramov,Ekaterina Tsapieva,Dmitry V. Dylov,Ivan Oseledets*

Main category: cs.CL

TL;DR: CLARITY是一个基于AI的临床辅助平台，结合有限状态机与大语言模型，实现高效、准确的患者分诊与专科转介，性能超越人工且咨询时间更短。


<details>
  <summary>Details</summary>
Motivation: 为提高医疗系统中患者到专科医生的转诊效率和准确性，解决人工分诊耗时长、资源紧张的问题，亟需一个自动化、可扩展的智能临床辅助系统。

Method: 采用混合架构，结合有限状态机（FSM）管理结构化对话流程，以及基于大语言模型（LLM）的协作智能体进行症状分析与优先级判断，构建于模块化微服务框架之上，便于集成与扩展。

Result: 在大规模医院IT平台部署中完成超过5.5万次对话，2500条由专家标注验证；结果显示CLARITY在首次转诊准确率上超过人类水平，且咨询时长缩短至人工的三分之一。

Conclusion: CLARITY具备高效、准确、可扩展的临床辅助能力，能够有效集成至现有医疗IT系统，显著提升患者分诊与转诊效率，具有广泛的临床应用前景。

Abstract: We present CLARITY (Clinical Assistant for Routing, Inference, and Triage),
an AI-driven platform designed to facilitate patient-to-specialist routing,
clinical consultations, and severity assessment of patients' conditions. Its
hybrid architecture combines a Finite State Machine (FSM) for structured
dialogue flows with collaborative agents that employ Large Language Model (LLM)
to analyze symptoms and prioritize referrals to appropriate specialists. Built
on a modular microservices framework, CLARITY ensures safe, efficient, and
robust performance, flexible and readily scalable to meet the demands of
existing workflows and IT solutions in healthcare.
  We report integration of our clinical assistant into a large-scale
nation-wide inter-hospital IT platform, with over 55,000 content-rich user
dialogues completed within the two months of deployment, 2,500 of which were
expert-annotated for a consequent validation. The validation results show that
CLARITY surpasses human-level performance in terms of the first-attempt routing
precision, naturally requiring up to 3 times shorter duration of the
consultation than with a human.

</details>


### [100] [Unraveling Syntax: How Language Models Learn Context-Free Grammars](https://arxiv.org/abs/2510.02524)
*Laura Ying Schulz,Daniel Mitropolsky,Tomaso Poggio*

Main category: cs.CL

TL;DR: 提出了一种基于PCFG生成的合成语言研究语言模型学习语法动态的新框架，揭示了Transformer并行学习子语法结构，且在深层递归结构上存在挑战。


<details>
  <summary>Details</summary>
Motivation: 理解语言模型如何获取语法结构，尤其是其学习动态尚不明确，本文旨在通过可控的合成语言环境系统研究该问题。

Method: 基于概率上下文无关文法（PCFG）生成合成语言，训练小型模型，分析其在不同语法复杂度、递归深度和子语法结构下的学习动态，并推导训练损失和KL散度的递归公式。

Result: 发现Transformer并行降低所有子语法的损失（不同于儿童的渐进学习）；子语法预训练可改善小模型的最终损失，且其内部表征更符合语法结构；模型在深层递归结构上表现困难。

Conclusion: PCFG合成语言是研究语言模型学习动态的有效测试平台，为理解神经网络如何学习语法开辟了新的研究方向。

Abstract: We introduce a new framework for understanding how language models acquire
syntax. While large models achieve impressive results, little is known about
their learning dynamics. Our approach starts with the observation that most
domains of interest, such as natural language syntax, coding languages,
arithmetic problems, are captured by probabilistic context-free grammars
(PCFGs). We study the learning dynamics of small models trained on synthetic
languages generated from PCFGs, enabling precise control over grammar
complexity, recursion depth, and subgrammar structure. We prove several
general, recursive formulae for the training loss and Kullback-Leibler
divergence over the subgrammar structure of a PCFG. Empirically, we find that
unlike children, who first master simple substructures before progressing to
more complex constructions, transformers reduce loss across all subgrammars in
parallel. We further show that subgrammar pretraining can improve the final
loss for smaller models, and that pretrained models develop internal
representations more aligned with the grammar's substructure. Finally, we
demonstrate that models struggle with deeper recursive structures (a limitation
even of large language models), revealing fundamental challenges in how neural
networks represent hierarchical syntax. Overall, our work initiates the study
of the learning dynamics of transformers on PCFGs as a versatile testbed for
probing learning in language models, opening a research direction with many
open questions.

</details>


### [101] [Hierarchical Semantic Retrieval with Cobweb](https://arxiv.org/abs/2510.02539)
*Anant Gupta,Karthik Singaravadivelan,Zekun Wang*

Main category: cs.CL

TL;DR: 提出了一种层次感知的文档检索框架Cobweb，利用句子嵌入构建原型树，实现从粗到细的检索，提升鲁棒性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有神经检索方法常将语料视为扁平向量集合，忽视了语料结构，导致解释性差且对嵌入质量敏感。

Method: 提出Cobweb框架，将句子嵌入组织成原型树，通过粗到细遍历进行检索；设计两种推理方法：广义最佳优先搜索和轻量级路径求和排序器。

Result: 在MS MARCO和QQP数据集上，使用BERT/T5等强编码器时性能与点积搜索相当，而在GPT-2等较弱嵌入下显著优于点积搜索，表现出更强的鲁棒性。

Conclusion: Cobweb框架在保持检索效果的同时，提升了对嵌入质量的鲁棒性、可扩展性，并通过层次化原型提供了可解释的检索路径。

Abstract: Neural document retrieval often treats a corpus as a flat cloud of vectors
scored at a single granularity, leaving corpus structure underused and
explanations opaque. We use Cobweb--a hierarchy-aware framework--to organize
sentence embeddings into a prototype tree and rank documents via coarse-to-fine
traversal. Internal nodes act as concept prototypes, providing multi-granular
relevance signals and a transparent rationale through retrieval paths. We
instantiate two inference approaches: a generalized best-first search and a
lightweight path-sum ranker. We evaluate our approaches on MS MARCO and QQP
with encoder (e.g., BERT/T5) and decoder (GPT-2) representations. Our results
show that our retrieval approaches match the dot product search on strong
encoder embeddings while remaining robust when kNN degrades: with GPT-2
vectors, dot product performance collapses whereas our approaches still
retrieve relevant results. Overall, our experiments suggest that Cobweb
provides competitive effectiveness, improved robustness to embedding quality,
scalability, and interpretable retrieval via hierarchical prototypes.

</details>


### [102] [Knowledge-Graph Based RAG System Evaluation Framework](https://arxiv.org/abs/2510.02549)
*Sicheng Dong,Vahid Zolfaghari,Nenad Petrovic,Alois Knoll*

Main category: cs.CL

TL;DR: 本文提出了一种基于知识图谱（KG）的检索增强生成（RAG）系统评估新方法，通过引入多跳推理和语义社区聚类，提升了评估的全面性和敏感性，并验证了其与人类判断的更高相关性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统评估指标难以有效捕捉LLM生成内容的关键特征，尤其是高流畅性和自然性下的语义差异，亟需更精细、深入的评估框架。

Method: 受RAGAS启发，构建基于知识图谱的评估范式，融合多跳推理和语义社区聚类技术，生成更全面的评分指标，并与RAGAS对比，构建人工标注子集以评估与人工判断的相关性。

Result: 实验表明，该KG-based方法在捕捉生成结果的细微语义差异上比RAGAS更敏感，且与人类判断的相关性更高，验证了其有效性。

Conclusion: 基于KG的评估方法为RAG系统提供了更深入、细致的性能分析视角，有助于推动RAG评估体系的发展，未来可进一步探索其在复杂推理任务中的应用。

Abstract: Large language models (LLMs) has become a significant research focus and is
utilized in various fields, such as text generation and dialog systems. One of
the most essential applications of LLM is Retrieval Augmented Generation (RAG),
which greatly enhances generated content's reliability and relevance. However,
evaluating RAG systems remains a challenging task. Traditional evaluation
metrics struggle to effectively capture the key features of modern
LLM-generated content that often exhibits high fluency and naturalness.
Inspired by the RAGAS tool, a well-known RAG evaluation framework, we extended
this framework into a KG-based evaluation paradigm, enabling multi-hop
reasoning and semantic community clustering to derive more comprehensive
scoring metrics. By incorporating these comprehensive evaluation criteria, we
gain a deeper understanding of RAG systems and a more nuanced perspective on
their performance. To validate the effectiveness of our approach, we compare
its performance with RAGAS scores and construct a human-annotated subset to
assess the correlation between human judgments and automated metrics. In
addition, we conduct targeted experiments to demonstrate that our KG-based
evaluation method is more sensitive to subtle semantic differences in generated
outputs. Finally, we discuss the key challenges in evaluating RAG systems and
highlight potential directions for future research.

</details>


### [103] [Transcribe, Translate, or Transliterate: An Investigation of Intermediate Representations in Spoken Language Models](https://arxiv.org/abs/2510.02569)
*Tolúl\d{o}pé Ògúnrèmí,Christopher D. Manning,Dan Jurafsky,Karen Livescu*

Main category: cs.CL

TL;DR: 研究了三种口语语言模型中模态适配器（MA）的表示策略，发现基于Whisper编码器的模型使用英语为基础的中介语言表示语义，而其他模型则用英语词汇表达输入的语音信息。


<details>
  <summary>Details</summary>
Motivation: 了解模态适配器如何转换表示，以增进对多模态模型内部工作机制的理解。

Method: 通过查找最接近解码器语言模型词汇的MA表示，分析三个不同SLM中的MA输出表示。

Result: 发现了两种MA表示策略：一种是基于语义的英语中介语言表示，另一种是基于语音但以英语词表达的方式。

Conclusion: MA表示策略的选择可能取决于语音编码器是否仅用于语音识别或还用于翻译任务。

Abstract: Spoken language models (SLMs) that integrate speech with large language
models (LMs) rely on modality adapters (MAs) to map the output of speech
encoders to a representation that is understandable to the decoder LM. Yet we
know very little about how these crucial MAs transform representations. Here we
examine the MA output representation in three SLMs (SALMONN, Qwen2-Audio and
Phi-4-Multimodal-Instruct). By finding the nearest decoder LM token to an MA
representation, we uncover two strategies for MA representations. For models
using a Whisper encoder, MAs appear to represent the meaning of the input using
an English-based interlingua, allowing them to handle languages unseen in
instruction tuning. For models that don't, like Phi-4-Multimodal-Instruct, MAs
instead represent the phonetics of the input, but expressed with English words.
We hypothesise that which arises depends on whether the speech encoder is
trained only for speech recognition or also for translation.

</details>


### [104] [Evaluation Framework for Highlight Explanations of Context Utilisation in Language Models](https://arxiv.org/abs/2510.02629)
*Jingyi Sun,Pepa Atanasova,Sagnik Ray Choudhury,Sekh Mainul Islam,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 本文提出了首个用于评估高亮解释（HE）在上下文归因中有效性的黄金标准框架，并使用受控测试用例评估了四种HE方法在不同场景下的表现，发现现有方法在长上下文和位置偏差方面存在挑战。


<details>
  <summary>Details</summary>
Motivation: 语言模型如何利用上下文信息对用户不透明，现有解释方法缺乏对上下文使用准确性的直接评估，因此需要一个可靠的评估框架来衡量解释方法的有效性。

Method: 构建了一个具有已知真实上下文使用的受控测试框架，评估了三种现有HE方法和一种基于机械可解释性的新方法MechLight，在四种上下文场景、四个数据集和五种语言模型上的表现。

Result: MechLight在所有场景中表现最佳，但所有方法在处理长上下文时性能下降，且表现出位置偏差，说明当前解释方法存在根本性局限。

Conclusion: 需要新方法来克服现有高亮解释在长上下文和位置偏差方面的不足，以实现可扩展且可靠的上下文使用解释。

Abstract: Context utilisation, the ability of Language Models (LMs) to incorporate
relevant information from the provided context when generating responses,
remains largely opaque to users, who cannot determine whether models draw from
parametric memory or provided context, nor identify which specific context
pieces inform the response. Highlight explanations (HEs) offer a natural
solution as they can point the exact context pieces and tokens that influenced
model outputs. However, no existing work evaluates their effectiveness in
accurately explaining context utilisation. We address this gap by introducing
the first gold standard HE evaluation framework for context attribution, using
controlled test cases with known ground-truth context usage, which avoids the
limitations of existing indirect proxy evaluations. To demonstrate the
framework's broad applicability, we evaluate four HE methods -- three
established techniques and MechLight, a mechanistic interpretability approach
we adapt for this task -- across four context scenarios, four datasets, and
five LMs. Overall, we find that MechLight performs best across all context
scenarios. However, all methods struggle with longer contexts and exhibit
positional biases, pointing to fundamental challenges in explanation accuracy
that require new approaches to deliver reliable context utilisation
explanations at scale.

</details>


### [105] [Mind the Gap: Linguistic Divergence and Adaptation Strategies in Human-LLM Assistant vs. Human-Human Interactions](https://arxiv.org/abs/2510.02645)
*Fulei Zhang,Zhou Yu*

Main category: cs.CL

TL;DR: 用户与LLM聊天机器人交互时的沟通风格与人类客服显著不同，表现为语法流畅性、礼貌性和词汇多样性差异；研究发现使用风格多样化数据增强训练可显著提升模型适应性，而推理时消息重构效果有限。


<details>
  <summary>Details</summary>
Motivation: 探索在LLM广泛应用于客服场景下，用户与聊天机器人和人类代理沟通方式的差异，并提升模型对上线后沟通风格变化的适应能力。

Method: 分析用户与LLM和人类代理交互的语言数据，比较语法流畅性、礼貌性和词汇多样性；实验评估数据增强（后训练阶段）和推理时用户消息重构两种策略对模型性能的影响。

Result: 用户与聊天机器人交互时语言风格显著不同；基于风格多样化数据训练的模型显著优于仅使用原始或单一风格数据训练的模型，而推理时重构效果较弱。

Conclusion: 为提升LLM在真实场景中的鲁棒性，应使用包含多样化沟通风格的数据进行训练，以更好适应用户与机器交互时的语言变化。

Abstract: As Large Language Models (LLMs) are increasingly deployed in customer-facing
applications, a critical yet underexplored question is how users communicate
differently with LLM chatbots compared to human agent. In this study, we
present empirical evidence that users adopt distinct communication styles when
users interact with chatbots versus human agents. Our analysis reveals
significant differences in grammatical fluency, politeness, and lexical
diversity in user language between the two settings. These findings suggest
that models trained exclusively on human-human interaction data may not
adequately accommodate the communication style shift that occurs once an LLM
chatbot is deployed. To enhance LLM robustness to post-launch communication
style changes, we experimented with two strategies: (1) data augmentation
during the post-training phase and (2) inference-time user message
reformulation. Our results indicate that models trained on stylistically
diverse datasets significantly outperform those trained exclusively on original
or stylistically uniform datasets, while inference-time reformulation proved
less effective. These insights help us to better adapt our models for improved
LLM-user interaction experiences.

</details>


### [106] [SoT: Structured-of-Thought Prompting Guides Multilingual Reasoning in Large Language Models](https://arxiv.org/abs/2510.02648)
*Rui Qi,Zhibo Man,Yufeng Chen,Fengran Mo,Jinan Xu,Kaiyu Huang*

Main category: cs.CL

TL;DR: 提出了一种无需训练的多语言推理增强方法SoT，通过语言思维转换和结构化知识转换提升LLM在低资源语言中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大模型的复杂推理能力难以迁移至低资源语言，受限于多语言推理中的资源不足问题。

Method: 提出Structured-of-Thought（SoT）方法，包含语言思维转换和结构化知识转换两个步骤，将语言特定的语义转化为语言无关的结构化表示，引导模型进行一致且集中的跨语言推理。

Result: SoT在多个多语言推理基准上优于多种强基线方法，适用于不同LLM主干模型，并可与其他无需训练策略结合进一步提升性能。

Conclusion: SoT有效提升了大模型在多语言场景下的推理一致性与表现，为低资源语言推理提供了高效、实用的解决方案。

Abstract: Recent developments have enabled Large Language Models (LLMs) to engage in
complex reasoning tasks through deep thinking. However, the capacity of
reasoning has not been successfully transferred to non-high-resource languages
due to resource constraints, which struggles with multilingual reasoning tasks.
To this end, we propose Structured-of-Thought (SoT), a training-free method
that improves the performance on multilingual reasoning through a multi-step
transformation: Language Thinking Transformation and Structured Knowledge
Transformation. The SoT method converts language-specific semantic information
into language-agnostic structured representations, enabling the models to
understand the query in different languages more sophisticated. Besides, SoT
effectively guides LLMs toward more concentrated reasoning to maintain
consistent underlying reasoning pathways when handling cross-lingual variations
in expression. Experimental results demonstrate that SoT outperforms several
strong baselines on multiple multilingual reasoning benchmarks when adapting to
various backbones of LLMs. It can also be integrated with other training-free
strategies for further improvements. Our code is available at
https://github.com/Cherry-qwq/SoT.

</details>


### [107] [Self-Improvement in Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2510.02665)
*Shijian Deng,Kai Wang,Tianyu Yang,Harsh Singh,Yapeng Tian*

Main category: cs.CL

TL;DR: 这是首个关于多模态大语言模型（MLLM）自提升的综述，系统梳理了数据收集、数据组织和模型优化三方面的研究进展，并讨论了常用评估方法、下游应用及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型的自提升已取得进展，但其在多模态领域的扩展仍处于初期，有必要系统总结现有工作以推动MLLM的自提升发展。

Method: 从数据收集、数据组织和模型优化三个角度对现有MLLM自提升方法进行分类和综述，并总结常用的评估方式与应用场景。

Result: 提供了MLLM自提升领域的全面综述，涵盖了当前主要方法、技术路径与应用实践。

Conclusion: 总结了该领域面临的开放性挑战，指出了未来研究方向，为MLLM的自提升研究提供了系统性参考。

Abstract: Recent advancements in self-improvement for Large Language Models (LLMs) have
efficiently enhanced model capabilities without significantly increasing costs,
particularly in terms of human effort. While this area is still relatively
young, its extension to the multimodal domain holds immense potential for
leveraging diverse data sources and developing more general self-improving
models. This survey is the first to provide a comprehensive overview of
self-improvement in Multimodal LLMs (MLLMs). We provide a structured overview
of the current literature and discuss methods from three perspectives: 1) data
collection, 2) data organization, and 3) model optimization, to facilitate the
further development of self-improvement in MLLMs. We also include commonly used
evaluations and downstream applications. Finally, we conclude by outlining open
challenges and future research directions.

</details>


### [108] [Uncertainty as Feature Gaps: Epistemic Uncertainty Quantification of LLMs in Contextual Question-Answering](https://arxiv.org/abs/2510.02671)
*Yavuz Bakman,Sungmin Kang,Zhiqi Huang,Duygu Nur Yaldiz,Catarina G. Belém,Chenyang Zhu,Anoop Kumar,Alfy Samuel,Salman Avestimehr,Daben Liu,Sai Praneeth Karimireddy*

Main category: cs.CL

TL;DR: 本文提出了一种理论上有据可依的方法，用于量化上下文问答任务中的认知不确定性，通过分解交叉熵并逼近理想模型，引入了任务无关的标记级不确定性度量，并基于上下文依赖性、理解和诚实性三个特征构建不确定性评分，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管不确定性量化在闭卷问答中受到广泛关注，但在实际应用中更加重要的上下文问答中的不确定性量化仍为空白，本文旨在填补这一研究空白。

Method: 提出一种任务无关的标记级不确定性度量，将其分解以分离认知不确定性，并用理想模型逼近真实分布，推导出认知不确定性的上界；将其应用于上下文问答任务，提取上下文依赖性、理解与诚实性三个特征，并通过少量标注样本和自上而下的可解释性方法集成形成不确定性评分。

Result: 在多个上下文问答基准（包括分布内和分布外场景）上的实验表明，该方法显著优于当前最先进的无监督和有监督不确定性量化方法，PRR指标最大提升达13点，且推理开销极低。

Conclusion: 所提出的方法为上下文问答中的认知不确定性量化提供了理论基础和高效实用的解决方案，具有良好的泛化性和应用前景。

Abstract: Uncertainty Quantification (UQ) research has primarily focused on closed-book
factual question answering (QA), while contextual QA remains unexplored,
despite its importance in real-world applications. In this work, we focus on UQ
for the contextual QA task and propose a theoretically grounded approach to
quantify epistemic uncertainty. We begin by introducing a task-agnostic,
token-level uncertainty measure defined as the cross-entropy between the
predictive distribution of the given model and the unknown true distribution.
By decomposing this measure, we isolate the epistemic component and approximate
the true distribution by a perfectly prompted, idealized model. We then derive
an upper bound for epistemic uncertainty and show that it can be interpreted as
semantic feature gaps in the given model's hidden representations relative to
the ideal model. We further apply this generic framework to the contextual QA
task and hypothesize that three features approximate this gap: context-reliance
(using the provided context rather than parametric knowledge), context
comprehension (extracting relevant information from context), and honesty
(avoiding intentional lies). Using a top-down interpretability approach, we
extract these features by using only a small number of labeled samples and
ensemble them to form a robust uncertainty score. Experiments on multiple QA
benchmarks in both in-distribution and out-of-distribution settings show that
our method substantially outperforms state-of-the-art unsupervised
(sampling-free and sampling-based) and supervised UQ methods, achieving up to a
13-point PRR improvement while incurring a negligible inference overhead.

</details>


### [109] [Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks](https://arxiv.org/abs/2510.02712)
*Yubo Li,Ramayya Krishnan,Rema Padman*

Main category: cs.CL

TL;DR: 本研究首次采用生存分析方法评估大语言模型在多轮对话中的鲁棒性，发现渐进式语义漂移反而能显著降低对话失败风险，而突变式漂移则极具破坏性。


<details>
  <summary>Details</summary>
Motivation: 现有评测框架多关注单轮或静态任务，难以捕捉多轮对话中随时间演化的性能退化问题，因此需要一种能建模时间动态性的新评估范式。

Method: 收集了9个最先进大模型共36,951轮对话数据，采用Cox比例风险模型、加速失效时间模型（AFT）和随机生存森林等生存分析方法，将对话失败建模为时间事件过程。

Result: 发现突变式语义漂移（P2P）显著增加对话失败风险，而渐进式漂移具有保护作用，显著延长对话寿命；AFT模型在交互效应下表现出优异的区分度和校准能力。

Conclusion: 生存分析为评估大模型对话鲁棒性提供了有力框架，挑战了语义一致性在对话系统中绝对必要的传统认知，并为构建更稳健的对话代理提供了新设计思路。

Abstract: Large Language Models (LLMs) have revolutionized conversational AI, yet their
robustness in extended multi-turn dialogues remains poorly understood. Existing
evaluation frameworks focus on static benchmarks and single-turn assessments,
failing to capture the temporal dynamics of conversational degradation that
characterize real-world interactions. In this work, we present the first
comprehensive survival analysis of conversational AI robustness, analyzing
36,951 conversation turns across 9 state-of-the-art LLMs to model failure as a
time-to-event process. Our survival modeling framework-employing Cox
proportional hazards, Accelerated Failure Time, and Random Survival Forest
approaches-reveals extraordinary temporal dynamics. We find that abrupt,
prompt-to-prompt(P2P) semantic drift is catastrophic, dramatically increasing
the hazard of conversational failure. In stark contrast, gradual, cumulative
drift is highly protective, vastly reducing the failure hazard and enabling
significantly longer dialogues. AFT models with interactions demonstrate
superior performance, achieving excellent discrimination and exceptional
calibration. These findings establish survival analysis as a powerful paradigm
for evaluating LLM robustness, offer concrete insights for designing resilient
conversational agents, and challenge prevailing assumptions about the necessity
of semantic consistency in conversational AI Systems.

</details>


### [110] [TravelBench : Exploring LLM Performance in Low-Resource Domains](https://arxiv.org/abs/2510.02719)
*Srinivas Billa,Xiaonan Jing*

Main category: cs.CL

TL;DR: 本文提出了一个针对低资源领域的旅行领域数据集集合，用于评估大语言模型在真实场景下的性能，并发现通用基准测试结果不足以反映模型在特定领域的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）基准测试在低资源任务上提供的信息有限，难以有效开发解决方案。为了填补这一空白，需要构建更贴近实际应用场景的评估数据集。

Method: 收集并整理了来自真实场景的14个旅行领域数据集，涵盖7种常见的自然语言处理任务，并对多个LLM进行了系统性性能分析，包括准确性、扩展行为和推理能力。

Result: 实验结果表明，尽管投入大量训练计算资源，现成的LLM在复杂且领域特定的任务中仍遇到性能瓶颈；通用基准测试结果无法准确预测其在低资源任务中的表现；此外，推理能力的引入对小型LLM的提升更为显著。

Conclusion: 针对特定领域特别是低资源领域的模型评估需要专门设计的数据集和方法，未来的研究应更关注领域适配与推理能力对模型性能的影响。

Abstract: Results on existing LLM benchmarks capture little information over the model
capabilities in low-resource tasks, making it difficult to develop effective
solutions in these domains. To address these challenges, we curated 14
travel-domain datasets spanning 7 common NLP tasks using anonymised data from
real-world scenarios, and analysed the performance across LLMs. We report on
the accuracy, scaling behaviour, and reasoning capabilities of LLMs in a
variety of tasks. Our results confirm that general benchmarking results are
insufficient for understanding model performance in low-resource tasks. Despite
the amount of training FLOPs, out-of-the-box LLMs hit performance bottlenecks
in complex, domain-specific scenarios. Furthermore, reasoning provides a more
significant boost for smaller LLMs by making the model a better judge on
certain tasks.

</details>


### [111] [PGMEL: Policy Gradient-based Generative Adversarial Network for Multimodal Entity Linking](https://arxiv.org/abs/2510.02726)
*KM Pooja,Cheng Long,Aixin Sun*

Main category: cs.CL

TL;DR: 提出了一种基于策略梯度的生成对抗网络PGMEL，用于多模态实体链接，通过生成高质量负样本提升表示学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分探索负样本选择对多模态实体链接中表示学习的影响，本文旨在填补这一空白。

Method: 构建生成对抗框架，其中生成器生成高质量负样本，判别器进行度量学习，生成器通过策略梯度方法优化。

Result: 在Wiki-MEL、Richpedia-MEL和WikiDiverse数据集上，PGMEL优于现有最先进方法，能学习更具挑战性的负样本表示。

Conclusion: 引入对抗生成机制并利用策略梯度优化负样本生成，可有效提升多模态实体链接性能。

Abstract: The task of entity linking, which involves associating mentions with their
respective entities in a knowledge graph, has received significant attention
due to its numerous potential applications. Recently, various multimodal entity
linking (MEL) techniques have been proposed, targeted to learn comprehensive
embeddings by leveraging both text and vision modalities. The selection of
high-quality negative samples can potentially play a crucial role in
metric/representation learning. However, to the best of our knowledge, this
possibility remains unexplored in existing literature within the framework of
MEL. To fill this gap, we address the multimodal entity linking problem in a
generative adversarial setting where the generator is responsible for
generating high-quality negative samples, and the discriminator is assigned the
responsibility for the metric learning tasks. Since the generator is involved
in generating samples, which is a discrete process, we optimize it using policy
gradient techniques and propose a policy gradient-based generative adversarial
network for multimodal entity linking (PGMEL). Experimental results based on
Wiki-MEL, Richpedia-MEL and WikiDiverse datasets demonstrate that PGMEL learns
meaningful representation by selecting challenging negative samples and
outperforms state-of-the-art methods.

</details>


### [112] [IndiCASA: A Dataset and Bias Evaluation Framework in LLMs Using Contrastive Embedding Similarity in the Indian Context](https://arxiv.org/abs/2510.02742)
*Santhosh G S,Akshay Govind S,Gokul S Krishnan,Balaraman Ravindran,Sriraam Natarajan*

Main category: cs.CL

TL;DR: 提出了一种基于对比学习编码器的评估框架和印度多维度偏见数据集IndiCASA，用于细粒度检测大语言模型中的文化偏见，发现现有开源LLM普遍存在偏见，尤其是残疾相关偏见较严重。


<details>
  <summary>Details</summary>
Motivation: 现有偏见评估方法在捕捉印度多元文化背景下的细微刻板印象方面存在不足，且高风险应用场景需要更严谨的偏见评估。

Method: 构建基于对比学习的编码器，通过嵌入相似性捕捉细粒度偏见，并提出新数据集IndiCASA，包含2,575个人工验证的句子，覆盖种姓、性别、宗教、残疾和经济地位五个维度。

Result: 对多个开源大模型的评估显示，所有模型均存在一定程度的刻板偏见，其中残疾相关偏见最为显著，宗教偏见较低，可能得益于全球去偏见努力。

Conclusion: 需针对特定文化背景开发更细致的偏见评估工具，并推动更公平的模型开发实践。

Abstract: Large Language Models (LLMs) have gained significant traction across critical
domains owing to their impressive contextual understanding and generative
capabilities. However, their increasing deployment in high stakes applications
necessitates rigorous evaluation of embedded biases, particularly in culturally
diverse contexts like India where existing embedding-based bias assessment
methods often fall short in capturing nuanced stereotypes. We propose an
evaluation framework based on a encoder trained using contrastive learning that
captures fine-grained bias through embedding similarity. We also introduce a
novel dataset - IndiCASA (IndiBias-based Contextually Aligned Stereotypes and
Anti-stereotypes) comprising 2,575 human-validated sentences spanning five
demographic axes: caste, gender, religion, disability, and socioeconomic
status. Our evaluation of multiple open-weight LLMs reveals that all models
exhibit some degree of stereotypical bias, with disability related biases being
notably persistent, and religion bias generally lower likely due to global
debiasing efforts demonstrating the need for fairer model development.

</details>


### [113] [The Path of Self-Evolving Large Language Models: Achieving Data-Efficient Learning via Intrinsic Feedback](https://arxiv.org/abs/2510.02752)
*Hangfan Zhang,Siyuan Xu,Zhimeng Guo,Huaisheng Zhu,Shicheng Liu,Xinrun Wang,Qiaosheng Zhang,Yang Chen,Peng Ye,Lei Bai,Shuyue Hu*

Main category: cs.CL

TL;DR: 提出一种基于自我意识的强化学习方法，通过让大语言模型自主生成并尝试解决任务，在极少外部数据依赖下显著提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习训练大语言模型需要大量标注数据，成本高昂；本文旨在探索如何利用最小化数据提升模型推理能力。

Method: 提出两种基于自我意识的机制：一是自我感知难度预测，模型评估任务难度并选择具有挑战但可解的任务；二是自我极限突破，当任务超出能力范围时主动请求外部数据。训练过程中模型交替提出和解决任务。

Result: 在九个基准测试上实现了53.8%的相对性能提升，仅使用不到1.2%的额外数据。

Conclusion: 自我意识强化学习能有效减少数据依赖，展现自进化智能体训练的巨大潜力。

Abstract: Reinforcement learning (RL) has demonstrated potential in enhancing the
reasoning capabilities of large language models (LLMs), but such training
typically demands substantial efforts in creating and annotating data. In this
work, we explore improving LLMs through RL with minimal data. Our approach
alternates between the LLM proposing a task and then attempting to solve it. To
minimize data dependency, we introduce two novel mechanisms grounded in
self-awareness: (1) self-aware difficulty prediction, where the model learns to
assess task difficulty relative to its own abilities and prioritize challenging
yet solvable tasks, and (2) self-aware limit breaking, where the model
recognizes when a task is beyond its capability boundary and proactively
requests external data to break through that limit. Extensive experiments on
nine benchmarks showing a 53.8% relative improvement with less than 1.2% extra
data demonstrate the efficacy of self-aware RL and underscore the promise of
self-evolving agent training.

</details>


### [114] [XTRA: Cross-Lingual Topic Modeling with Topic and Representation Alignments](https://arxiv.org/abs/2510.02788)
*Tien Phat Nguyen,Vu Minh Ngo,Tung Nguyen,Linh Van Ngo,Duc Anh Nguyen,Sang Dinh,Trung Le*

Main category: cs.CL

TL;DR: 提出了一种名为XTRA的跨语言主题建模新框架，通过统一词袋模型和多语言嵌入，并引入表示对齐和主题对齐机制，显著提升了主题一致性、多样性和跨语言对齐质量。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言主题建模方法在主题一致性与跨语言一致性对齐方面表现不足，需要一种能同时提升主题可解释性和对齐效果的新方法。

Method: XTRA框架结合词袋模型与多语言嵌入，引入两种对齐机制：（1）表示对齐——通过对比学习在共享语义空间中对齐文档-主题分布；（2）主题对齐——将主题-词分布投影到同一空间以增强跨语言一致性。

Result: 在多语言语料库上的实验表明，XTRA在主题一致性、多样性及对齐质量方面显著优于强基线模型。

Conclusion: XTRA通过双重对齐机制有效实现了跨语言主题建模，生成的主题既可解释又在多种语言间保持一致，推动了该领域的性能提升。

Abstract: Cross-lingual topic modeling aims to uncover shared semantic themes across
languages. Several methods have been proposed to address this problem,
leveraging both traditional and neural approaches. While previous methods have
achieved some improvements in topic diversity, they often struggle to ensure
high topic coherence and consistent alignment across languages. We propose XTRA
(Cross-Lingual Topic Modeling with Topic and Representation Alignments), a
novel framework that unifies Bag-of-Words modeling with multilingual
embeddings. XTRA introduces two core components: (1) representation alignment,
aligning document-topic distributions via contrastive learning in a shared
semantic space; and (2) topic alignment, projecting topic-word distributions
into the same space to enforce crosslingual consistency. This dual mechanism
enables XTRA to learn topics that are interpretable (coherent and diverse) and
well-aligned across languages. Experiments on multilingual corpora confirm that
XTRA significantly outperforms strong baselines in topic coherence, diversity,
and alignment quality. Code and reproducible scripts are available at https:
//github.com/tienphat140205/XTRA.

</details>


### [115] [A Computational Framework for Interpretable Text-Based Personality Assessment from Social Media](https://arxiv.org/abs/2510.02811)
*Matej Gjurković*

Main category: cs.CL

TL;DR: 本论文针对人格评估中的数据稀缺和NLP与人格心理学脱节问题，提出了两个大型标注数据集（MBTI9k和PANDORA）及一种可解释的人格评估框架SIMPA，利用语义匹配实现高效、准确且可解释的人格预测。


<details>
  <summary>Details</summary>
Motivation: 现有自动人格评估方法受限于缺乏大规模标注数据集以及自然语言处理与人格心理学之间的脱节，导致模型有效性与可解释性不足。

Method: 从Reddit收集并构建MBTI9k和PANDORA两个数据集，结合MBTI和大五人格模型及人口统计信息；提出SIMPA框架，通过语义相似性将用户生成文本与标准化问卷条目匹配，实现可解释的人格评估。

Result: 实验证明人口统计变量影响模型有效性；SIMPA在多个数据集上实现了与人类评估相当的人格预测效果，同时具备高可解释性和效率。

Conclusion: SIMPA为可解释的人格计算提供了有效路径，其模型无关、分层线索检测和可扩展设计可推广至其他复杂标签体系的研究与应用。

Abstract: Personality refers to individual differences in behavior, thinking, and
feeling. With the growing availability of digital footprints, especially from
social media, automated methods for personality assessment have become
increasingly important. Natural language processing (NLP) enables the analysis
of unstructured text data to identify personality indicators. However, two main
challenges remain central to this thesis: the scarcity of large,
personality-labeled datasets and the disconnect between personality psychology
and NLP, which restricts model validity and interpretability. To address these
challenges, this thesis presents two datasets -- MBTI9k and PANDORA --
collected from Reddit, a platform known for user anonymity and diverse
discussions. The PANDORA dataset contains 17 million comments from over 10,000
users and integrates the MBTI and Big Five personality models with demographic
information, overcoming limitations in data size, quality, and label coverage.
Experiments on these datasets show that demographic variables influence model
validity. In response, the SIMPA (Statement-to-Item Matching Personality
Assessment) framework was developed - a computational framework for
interpretable personality assessment that matches user-generated statements
with validated questionnaire items. By using machine learning and semantic
similarity, SIMPA delivers personality assessments comparable to human
evaluations while maintaining high interpretability and efficiency. Although
focused on personality assessment, SIMPA's versatility extends beyond this
domain. Its model-agnostic design, layered cue detection, and scalability make
it suitable for various research and practical applications involving complex
label taxonomies and variable cue associations with target concepts.

</details>


### [116] [StepChain GraphRAG: Reasoning Over Knowledge Graphs for Multi-Hop Question Answering](https://arxiv.org/abs/2510.02827)
*Tengjun Ni,Xin Yuan,Shenghong Li,Kai Wu,Ren Ping Liu,Wei Ni,Wenjie Zhang*

Main category: cs.CL

TL;DR: StepChain GraphRAG 是一种结合问题分解与广度优先搜索推理流程的检索增强生成框架，通过动态构建知识图谱中的证据链，显著提升多跳问答的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成方法在多跳问答中难以有效整合迭代推理与外部知识检索，导致性能受限且缺乏可解释性。

Method: 提出 StepChain GraphRAG 框架：首先构建语料库的全局索引；在推理时按需将检索到的段落动态解析为知识图谱，并将复杂问题分解为子问题；对每个子问题采用基于 BFS 的推理流程，在图上动态扩展以形成明确的证据链。

Result: 在 MuSiQue、2WikiMultiHopQA 和 HotpotQA 上实现了最先进的 EM 和 F1 分数，平均提升 EM 2.57%、F1 2.13%，在 HotpotQA 上提升最大（+4.70% EM, +3.44% F1），并增强了推理过程的可解释性。

Conclusion: StepChain GraphRAG 有效提升了多跳问答的性能与透明度，未来工作需解决计算开销和大模型幻觉问题以进一步提高效率与可靠性。

Abstract: Recent progress in retrieval-augmented generation (RAG) has led to more
accurate and interpretable multi-hop question answering (QA). Yet, challenges
persist in integrating iterative reasoning steps with external knowledge
retrieval. To address this, we introduce StepChain GraphRAG, a framework that
unites question decomposition with a Breadth-First Search (BFS) Reasoning Flow
for enhanced multi-hop QA. Our approach first builds a global index over the
corpus; at inference time, only retrieved passages are parsed on-the-fly into a
knowledge graph, and the complex query is split into sub-questions. For each
sub-question, a BFS-based traversal dynamically expands along relevant edges,
assembling explicit evidence chains without overwhelming the language model
with superfluous context. Experiments on MuSiQue, 2WikiMultiHopQA, and HotpotQA
show that StepChain GraphRAG achieves state-of-the-art Exact Match and F1
scores. StepChain GraphRAG lifts average EM by 2.57% and F1 by 2.13% over the
SOTA method, achieving the largest gain on HotpotQA (+4.70% EM, +3.44% F1).
StepChain GraphRAG also fosters enhanced explainability by preserving the
chain-of-thought across intermediate retrieval steps. We conclude by discussing
how future work can mitigate the computational overhead and address potential
hallucinations from large language models to refine efficiency and reliability
in multi-hop QA.

</details>


### [117] [Evaluating Large Language Models for IUCN Red List Species Information](https://arxiv.org/abs/2510.02830)
*Shinya Uryu*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在物种保护评估中表现出显著的知识-推理鸿沟：虽能准确分类物种（94.9%），但在评估保护状态方面表现极差（27.2%），且存在偏好 charismatic 脊椎动物的系统性偏差，需依赖人类专家监督以实现负责任应用。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在IUCN红色名录物种评估中的可靠性，揭示其在保护生物学应用中的潜力与局限。

Method: 对五个主流LLM在21,955个物种的四个IUCN评估维度（分类、保护状态、分布、威胁）上进行系统性验证。

Result: 模型在分类任务上表现优异（94.9%准确率），但在保护状态评估等推理任务上严重失准（仅27.2%）；普遍存在对 charismatic 脊椎动物的系统性偏见。

Conclusion: LLM可作为信息检索工具辅助保护工作，但因架构限制和推理缺陷，必须由人类专家主导关键决策，建议采取人机协作的混合模式。

Abstract: Large Language Models (LLMs) are rapidly being adopted in conservation to
address the biodiversity crisis, yet their reliability for species evaluation
is uncertain. This study systematically validates five leading models on 21,955
species across four core IUCN Red List assessment components: taxonomy,
conservation status, distribution, and threats. A critical paradox was
revealed: models excelled at taxonomic classification (94.9%) but consistently
failed at conservation reasoning (27.2% for status assessment). This
knowledge-reasoning gap, evident across all models, suggests inherent
architectural constraints, not just data limitations. Furthermore, models
exhibited systematic biases favoring charismatic vertebrates, potentially
amplifying existing conservation inequities. These findings delineate clear
boundaries for responsible LLM deployment: they are powerful tools for
information retrieval but require human oversight for judgment-based decisions.
A hybrid approach is recommended, where LLMs augment expert capacity while
human experts retain sole authority over risk assessment and policy.

</details>


### [118] [Constraint Satisfaction Approaches to Wordle: Novel Heuristics and Cross-Lexicon Validation](https://arxiv.org/abs/2510.02855)
*Jahidul Arafat,Fariha Tasmin,Sanjaya Poudel,Kamrujjaman,Eftakhar Ahmed Arnob,Ahsan Habib Tareq*

Main category: cs.CL

TL;DR: 本文首次系统地将Wordle建模为约束满足问题（CSP），提出CSP感知的熵优化与概率性CSP框架，在准确率、鲁棒性和运行效率上均优于传统方法，并验证了其跨语言可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有Wordle求解器依赖信息论熵最大化或频率启发式，缺乏对约束的正式建模；本文旨在建立首个完整的CSP建模范式，提升求解效率与鲁棒性。

Method: 提出CSP-Aware Entropy，在约束传播后计算信息增益；构建融合贝叶斯词频先验与逻辑约束的Probabilistic CSP框架；在英语和西班牙语词汇上进行交叉验证。

Result: 在2,315个英文词上，CSP-Aware Entropy平均3.54次猜测成功率达99.9%，比前向检查快46%；在10%噪声下性能领先5.3个百分点；Probabilistic CSP在0-20%噪声下均实现100%成功；西班牙语测试无调优下达88%成功率。

Conclusion: 基于形式化CSP建模、约束感知启发式与概率逻辑融合的方法在结构化谜题求解中显著优于传统方法，建立了新性能基准，且具备跨语言适用性。

Abstract: Wordle presents an algorithmically rich testbed for constraint satisfaction
problem (CSP) solving. While existing solvers rely on information-theoretic
entropy maximization or frequency-based heuristics without formal constraint
treatment, we present the first comprehensive CSP formulation of Wordle with
novel constraint-aware solving strategies. We introduce CSP-Aware Entropy,
computing information gain after constraint propagation rather than on raw
candidate sets, and a Probabilistic CSP framework integrating Bayesian
word-frequency priors with logical constraints. Through evaluation on 2,315
English words, CSP-Aware Entropy achieves 3.54 average guesses with 99.9%
success rate, a statistically significant 1.7% improvement over Forward
Checking (t=-4.82, p<0.001, Cohen's d=0.07) with 46% faster runtime (12.9ms
versus 23.7ms per guess). Under 10% noise, CSP-aware approaches maintain 5.3
percentage point advantages (29.0% versus 23.7%, p=0.041), while Probabilistic
CSP achieves 100% success across all noise levels (0-20%) through constraint
recovery mechanisms. Cross-lexicon validation on 500 Spanish words demonstrates
88% success with zero language-specific tuning, validating that core CSP
principles transfer across languages despite an 11.2 percentage point gap from
linguistic differences (p<0.001, Fisher's exact test). Our open-source
implementation with 34 unit tests achieving 91% code coverage provides
reproducible infrastructure for CSP research. The combination of formal CSP
treatment, constraint-aware heuristics, probabilistic-logical integration,
robustness analysis, and cross-lexicon validation establishes new performance
benchmarks demonstrating that principled constraint satisfaction techniques
outperform classical information-theoretic and learning-based approaches for
structured puzzle-solving domains.

</details>


### [119] [Self-Reflective Generation at Test Time](https://arxiv.org/abs/2510.02919)
*Jian Mu,Qixin Zhang,Zhiyong Wang,Menglin Yang,Shuang Qiu,Chengwei Qin,Zhongxiang Dai,Yao Shu*

Main category: cs.CL

TL;DR: 提出了一种名为SRGen的轻量级测试时自反思生成框架，通过在生成过程中识别高不确定性token并进行即时修正，显著提升了大语言模型在复杂数学推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的自反思方法要么在完整草稿上进行修改，要么依赖昂贵的训练过程进行自我修正，效率低且反应滞后，难以应对LLM生成过程中因早期错误导致的级联误差问题。

Method: SRGen在生成token时利用动态熵阈值识别高不确定性token，并基于已生成的上下文训练特定的修正向量，以调整该token的概率分布，实现生成前的自反思修正。

Result: 在多个复杂数学推理基准和不同LLM上验证，SRGen显著增强了模型推理能力，例如在AIME2024上，DeepSeek-R1-Distill-Qwen-7B的Pass@1提升12.0%，Cons@5提升13.3%，并表现出良好的兼容性和低开销。

Conclusion: SRGen是一种即插即用的方法，能将自反思机制有效集成到生成过程中，提升LLM推理的可靠性，且具有广泛组合潜力。

Abstract: Large language models (LLMs) increasingly solve complex reasoning tasks via
long chain-of-thought, but their forward-only autoregressive generation process
is fragile; early token errors can cascade, which creates a clear need for
self-reflection mechanisms. However, existing self-reflection either performs
revisions over full drafts or learns self-correction via expensive training,
both fundamentally reactive and inefficient. To address this, we propose
Self-Reflective Generation at Test Time (SRGen), a lightweight test-time
framework that reflects before generating at uncertain points. During token
generation, SRGen utilizes dynamic entropy thresholding to identify
high-uncertainty tokens. For each identified token, it trains a specific
corrective vector, which fully exploits the already generated context for a
self-reflective generation to correct the token probability distribution. By
retrospectively analyzing the partial output, this self-reflection enables more
trustworthy decisions, thereby significantly reducing the probability of errors
at highly uncertain points. Evaluated on challenging mathematical reasoning
benchmarks and a diverse set of LLMs, SRGen can consistently strengthen model
reasoning: improvements in single-pass quality also translate into stronger
self-consistency voting. Especially, on AIME2024 with
DeepSeek-R1-Distill-Qwen-7B, SRGen yields absolute improvements of +12.0% on
Pass@1 and +13.3% on Cons@5. Moreover, our findings position SRGen as a
plug-and-play method that integrates reflection into the generation process for
reliable LLM reasoning, achieving consistent gains with bounded overhead and
broad composability with other training-time (e.g., RLHF) and test-time (e.g.,
SLOT) techniques.

</details>


### [120] [Finding Diamonds in Conversation Haystacks: A Benchmark for Conversational Data Retrieval](https://arxiv.org/abs/2510.02938)
*Yohan Lee,Yongwoo Song,Sangyeop Kim*

Main category: cs.CL

TL;DR: 提出首个用于评估对话数据检索性能的基准CDR，包含1.6k查询和9.1k对话，揭示当前模型在对话数据检索上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有检索系统主要面向文档，难以有效处理对话数据中的复杂语义和上下文依赖，缺乏标准 benchmarks 来评估对话数据检索性能。

Method: 构建CDR基准，涵盖五个分析任务的1.6k查询和9.1k对话，评估16种主流嵌入模型的表现，并进行错误分析。

Result: 最优模型在NDCG@10上仅达到约0.51，显著低于文档检索性能，揭示了对话数据检索的重大挑战。

Conclusion: CDR为对话数据检索提供了可靠的评估标准，突显了该领域需解决的关键问题，如隐含状态识别、轮次动态和上下文指代。

Abstract: We present the Conversational Data Retrieval (CDR) benchmark, the first
comprehensive test set for evaluating systems that retrieve conversation data
for product insights. With 1.6k queries across five analytical tasks and 9.1k
conversations, our benchmark provides a reliable standard for measuring
conversational data retrieval performance. Our evaluation of 16 popular
embedding models shows that even the best models reach only around NDCG@10 of
0.51, revealing a substantial gap between document and conversational data
retrieval capabilities. Our work identifies unique challenges in conversational
data retrieval (implicit state recognition, turn dynamics, contextual
references) while providing practical query templates and detailed error
analysis across different task categories. The benchmark dataset and code are
available at https://github.com/l-yohai/CDR-Benchmark.

</details>


### [121] [Leave No TRACE: Black-box Detection of Copyrighted Dataset Usage in Large Language Models via Watermarking](https://arxiv.org/abs/2510.02962)
*Jingqi Zhang,Ruibo Chen,Yingqing Yang,Peihua Mai,Heng Huang,Yan Pang*

Main category: cs.CL

TL;DR: 提出TRACE框架，实现对大语言模型微调中使用版权数据集的完全黑盒检测，具有高检测能力、多数据集归因和支持持续预训练后的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有成员推理攻击和数据集推断方法大多需要访问内部信号或依赖手工提示与干净参考数据集，限制了实际应用；水印技术可能影响文本质量或任务性能，因此需要一种实用且高效的黑盒检测方法。

Method: 提出TRACE框架，通过私钥引导在数据集中嵌入无损水印，并在检测时利用微调对水印数据的‘放射性’效应，结合熵门控机制选择高不确定性token进行评分以增强检测能力。

Result: 在多种数据集和模型族上，TRACE consistently 实现显著检测效果（p<0.05），具备极强统计证据，支持多数据集归因，且在大规模非水印语料继续预训练后仍保持鲁棒性。

Conclusion: TRACE为大语言模型微调中版权数据使用的可靠黑盒验证提供了实用路径。

Abstract: Large Language Models (LLMs) are increasingly fine-tuned on smaller,
domain-specific datasets to improve downstream performance. These datasets
often contain proprietary or copyrighted material, raising the need for
reliable safeguards against unauthorized use. Existing membership inference
attacks (MIAs) and dataset-inference methods typically require access to
internal signals such as logits, while current black-box approaches often rely
on handcrafted prompts or a clean reference dataset for calibration, both of
which limit practical applicability. Watermarking is a promising alternative,
but prior techniques can degrade text quality or reduce task performance. We
propose TRACE, a practical framework for fully black-box detection of
copyrighted dataset usage in LLM fine-tuning. \texttt{TRACE} rewrites datasets
with distortion-free watermarks guided by a private key, ensuring both text
quality and downstream utility. At detection time, we exploit the radioactivity
effect of fine-tuning on watermarked data and introduce an entropy-gated
procedure that selectively scores high-uncertainty tokens, substantially
amplifying detection power. Across diverse datasets and model families, TRACE
consistently achieves significant detections (p<0.05), often with extremely
strong statistical evidence. Furthermore, it supports multi-dataset attribution
and remains robust even after continued pretraining on large non-watermarked
corpora. These results establish TRACE as a practical route to reliable
black-box verification of copyrighted dataset usage. We will make our code
available at: https://github.com/NusIoraPrivacy/TRACE.

</details>


### [122] [Grounding Large Language Models in Clinical Evidence: A Retrieval-Augmented Generation System for Querying UK NICE Clinical Guidelines](https://arxiv.org/abs/2510.02967)
*Matthew Lewis,Samuel Thio,Richard JB Dobson,Spiros Denaxas*

Main category: cs.CL

TL;DR: 本文提出了一种用于查询英国国家健康与护理卓越中心（NICE）临床指南的检索增强生成（RAG）系统，通过结合大语言模型与混合检索机制，显著提升了信息获取的准确性和可靠性，尤其在回答忠实性方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 由于NICE临床指南数量庞大、内容冗长，在时间紧迫的医疗环境中难以高效利用，因此需要一种能快速准确响应自然语言查询的智能系统。

Method: 构建一个基于混合嵌入机制的RAG系统，对从300份指南中提取的10,195个文本片段进行索引，并在7,901个查询上评估其检索性能；在生成阶段使用70个人工标注的问答对评估RAG增强模型的表现。

Result: 检索模块表现出色，MRR为0.814，首条召回率为81%，前十条召回率达99.1%；生成阶段RAG增强的O4-Mini模型忠实度达到99.5%（提升64.7个百分点），远超Meditron3-8B的43%，且上下文精准度均为1。

Conclusion: RAG是一种高效、可靠且可扩展的方法，能够将生成式AI安全地应用于医疗领域，提升医护人员对临床指南的可及性与使用效率。

Abstract: This paper presents the development and evaluation of a Retrieval-Augmented
Generation (RAG) system for querying the United Kingdom's National Institute
for Health and Care Excellence (NICE) clinical guidelines using Large Language
Models (LLMs). The extensive length and volume of these guidelines can impede
their utilisation within a time-constrained healthcare system, a challenge this
project addresses through the creation of a system capable of providing users
with precisely matched information in response to natural language queries. The
system's retrieval architecture, composed of a hybrid embedding mechanism, was
evaluated against a database of 10,195 text chunks derived from three hundred
guidelines. It demonstrates high performance, with a Mean Reciprocal Rank (MRR)
of 0.814, a Recall of 81% at the first chunk and of 99.1% within the top ten
retrieved chunks, when evaluated on 7901 queries.
  The most significant impact of the RAG system was observed during the
generation phase. When evaluated on a manually curated dataset of seventy
question-answer pairs, RAG-enhanced models showed substantial gains in
performance. Faithfulness, the measure of whether an answer is supported by the
source text, was increased by 64.7 percentage points to 99.5% for the
RAG-enhanced O4-Mini model and significantly outperformed the medical-focused
Meditron3-8B LLM, which scored 43%. This, combined with a perfect Context
Precision score of 1 for all RAG-enhanced models, confirms the system's ability
to prevent information fabrication by grounding its answers in relevant source
material. This study thus establishes RAG as an effective, reliable, and
scalable approach for applying generative AI in healthcare, enabling
cost-effective access to medical guidelines.

</details>


### [123] [Semantic Differentiation in Speech Emotion Recognition: Insights from Descriptive and Expressive Speech Roles](https://arxiv.org/abs/2510.03060)
*Rongchen Guo,Vincent Francoeur,Isar Nejadgholi,Sylvain Gagnon,Miodrag Bolic*

Main category: cs.CL

TL;DR: 该研究区分了语音中的描述性语义和表达性语义，发现前者与意图情绪相关，后者与诱发情绪相关，有助于提升语音情绪识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 语音情绪识别（SER）在人机交互中至关重要，但情感细微差别的复杂性限制了其准确性。因此，需区分语音中的语义类型以提升识别效果。

Method: 记录参与者观看情绪化电影片段后的语音描述，收集意图情绪标签、自我报告情绪反应和效价/唤醒评分，分析描述性与表达性语义与情绪的关系。

Result: 实验表明，描述性语义与意图情绪一致，表达性语义与诱发情绪（如自我报告的情绪和效价/唤醒）相关。

Conclusion: 研究结果有助于改进人机交互中的语音情绪识别系统，推动更具备上下文感知能力的人工智能发展。

Abstract: Speech Emotion Recognition (SER) is essential for improving human-computer
interaction, yet its accuracy remains constrained by the complexity of
emotional nuances in speech. In this study, we distinguish between descriptive
semantics, which represents the contextual content of speech, and expressive
semantics, which reflects the speaker's emotional state. After watching
emotionally charged movie segments, we recorded audio clips of participants
describing their experiences, along with the intended emotion tags for each
clip, participants' self-rated emotional responses, and their valence/arousal
scores. Through experiments, we show that descriptive semantics align with
intended emotions, while expressive semantics correlate with evoked emotions.
Our findings inform SER applications in human-AI interaction and pave the way
for more context-aware AI systems.

</details>


### [124] [Semantic Similarity in Radiology Reports via LLMs and NER](https://arxiv.org/abs/2510.03102)
*Beth Pearson,Ahmed Adnan,Zahraa Abdallah*

Main category: cs.CL

TL;DR: 本文提出Llama-EntScore方法，结合Llama 3.1和命名实体识别（NER），通过可调权重生成可解释的放射学报告语义相似性评分，优于单独使用大语言模型或NER方法。


<details>
  <summary>Details</summary>
Motivation: 帮助初级放射科医生识别初稿与终稿报告之间的语义差异，提升诊断准确性和培训效果，弥补现有大语言模型和NER方法在专业领域语义相似性评估中的不足。

Method: 比较多种大语言模型在报告对比中的表现，评估基于NER的传统方法，进而提出Llama-EntScore方法，融合Llama 3.1和NER，并引入可调权重机制生成量化相似性评分及解释性反馈。

Result: Llama-EntScore在与放射科医生提供的真值评分对比中达到67%的完全匹配准确率和93%的±1误差内准确率，优于单独使用大语言模型或NER方法。

Conclusion: 结合大语言模型与NER并引入可调权重的Llama-EntScore能更准确、可解释地评估放射学报告的语义差异，有助于临床培训和报告质量提升。

Abstract: Radiology report evaluation is a crucial part of radiologists' training and
plays a key role in ensuring diagnostic accuracy. As part of the standard
reporting workflow, a junior radiologist typically prepares a preliminary
report, which is then reviewed and edited by a senior radiologist to produce
the final report. Identifying semantic differences between preliminary and
final reports is essential for junior doctors, both as a training tool and to
help uncover gaps in clinical knowledge. While AI in radiology is a rapidly
growing field, the application of large language models (LLMs) remains
challenging due to the need for specialised domain knowledge. In this paper, we
explore the ability of LLMs to provide explainable and accurate comparisons of
reports in the radiology domain. We begin by comparing the performance of
several LLMs in comparing radiology reports. We then assess a more traditional
approach based on Named-Entity-Recognition (NER). However, both approaches
exhibit limitations in delivering accurate feedback on semantic similarity. To
address this, we propose Llama-EntScore, a semantic similarity scoring method
using a combination of Llama 3.1 and NER with tunable weights to emphasise or
de-emphasise specific types of differences. Our approach generates a
quantitative similarity score for tracking progress and also gives an
interpretation of the score that aims to offer valuable guidance in reviewing
and refining their reporting. We find our method achieves 67% exact-match
accuracy and 93% accuracy within +/- 1 when compared to radiologist-provided
ground truth scores - outperforming both LLMs and NER used independently. Code
is available at:
\href{https://github.com/otmive/llama_reports}{github.com/otmive/llama\_reports}

</details>


### [125] [SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?](https://arxiv.org/abs/2510.03120)
*Zhaojun Sun,Xuzhou Zhu,Xuanhe Zhou,Xin Tong,Shuo Wang,Jie Fu,Guoliang Li,Zhiyuan Liu,Fan Wu*

Main category: cs.CL

TL;DR: 提出了一种细粒度、基于测验的评估框架SurveyBench，用于评估自动生成学术综述的质量，发现现有LLM4Survey方法与人类水平仍有显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有自动综述生成方法输出质量有限，缺乏严格且与读者需求对齐的评估基准，难以揭示其缺陷。

Method: 构建SurveyBench框架，包含来自arXiv的广泛主题和高质量综述数据集、多维度评估指标体系（提纲、内容、非文本质量）以及内容和测验双模式评估协议。

Result: 实验证明，SurveyBench能有效挑战现有LLM4Survey方法，其在基于内容的评估中平均比人类低21%。

Conclusion: SurveyBench为自动综述生成提供了更严格和读者对齐的评估标准，揭示了当前方法的不足，推动该领域发展。

Abstract: Academic survey writing, which distills vast literature into a coherent and
insightful narrative, remains a labor-intensive and intellectually demanding
task. While recent approaches, such as general DeepResearch agents and
survey-specialized methods, can generate surveys automatically (a.k.a.
LLM4Survey), their outputs often fall short of human standards and there lacks
a rigorous, reader-aligned benchmark for thoroughly revealing their
deficiencies. To fill the gap, we propose a fine-grained, quiz-driven
evaluation framework SurveyBench, featuring (1) typical survey topics source
from recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys;
(2) a multifaceted metric hierarchy that assesses the outline quality (e.g.,
coverage breadth, logical coherence), content quality (e.g., synthesis
granularity, clarity of insights), and non-textual richness; and (3) a
dual-mode evaluation protocol that includes content-based and quiz-based
answerability tests, explicitly aligned with readers' informational needs.
Results show SurveyBench effectively challenges existing LLM4Survey approaches
(e.g., on average 21% lower than human in content-based evaluation).

</details>


### [126] [Beyond the Final Layer: Intermediate Representations for Better Multilingual Calibration in Large Language Models](https://arxiv.org/abs/2510.03136)
*Ej Zhou,Caiqi Zhang,Tiancheng Hu,Chengzu Li,Nigel Collier,Ivan Vulić,Anna Korhonen*

Main category: cs.CL

TL;DR: 本研究首次系统探讨了大规模多语言场景下大语言模型的置信度校准问题，发现非英语语言普遍存在校准效果差的问题，提出无需训练的层间集成方法LACE，通过选择最优中间层组合提升多语言校准性能。


<details>
  <summary>Details</summary>
Motivation: 尽管置信度校准对大语言模型的可靠部署至关重要，但在多语言场景下的研究仍不足。观察到非英语语言校准表现较差，作者希望探究其成因并提出无需训练的解决方案，以提升全球范围内语言模型的公平性与可信度。

Method: 在六个模型家族和超过100种语言上进行大规模校准分析；通过分析模型内部表示，发现最终层受英语中心训练影响较大；提出基于中间层的训练免费方法，如语言感知置信集成（LACE），自适应选择每种语言的最优层组合进行置信度估计。

Result: 实验证明非英语语言普遍存在更差的校准性能；最终层提供较差的多语言置信信号，而晚期中间层更可靠；LACE等方法显著提升多语言校准效果，且无需额外训练。

Conclusion: 英语中心的训练带来了多语言校准的隐性代价，仅依赖最终层会损害非英语语言的可靠性；通过利用中间层信息可构建更公平、可信的多语言大模型，为校准研究提供了新方向。

Abstract: Confidence calibration, the alignment of a model's predicted confidence with
its actual accuracy, is crucial for the reliable deployment of Large Language
Models (LLMs). However, this critical property remains largely under-explored
in multilingual contexts. In this work, we conduct the first large-scale,
systematic studies of multilingual calibration across six model families and
over 100 languages, revealing that non-English languages suffer from
systematically worse calibration. To diagnose this, we investigate the model's
internal representations and find that the final layer, biased by
English-centric training, provides a poor signal for multilingual confidence.
In contrast, our layer-wise analysis uncovers a key insight that
late-intermediate layers consistently offer a more reliable and
better-calibrated signal. Building on this, we introduce a suite of
training-free methods, including Language-Aware Confidence Ensemble (LACE),
which adaptively selects an optimal ensemble of layers for each specific
language. Our study highlights the hidden costs of English-centric alignment
and offer a new path toward building more globally equitable and trustworthy
LLMs by looking beyond the final layer.

</details>


### [127] [EditLens: Quantifying the Extent of AI Editing in Text](https://arxiv.org/abs/2510.03154)
*Katherine Thai,Bradley Emi,Elyas Masrour,Mohit Iyyer*

Main category: cs.CL

TL;DR: 提出并验证了轻量级相似性度量方法，利用这些度量训练了一个名为EditLens的回归模型，能够有效检测和量化AI对人类文本的编辑程度，在二元和三元分类任务中达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要集中在检测完全由AI生成的文本，而忽视了AI编辑文本的检测，这在实际应用中非常重要，因为很多情况下AI只是对人类撰写的文本进行了修改。

Method: 提出了使用轻量级相似性度量来量化AI编辑的程度，并通过人工标注验证这些度量的有效性；基于这些相似性度量作为中间监督信号，训练了一个回归模型EditLens来预测文本中AI编辑的程度。

Result: EditLens模型在二元分类（F1=94.7%）和三元分类（F1=90.4%）任务上达到了最先进的性能，能够准确地区分人类撰写、AI生成和混合编写的文本。

Conclusion: 研究表明AI编辑的文本可以被有效检测，并且AI对人类文本的修改程度也可以被量化，这对作者归属、教育和政策制定具有重要意义。

Abstract: A significant proportion of queries to large language models ask them to edit
user-provided text, rather than generate new text from scratch. While previous
work focuses on detecting fully AI-generated text, we demonstrate that
AI-edited text is distinguishable from human-written and AI-generated text.
First, we propose using lightweight similarity metrics to quantify the
magnitude of AI editing present in a text given the original human-written text
and validate these metrics with human annotators. Using these similarity
metrics as intermediate supervision, we then train EditLens, a regression model
that predicts the amount of AI editing present within a text. Our model
achieves state-of-the-art performance on both binary (F1=94.7%) and ternary
(F1=90.4%) classification tasks in distinguishing human, AI, and mixed writing.
Not only do we show that AI-edited text can be detected, but also that the
degree of change made by AI to human writing can be detected, which has
implications for authorship attribution, education, and policy. Finally, as a
case study, we use our model to analyze the effects of AI-edits applied by
Grammarly, a popular writing assistance tool. To encourage further research, we
commit to publicly releasing our models and dataset.

</details>


### [128] [Neural Correlates of Language Models Are Specific to Human Language](https://arxiv.org/abs/2510.03156)
*Iñigo Parra*

Main category: cs.CL

TL;DR: 该研究验证了大型语言模型隐藏状态与fMRI脑响应之间的相关性在多种条件下仍稳健，确认并加强了先前的研究结果。


<details>
  <summary>Details</summary>
Motivation: 检验之前发现的大型语言模型与大脑状态表征相似性是否对多种潜在问题具有鲁棒性。

Method: 通过降维、使用新的相似性度量方法、比较不同训练模型以及分析位置编码的影响来测试先前结果的稳健性。

Result: 发现先前结果在降维后依然成立，新相似性度量方法证实了结果，且相关性仅在训练于人类语言的模型中出现，并依赖于模型中的位置编码。

Conclusion: 结果支持大型语言模型与人脑语言处理之间存在表征相似性，增强了对这些模型生物学合理性和可解释性的理解。

Abstract: Previous work has shown correlations between the hidden states of large
language models and fMRI brain responses, on language tasks. These correlations
have been taken as evidence of the representational similarity of these models
and brain states. This study tests whether these previous results are robust to
several possible concerns. Specifically this study shows: (i) that the previous
results are still found after dimensionality reduction, and thus are not
attributable to the curse of dimensionality; (ii) that previous results are
confirmed when using new measures of similarity; (iii) that correlations
between brain representations and those from models are specific to models
trained on human language; and (iv) that the results are dependent on the
presence of positional encoding in the models. These results confirm and
strengthen the results of previous research and contribute to the debate on the
biological plausibility and interpretability of state-of-the-art large language
models.

</details>


### [129] [Topic Modeling as Long-Form Generation: Can Long-Context LLMs revolutionize NTM via Zero-Shot Prompting?](https://arxiv.org/abs/2510.03174)
*Xuan Xu,Haolun Li,Zhongliang Yang,Beilin Chu,Jia Song,Moxuan Xu,Linna Zhou*

Main category: cs.CL

TL;DR: 本文提出一种基于大语言模型（LLM）的新型主题建模范式，将主题建模视为长文本生成任务，通过零样本提示实现，并与传统神经主题模型（NTM）进行系统比较。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，传统神经主题模型可能已过时，本文旨在探索LLM是否能在主题建模任务中超越NTM，并重新定义主题建模的范式。

Method: 将主题建模重构为长文本生成任务，采用零样本提示方法：采样数据子集，通过设计提示词生成主题及代表性文本，并利用关键词匹配进行文本分配。

Result: 实验系统比较了NTM与LLM在主题质量上的表现，结果显示在零样本设置下，LLM在某些指标上可与或优于传统NTM。

Conclusion: 大语言模型通过长文本生成范式在主题建模中展现出潜力，挑战了传统NTM的有效性，提示‘多数NTM可能已过时’的观点值得重视。

Abstract: Traditional topic models such as neural topic models rely on inference and
generation networks to learn latent topic distributions. This paper explores a
new paradigm for topic modeling in the era of large language models, framing TM
as a long-form generation task whose definition is updated in this paradigm. We
propose a simple but practical approach to implement LLM-based topic model
tasks out of the box (sample a data subset, generate topics and representative
text with our prompt, text assignment with keyword match). We then investigate
whether the long-form generation paradigm can beat NTMs via zero-shot
prompting. We conduct a systematic comparison between NTMs and LLMs in terms of
topic quality and empirically examine the claim that "a majority of NTMs are
outdated."

</details>


### [130] [Model-Based Ranking of Source Languages for Zero-Shot Cross-Lingual Transfer](https://arxiv.org/abs/2510.03202)
*Abteen Ebrahimi,Adam Wiemerslage,Katharina von der Wense*

Main category: cs.CL

TL;DR: 提出NN-Rank算法，利用多语言模型的隐藏表示和无标签目标语言数据进行跨语言迁移的源语言排序，在POS和NER任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在跨语言迁移中选择合适的源语言对目标语言性能至关重要，现有方法依赖语言学特征或词汇特征，难以充分利用语义信息。

Method: 提出NN-Rank算法，基于多语言模型的隐藏表示和少量无标签目标语言数据计算源语言与目标语言的相似度，进行源语言排序。

Result: 在POS和NER任务上，使用领域内数据时NN-Rank比现有最优方法平均提升最多35.56 NDCG（POS）和18.14 NDCG（NER）；即使使用圣经等领域外数据，表现仍具竞争力；仅用25个样例即可达到使用全部数据92.8%的NDCG性能。

Conclusion: NN-Rank能有效利用多语言模型表示和少量无标签目标数据进行源语言排序，性能优越且对数据量需求低，适用于资源稀缺语言。

Abstract: We present NN-Rank, an algorithm for ranking source languages for
cross-lingual transfer, which leverages hidden representations from
multilingual models and unlabeled target-language data. We experiment with two
pretrained multilingual models and two tasks: part-of-speech tagging (POS) and
named entity recognition (NER). We consider 51 source languages and evaluate on
56 and 72 target languages for POS and NER, respectively. When using in-domain
data, NN-Rank beats state-of-the-art baselines that leverage lexical and
linguistic features, with average improvements of up to 35.56 NDCG for POS and
18.14 NDCG for NER. As prior approaches can fall back to language-level
features if target language data is not available, we show that NN-Rank remains
competitive using only the Bible, an out-of-domain corpus available for a large
number of languages. Ablations on the amount of unlabeled target data show
that, for subsets consisting of as few as 25 examples, NN-Rank produces
high-quality rankings which achieve 92.8% of the NDCG achieved using all
available target data for ranking.

</details>


### [131] [FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of Web Agents](https://arxiv.org/abs/2510.03204)
*Imene Kerboua,Sahar Omidi Shayegan,Megh Thakkar,Xing Han Lù,Léo Boisvert,Massimo Caccia,Jérémy Espinas,Alexandre Aussem,Véronique Eglin,Alexandre Lacoste*

Main category: cs.CL

TL;DR: FocusAgent是一种利用轻量级语言模型检索器从网页的可访问性树中提取与任务最相关文本的方法，能有效减少输入长度、计算成本和安全风险，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 现有的网页代理在处理超长网页内容时面临上下文过长、计算开销大和安全性差（如提示注入攻击）的问题，而现有剪枝策略往往丢失关键信息或保留过多无关内容。

Method: 提出FocusAgent，使用轻量级LLM检索器，根据任务目标从可访问性树（AxTree）中提取最相关的行，过滤噪声和无关内容，实现更高效和安全的推理。

Result: 在WorkArena和WebArena基准上，FocusAgent在减少超过50%观测大小的同时，性能与强基线相当；其变体显著降低了提示注入攻击（如横幅和弹窗攻击）的成功率，且在无攻击环境下保持任务成功率。

Conclusion: 基于LLM的定向检索是一种构建高效、有效且安全的网页代理的实用且鲁棒的策略。

Abstract: Web agents powered by large language models (LLMs) must process lengthy web
page observations to complete user goals; these pages often exceed tens of
thousands of tokens. This saturates context limits and increases computational
cost processing; moreover, processing full pages exposes agents to security
risks such as prompt injection. Existing pruning strategies either discard
relevant content or retain irrelevant context, leading to suboptimal action
prediction. We introduce FocusAgent, a simple yet effective approach that
leverages a lightweight LLM retriever to extract the most relevant lines from
accessibility tree (AxTree) observations, guided by task goals. By pruning
noisy and irrelevant content, FocusAgent enables efficient reasoning while
reducing vulnerability to injection attacks. Experiments on WorkArena and
WebArena benchmarks show that FocusAgent matches the performance of strong
baselines, while reducing observation size by over 50%. Furthermore, a variant
of FocusAgent significantly reduces the success rate of prompt-injection
attacks, including banner and pop-up attacks, while maintaining task success
performance in attack-free settings. Our results highlight that targeted
LLM-based retrieval is a practical and robust strategy for building web agents
that are efficient, effective, and secure.

</details>


### [132] [Cache-to-Cache: Direct Semantic Communication Between Large Language Models](https://arxiv.org/abs/2510.03215)
*Tianyu Fu,Zihan Min,Hanling Zhang,Jichao Yan,Guohao Dai,Wanli Ouyang,Yu Wang*

Main category: cs.CL

TL;DR: 提出Cache-to-Cache（C2C）新范式，通过直接在LLM间传递KV-Cache实现超越文本的语义通信，显著提升性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM系统通过文本通信导致语义信息丢失和生成延迟，限制了性能提升，因此探索超越文本的通信方式。

Method: 提出C2C框架，利用神经网络投影并融合源模型与目标模型的KV-Cache，通过可学习的门控机制选择受益层，实现深层语义直接传输。

Result: C2C相较单个模型平均准确率提升8.5-10.5%，优于文本通信3.0-5.0%，并实现平均2.0倍的延迟降低。

Conclusion: C2C是一种高效、高速的LLM间通信新范式，验证了KV-Cache作为语义通信媒介的可行性与优势。

Abstract: Multi-LLM systems harness the complementary strengths of diverse Large
Language Models, achieving performance and efficiency gains unattainable by a
single model. In existing designs, LLMs communicate through text, forcing
internal representations to be transformed into output token sequences. This
process both loses rich semantic information and incurs token-by-token
generation latency. Motivated by these limitations, we ask: Can LLMs
communicate beyond text? Oracle experiments show that enriching the KV-Cache
semantics can improve response quality without increasing cache size,
supporting KV-Cache as an effective medium for inter-model communication. Thus,
we propose Cache-to-Cache (C2C), a new paradigm for direct semantic
communication between LLMs. C2C uses a neural network to project and fuse the
source model's KV-cache with that of the target model to enable direct semantic
transfer. A learnable gating mechanism selects the target layers that benefit
from cache communication. Compared with text communication, C2C utilizes the
deep, specialized semantics from both models, while avoiding explicit
intermediate text generation. Experiments show that C2C achieves 8.5-10.5%
higher average accuracy than individual models. It further outperforms the text
communication paradigm by approximately 3.0-5.0%, while delivering an average
2.0x speedup in latency. Our code is available at
https://github.com/thu-nics/C2C.

</details>


### [133] [Self-Anchor: Large Language Model Reasoning via Step-by-step Attention Alignment](https://arxiv.org/abs/2510.03223)
*Hongxiang Zhang,Yuan Tian,Tianyi Zhang*

Main category: cs.CL

TL;DR: 提出Self-Anchor方法，通过结构化推理路径并引导模型注意力，提升大语言模型在复杂推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决长推理链中关键中间步骤和原始提示被忽略的问题，从而减少推理错误。

Method: 将推理过程分解为结构化计划，并自动对齐模型注意力到最相关的推理步骤。

Result: 在六个基准上优于现有的最先进提示方法，显著缩小了非推理模型与专用推理模型之间的性能差距。

Conclusion: Self-Anchor有望使大多数大语言模型无需重训练即可胜任复杂推理任务。

Abstract: To solve complex reasoning tasks for Large Language Models (LLMs),
prompting-based methods offer a lightweight alternative to fine-tuning and
reinforcement learning. However, as reasoning chains extend, critical
intermediate steps and the original prompt will be buried in the context,
receiving insufficient attention and leading to errors. In this paper, we
propose Self-Anchor, a novel pipeline that leverages the inherent structure of
reasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories
into structured plans and automatically aligns the model's attention to the
most relevant inference steps, allowing the model to maintain focus throughout
generation. Our experiment shows that Self-Anchor outperforms SOTA prompting
methods across six benchmarks. Notably, Self-Anchor significantly reduces the
performance gap between ``non-reasoning'' models and specialized reasoning
models, with the potential to enable most LLMs to tackle complex reasoning
tasks without retraining.

</details>


### [134] [Reward Models are Metrics in a Trench Coat](https://arxiv.org/abs/2510.03231)
*Sebastian Gehrmann*

Main category: cs.CL

TL;DR: 本文探讨了奖励模型和评估指标这两个独立研究领域之间的重叠与脱节，指出它们面临相似挑战，并主张加强协作以改善模型性能。


<details>
  <summary>Details</summary>
Motivation: 奖励模型和评估指标在大型语言模型训练中各自独立发展，导致术语重复和共同问题反复出现，本文旨在推动两领域的融合以解决共性问题。

Method: 通过对比分析奖励模型与评估指标在任务中的表现，并对两个领域进行系统综述，识别可协作的研究方向。

Result: 发现评估指标在某些任务上优于奖励模型，并识别出偏好获取、避免虚假相关性与奖励攻击、校准感知的元评估等关键交叉研究方向。

Conclusion: 加强奖励模型与评估指标领域的协作有助于克服当前挑战，推动更可靠、高效的AI评估机制发展。

Abstract: The emergence of reinforcement learning in post-training of large language
models has sparked significant interest in reward models. Reward models assess
the quality of sampled model outputs to generate training signals. This task is
also performed by evaluation metrics that monitor the performance of an AI
model. We find that the two research areas are mostly separate, leading to
redundant terminology and repeated pitfalls. Common challenges include
susceptibility to spurious correlations, impact on downstream reward hacking,
methods to improve data quality, and approaches to meta-evaluation. Our
position paper argues that a closer collaboration between the fields can help
overcome these issues. To that end, we show how metrics outperform reward
models on specific tasks and provide an extensive survey of the two areas.
Grounded in this survey, we point to multiple research topics in which closer
alignment can improve reward models and metrics in areas such as preference
elicitation methods, avoidance of spurious correlations and reward hacking, and
calibration-aware meta-evaluation.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [135] [Accelerated Convolutive Transfer Function-Based Multichannel NMF Using Iterative Source Steering](https://arxiv.org/abs/2510.02382)
*Xuemai Xie,Xianrui Wang,Liyuan Zhang,Yichen Yang,Shoji Makino*

Main category: cs.SD

TL;DR: 提出了一种基于迭代源引导（ISS）的高效CTF-MNMF变体，避免了矩阵求逆，显著降低了计算复杂度，同时保持了与原方法相当或更优的分离性能。


<details>
  <summary>Details</summary>
Motivation: 原始CTF-MNMF方法因每源需矩阵求逆导致高计算成本，限制了其实际部署，本文旨在降低计算复杂度。

Method: 将无需矩阵求逆的迭代源引导（ISS）更新规则引入CTF-MNMF，用于更新分离滤波器。

Result: 实验结果表明，所提方法在显著降低计算复杂度的同时，实现了与原CTF-MNMF相当甚至更优的分离性能。

Conclusion: 所提出的ISS-based CTF-MNMF是一种高效且高性能的盲源分离方法，适合在实际中部署。

Abstract: Among numerous blind source separation (BSS) methods, convolutive transfer
function-based multichannel non-negative matrix factorization (CTF-MNMF) has
demonstrated strong performance in highly reverberant environments by modeling
multi-frame correlations of delayed source signals. However, its practical
deployment is hindered by the high computational cost associated with the
iterative projection (IP) update rule, which requires matrix inversion for each
source. To address this issue, we propose an efficient variant of CTF-MNMF that
integrates iterative source steering (ISS), a matrix inversion-free update rule
for separation filters. Experimental results show that the proposed method
achieves comparable or superior separation performance to the original
CTF-MNMF, while significantly reducing the computational complexity.

</details>


### [136] [Linear RNNs for autoregressive generation of long music samples](https://arxiv.org/abs/2510.02401)
*Konrad Szewczyk,Daniel Gallo Fernández,James Townsend*

Main category: cs.SD

TL;DR: 提出HarmonicRNN模型，利用线性RNN和上下文并行技术在长序列音频建模中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 直接自回归生成原始音频波形具有挑战性，因序列长且多时间尺度结构复杂，传统方法效果有限。

Method: 采用深度状态空间模型（线性RNN），探索不同架构设计，并使用上下文并行训练长达一分钟（1M tokens）的序列。

Result: HarmonicRNN在小规模数据集上实现了最先进的对数似然和感知指标。

Conclusion: 线性RNN在原始音频建模中具有强大潜力，适当架构和训练策略可显著提升性能。

Abstract: Directly learning to generate audio waveforms in an autoregressive manner is
a challenging task, due to the length of the raw sequences and the existence of
important structure on many different timescales. Traditional approaches based
on recurrent neural networks, as well as causal convolutions and
self-attention, have only had limited success on this task. However, recent
work has shown that deep state space models, also referred to as linear RNNs,
can be highly efficient in this context. In this work, we push the boundaries
of linear RNNs applied to raw audio modeling, investigating the effects of
different architectural choices and using context-parallelism to enable
training on sequences up to one minute (1M tokens) in length. We present a
model, HarmonicRNN, which attains state of the art log-likelihoods and
perceptual metrics on small-scale datasets.

</details>


### [137] [Latent Multi-view Learning for Robust Environmental Sound Representations](https://arxiv.org/abs/2510.02500)
*Sivan Sing,Julia Wilkins,Magdalena Fuentes,Juan Pablo Bello*

Main category: cs.SD

TL;DR: 提出了一种结合对比学习和生成方法的多视图学习框架，用于环境声音表示学习，通过编码音频潜在特征到视图特异和共用子空间，提升了声音源和传感器分类性能。


<details>
  <summary>Details</summary>
Motivation: 探索对比学习和生成方法在统一框架下的互补性，以更好地利用无标签数据进行环境声音表示学习。

Method: 构建一个多视图学习框架，将对比学习原则融入生成管道，通过对比学习引导子空间间的信息流动，同时利用重构损失保持整体信息。

Result: 在城市声音传感器网络数据集上，该方法在声音源和传感器分类任务中优于传统自监督学习技术，并展示了在不同训练配置下解耦环境声音属性的潜力。

Conclusion: 所提框架有效融合了对比与生成方法的优势，提升了环境声音表示学习的性能，并展现出对声音属性解耦的良好潜力。

Abstract: Self-supervised learning (SSL) approaches, such as contrastive and generative
methods, have advanced environmental sound representation learning using
unlabeled data. However, how these approaches can complement each other within
a unified framework remains relatively underexplored. In this work, we propose
a multi-view learning framework that integrates contrastive principles into a
generative pipeline to capture sound source and device information. Our method
encodes compressed audio latents into view-specific and view-common subspaces,
guided by two self-supervised objectives: contrastive learning for targeted
information flow between subspaces, and reconstruction for overall information
preservation. We evaluate our method on an urban sound sensor network dataset
for sound source and sensor classification, demonstrating improved downstream
performance over traditional SSL techniques. Additionally, we investigate the
model's potential to disentangle environmental sound attributes within the
structured latent space under varied training configurations.

</details>


### [138] [TART: A Comprehensive Tool for Technique-Aware Audio-to-Tab Guitar Transcription](https://arxiv.org/abs/2510.02597)
*Akshaj Gupta,Andrea Guzman,Anagha Badriprasad,Hwi Joo Park,Upasana Puranik,Robin Netzorg,Jiachen Lian,Gopala Krishna Anumanchipalli*

Main category: cs.SD

TL;DR: 提出了一种四阶段端到端的自动吉他乐谱转录 pipeline，可从音频生成包含准确指法和表现技法标签的详细吉他谱。


<details>
  <summary>Details</summary>
Motivation: 现有吉他自动转录系统无法识别表现技法（如滑音、弯音、打击音）且常错误分配弦和品，同时模型训练数据有限，泛化能力差。

Method: 构建四阶段系统：(1) 借助适配吉他数据的钢琴转录模型将音频转为MIDI音高；(2) 使用MLP分类表现技法；(3) Transformer模型分配弦和品；(4) LSTM生成最终的吉他谱。

Result: 首次实现从吉他音频端到端生成包含准确指法和表现技法标注的详细吉他谱。

Conclusion: 该框架显著提升了吉他自动转录的细节精度与实用性，推动了AMT在吉他领域的应用。

Abstract: Automatic Music Transcription (AMT) has advanced significantly for the piano,
but transcription for the guitar remains limited due to several key challenges.
Existing systems fail to detect and annotate expressive techniques (e.g.,
slides, bends, percussive hits) and incorrectly map notes to the wrong string
and fret combination in the generated tablature. Furthermore, prior models are
typically trained on small, isolated datasets, limiting their generalizability
to real-world guitar recordings. To overcome these limitations, we propose a
four-stage end-to-end pipeline that produces detailed guitar tablature directly
from audio. Our system consists of (1) Audio-to-MIDI pitch conversion through a
piano transcription model adapted to guitar datasets; (2) MLP-based expressive
technique classification; (3) Transformer-based string and fret assignment; and
(4) LSTM-based tablature generation. To the best of our knowledge, this
framework is the first to generate detailed tablature with accurate fingerings
and expressive labels from guitar audio.

</details>


### [139] [Flamed-TTS: Flow Matching Attention-Free Models for Efficient Generating and Dynamic Pacing Zero-shot Text-to-Speech](https://arxiv.org/abs/2510.02848)
*Hieu-Nghia Huynh-Nguyen,Huynh Nguyen Dang,Ngoc-Son Nguyen,Van Nguyen*

Main category: cs.SD

TL;DR: 本文提出Flamed-TTS，一种新型零样本文本到语音（TTS）框架，通过改进流匹配训练范式并结合离散与连续表示，在低计算成本、低延迟的同时实现了高保真度和丰富的时序多样性。


<details>
  <summary>Details</summary>
Motivation: 现有零样本TTS方法在合成可靠性、推理速度和计算开销方面存在挑战，且对语音自然性至关重要的时序多样性探索不足。

Method: 重新设计流匹配训练范式，融合离散和连续语音表示以建模不同语音属性，兼顾合成质量与效率。

Result: 实验表明，Flamed-TTS在可懂度、自然性、说话人相似性、声学特征保持和动态语速上优于现有最先进模型，WER低至4%，且推理延迟低、语音保真度高。

Conclusion: Flamed-TTS在实现高质量零样本TTS的同时，有效平衡了性能、速度与计算成本，推动了时序多样性在语音合成中的应用。

Abstract: Zero-shot Text-to-Speech (TTS) has recently advanced significantly, enabling
models to synthesize speech from text using short, limited-context prompts.
These prompts serve as voice exemplars, allowing the model to mimic speaker
identity, prosody, and other traits without extensive speaker-specific data.
Although recent approaches incorporating language models, diffusion, and flow
matching have proven their effectiveness in zero-shot TTS, they still encounter
challenges such as unreliable synthesis caused by token repetition or
unexpected content transfer, along with slow inference and substantial
computational overhead. Moreover, temporal diversity-crucial for enhancing the
naturalness of synthesized speech-remains largely underexplored. To address
these challenges, we propose Flamed-TTS, a novel zero-shot TTS framework that
emphasizes low computational cost, low latency, and high speech fidelity
alongside rich temporal diversity. To achieve this, we reformulate the flow
matching training paradigm and incorporate both discrete and continuous
representations corresponding to different attributes of speech. Experimental
results demonstrate that Flamed-TTS surpasses state-of-the-art models in terms
of intelligibility, naturalness, speaker similarity, acoustic characteristics
preservation, and dynamic pace. Notably, Flamed-TTS achieves the best WER of 4%
compared to the leading zero-shot TTS baselines, while maintaining low latency
in inference and high fidelity in generated speech. Code and audio samples are
available at our demo page https://flamed-tts.github.io.

</details>


### [140] [Forensic Similarity for Speech Deepfakes](https://arxiv.org/abs/2510.02864)
*Viola Negroni,Davide Salvi,Daniele Ugo Leonzio,Paolo Bestagini,Stefano Tubaro*

Main category: cs.SD

TL;DR: 本文提出了一种名为“语音深度伪造的法医相似性”（Forensic Similarity for Speech Deepfakes）的数字音频取证方法，通过深度学习模型判断两个音频片段是否具有相同的法医痕迹，无需事先知晓痕迹类型，具有强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的音频取证方法通常依赖于已知伪造痕迹的先验知识，难以应对未知或新型伪造技术。受图像领域法医相似性研究的启发，本文旨在构建一种无需先验知识、能泛化到未知音频伪造痕迹的通用检测方法。

Method: 提出一个两部分深度学习系统：一是基于语音深度伪造检测器的特征提取器，二是浅层神经网络（相似性网络）。该系统将一对音频片段映射为一个相似性得分，判断其是否含有相同法医痕迹。系统在源验证和拼接检测两个任务上进行评估。

Result: 实验表明，该方法在源验证任务中能有效识别两个样本是否来自同一生成模型，并在拼接检测中展现应用潜力。系统对多种已知和前所未见的法医痕迹均表现出良好泛化能力。

Conclusion: 所提出的法医相似性方法无需针对特定伪造痕迹进行训练，即可在数字音频取证中实现对未知伪造技术的鲁棒检测，展现出高度的灵活性和实际应用价值。

Abstract: In this paper, we introduce a digital audio forensics approach called
Forensic Similarity for Speech Deepfakes, which determines whether two audio
segments contain the same forensic traces or not. Our work is inspired by prior
work in the image domain on forensic similarity, which proved strong
generalization capabilities against unknown forensic traces, without requiring
prior knowledge of them at training time. To achieve this in the audio setting,
we propose a two-part deep-learning system composed of a feature extractor
based on a speech deepfake detector backbone and a shallow neural network,
referred to as the similarity network. This system maps pairs of audio segments
to a score indicating whether they contain the same or different forensic
traces. We evaluate the system on the emerging task of source verification,
highlighting its ability to identify whether two samples originate from the
same generative model. Additionally, we assess its applicability to splicing
detection as a complementary use case. Experiments show that the method
generalizes to a wide range of forensic traces, including previously unseen
ones, illustrating its flexibility and practical value in digital audio
forensics.

</details>


### [141] [WavInWav: Time-domain Speech Hiding via Invertible Neural Network](https://arxiv.org/abs/2510.02915)
*Wei Fan,Kejiang Chen,Xiangkun Wang,Weiming Zhang,Nenghai Yu*

Main category: cs.SD

TL;DR: 提出了一种基于流式可逆神经网络的新方法，通过引入时频损失和加密技术，显著提升了音频隐写中秘密信息的恢复质量和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有音频隐写方法在时频关系建模上存在局限，导致秘密音频恢复质量不佳，需提高可逆性和鲁棒性。

Method: 采用基于流的可逆神经网络，建立宿主音频、秘密音频和含密音频之间的直接映射，并引入时频域损失函数优化时域信号，同时结合加密技术保护隐藏数据。

Result: 在VCTK和LibriSpeech数据集上实验表明，该方法在主客观指标上均优于先前方法，并对多种噪声具有鲁棒性。

Conclusion: 所提方法有效提升了音频隐写的信息恢复质量与安全性，适用于实际的定向安全通信场景。

Abstract: Data hiding is essential for secure communication across digital media, and
recent advances in Deep Neural Networks (DNNs) provide enhanced methods for
embedding secret information effectively. However, previous audio hiding
methods often result in unsatisfactory quality when recovering secret audio,
due to their inherent limitations in the modeling of time-frequency
relationships. In this paper, we explore these limitations and introduce a new
DNN-based approach. We use a flow-based invertible neural network to establish
a direct link between stego audio, cover audio, and secret audio, enhancing the
reversibility of embedding and extracting messages. To address common issues
from time-frequency transformations that degrade secret audio quality during
recovery, we implement a time-frequency loss on the time-domain signal. This
approach not only retains the benefits of time-frequency constraints but also
enhances the reversibility of message recovery, which is vital for practical
applications. We also add an encryption technique to protect the hidden data
from unauthorized access. Experimental results on the VCTK and LibriSpeech
datasets demonstrate that our method outperforms previous approaches in terms
of subjective and objective metrics and exhibits robustness to various types of
noise, suggesting its utility in targeted secure communication scenarios.

</details>


### [142] [SALSA-V: Shortcut-Augmented Long-form Synchronized Audio from Videos](https://arxiv.org/abs/2510.02916)
*Amir Dellali,Luca A. Lanzendörfer,Florian Grötschla,Roger Wattenhofer*

Main category: cs.SD

TL;DR: SALSA-V 是一种多模态视频到音频生成模型，能够从无声视频中生成高度同步、高保真、长时音频，支持无约束长度音频生成，并可在仅8步采样中快速生成高质量音频，适用于实时应用和专业音频合成任务。


<details>
  <summary>Details</summary>
Motivation: 现有的视频到音频生成方法在音频-视觉对齐、同步性和生成效率方面存在局限，难以满足长时、高保真音频生成及实时应用需求，尤其在专业声音设计等场景中应用受限。

Method: 提出 SALSA-V 模型，采用掩码扩散目标实现音频条件生成，支持任意长度音频合成；引入快捷损失（shortcut loss）加速生成过程；训练中使用随机掩码以匹配参考音频的频谱特征。

Result: SALSA-V 在音频-视觉对齐和同步性方面显著优于现有最先进方法，定量评估和人类听觉实验均验证其优越性；仅需8步采样即可生成高质量音频，具备近实时生成能力。

Conclusion: SALSA-V 实现了高效、高质量、长时视频到音频生成，具备良好的同步性和频谱匹配能力，适用于Foley生成、声音设计等专业场景，为实际应用提供了可行方案。

Abstract: We propose SALSA-V, a multimodal video-to-audio generation model capable of
synthesizing highly synchronized, high-fidelity long-form audio from silent
video content. Our approach introduces a masked diffusion objective, enabling
audio-conditioned generation and the seamless synthesis of audio sequences of
unconstrained length. Additionally, by integrating a shortcut loss into our
training process, we achieve rapid generation of high-quality audio samples in
as few as eight sampling steps, paving the way for near-real-time applications
without requiring dedicated fine-tuning or retraining. We demonstrate that
SALSA-V significantly outperforms existing state-of-the-art methods in both
audiovisual alignment and synchronization with video content in quantitative
evaluation and a human listening study. Furthermore, our use of random masking
during training enables our model to match spectral characteristics of
reference audio samples, broadening its applicability to professional audio
synthesis tasks such as Foley generation and sound design.

</details>


### [143] [AudioToolAgent: An Agentic Framework for Audio-Language Models](https://arxiv.org/abs/2510.02995)
*Gijs Wijngaard,Elia Formisano,Michel Dumontier*

Main category: cs.SD

TL;DR: 本文提出AudioToolAgent框架，通过中心化LLM代理协调音频-语言模型作为工具，实现无需训练的多步音频理解与问答，达到多个基准上的最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型音频-语言模型（LALMs）在多步推理和工具调用方面能力不足，而当前大型语言模型（LLMs）已具备较强的推理与工具使用能力，因此需要构建一个能结合二者优势的框架。

Method: 设计AudioToolAgent框架，由一个中心LLM代理调度不同的音频处理工具（如音频问答和语音识别），通过工具选择、提问、输出验证与Monte Carlo采样分析最优配置，实现模块化协作。

Result: 在MMAU、MMAR和MMAU-Pro三个基准上分别达到74.10%、68.80%和57.96%的准确率，为当前最优结果；通过374种配置的Shapley值分析识别出高效组合。

Conclusion: AudioToolAgent通过模块化设计实现了高性能、低成本的音频理解，无需训练即可集成新工具，具有良好的可扩展性与实用性。

Abstract: Large Audio-Language Models (LALMs) perform well on audio understanding tasks
but lack multi-step reasoning and tool-calling found in recent Large Language
Models (LLMs). This paper presents AudioToolAgent, a framework that coordinates
audio-language models as tools via a central LLM agent that accesses tool
adapters for audio question answering and speech-to-text. The agent selects
tools, asks follow-up questions, and compares outputs for verification.
Experiments with MMAU, MMAR, and MMAU-Pro show state-of-the-art accuracy: up to
74.10% on MMAU, 68.80% on MMAR, and 57.96% on MMAU-Pro. Monte Carlo sampling
for shapley values across 374 configurations identifies effective agent-tool
combinations. The modular design allows integration of new tools and eliminates
the use of data and training costs. Code and reproduction materials are
available at: github.com/GLJS/AudioToolAgent

</details>

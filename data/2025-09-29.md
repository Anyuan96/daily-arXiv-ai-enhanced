<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 159]
- [cs.CL](#cs.CL) [Total: 109]
- [cs.SD](#cs.SD) [Total: 16]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Random Direct Preference Optimization for Radiography Report Generation](https://arxiv.org/abs/2509.21351)
*Valentin Samokhin,Boris Shirokikh,Mikhail Goncharov,Dmitriy Umerenkov,Maksim Bobrin,Ivan Oseledets,Dmitry Dylov,Mikhail Belyaev*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，通过随机直接偏好优化提高放射报告生成的准确性，实验结果表明可以在不增加任何训练数据的情况下提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着放射科医生工作量的增加，放射报告生成（RRG）成为一种有前景的工具，但现有方法尚未达到临床应用所需的质量。

Method: 我们提出了一种模型无关的框架，通过直接偏好优化（DPO）增强放射报告生成的准确性，采用随机对比采样构建训练对，避免了对奖励模型或人类偏好注释的需求。

Result: 我们的实验显示，在三种最新模型的基础上，随机直接偏好优化方法能够提高临床性能指标，提升幅度高达5%。

Conclusion: 我们的随机直接偏好优化方法显著提高了放射报告生成的临床性能，达到每种模型提高5%的效果，并且不需要额外的训练数据。

Abstract: Radiography Report Generation (RRG) has gained significant attention in
medical image analysis as a promising tool for alleviating the growing workload
of radiologists. However, despite numerous advancements, existing methods have
yet to achieve the quality required for deployment in real-world clinical
settings. Meanwhile, large Visual Language Models (VLMs) have demonstrated
remarkable progress in the general domain by adopting training strategies
originally designed for Large Language Models (LLMs), such as alignment
techniques. In this paper, we introduce a model-agnostic framework to enhance
RRG accuracy using Direct Preference Optimization (DPO). Our approach leverages
random contrastive sampling to construct training pairs, eliminating the need
for reward models or human preference annotations. Experiments on supplementing
three state-of-the-art models with our Random DPO show that our method improves
clinical performance metrics by up to 5%, without requiring any additional
training data.

</details>


### [2] [Improving Autism Detection with Multimodal Behavioral Analysis](https://arxiv.org/abs/2509.21352)
*William Saakyan,Matthias Norden,Lola Eversmann,Simon Kirsch,Muyu Lin,Simon Guendelman,Isabel Dziobek,Hanna Drimalla*

Main category: cs.CV

TL;DR: 针对自闭症谱系障碍的诊断复杂性，本文提出了一种多模态分析的方法，通过引入新的统计描述符提高了基于注视的分类准确率，并展示了视频筛查工具的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于自闭症谱系障碍（ASC）的诊断复杂且资源密集，已有多种计算机辅助诊断支持方法被提出，通过分析患者视频数据中的行为线索来检测自闭症。

Method: 通过多模态分析面部表情、声音韵律、头部动作、心率变异性（HRV）和注视行为，采用最新的统计描述符来量化眼睛注视角度的变异性。

Result: 将基于注视的分类准确率从64%提高到69%，最终通过晚期融合实现了74%的分类准确率。

Conclusion: 我们的研究结果突显了可扩展的视频筛查工具在自闭症评估中的潜力。

Abstract: Due to the complex and resource-intensive nature of diagnosing Autism
Spectrum Condition (ASC), several computer-aided diagnostic support methods
have been proposed to detect autism by analyzing behavioral cues in patient
video data. While these models show promising results on some datasets, they
struggle with poor gaze feature performance and lack of real-world
generalizability. To tackle these challenges, we analyze a standardized video
dataset comprising 168 participants with ASC (46% female) and 157 non-autistic
participants (46% female), making it, to our knowledge, the largest and most
balanced dataset available. We conduct a multimodal analysis of facial
expressions, voice prosody, head motion, heart rate variability (HRV), and gaze
behavior. To address the limitations of prior gaze models, we introduce novel
statistical descriptors that quantify variability in eye gaze angles, improving
gaze-based classification accuracy from 64% to 69% and aligning computational
findings with clinical research on gaze aversion in ASC. Using late fusion, we
achieve a classification accuracy of 74%, demonstrating the effectiveness of
integrating behavioral markers across multiple modalities. Our findings
highlight the potential for scalable, video-based screening tools to support
autism assessment.

</details>


### [3] [KV-Efficient VLA: A Method of Speed up Vision Language Model with RNN-Gated Chunked KV Cache](https://arxiv.org/abs/2509.21354)
*Wanshun Xu,Long Zhuang*

Main category: cs.CV

TL;DR: 提出KV-Efficient VLA模型，通过轻量内存压缩机制提高VLA模型推理效率，减少内存使用，适用于实时部署。


<details>
  <summary>Details</summary>
Motivation: VLA模型在推理过程中面临注意力计算的平方成本和键值内存的无限增长问题，限制了其在实时部署中的可扩展性。

Method: 提出了一种基于轻量级机制的KV内存压缩框架，通过选择性保留高效上下文来解决推理效率问题。

Result: 通过重新设计KV缓存并使用门控模块进行上下文过滤，KV-Efficient VLA实现了1.21倍的推理速度提升和36%的内存减少。

Conclusion: KV-Efficient VLA显著提高了推理速度和内存利用率，同时对任务成功率影响较小。

Abstract: Vision-Language-Action (VLA) models promise unified robotic perception and
control, yet their scalability is constrained by the quadratic cost of
attention and the unbounded growth of key-value (KV) memory during long-horizon
inference. While recent methods improve generalization through scaling backbone
architectures, they often neglect the inference inefficiencies critical to
real-time deployment. In this work, we present KV-Efficient VLA, a
model-agnostic memory compression framework that addresses these limitations by
introducing a lightweight, training-friendly mechanism to selectively retain
high-utility context. Our method partitions the KV cache into fixed size chunks
and employs a recurrent gating module to summarize and filter historical
context according to learned utility scores. This design preserves recent
fine-grained detail while aggressively pruning stale, low-relevance memory, all
while maintaining causality. Theoretically, KV-Efficient VLA yields up to 1.21x
inference speedup and 36% KV memory reduction, with minimal impact on task
success. Our method integrates seamlessly into existing autoregressive and
hybrid VLA stacks, enabling scalable inference without modifying training
pipelines or downstream control logic.

</details>


### [4] [Phrase-grounded Fact-checking for Automatically Generated Chest X-ray Reports](https://arxiv.org/abs/2509.21356)
*Razi Mahmood,Diego Machado-Reyes,Joy Wu,Parisa Kaviani,Ken C. L. Wong,Niharika D'Souza,Mannudeep Kalra,Ge Wang,Pingkun Yan,Tanveer Syeda-Mahmood*

Main category: cs.CV

TL;DR: 本文提出了一种新的事实检查模型，旨在检测和修正自动生成的胸部放射报告中的错误，表现出优越的准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型生成的放射报告存在事实错误和幻觉，制约了其临床转化，急需一种有效的错误检测模型。

Method: 通过对真实报告进行扰动，构建一个大规模合成数据集，然后使用新的多标签跨模态对比回归网络进行训练。

Result: 该模型在多种X光数据集上实现了极高的准确性和定位能力，并显示出对多种报告生成器的错误检测的有效性，达到与真实基准验证的0.997的一致性相关系数。

Conclusion: 提出了一种新型的短语基础事实检查模型，以检测自动生成的胸部放射报告中的错误和其指示位置，表现出较高的准确性和鲁棒性，增强了模型在临床应用中的实用性。

Abstract: With the emergence of large-scale vision language models (VLM), it is now
possible to produce realistic-looking radiology reports for chest X-ray images.
However, their clinical translation has been hampered by the factual errors and
hallucinations in the produced descriptions during inference. In this paper, we
present a novel phrase-grounded fact-checking model (FC model) that detects
errors in findings and their indicated locations in automatically generated
chest radiology reports.
  Specifically, we simulate the errors in reports through a large synthetic
dataset derived by perturbing findings and their locations in ground truth
reports to form real and fake findings-location pairs with images. A new
multi-label cross-modal contrastive regression network is then trained on this
dataset. We present results demonstrating the robustness of our method in terms
of accuracy of finding veracity prediction and localization on multiple X-ray
datasets. We also show its effectiveness for error detection in reports of SOTA
report generators on multiple datasets achieving a concordance correlation
coefficient of 0.997 with ground truth-based verification, thus pointing to its
utility during clinical inference in radiology workflows.

</details>


### [5] [MDF-MLLM: Deep Fusion Through Cross-Modal Feature Alignment for Contextually Aware Fundoscopic Image Classification](https://arxiv.org/abs/2509.21358)
*Jason Jordan,Mohammadreza Akbari Lor,Peter Koulen,Mei-Ling Shyu,Shu-Ching Chen*

Main category: cs.CV

TL;DR: 本研究通过多模态深度学习架构MDF-MLLM，显著提升了视网膜图像分类的准确率，为临床应用奠定基础。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大语言模型难以捕捉对视网膜疾病诊断至关重要的低层次空间细节，因此需要开发出更有效的模型以提高分类准确性。

Method: 采用了一种新的多模态深度学习架构，整合细粒度图像特征与全局文本上下文，利用LLaMA 3.2 11B MLLM和U-Net实现了跨注意力融合。

Result: MDF-MLLM在双重疾病分类任务中达到了94%的准确率，相较于基线模型提高了56%，召回率和F1得分分别提高了67%和35%。

Conclusion: MDF-MLLM模型展示了其在视网膜图像分类中的优越性能，并为临床决策支持系统的实际应用提供了潜在价值。

Abstract: This study aimed to enhance disease classification accuracy from retinal
fundus images by integrating fine-grained image features and global textual
context using a novel multimodal deep learning architecture. Existing
multimodal large language models (MLLMs) often struggle to capture low-level
spatial details critical for diagnosing retinal diseases such as glaucoma,
diabetic retinopathy, and retinitis pigmentosa. This model development and
validation study was conducted on 1,305 fundus image-text pairs compiled from
three public datasets (FIVES, HRF, and StoneRounds), covering acquired and
inherited retinal diseases, and evaluated using classification accuracy and
F1-score. The MDF-MLLM integrates skip features from four U-Net encoder layers
into cross-attention blocks within a LLaMA 3.2 11B MLLM. Vision features are
patch-wise projected and fused using scaled cross-attention and FiLM-based
U-Net modulation. Baseline MLLM achieved 60% accuracy on the dual-type disease
classification task. MDF-MLLM, with both U-Net and MLLM components fully
fine-tuned during training, achieved a significantly higher accuracy of 94%,
representing a 56% improvement. Recall and F1-scores improved by as much as 67%
and 35% over baseline, respectively. Ablation studies confirmed that the
multi-depth fusion approach contributed to substantial gains in spatial
reasoning and classification, particularly for inherited diseases with rich
clinical text. MDF-MLLM presents a generalizable, interpretable, and modular
framework for fundus image classification, outperforming traditional MLLM
baselines through multi-scale feature fusion. The architecture holds promise
for real-world deployment in clinical decision support systems. Future work
will explore synchronized training techniques, a larger pool of diseases for
more generalizability, and extending the model for segmentation tasks.

</details>


### [6] [Multimodal Prompt Decoupling Attack on the Safety Filters in Text-to-Image Models](https://arxiv.org/abs/2509.21360)
*Xingkai Peng,Jun Jiang,Meng Tong,Shuai Li,Weiming Zhang,Nenghai Yu,Kejiang Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的攻击方法MPDA，该方法通过将不安全提示分解为伪安全和有害提示，成功绕过文本过滤器，实现生成NSFW内容。


<details>
  <summary>Details</summary>
Motivation: 现有的越狱方法主要依赖文本提示，未探索基于图像输入的潜在漏洞，因此需要一种新方法来有效 bypass 安全过滤器。

Method: MPDA方法包含三个核心步骤：首先利用大语言模型(LLM)解耦不安全提示；其次将其重写为自然对抗提示；最后通过视觉语言模型生成图像标题，确保生成图像和原始提示之间的语义一致性。

Result: 通过多模态提示解耦攻击(MPDA)能在文本和图像输入的交互中产生更高效的NSFW内容生成，从而提高了攻击的成功率。

Conclusion: 提出的多模态提示解耦攻击(MPDA)能有效绕过现有的安全过滤器，为生成NSFW内容提供了一种新方法。

Abstract: Text-to-image (T2I) models have been widely applied in generating
high-fidelity images across various domains. However, these models may also be
abused to produce Not-Safe-for-Work (NSFW) content via jailbreak attacks.
Existing jailbreak methods primarily manipulate the textual prompt, leaving
potential vulnerabilities in image-based inputs largely unexplored. Moreover,
text-based methods face challenges in bypassing the model's safety filters. In
response to these limitations, we propose the Multimodal Prompt Decoupling
Attack (MPDA), which utilizes image modality to separate the harmful semantic
components of the original unsafe prompt. MPDA follows three core steps:
firstly, a large language model (LLM) decouples unsafe prompts into pseudo-safe
prompts and harmful prompts. The former are seemingly harmless sub-prompts that
can bypass filters, while the latter are sub-prompts with unsafe semantics that
trigger filters. Subsequently, the LLM rewrites the harmful prompts into
natural adversarial prompts to bypass safety filters, which guide the T2I model
to modify the base image into an NSFW output. Finally, to ensure semantic
consistency between the generated NSFW images and the original unsafe prompts,
the visual language model generates image captions, providing a new pathway to
guide the LLM in iterative rewriting and refining the generated content.

</details>


### [7] [A Mutual Learning Method for Salient Object Detection with intertwined Multi-Supervision--Revised](https://arxiv.org/abs/2509.21363)
*Runmin Wu,Mengyang Feng,Wenlong Guan,Dong Wang,Huchuan Lu,Errui Ding*

Main category: cs.CV

TL;DR: 提出了一种新方法，通过互学习模块结合多种检测任务，改善了显著性物体和边缘检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决显著性检测中的不完整预测和边界不准确性的问题。

Method: 通过互学习模块（MLM）训练显著性检测网络，同时利用显著性目标检测、前景轮廓检测和边缘检测的监督。

Result: 在七个具有挑战性的数据集上进行了广泛实验，证明了该方法显著提高了性能。

Conclusion: 该方法在显著性目标检测和边缘检测中均达到了最先进的结果。

Abstract: Though deep learning techniques have made great progress in salient object
detection recently, the predicted saliency maps still suffer from incomplete
predictions due to the internal complexity of objects and inaccurate boundaries
caused by strides in convolution and pooling operations. To alleviate these
issues, we propose to train saliency detection networks by exploiting the
supervision from not only salient object detection, but also foreground contour
detection and edge detection. First, we leverage salient object detection and
foreground contour detection tasks in an intertwined manner to generate
saliency maps with uniform highlight. Second, the foreground contour and edge
detection tasks guide each other simultaneously, thereby leading to precise
foreground contour prediction and reducing the local noises for edge
prediction. In addition, we develop a novel mutual learning module (MLM) which
serves as the building block of our method. Each MLM consists of multiple
network branches trained in a mutual learning manner, which improves the
performance by a large margin. Extensive experiments on seven challenging
datasets demonstrate that the proposed method has delivered state-of-the-art
results in both salient object detection and edge detection.

</details>


### [8] [MAJORScore: A Novel Metric for Evaluating Multimodal Relevance via Joint Representation](https://arxiv.org/abs/2509.21365)
*Zhicheng Du,Qingyang Shi,Jiasheng Lu,Yingshan Liang,Xinyu Zhang,Yiran Wang,Peiwu Qin*

Main category: cs.CV

TL;DR: 本文提出了一种名为 MAJORScore 的新评估指标，能够有效评估 N 种（N>=3）模态之间的相关性，超越传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的评估指标主要局限于两个模态之间的关联性，无法满足多模态相似性评估的需求。

Method: 通过多模态联合表示的方法，整合多种模态于同一潜在空间，以实现对相关性的准确评估。

Result: MAJORScore 在一致模态下提升了 26.03%-64.29%，而在不一致模态下降低了 13.28%-20.54%。

Conclusion: MAJORScore 是一种新颖的评估指标，能够有效评估多种模态之间的相关性，并在大规模多模态数据集和模型性能评估中表现出色。

Abstract: The multimodal relevance metric is usually borrowed from the embedding
ability of pretrained contrastive learning models for bimodal data, which is
used to evaluate the correlation between cross-modal data (e.g., CLIP).
However, the commonly used evaluation metrics are only suitable for the
associated analysis between two modalities, which greatly limits the evaluation
of multimodal similarity. Herein, we propose MAJORScore, a brand-new evaluation
metric for the relevance of multiple modalities (N modalities, N>=3) via
multimodal joint representation for the first time. The ability of multimodal
joint representation to integrate multiple modalities into the same latent
space can accurately represent different modalities at one scale, providing
support for fair relevance scoring. Extensive experiments have shown that
MAJORScore increases by 26.03%-64.29% for consistent modality and decreases by
13.28%-20.54% for inconsistence compared to existing methods. MAJORScore serves
as a more reliable metric for evaluating similarity on large-scale multimodal
datasets and multimodal model performance evaluation.

</details>


### [9] [Safety Assessment of Scaffolding on Construction Site using AI](https://arxiv.org/abs/2509.21368)
*Sameer Prabhu,Amit Patwardhan,Ramin Karim*

Main category: cs.CV

TL;DR: 本文探讨了通过云端AI平台优化脚手架检查的准确性，以提升施工安全，减少人工检查时间。


<details>
  <summary>Details</summary>
Motivation: 施工行业的安全评估至关重要，但目前的视觉检查方法费时且易受人为错误影响，可能导致不安全的条件。

Method: 开发了一个基于云的AI平台，用于处理和分析脚手架结构的点云数据，通过比较认证参考数据和最新的点云数据来检测结构修改。

Result: 研究表明，该系统能够自动监控脚手架，减少人工检查所需的时间和精力，同时提高安全性。

Conclusion: 提出的云端AI平台通过处理和分析脚手架的点云数据，提高了脚手架检查的准确性，从而增强了施工现场的安全性。

Abstract: In the construction industry, safety assessment is vital to ensure both the
reliability of assets and the safety of workers. Scaffolding, a key structural
support asset requires regular inspection to detect and identify alterations
from the design rules that may compromise the integrity and stability. At
present, inspections are primarily visual and are conducted by site manager or
accredited personnel to identify deviations. However, visual inspection is
time-intensive and can be susceptible to human errors, which can lead to unsafe
conditions. This paper explores the use of Artificial Intelligence (AI) and
digitization to enhance the accuracy of scaffolding inspection and contribute
to the safety improvement. A cloud-based AI platform is developed to process
and analyse the point cloud data of scaffolding structure. The proposed system
detects structural modifications through comparison and evaluation of certified
reference data with the recent point cloud data. This approach may enable
automated monitoring of scaffolding, reducing the time and effort required for
manual inspections while enhancing the safety on a construction site.

</details>


### [10] [Automated Prompt Generation for Creative and Counterfactual Text-to-image Synthesis](https://arxiv.org/abs/2509.21375)
*Aleksa Jelaca,Ying Jiao,Chang Tian,Marie-Francine Moens*

Main category: cs.CV

TL;DR: 本文提出了一种新框架，通过自动化提示重写和图像评估，提升文本到图像生成中的反事实控制能力，尤其在反事实大小方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 反事实控制能力是文本到图像生成中的一项关键挑战，尤其是当需要创建与常识模式相悖的图像时。

Method: 研究构建了一个包含图像评估器、监督提示重写器和DPO训练的排序器的框架，以适应基本提示并生成改进的提示。

Result: 方法超越了当前的先进基线和ChatGPT-4o，构建了第一个反事实大小的文本-图像数据集，并显著提升了图像评估器性能，达到114%的改进。

Conclusion: 本研究提出了一种自动化的提示工程框架，有效提升了对反事实图像生成的控制能力，特别是在反事实大小的生成上表现优异，并为未来研究奠定了基础。

Abstract: Text-to-image generation has advanced rapidly with large-scale multimodal
training, yet fine-grained controllability remains a critical challenge.
Counterfactual controllability, defined as the capacity to deliberately
generate images that contradict common-sense patterns, remains a major
challenge but plays a crucial role in enabling creativity and exploratory
applications. In this work, we address this gap with a focus on counterfactual
size (e.g., generating a tiny walrus beside a giant button) and propose an
automatic prompt engineering framework that adapts base prompts into revised
prompts for counterfactual images. The framework comprises three components: an
image evaluator that guides dataset construction by identifying successful
image generations, a supervised prompt rewriter that produces revised prompts,
and a DPO-trained ranker that selects the optimal revised prompt. We construct
the first counterfactual size text-image dataset and enhance the image
evaluator by extending Grounded SAM with refinements, achieving a 114 percent
improvement over its backbone. Experiments demonstrate that our method
outperforms state-of-the-art baselines and ChatGPT-4o, establishing a
foundation for future research on counterfactual controllability.

</details>


### [11] [In silico Deep Learning Protocols for Label-Free Super-Resolution Microscopy: A Comparative Study of Network Architectures and SNR Dependence](https://arxiv.org/abs/2509.21376)
*Shiraz S Kaderuppan,Jonathan Mar,Andrew Irvine,Anurag Sharma,Muhammad Ramadan Saifuddin,Wai Leong Eugene Wong,Wai Lok Woo*

Main category: cs.CV

TL;DR: 本研究探索了利用深度神经网络提高非荧光超分辨光学显微镜的性能，结果显示O-Net和Theta-Net在不同信噪比下互补，为显微镜技术提供了经济性解决方案。


<details>
  <summary>Details</summary>
Motivation: 光学显微镜的侧面分辨率有限，通常在200nm左右，本研究旨在探索一种经济的超分辨光学显微镜方法。

Method: 评估两种深度神经网络架构（O-Net和Theta-Net）在解析纳米级特征方面的表现，通过设计的测试目标进行验证。

Result: O-Net和Theta-Net在超分辨成像上表现良好，但在不同信噪比下表现出不同的优势， O-Net在高信噪比下表现更好，而Theta-Net则在低信噪比下更具优势。

Conclusion: 本研究表明，在深度神经网络模型应用于非荧光光学纳米显微镜时，O-Net和Theta-Net模型在图像超分辨率表现上是互补的，受图像信噪比影响显著。

Abstract: The field of optical microscopy spans across numerous industries and research
domains, ranging from education to healthcare, quality inspection and analysis.
Nonetheless, a key limitation often cited by optical microscopists refers to
the limit of its lateral resolution (typically defined as ~200nm), with
potential circumventions involving either costly external modules (e.g.
confocal scan heads, etc) and/or specialized techniques [e.g. super-resolution
(SR) fluorescent microscopy]. Addressing these challenges in a normal
(non-specialist) context thus remains an aspect outside the scope of most
microscope users & facilities. This study thus seeks to evaluate an alternative
& economical approach to achieving SR optical microscopy, involving
non-fluorescent phase-modulated microscopical modalities such as Zernike phase
contrast (PCM) and differential interference contrast (DIC) microscopy. Two in
silico deep neural network (DNN) architectures which we developed previously
(termed O-Net and Theta-Net) are assessed on their abilities to resolve a
custom-fabricated test target containing nanoscale features calibrated via
atomic force microscopy (AFM). The results of our study demonstrate that
although both O-Net and Theta-Net seemingly performed well when super-resolving
these images, they were complementary (rather than competing) approaches to be
considered for image SR, particularly under different image signal-to-noise
ratios (SNRs). High image SNRs favoured the application of O-Net models, while
low SNRs inclined preferentially towards Theta-Net models. These findings
demonstrate the importance of model architectures (in conjunction with the
source image SNR) on model performance and the SR quality of the generated
images where DNN models are utilized for non-fluorescent optical nanoscopy,
even where the same training dataset & number of epochs are being used.

</details>


### [12] [Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation](https://arxiv.org/abs/2509.21377)
*Yinfeng Yu,Hailong Zhang,Meiling Zhu*

Main category: cs.CV

TL;DR: 本论文提出DMTF-AVN，通过动态融合视觉和听觉信息，在音频视觉导航中实现了最优秀的表现。


<details>
  <summary>Details</summary>
Motivation: 在多模态导航中有效利用视觉和听觉信号的深层感知背景。

Method: 采用多目标架构和改进的Transformer机制，过滤和融合跨模态信息。

Result: DMTF-AVN在Replica和Matterport3D数据集上实现了最先进的性能，在成功率、路径效率和场景适应性方面超越了现有方法。

Conclusion: DMTF-AVN模型在音频视觉导航中表现出色，具有强大的可扩展性和通用性。

Abstract: Audiovisual embodied navigation enables robots to locate audio sources by
dynamically integrating visual observations from onboard sensors with the
auditory signals emitted by the target. The core challenge lies in effectively
leveraging multimodal cues to guide navigation. While prior works have explored
basic fusion of visual and audio data, they often overlook deeper perceptual
context. To address this, we propose the Dynamic Multi-Target Fusion for
Efficient Audio-Visual Navigation (DMTF-AVN). Our approach uses a multi-target
architecture coupled with a refined Transformer mechanism to filter and
selectively fuse cross-modal information. Extensive experiments on the Replica
and Matterport3D datasets demonstrate that DMTF-AVN achieves state-of-the-art
performance, outperforming existing methods in success rate (SR), path
efficiency (SPL), and scene adaptation (SNA). Furthermore, the model exhibits
strong scalability and generalizability, paving the way for advanced multimodal
fusion strategies in robotic navigation. The code and videos are available at
  https://github.com/zzzmmm-svg/DMTF.

</details>


### [13] [SAEmnesia: Erasing Concepts in Diffusion Models with Sparse Autoencoders](https://arxiv.org/abs/2509.21379)
*Enrico Cassano,Riccardo Renzulli,Marco Nurisso,Mirko Zaffaroni,Alan Perotti,Marco Grangetto*

Main category: cs.CV

TL;DR: SAEmnesia是一种新方法，通过提高神经元与概念的关联性，显著改善文本到图像扩散模型的概念反学习效果。


<details>
  <summary>Details</summary>
Motivation: 有效地在文本到图像扩散模型中实现概念反学习，需要精确定位模型潜在空间中的概念表示。

Method: SAEmnesia是一种受监督的稀疏自编码器训练方法，通过系统的概念标签促进一对一的概念-神经元映射。

Result: SAEmnesia在UnlearnCanvas基准上提高了9.22%的准确率，并在9个对象移除的反学习任务中提高了28.4%的反学习准确性。

Conclusion: SAEmnesia在UnlearnCanvas基准上相较于现有技术有显著的提升，并在序列反学习任务中表现出更好的扩展性。

Abstract: Effective concept unlearning in text-to-image diffusion models requires
precise localization of concept representations within the model's latent
space. While sparse autoencoders successfully reduce neuron polysemanticity
(i.e., multiple concepts per neuron) compared to the original network,
individual concept representations can still be distributed across multiple
latent features, requiring extensive search procedures for concept unlearning.
We introduce SAEmnesia, a supervised sparse autoencoder training method that
promotes one-to-one concept-neuron mappings through systematic concept
labeling, mitigating feature splitting and promoting feature centralization.
Our approach learns specialized neurons with significantly stronger concept
associations compared to unsupervised baselines. The only computational
overhead introduced by SAEmnesia is limited to cross-entropy computation during
training. At inference time, this interpretable representation reduces
hyperparameter search by 96.67% with respect to current approaches. On the
UnlearnCanvas benchmark, SAEmnesia achieves a 9.22% improvement over the
state-of-the-art. In sequential unlearning tasks, we demonstrate superior
scalability with a 28.4% improvement in unlearning accuracy for 9-object
removal.

</details>


### [14] [Coreset selection based on Intra-class diversity](https://arxiv.org/abs/2509.21380)
*Imran Ashraf,Mukhtar Ullah,Muhammad Faisal Nadeem,Muhammad Nouman Noor*

Main category: cs.CV

TL;DR: 本研究提出了一种智能轻量级的核心集选择机制，通过提取类内多样性来改善深度学习模型在生物医学成像分类中的表现，优于传统随机采样方法。


<details>
  <summary>Details</summary>
Motivation: 随着数据集规模的增长，选择代表性数据子集进行训练和超参数搜索的研究成为热点问题。

Method: 通过提取类内多样性，形成每类的聚类，用于最终的样本选择。

Result: 在广泛的分类实验中，所提出的机制在多个性能指标上都超过了随机采样方法。

Conclusion: 提出的方法在各项性能指标上优于随机采样方法，验证了其有效性。

Abstract: Deep Learning models have transformed various domains, including the
healthcare sector, particularly biomedical image classification by learning
intricate features and enabling accurate diagnostics pertaining to complex
diseases. Recent studies have adopted two different approaches to train DL
models: training from scratch and transfer learning. Both approaches demand
substantial computational time and resources due to the involvement of massive
datasets in model training. These computational demands are further increased
due to the design-space exploration required for selecting optimal
hyperparameters, which typically necessitates several training rounds. With the
growing sizes of datasets, exploring solutions to this problem has recently
gained the research community's attention. A plausible solution is to select a
subset of the dataset for training and hyperparameter search. This subset,
referred to as the corset, must be a representative set of the original
dataset. A straightforward approach to selecting the coreset could be employing
random sampling, albeit at the cost of compromising the representativeness of
the original dataset. A critical limitation of random sampling is the bias
towards the dominant classes in an imbalanced dataset. Even if the dataset has
inter-class balance, this random sampling will not capture intra-class
diversity. This study addresses this issue by introducing an intelligent,
lightweight mechanism for coreset selection. Specifically, it proposes a method
to extract intra-class diversity, forming per-class clusters that are utilized
for the final sampling. We demonstrate the efficacy of the proposed methodology
by conducting extensive classification experiments on a well-known biomedical
imaging dataset. Results demonstrate that the proposed scheme outperforms the
random sampling approach on several performance metrics for uniform conditions.

</details>


### [15] [The LongiMam model for improved breast cancer risk prediction using longitudinal mammograms](https://arxiv.org/abs/2509.21383)
*Manel Rakez,Thomas Louis,Julien Guillaumin,Foucauld Chamming's,Pierre Fillard,Brice Amadeo,Virginie Rondeau*

Main category: cs.CV

TL;DR: LongiMam模型通过整合多次乳腺X光检查数据来提高乳腺癌预测能力，支持在筛查程序中优化风险分层，已公开为开源软件。


<details>
  <summary>Details</summary>
Motivation: 开发一个强健的乳腺癌筛查模型，该模型可以利用纵向成像数据，克服现有深度学习模型在现实世界中面临的不平衡结果分布和异质性随访的问题。

Method: LongiMam是一个端到端的深度学习模型，结合卷积神经网络和递归神经网络，旨在捕捉预测乳腺癌的空间和时间模式。

Result: LongiMam在包括多个场景（先前考试数量和组成变化）的一系列实验中，始终提高了预测准确率，特别是在观察到乳腺密度变化的女性中表现最好，且在多个关键风险群体中均显示出有效性。

Conclusion: LongiMam模型通过整合当前和最多四张先前的乳腺X光检查，显著提高了乳腺癌预测效果，支持在筛查项目中使用重复的乳腺X光检查以优化风险分层。

Abstract: Risk-adapted breast cancer screening requires robust models that leverage
longitudinal imaging data. Most current deep learning models use single or
limited prior mammograms and lack adaptation for real-world settings marked by
imbalanced outcome distribution and heterogeneous follow-up. We developed
LongiMam, an end-to-end deep learning model that integrates both current and up
to four prior mammograms. LongiMam combines a convolutional and a recurrent
neural network to capture spatial and temporal patterns predictive of breast
cancer. The model was trained and evaluated using a large, population-based
screening dataset with disproportionate case-to-control ratio typical of
clinical screening. Across several scenarios that varied in the number and
composition of prior exams, LongiMam consistently improved prediction when
prior mammograms were included. The addition of prior and current visits
outperformed single-visit models, while priors alone performed less well,
highlighting the importance of combining historical and recent information.
Subgroup analyses confirmed the model's efficacy across key risk groups,
including women with dense breasts and those aged 55 years or older. Moreover,
the model performed best in women with observed changes in mammographic density
over time. These findings demonstrate that longitudinal modeling enhances
breast cancer prediction and support the use of repeated mammograms to refine
risk stratification in screening programs. LongiMam is publicly available as
open-source software.

</details>


### [16] [Assessing the Alignment of Popular CNNs to the Brain for Valence Appraisal](https://arxiv.org/abs/2509.21384)
*Laurent Mertens,Elahe' Yargholi,Laura Van Hove,Hans Op de Beeck,Jan Van den Stock,Joost Vennekens*

Main category: cs.CV

TL;DR: 本文探讨CNN模型在社会认知领域的表现，发现其在处理复杂情境时能力不足，并提出新的分析框架Object2Brain。


<details>
  <summary>Details</summary>
Motivation: 研究CNN模型与人脑之间的对应关系，尤其是在社会认知这一复杂脑过程中的体现。

Method: 通过对流行CNN架构与人类行为及fMRI数据的相关性分析，以评估其在图像情感评估中的对齐程度。并引入Object2Brain框架，结合GradCAM和物体检测进行深入分析。

Result: 尽管不同CNN架构在相关性趋势上相似，但在对不同物体类别的敏感性上显示出差异。

Conclusion: CNN模型在处理复杂的社会认知任务时表现不佳，未能有效反映人脑的高阶处理能力。

Abstract: Convolutional Neural Networks (CNNs) are a popular type of computer model
that have proven their worth in many computer vision tasks. Moreover, they form
an interesting study object for the field of psychology, with shown
correspondences between the workings of CNNs and the human brain. However,
these correspondences have so far mostly been studied in the context of general
visual perception. In contrast, this paper explores to what extent this
correspondence also holds for a more complex brain process, namely social
cognition. To this end, we assess the alignment between popular CNN
architectures and both human behavioral and fMRI data for image valence
appraisal through a correlation analysis. We show that for this task CNNs
struggle to go beyond simple visual processing, and do not seem to reflect
higher-order brain processing. Furthermore, we present Object2Brain, a novel
framework that combines GradCAM and object detection at the CNN-filter level
with the aforementioned correlation analysis to study the influence of
different object classes on the CNN-to-human correlations. Despite similar
correlation trends, different CNN architectures are shown to display different
object class sensitivities.

</details>


### [17] [Debugging Concept Bottleneck Models through Removal and Retraining](https://arxiv.org/abs/2509.21385)
*Eric Enouen,Sainyam Galhotra*

Main category: cs.CV

TL;DR: CBMs通过可解释的概念预测任务标签，但在专家干预时可能存在系统性不一致。我们提出CBDebug框架，通过移除不良概念并重训练模型，显著提高了模型表现。


<details>
  <summary>Details</summary>
Motivation: 解决CBM与专家推理之间的系统性不一致，特别是在模型从偏见数据中学习快捷方式的情况下。

Method: 提出了一种可解释的调试框架，包括概念移除和重训练两个步骤，使用CBDebug将用户反馈转化为样本级辅助标签。

Result: 通过真实和自动化专家反馈评估，CBDebug在处理已知虚假相关性的多种CBM架构上表现优越。

Conclusion: CBDebug在多个CBM架构及基准上显著优于以往的重训练方法，提高了模型对不良概念的抗干扰能力。

Abstract: Concept Bottleneck Models (CBMs) use a set of human-interpretable concepts to
predict the final task label, enabling domain experts to not only validate the
CBM's predictions, but also intervene on incorrect concepts at test time.
However, these interventions fail to address systemic misalignment between the
CBM and the expert's reasoning, such as when the model learns shortcuts from
biased data. To address this, we present a general interpretable debugging
framework for CBMs that follows a two-step process of Removal and Retraining.
In the Removal step, experts use concept explanations to identify and remove
any undesired concepts. In the Retraining step, we introduce CBDebug, a novel
method that leverages the interpretability of CBMs as a bridge for converting
concept-level user feedback into sample-level auxiliary labels. These labels
are then used to apply supervised bias mitigation and targeted augmentation,
reducing the model's reliance on undesired concepts. We evaluate our framework
with both real and automated expert feedback, and find that CBDebug
significantly outperforms prior retraining methods across multiple CBM
architectures (PIP-Net, Post-hoc CBM) and benchmarks with known spurious
correlations.

</details>


### [18] [WAVE: Learning Unified & Versatile Audio-Visual Embeddings with Multimodal LLM](https://arxiv.org/abs/2509.21990)
*Changli Tang,Qinfan Xiao,Ke Mei,Tianyi Wang,Fengyun Rao,Chao Zhang*

Main category: cs.CV

TL;DR: WAVE是首个统一音频、视频和文本的多模态嵌入模型，具有出色的跨模态检索和多模态问答能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在动态模态（如音频和视频）应用上尚未充分研究。

Method: 采用新颖的层次特征融合策略和联合多模态、多任务训练方法。

Result: WAVE在MMEB-v2视频基准上设立了新的最先进的性能，并在音频和视频到音频的检索上取得了优越的结果。

Conclusion: WAVE是一种创新的多模态嵌入模型，通过联合训练实现了文本、音频和视频的统一表示，展现了在跨模态检索和多模态问答中的优越性能。

Abstract: While embeddings from multimodal large language models (LLMs) excel as
general-purpose representations, their application to dynamic modalities like
audio and video remains underexplored. We introduce WAVE (\textbf{u}nified \&
\textbf{v}ersatile \textbf{a}udio-\textbf{v}isual \textbf{e}mbeddings), the
first LLM-based embedding that creates a unified representation space for text,
audio, and video modalities. WAVE employs a novel hierarchical feature fusion
strategy and a joint multi-modal, multi-task training approach to enable two
key capabilities: any-to-any cross-modal retrieval and the generation of
prompt-aware embeddings tailored to user instructions. Experimentally, WAVE
sets a new state-of-the-art on the MMEB-v2 video benchmark and achieves
superior results in audio and video-to-audio retrieval. Its prompt-aware nature
also yields remarkable performance in multimodal question answering,
significantly outperforming existing embedding models. Ablation studies
validate our joint training strategy, demonstrating improved performance across
all modalities. With a newly introduced benchmark for versatile audio-visual
learning, WAVE opens up broad possibilities for cross-modal, any-to-any
applications. Our code, checkpoints, and data will be released.

</details>


### [19] [ShipwreckFinder: A QGIS Tool for Shipwreck Detection in Multibeam Sonar Data](https://arxiv.org/abs/2509.21386)
*Anja Sheppard,Tyler Smithline,Andrew Scheffer,David Smith,Advaith V. Sethuraman,Ryan Bird,Sabrina Lin,Katherine A. Skinner*

Main category: cs.CV

TL;DR: 本文介绍了ShipwreckFinder，一个开源的QGIS插件，利用深度学习从多波束声纳数据中自动检测沉船，显著提高了检测效率。


<details>
  <summary>Details</summary>
Motivation: 沉船是海事历史的重要标志，手动检查水深数据既费时又需要专家分析，因此需要一种更自动化的方法来进行沉船检测。

Method: 本研究开发了一个开源QGIS插件，通过深度学习对多波束声纳数据进行沉船检测。

Result: 通过我们的工具，用户能够自动预处理水深数据，进行深度学习推理，生成像素级分割掩码或预测的沉船的边界框，且在性能上优于其他方法。

Conclusion: 我们的开源工具在检测船只沉没物方面表现优越，提供了更高效的解决方案。

Abstract: In this paper, we introduce ShipwreckFinder, an open-source QGIS plugin that
detects shipwrecks from multibeam sonar data. Shipwrecks are an important
historical marker of maritime history, and can be discovered through manual
inspection of bathymetric data. However, this is a time-consuming process and
often requires expert analysis. Our proposed tool allows users to automatically
preprocess bathymetry data, perform deep learning inference, threshold model
outputs, and produce either pixel-wise segmentation masks or bounding boxes of
predicted shipwrecks. The backbone of this open-source tool is a deep learning
model, which is trained on a variety of shipwreck data from the Great Lakes and
the coasts of Ireland. Additionally, we employ synthetic data generation in
order to increase the size and diversity of our dataset. We demonstrate
superior segmentation performance with our open-source tool and training
pipeline as compared to a deep learning-based ArcGIS toolkit and a more
classical inverse sinkhole detection method. The open-source tool can be found
at https://github.com/umfieldrobotics/ShipwreckFinderQGISPlugin.

</details>


### [20] [High-Quality Sound Separation Across Diverse Categories via Visually-Guided Generative Modeling](https://arxiv.org/abs/2509.22063)
*Chao Huang,Susan Liang,Yapeng Tian,Anurag Kumar,Chenliang Xu*

Main category: cs.CV

TL;DR: DAVIS是一种创新的扩散模型基础音频-视觉分离框架，显著提高了声音分离质量，克服了传统方法的限制。


<details>
  <summary>Details</summary>
Motivation: 现有方法在音频分离中局限于掩膜回归，难以捕捉复杂的数据分布，影响了不同类别声音的高质量分离。

Method: 提出了一个基于扩散模型的音频-视觉分离框架，结合了去噪扩散概率模型和流匹配，集成在一个专门的分离U-Net架构中。

Result: DAVIS的两个变体在标准AVE和MUSIC数据集上的表现超越了领先方法，展示了其在音频-视觉源分离任务中的有效性。

Conclusion: DAVIS框架在音频-视觉源分离任务中，利用生成模型显著提升了音频分离质量。

Abstract: We propose DAVIS, a Diffusion-based Audio-VIsual Separation framework that
solves the audio-visual sound source separation task through generative
learning. Existing methods typically frame sound separation as a mask-based
regression problem, achieving significant progress. However, they face
limitations in capturing the complex data distribution required for
high-quality separation of sounds from diverse categories. In contrast, DAVIS
circumvents these issues by leveraging potent generative modeling paradigms,
specifically Denoising Diffusion Probabilistic Models (DDPM) and the more
recent Flow Matching (FM), integrated within a specialized Separation U-Net
architecture. Our framework operates by synthesizing the desired separated
sound spectrograms directly from a noise distribution, conditioned concurrently
on the mixed audio input and associated visual information. The inherent nature
of its generative objective makes DAVIS particularly adept at producing
high-quality sound separations for diverse sound categories. We present
comparative evaluations of DAVIS, encompassing both its DDPM and Flow Matching
variants, against leading methods on the standard AVE and MUSIC datasets. The
results affirm that both variants surpass existing approaches in separation
quality, highlighting the efficacy of our generative framework for tackling the
audio-visual source separation task.

</details>


### [21] [Do Sparse Subnetworks Exhibit Cognitively Aligned Attention? Effects of Pruning on Saliency Map Fidelity, Sparsity, and Concept Coherence](https://arxiv.org/abs/2509.21387)
*Sanish Suwal,Dipkamal Bhusal,Michael Clifford,Nidhi Rastogi*

Main category: cs.CV

TL;DR: 研究发现适度剪枝可以提高模型的可解释性，但过度剪枝会影响模型的概念清晰度和显著性图的效果。


<details>
  <summary>Details</summary>
Motivation: 探讨剪枝对模型可解释性的影响，尤其是如何影响显著性映射和概念表示。

Method: 采用基于幅度的剪枝和微调方法，评估低级显著性图和高级概念表示.

Result: 轻度到中度剪枝提高显著性图的专注度和真实性，而激进剪枝则合并异构特征，降低了显著性图的稀疏性和概念一致性。

Conclusion: 过度剪枝会降低模型的可解释性，尽管保持了准确性。

Abstract: Prior works have shown that neural networks can be heavily pruned while
preserving performance, but the impact of pruning on model interpretability
remains unclear. In this work, we investigate how magnitude-based pruning
followed by fine-tuning affects both low-level saliency maps and high-level
concept representations. Using a ResNet-18 trained on ImageNette, we compare
post-hoc explanations from Vanilla Gradients (VG) and Integrated Gradients (IG)
across pruning levels, evaluating sparsity and faithfulness. We further apply
CRAFT-based concept extraction to track changes in semantic coherence of
learned concepts. Our results show that light-to-moderate pruning improves
saliency-map focus and faithfulness while retaining distinct, semantically
meaningful concepts. In contrast, aggressive pruning merges heterogeneous
features, reducing saliency map sparsity and concept coherence despite
maintaining accuracy. These findings suggest that while pruning can shape
internal representations toward more human-aligned attention patterns,
excessive pruning undermines interpretability.

</details>


### [22] [TUN3D: Towards Real-World Scene Understanding from Unposed Images](https://arxiv.org/abs/2509.21388)
*Anton Konushin,Nikita Drozdov,Bulat Gabdullin,Alexey Zakharov,Anna Vorontsova,Danila Rukhovich,Maksim Kolodiazhnyi*

Main category: cs.CV

TL;DR: TUN3D是首个在多视图图像输入下实现布局估计和3D物体检测的方法，突破了点云输入的局限，展示了在室内场景理解中的新基准表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖点云输入，这对大多数不具备深度传感器的消费者相机造成了限制，视觉数据更为普遍，因此需要探索新的解决方案。

Method: TUN3D采用轻量级稀疏卷积骨干网络和两个专用头，用于3D物体检测和布局估计，结合新的参数化墙体表示。

Result: 经过广泛实验，TUN3D在三个具有挑战性的场景理解基准上都显示出卓越性能，包括使用真实点云、已标定图像和未标定图像。

Conclusion: TUN3D在室内场景理解中推出了一种创新方法，通过多视图图像实现布局估计和3D物体检测的联动，建立了新的性能标准。

Abstract: Layout estimation and 3D object detection are two fundamental tasks in indoor
scene understanding. When combined, they enable the creation of a compact yet
semantically rich spatial representation of a scene. Existing approaches
typically rely on point cloud input, which poses a major limitation since most
consumer cameras lack depth sensors and visual-only data remains far more
common. We address this issue with TUN3D, the first method that tackles joint
layout estimation and 3D object detection in real scans, given multi-view
images as input, and does not require ground-truth camera poses or depth
supervision. Our approach builds on a lightweight sparse-convolutional backbone
and employs two dedicated heads: one for 3D object detection and one for layout
estimation, leveraging a novel and effective parametric wall representation.
Extensive experiments show that TUN3D achieves state-of-the-art performance
across three challenging scene understanding benchmarks: (i) using ground-truth
point clouds, (ii) using posed images, and (iii) using unposed images. While
performing on par with specialized 3D object detection methods, TUN3D
significantly advances layout estimation, setting a new benchmark in holistic
indoor scene understanding. Code is available at
https://github.com/col14m/tun3d .

</details>


### [23] [Large AI Model-Enabled Generative Semantic Communications for Image Transmission](https://arxiv.org/abs/2509.21394)
*Qiyu Ma,Wanli Ni,Zhijin Qin*

Main category: cs.CV

TL;DR: 该论文提出了一种创新的生成语义通信系统，通过细分图像区域和采用轻量级部署策略，提高了图像传输的效率与质量。


<details>
  <summary>Details</summary>
Motivation: 生成性人工智能的快速发展带来了提高语义通信系统中图像传输效率和准确性的显著机遇，同时现有方法忽视了图像不同区域的重要性差异。

Method: 将图像细分为关键区域和非关键区域，关键区域通过图像导向的语义编码器处理，非关键区域通过图像到文本建模方法高效压缩，同时采用轻量级部署策略来降低存储和计算需求。

Result: 仿真结果显示，所提出的系统在语义保真度和视觉质量方面超越了传统方法。

Conclusion: 所提出的生成语义通信系统在语义保真度和视觉质量方面都优于传统方法，确认其在图像传输任务中的有效性。

Abstract: The rapid development of generative artificial intelligence (AI) has
introduced significant opportunities for enhancing the efficiency and accuracy
of image transmission within semantic communication systems. Despite these
advancements, existing methodologies often neglect the difference in importance
of different regions of the image, potentially compromising the reconstruction
quality of visually critical content. To address this issue, we introduce an
innovative generative semantic communication system that refines semantic
granularity by segmenting images into key and non-key regions. Key regions,
which contain essential visual information, are processed using an image
oriented semantic encoder, while non-key regions are efficiently compressed
through an image-to-text modeling approach. Additionally, to mitigate the
substantial storage and computational demands posed by large AI models, the
proposed system employs a lightweight deployment strategy incorporating model
quantization and low-rank adaptation fine-tuning techniques, significantly
boosting resource utilization without sacrificing performance. Simulation
results demonstrate that the proposed system outperforms traditional methods in
terms of both semantic fidelity and visual quality, thereby affirming its
effectiveness for image transmission tasks.

</details>


### [24] [mmHSense: Multi-Modal and Distributed mmWave ISAC Datasets for Human Sensing](https://arxiv.org/abs/2509.21396)
*Nabeel Nisar Bhat,Maksim Karnaukh,Stein Vandenbroeke,Wouter Lemoine,Jakob Struye,Jesus Omar Lacruz,Siddhartha Kumar,Mohammad Hossein Moghaddam,Joerg Widmer,Rafael Berkvens,Jeroen Famaey*

Main category: cs.CV

TL;DR: mmHSense数据集为ISAC系统中的人类感知研究提供了支持，通过验证展示了应用潜力，并引入了高效微调技术以降低复杂度。


<details>
  <summary>Details</summary>
Motivation: 为支持集成传感与通信的研究，尤其是在人类感知和信号处理方面，创建开放标记的mmWave数据集。

Method: 提供了开放标记的毫米波数据集，以支持在集成传感与通信系统中的人类感知研究，包含实验设置和信号特征描述。

Result: 数据集支持手势识别、人员识别、姿态估计和定位等多种应用，有效推动了mmWave ISAC领域的研究。

Conclusion: mmHSense数据集通过多种下游任务的验证，展示了其在mmWave ISAC系统中的应用潜力，并引入了参数高效的微调方法，以降低计算复杂度，保持性能。

Abstract: This article presents mmHSense, a set of open labeled mmWave datasets to
support human sensing research within Integrated Sensing and Communication
(ISAC) systems. The datasets can be used to explore mmWave ISAC for various end
applications such as gesture recognition, person identification, pose
estimation, and localization. Moreover, the datasets can be used to develop and
advance signal processing and deep learning research on mmWave ISAC. This
article describes the testbed, experimental settings, and signal features for
each dataset. Furthermore, the utility of the datasets is demonstrated through
validation on a specific downstream task. In addition, we demonstrate the use
of parameter-efficient fine-tuning to adapt ISAC models to different tasks,
significantly reducing computational complexity while maintaining performance
on prior tasks.

</details>


### [25] [Skeleton Sparsification and Densification Scale-Spaces](https://arxiv.org/abs/2509.21398)
*Julia Gierke,Pascal Peter*

Main category: cs.CV

TL;DR: 提出了一种新的骨架化尺度空间方法，通过结合稀疏化和层次化简化，解决了哈密尔顿-雅可比骨架在噪声下的敏感性问题，并在实际应用中显示出良好的效果。


<details>
  <summary>Details</summary>
Motivation: 处理哈密尔顿-雅可比骨架的噪声敏感性，并引入层次化简化的方法。

Method: 引入骨架化尺度空间，结合骨架的稀疏化实现形状的分层简化，满足关键尺度空间属性。

Result: 通过概念验证实验，验证了框架在实际应用中的有效性。

Conclusion: 该框架有效提升了骨架化的鲁棒性，形状压缩和用于增材制造的刚度增强等实际任务的效果。

Abstract: The Hamilton-Jacobi skeleton, also known as the medial axis, is a powerful
shape descriptor that represents binary objects in terms of the centres of
maximal inscribed discs. Despite its broad applicability, the medial axis
suffers from sensitivity to noise: minor boundary variations can lead to
disproportionately large and undesirable expansions of the skeleton. Classical
pruning methods mitigate this shortcoming by systematically removing extraneous
skeletal branches. This sequential simplification of skeletons resembles the
principle of sparsification scale-spaces that embed images into a family of
reconstructions from increasingly sparse pixel representations.
  We combine both worlds by introducing skeletonisation scale-spaces: They
leverage sparsification of the medial axis to achieve hierarchical
simplification of shapes. Unlike conventional pruning, our framework inherently
satisfies key scale-space properties such as hierarchical architecture,
controllable simplification, and equivariance to geometric transformations. We
provide a rigorous theoretical foundation in both continuous and discrete
formulations and extend the concept further with densification. This allows
inverse progression from coarse to fine scales and can even reach beyond the
original skeleton to produce overcomplete shape representations with relevancy
for practical applications.
  Through proof-of-concept experiments, we demonstrate the effectiveness of our
framework for practical tasks including robust skeletonisation, shape
compression, and stiffness enhancement for additive manufacturing.

</details>


### [26] [Downscaling climate projections to 1 km with single-image super resolution](https://arxiv.org/abs/2509.21399)
*Petr Košťál,Pavel Kordík,Ondřej Podsztavek*

Main category: cs.CV

TL;DR: 通过单图像超分辨率模型，将低分辨率气候预测下调至1 km分辨率，保持气候指标的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前气候预测分辨率低，限制了其实际应用，因此需要探索更高分辨率的气候预测方法。

Method: 利用单图像超分辨率模型进行统计性下调气候预测，采用高分辨率观测数据集进行训练，并通过观测气候指数进行评估。

Result: 实验表明，应用单图像超分辨率模型后，气候指标的误差与低分辨率气候预测相比并未增加。

Conclusion: 单图像超分辨率模型能够将气候预测的空间分辨率从12.5 km提高到1 km，而不会增加气候指标的误差。

Abstract: High-resolution climate projections are essential for local decision-making.
However, available climate projections have low spatial resolution (e.g. 12.5
km), which limits their usability. We address this limitation by leveraging
single-image super-resolution models to statistically downscale climate
projections to 1-km resolution. Since high-resolution climate projections are
unavailable for training, we train models on a high-resolution observational
gridded data set and apply them to low-resolution climate projections. We
propose a climate indicator-based assessment using observed climate indices
computed at weather station locations to evaluate the downscaled climate
projections without ground-truth high-resolution climate projections.
Experiments on daily mean temperature demonstrate that single-image
super-resolution models can downscale climate projections without increasing
the error of climate indicators compared to low-resolution climate projections.

</details>


### [27] [JaiLIP: Jailbreaking Vision-Language Models via Loss Guided Image Perturbation](https://arxiv.org/abs/2509.21401)
*Md Jueal Mia,M. Hadi Amini*

Main category: cs.CV

TL;DR: 本研究提出JaiLIP，一个有效的图像基础的越狱攻击方法，能够生成不易察觉的对抗性图像，强调了防御VLMs的必要性。


<details>
  <summary>Details</summary>
Motivation: 随着VLMs在多模态推理任务中的能力显著提升，潜在的误用或安全对齐问题日益严重，尤其是图像基础的扰动攻击更为有效。

Method: 提出了一种图像空间的越狱攻击方法JaiLIP，通过最小化干净图像与对抗图像之间的均方误差损失和模型有害输出损失的联合目标进行优化。

Result: 实验结果表明，我们的方法能够生成高效且不易察觉的对抗图像，在产生毒性输出方面优于现有方法，并在运输领域的应用展示了攻击的实际可行性。

Conclusion: 我们的研究强调了图像基础的越狱攻击的实际挑战，以及对VLMs有效防御机制的需求。

Abstract: Vision-Language Models (VLMs) have remarkable abilities in generating
multimodal reasoning tasks. However, potential misuse or safety alignment
concerns of VLMs have increased significantly due to different categories of
attack vectors. Among various attack vectors, recent studies have demonstrated
that image-based perturbations are particularly effective in generating harmful
outputs. In the literature, many existing techniques have been proposed to
jailbreak VLMs, leading to unstable performance and visible perturbations. In
this study, we propose Jailbreaking with Loss-guided Image Perturbation
(JaiLIP), a jailbreaking attack in the image space that minimizes a joint
objective combining the mean squared error (MSE) loss between clean and
adversarial image with the models harmful-output loss. We evaluate our proposed
method on VLMs using standard toxicity metrics from Perspective API and
Detoxify. Experimental results demonstrate that our method generates highly
effective and imperceptible adversarial images, outperforming existing methods
in producing toxicity. Moreover, we have evaluated our method in the
transportation domain to demonstrate the attacks practicality beyond toxic text
generation in specific domain. Our findings emphasize the practical challenges
of image-based jailbreak attacks and the need for efficient defense mechanisms
for VLMs.

</details>


### [28] [Overview of ExpertLifeCLEF 2018: how far automated identification systems are from the best experts?](https://arxiv.org/abs/2509.21419)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: 本文通过LifeCLEF 2018 ExpertCLEF挑战评估了深度学习模型与人类专家在植物和动物识别中的性能，结果显示二者表现相当。


<details>
  <summary>Details</summary>
Motivation: 探讨自动化识别系统与人类专家之间的差距，量化其不确定性，分析不同观察者在识别生命体时的主观性。

Method: 通过LifeCLEF 2018 ExpertCLEF挑战，评估了19个深度学习系统与9名植物学专家的性能对比。

Result: 评估显示，深度学习模型的表现接近人类专家，提供了对参与研究团队的方法和结果的总结。

Conclusion: 目前，先进的深度学习模型的表现与最顶尖的人类专家相当，显示出自动化识别系统在植物和动物识别中的潜力。

Abstract: Automated identification of plants and animals has improved considerably in
the last few years, in particular thanks to the recent advances in deep
learning. The next big question is how far such automated systems are from the
human expertise. Indeed, even the best experts are sometimes confused and/or
disagree between each others when validating visual or audio observations of
living organism. A picture actually contains only a partial information that is
usually not sufficient to determine the right species with certainty.
Quantifying this uncertainty and comparing it to the performance of automated
systems is of high interest for both computer scientists and expert
naturalists. The LifeCLEF 2018 ExpertCLEF challenge presented in this paper was
designed to allow this comparison between human experts and automated systems.
In total, 19 deep-learning systems implemented by 4 different research teams
were evaluated with regard to 9 expert botanists of the French flora. The main
outcome of this work is that the performance of state-of-the-art deep learning
models is now close to the most advanced human expertise. This paper presents
more precisely the resources and assessments of the challenge, summarizes the
approaches and systems employed by the participating research groups, and
provides an analysis of the main outcomes.

</details>


### [29] [QuadGPT: Native Quadrilateral Mesh Generation with Autoregressive Models](https://arxiv.org/abs/2509.21420)
*Jian Liu,Chunshi Wang,Song Guo,Haohan Weng,Zhen Zhou,Zhiqi Li,Jiaao Yu,Yiling Zhu,Jing Xu,Biwen Lei,Zhuo Chen,Chunchao Guo*

Main category: cs.CV

TL;DR: QuadGPT 是一个端到端的自回归框架，用于生成高质量的四边形网格，超越了传统的三角形合并方法，显著改善了几何和拓扑品质。


<details>
  <summary>Details</summary>
Motivation: 优化现有生成模型生成的四边形网格的拓扑结构，解决之前方法中由于三角形合并导致的质量差的问题。

Method: 提出了一种自回归框架，通过统一的标记化方法处理三角形和四边形的混合拓扑，以及使用专门的强化学习微调方法tDPO以提高生成质量。

Result: QuadGPT在几何准确性和拓扑质量上显著超越了之前的三角形到四边形转换管道。

Conclusion: QuadGPT 在原生四边形网格生成方面建立了新的基准，并展示了将大型自回归模型与拓扑感知的强化学习精修相结合的能力。

Abstract: The generation of quadrilateral-dominant meshes is a cornerstone of
professional 3D content creation. However, existing generative models generate
quad meshes by first generating triangle meshes and then merging triangles into
quadrilaterals with some specific rules, which typically produces quad meshes
with poor topology. In this paper, we introduce QuadGPT, the first
autoregressive framework for generating quadrilateral meshes in an end-to-end
manner. QuadGPT formulates this as a sequence prediction paradigm,
distinguished by two key innovations: a unified tokenization method to handle
mixed topologies of triangles and quadrilaterals, and a specialized
Reinforcement Learning fine-tuning method tDPO for better generation quality.
Extensive experiments demonstrate that QuadGPT significantly surpasses previous
triangle-to-quad conversion pipelines in both geometric accuracy and
topological quality. Our work establishes a new benchmark for native quad-mesh
generation and showcases the power of combining large-scale autoregressive
models with topology-aware RL refinement for creating structured 3D assets.

</details>


### [30] [DyME: Dynamic Multi-Concept Erasure in Diffusion Models with Bi-Level Orthogonal LoRA Adaptation](https://arxiv.org/abs/2509.21433)
*Jiaqi Liu,Lan Zhang,Xiaoyong Yuan*

Main category: cs.CV

TL;DR: DyME是一个创新的按需抹除框架，解决了多概念抹除中的干扰问题，并在多个基准测试中显示出优越的性能。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型可能不自觉地再现受版权保护的风格和视觉概念，导致法律和伦理问题。

Method: 提出了一种按需抹除框架DyME，使用轻量级概念特定的LoRA适配器，并在推理时动态组合需要的适配器。

Result: DyME在ErasureBench-H和标准数据集上表现优于现有方法，实现了更高的多概念抹除保真度，同时降低了副作用。

Conclusion: DyME在多概念抹除中表现优异，减少了非目标内容的质量损失。

Abstract: Text-to-image diffusion models (DMs) inadvertently reproduce copyrighted
styles and protected visual concepts, raising legal and ethical concerns.
Concept erasure has emerged as a safeguard, aiming to selectively suppress such
concepts through fine-tuning. However, existing methods do not scale to
practical settings where providers must erase multiple and possibly conflicting
concepts. The core bottleneck is their reliance on static erasure: a single
checkpoint is fine-tuned to remove all target concepts, regardless of the
actual erasure needs at inference. This rigid design mismatches real-world
usage, where requests vary per generation, leading to degraded erasure success
and reduced fidelity for non-target content. We propose DyME, an on-demand
erasure framework that trains lightweight, concept-specific LoRA adapters and
dynamically composes only those needed at inference. This modular design
enables flexible multi-concept erasure, but naive composition causes
interference among adapters, especially when many or semantically related
concepts are suppressed. To overcome this, we introduce bi-level orthogonality
constraints at both the feature and parameter levels, disentangling
representation shifts and enforcing orthogonal adapter subspaces. We further
develop ErasureBench-H, a new hierarchical benchmark with
brand-series-character structure, enabling principled evaluation across
semantic granularities and erasure set sizes. Experiments on ErasureBench-H and
standard datasets (e.g., CIFAR-100, Imagenette) demonstrate that DyME
consistently outperforms state-of-the-art baselines, achieving higher
multi-concept erasure fidelity with minimal collateral degradation.

</details>


### [31] [VideoJudge: Bootstrapping Enables Scalable Supervision of MLLM-as-a-Judge for Video Understanding](https://arxiv.org/abs/2509.21451)
*Abdul Waheed,Zhen Wu,Dareen Alharthi,Seungone Kim,Bhiksha Raj*

Main category: cs.CV

TL;DR: VideoJudge是专门为视频理解模型评估设计的多模态语言模型，能有效提高评估性能，凸显了直接使用视频输入的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有用于视频理解模型评估的指标无法有效捕捉人类判断的细腻性，同时手动评估成本高，因此有必要开发一种新的评估工具。

Method: 通过生成器和评估器之间的互动来训练VideoJudge，生成器在目标评分的条件下生成响应，评估器则根据评分标准筛选响应。

Result: VideoJudge针对视频理解模型输出的评估表现优于多个大型多模态语言模型基线，而单一语言模型的评估效果则不如多模态模型。

Conclusion: VideoJudge是一种专门评估视频理解模型输出的多模态语言模型，显示出在多个基准测试中优于现有的基线模型，强调了在视频理解任务评估中直接提供视频输入的重要性。

Abstract: Precisely evaluating video understanding models remains challenging: commonly
used metrics such as BLEU, ROUGE, and BERTScore fail to capture the fineness of
human judgment, while obtaining such judgments through manual evaluation is
costly. Recent work has explored using large language models (LLMs) or
multimodal LLMs (MLLMs) as evaluators, but their extension to video
understanding remains relatively unexplored. In this work, we introduce
VideoJudge, a 3B and 7B-sized MLLM judge specialized to evaluate outputs from
video understanding models (\textit{i.e.}, text responses conditioned on
videos). To train VideoJudge, our recipe builds on the interplay between a
generator and an evaluator: the generator is prompted to produce responses
conditioned on a target rating, and responses not matching the evaluator's
rating are discarded. Across three out of four meta-evaluation benchmarks,
VideoJudge-7B outperforms larger MLLM judge baselines such as Qwen2.5-VL (32B
and 72B). Notably, we find that LLM judges (Qwen3) models perform worse than
MLLM judges (Qwen2.5-VL) and long chain-of-thought reasoning does not improve
performance, indicating that providing video inputs is crucial for evaluation
of video understanding tasks.

</details>


### [32] [Residual Vector Quantization For Communication-Efficient Multi-Agent Perception](https://arxiv.org/abs/2509.21464)
*Dereje Shenkut,B. V. K Vijaya Kumar*

Main category: cs.CV

TL;DR: ReVQom通过压缩特征维度，允许多智能体在更低的带宽下进行高效的协同感知，为实际的V2X部署提供了可能。


<details>
  <summary>Details</summary>
Motivation: 为了解决通信带宽限制对多智能体协同感知的可扩展性影响，提出了一种压缩中间特征的有效方法。

Method: ReVQom是一种端到端方法，通过简单的瓶颈网络和多阶段残差向量量化（RVQ）来压缩特征维度。

Result: 在DAIR-V2X真实世界的协同感知数据集上，ReVQom在30 bpp下实现了273倍压缩，在6 bpp下实现了1365倍压缩，同时在18 bpp下的表现与原始特征协同感知相当或更优。

Conclusion: ReVQom显著提高了多智能体协同感知的效率和准确性，促进了实际V2X部署。

Abstract: Multi-agent collaborative perception (CP) improves scene understanding by
sharing information across connected agents such as autonomous vehicles,
unmanned aerial vehicles, and robots. Communication bandwidth, however,
constrains scalability. We present ReVQom, a learned feature codec that
preserves spatial identity while compressing intermediate features. ReVQom is
an end-to-end method that compresses feature dimensions via a simple bottleneck
network followed by multi-stage residual vector quantization (RVQ). This allows
only per-pixel code indices to be transmitted, reducing payloads from 8192 bits
per pixel (bpp) of uncompressed 32-bit float features to 6-30 bpp per agent
with minimal accuracy loss. On DAIR-V2X real-world CP dataset, ReVQom achieves
273x compression at 30 bpp to 1365x compression at 6 bpp. At 18 bpp (455x),
ReVQom matches or outperforms raw-feature CP, and at 6-12 bpp it enables
ultra-low-bandwidth operation with graceful degradation. ReVQom allows
efficient and accurate multi-agent collaborative perception with a step toward
practical V2X deployment.

</details>


### [33] [Gender Stereotypes in Professional Roles Among Saudis: An Analytical Study of AI-Generated Images Using Language Models](https://arxiv.org/abs/2509.21466)
*Khaloud S. AlKhalifah,Malak Mashaabi,Hend Al-Khalifa*

Main category: cs.CV

TL;DR: 本研究分析了三种现代文本到图像AI模型在生成沙特专业人士图像时的性别和文化偏见，结果显示这些模型延续了社会性别偏见和文化不准确性，亟需改进训练数据和评估框架。


<details>
  <summary>Details</summary>
Motivation: 研究当代文本到图像人工智能模型在生成沙特阿拉伯专业人士图像时是否会延续性别刻板印象和文化不准确性。

Method: 分析了1,006张由ImageFX、DALL-E V3和Grok生成的图像，评估维度包括性别、服装外观、背景设置、活动互动和年龄。

Result: 发现ImageFX、Grok和DALL-E V3的结果中，男性图像分别占85\%、86.6\%和96\%，显示出明显的性别刻板印象，尤其是在领导和技术角色中。此外，所有三个模型都存在文化误解现象。

Conclusion: 当前的文本生成图像模型反映了训练数据中嵌入的人类社会偏见，这些模型在展示沙特阿拉伯专业人士时存在明显的性别不平衡和文化不准确性。

Abstract: This study investigates the extent to which contemporary Text-to-Image
artificial intelligence (AI) models perpetuate gender stereotypes and cultural
inaccuracies when generating depictions of professionals in Saudi Arabia. We
analyzed 1,006 images produced by ImageFX, DALL-E V3, and Grok for 56 diverse
Saudi professions using neutral prompts. Two trained Saudi annotators evaluated
each image on five dimensions: perceived gender, clothing and appearance,
background and setting, activities and interactions, and age. A third senior
researcher adjudicated whenever the two primary raters disagreed, yielding
10,100 individual judgements. The results reveal a strong gender imbalance,
with ImageFX outputs being 85\% male, Grok 86.6\% male, and DALL-E V3 96\%
male, indicating that DALL-E V3 exhibited the strongest overall gender
stereotyping. This imbalance was most evident in leadership and technical
roles. Moreover, cultural inaccuracies in clothing, settings, and depicted
activities were frequently observed across all three models.
Counter-stereotypical images often arise from cultural misinterpretations
rather than genuinely progressive portrayals. We conclude that current models
mirror societal biases embedded in their training data, generated by humans,
offering only a limited reflection of the Saudi labour market's gender dynamics
and cultural nuances. These findings underscore the urgent need for more
diverse training data, fairer algorithms, and culturally sensitive evaluation
frameworks to ensure equitable and authentic visual outputs.

</details>


### [34] [Reasoning-Enhanced Domain-Adaptive Pretraining of Multimodal Large Language Models for Short Video Content Moderation](https://arxiv.org/abs/2509.21486)
*Zixuan Wang,Yu Sun,Hongwei Wang,Baoyu Jing,Xiang Shen,Xin Dong,Zhuolin Hao,Hongyu Xiong,Yang Song*

Main category: cs.CV

TL;DR: 提出了一种增强推理的多模态大语言模型预训练方法，通过三个针对性任务改善不当内容检测的性能，显示出强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着短视频平台的发展，识别不当内容的重要性日益增加，现有方法缺乏跨问题通用性且需大量人工标注数据，因此提出了一种统一的不当内容检测方法。

Method: 采用了一种增强推理的多模态大语言模型预训练范式，包含了三个针对性的预训练任务：Caption、视觉问答（VQA）、以及思维链（CoT），以提高模型对视频内容及问题定义的理解能力和推理能力。

Result: 实验结果表明，我们的预训练方法在零样本和监督微调设置中显著提升了模型性能，并展示了对新出现、不曾见过的问题的强泛化能力。

Conclusion: 我们提出的预训练方法显著提升了多模态大语言模型在不当内容检测中的性能，尤其在零样本和监督微调设置中表现尤为优秀，同时展现了强大的泛化能力。

Abstract: Short video platforms are evolving rapidly, making the identification of
inappropriate content increasingly critical. Existing approaches typically
train separate and small classification models for each type of issue, which
requires extensive human-labeled data and lacks cross-issue generalization. We
propose a reasoning-enhanced multimodal large language model (MLLM) pretraining
paradigm for unified inappropriate content detection. To address the
distribution gap between short video content and the original pretraining data
of MLLMs, as well as the complex issue definitions, we introduce three targeted
pretraining tasks: (1) \textit{Caption}, to enhance the MLLM's perception of
video details; (2) \textit{Visual Question Answering (VQA)}, to deepen the
MLLM's understanding of issue definitions and annotation guidelines; (3)
\textit{Chain-of-Thought (CoT)}, to enhance the MLLM's reasoning capability.
Experimental results show that our pretraining approach significantly improves
the MLLM's performance in both zero-shot and supervised fine-tuning (SFT)
settings. In addition, our pretrained model demonstrates strong generalization
capabilities to emergent, previously unseen issues.

</details>


### [35] [Learning GUI Grounding with Spatial Reasoning from Visual Feedback](https://arxiv.org/abs/2509.21552)
*Yu Zhao,Wei-Ning Chen,Huseyin Atahan Inan,Samuel Kessler,Lu Wang,Lukas Wutschitz,Fangkai Yang,Chaoyun Zhang,Pasquale Minervini,Saravan Rajmohan,Robert Sim*

Main category: cs.CV

TL;DR: 本研究提出GUI-Cursor，通过交互式搜索任务改善GUI界面定位，显著提升了模型的准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 针对当前视觉语言模型在高分辨率复杂GUI布局中预测准确坐标的不足，将GUI定位重新定义为交互式搜索任务。

Method: 使用多步在线强化学习和密集基于轨迹的奖励函数训练GUI定位模型GUI-Cursor。

Result: GUI-Cursor在ScreenSpot-v2的准确率从88.8%提升至93.9%，在ScreenSpot-Pro的准确率从26.8%提升至56.5%。大部分实例的解决方案在两步内得出，对更复杂示例则适应性地进行更多步骤。

Conclusion: GUI-Cursor利用多步在线强化学习，显著提升了GUI定位的准确性，并在多个基准上达到了最先进的结果。

Abstract: Graphical User Interface (GUI) grounding is commonly framed as a coordinate
prediction task -- given a natural language instruction, generate on-screen
coordinates for actions such as clicks and keystrokes. However, recent Vision
Language Models (VLMs) often fail to predict accurate numeric coordinates when
processing high-resolution GUI images with complex layouts. To address this
issue, we reframe GUI grounding as an \emph{interactive search task}, where the
VLM generates actions to move a cursor in the GUI to locate UI elements. At
each step, the model determines the target object, evaluates the spatial
relations between the cursor and the target, and moves the cursor closer to the
target conditioned on the movement history. In this interactive process, the
rendered cursor provides visual feedback to help the model align its
predictions with the corresponding on-screen locations. We train our GUI
grounding model, GUI-Cursor, using multi-step online reinforcement learning
with a dense trajectory-based reward function. Our experimental results show
that GUI-Cursor, based on Qwen2.5-VL-7B, improves the GUI grounding accuracy
and achieves state-of-the-art results on ScreenSpot-v2 ($88.8\% \rightarrow
93.9\%$) and ScreenSpot-Pro ($26.8\% \rightarrow 56.5\%$). Moreover, we observe
that GUI-Cursor learns to solve the problem within two steps for 95\% of
instances and can adaptively conduct more steps on more difficult examples.

</details>


### [36] [X-CoT: Explainable Text-to-Video Retrieval via LLM-based Chain-of-Thought Reasoning](https://arxiv.org/abs/2509.21559)
*Prasanna Reddy Pulakurthi,Jiamian Wang,Majid Rabbani,Sohail Dianat,Raghuveer Rao,Zhiqiang Tao*

Main category: cs.CV

TL;DR: X-CoT是一个基于LLM CoT推理的可解释检索框架，解决了传统文本到视频检索系统的局限性，提升了性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频检索系统存在对低质量数据难以识别和解释、缺乏排名结果可解释性的问题。

Method: 提出X-CoT框架，基于LLM CoT推理，取代传统的基础于嵌入模型的相似性排名方法。

Result: X-CoT通过扩展基准和设计成对比较步骤，改进了检索性能，并提供了详细的推理基础。

Conclusion: X-CoT框架显著提升了文本到视频检索的性能，并提供了详尽的推理过程，有助于模型行为和数据质量的分析。

Abstract: Prevalent text-to-video retrieval systems mainly adopt embedding models for
feature extraction and compute cosine similarities for ranking. However, this
design presents two limitations. Low-quality text-video data pairs could
compromise the retrieval, yet are hard to identify and examine. Cosine
similarity alone provides no explanation for the ranking results, limiting the
interpretability. We ask that can we interpret the ranking results, so as to
assess the retrieval models and examine the text-video data? This work proposes
X-CoT, an explainable retrieval framework upon LLM CoT reasoning in place of
the embedding model-based similarity ranking. We first expand the existing
benchmarks with additional video annotations to support semantic understanding
and reduce data bias. We also devise a retrieval CoT consisting of pairwise
comparison steps, yielding detailed reasoning and complete ranking. X-CoT
empirically improves the retrieval performance and produces detailed
rationales. It also facilitates the model behavior and data quality analysis.
Code and data are available at: https://github.com/PrasannaPulakurthi/X-CoT.

</details>


### [37] [Unsupervised Defect Detection for Surgical Instruments](https://arxiv.org/abs/2509.21561)
*Joseph Huang,Yichi Zhang,Jingxi Yu,Wei Chen,Seunghyun Hwang,Qiang Qiu,Amy R. Reibman,Edward J. Delp,Fengqing Zhu*

Main category: cs.CV

TL;DR: 我们提出了一种方法，针对外科器械的缺陷检测，解决了传统方法中的 false positive、对小缺陷的敏感性不足和领域转移问题。


<details>
  <summary>Details</summary>
Motivation: 确保外科手术器械的安全需要可靠的视觉缺陷检测，但现有的自动检测方法在手术领域的应用效果不佳。

Method: 集成背景掩模、基于块的分析策略和高效的领域适应。

Result: 通过我们的方法，实现了外科器械影像中细微缺陷的可靠检测。

Conclusion: 我们提出了一种多功能的方法，能够有效适应外科器械的无监督缺陷检测，克服了传统方法的局限性。

Abstract: Ensuring the safety of surgical instruments requires reliable detection of
visual defects. However, manual inspection is prone to error, and existing
automated defect detection methods, typically trained on natural/industrial
images, fail to transfer effectively to the surgical domain. We demonstrate
that simply applying or fine-tuning these approaches leads to issues: false
positive detections arising from textured backgrounds, poor sensitivity to
small, subtle defects, and inadequate capture of instrument-specific features
due to domain shift. To address these challenges, we propose a versatile method
that adapts unsupervised defect detection methods specifically for surgical
instruments. By integrating background masking, a patch-based analysis
strategy, and efficient domain adaptation, our method overcomes these
limitations, enabling the reliable detection of fine-grained defects in
surgical instrument imagery.

</details>


### [38] [No Alignment Needed for Generation: Learning Linearly Separable Representations in Diffusion Models](https://arxiv.org/abs/2509.21565)
*Junno Yun,Yaşar Utku Alçalar,Mehmet Akçakaya*

Main category: cs.CV

TL;DR: 本文提出了一种新的正则化策略，通过提高中间层特征的线性可分性，以显著提高扩散模型的训练效率和生成质量。


<details>
  <summary>Details</summary>
Motivation: 改进大规模扩散模型的判别特征表示，并消除对大型预训练编码器的依赖。

Method: 基于促进中间层表示的线性可分性（LSEP）的正则化方法，直接将线性探测纳入网络的学习动态。

Result: 在SiTs架构上，FID达到1.46，表明训练效率和生成质量显著提升。

Conclusion: 提出了一种基于LSEP的正则化方法，在训练效率和生成质量上显著提高，尤其在流式变换器架构中表现优异。

Abstract: Efficient training strategies for large-scale diffusion models have recently
emphasized the importance of improving discriminative feature representations
in these models. A central line of work in this direction is representation
alignment with features obtained from powerful external encoders, which
improves the representation quality as assessed through linear probing.
Alignment-based approaches show promise but depend on large pretrained
encoders, which are computationally expensive to obtain. In this work, we
propose an alternative regularization for training, based on promoting the
Linear SEParability (LSEP) of intermediate layer representations. LSEP
eliminates the need for an auxiliary encoder and representation alignment,
while incorporating linear probing directly into the network's learning
dynamics rather than treating it as a simple post-hoc evaluation tool. Our
results demonstrate substantial improvements in both training efficiency and
generation quality on flow-based transformer architectures such as SiTs,
achieving an FID of 1.46 on $256 \times 256$ ImageNet dataset.

</details>


### [39] [Enhancing Contrastive Learning for Geolocalization by Discovering Hard Negatives on Semivariograms](https://arxiv.org/abs/2509.21573)
*Boyi Chen,Zhangyu Wang,Fabian Deuser,Johann Maximilian Zollner,Martin Werner*

Main category: cs.CV

TL;DR: 提出了一种新的空间正则化对比学习策略，通过建模空间相关性，改进了图像地理定位的效果。


<details>
  <summary>Details</summary>
Motivation: 解决传统对比学习方法忽视地理空间中的潜在空间依赖性导致的假负样本和难负样本问题。

Method: 提出了一种新颖的空间正则化对比学习策略，整合了一种半变差函数，通过与地理距离关联来捕捉视觉内容的预期空间相关性。

Result: 在OSV5M数据集上的实验证明，建模空间先验改善了基于图像的地理定位性能。

Conclusion: 通过明确建模空间先验，提高了图像地理定位的性能，特别是在更细粒度的区域。

Abstract: Accurate and robust image-based geo-localization at a global scale is
challenging due to diverse environments, visually ambiguous scenes, and the
lack of distinctive landmarks in many regions. While contrastive learning
methods show promising performance by aligning features between street-view
images and corresponding locations, they neglect the underlying spatial
dependency in the geographic space. As a result, they fail to address the issue
of false negatives -- image pairs that are both visually and geographically
similar but labeled as negatives, and struggle to effectively distinguish hard
negatives, which are visually similar but geographically distant. To address
this issue, we propose a novel spatially regularized contrastive learning
strategy that integrates a semivariogram, which is a geostatistical tool for
modeling how spatial correlation changes with distance. We fit the
semivariogram by relating the distance of images in feature space to their
geographical distance, capturing the expected visual content in a spatial
correlation. With the fitted semivariogram, we define the expected visual
dissimilarity at a given spatial distance as reference to identify hard
negatives and false negatives. We integrate this strategy into GeoCLIP and
evaluate it on the OSV5M dataset, demonstrating that explicitly modeling
spatial priors improves image-based geo-localization performance, particularly
at finer granularity.

</details>


### [40] [X-Streamer: Unified Human World Modeling with Audiovisual Interaction](https://arxiv.org/abs/2509.21574)
*You Xie,Tianpei Gu,Zenan Li,Chenxu Zhang,Guoxian Song,Xiaochen Zhao,Chao Liang,Jianwen Jiang,Hongyi Xu,Linjie Luo*

Main category: cs.CV

TL;DR: X-Streamer是一个多模态框架，能够通过流式输入实现数字人类的无缝视频互动。


<details>
  <summary>Details</summary>
Motivation: 开发能够实现文本、语音和视频无缝交互的数字人类代理，提升与用户的互动体验。

Method: 使用一个Thinker-Actor双变压器架构，通过流式多模态输入进行实时的开放式视频通话。

Result: X-Streamer能够运行实时视频聊天，并支持使用任意肖像进行长时间的一致视频通话。

Conclusion: X-Streamer开创了一种统一的人类世界建模方法，实现了与数字人类的长期一致性视频聊天，并为交互式数字人的发展铺平了道路。

Abstract: We introduce X-Streamer, an end-to-end multimodal human world modeling
framework for building digital human agents capable of infinite interactions
across text, speech, and video within a single unified architecture. Starting
from a single portrait, X-Streamer enables real-time, open-ended video calls
driven by streaming multimodal inputs. At its core is a Thinker-Actor
dual-transformer architecture that unifies multimodal understanding and
generation, turning a static portrait into persistent and intelligent
audiovisual interactions. The Thinker module perceives and reasons over
streaming user inputs, while its hidden states are translated by the Actor into
synchronized multimodal streams in real time. Concretely, the Thinker leverages
a pretrained large language-speech model, while the Actor employs a chunk-wise
autoregressive diffusion model that cross-attends to the Thinker's hidden
states to produce time-aligned multimodal responses with interleaved discrete
text and audio tokens and continuous video latents. To ensure long-horizon
stability, we design inter- and intra-chunk attentions with time-aligned
multimodal positional embeddings for fine-grained cross-modality alignment and
context retention, further reinforced by chunk-wise diffusion forcing and
global identity referencing. X-Streamer runs in real time on two A100 GPUs,
sustaining hours-long consistent video chat experiences from arbitrary
portraits and paving the way toward unified world modeling of interactive
digital humans.

</details>


### [41] [What Happens Next? Anticipating Future Motion by Generating Point Trajectories](https://arxiv.org/abs/2509.21592)
*Gabrijel Boduljak,Laurynas Karazija,Iro Laina,Christian Rupprecht,Andrea Vedaldi*

Main category: cs.CV

TL;DR: 本研究提出了一种新方法，通过条件生成运动轨迹，从单张图像中有效预测物体运动，超越了现有技术在物理场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 在无法观测物体速度或施加力的情况下，单张图像预测物体运动的能力尚未得到充分开发。

Method: 采用现代视频生成器的架构，输出运动轨迹而不是像素，通过条件生成密集轨迹网格来实现。

Result: 我们的模型在模拟数据上表现良好，且在机器人等下游应用中效果显著，在真实世界的直观物理数据集上也展示了准确性。

Conclusion: 我们的方法在单图像运动预测中表现出色，超过了现有技术，尤其是在处理物理场景方面。

Abstract: We consider the problem of forecasting motion from a single image, i.e.,
predicting how objects in the world are likely to move, without the ability to
observe other parameters such as the object velocities or the forces applied to
them. We formulate this task as conditional generation of dense trajectory
grids with a model that closely follows the architecture of modern video
generators but outputs motion trajectories instead of pixels. This approach
captures scene-wide dynamics and uncertainty, yielding more accurate and
diverse predictions than prior regressors and generators. We extensively
evaluate our method on simulated data, demonstrate its effectiveness on
downstream applications such as robotics, and show promising accuracy on
real-world intuitive physics datasets. Although recent state-of-the-art video
generators are often regarded as world models, we show that they struggle with
forecasting motion from a single image, even in simple physical scenarios such
as falling blocks or mechanical object interactions, despite fine-tuning on
such data. We show that this limitation arises from the overhead of generating
pixels rather than directly modeling motion.

</details>


### [42] [Temporal vs. Spatial: Comparing DINOv3 and V-JEPA2 Feature Representations for Video Action Analysis](https://arxiv.org/abs/2509.21595)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.CV

TL;DR: 本研究比较了DINOv3和V-JEPA2在视频动作识别中的表现，发现它们各有优势，适用于不同任务。


<details>
  <summary>Details</summary>
Motivation: 探讨视频分析系统中架构设计选择的差异，帮助选择合适的特征提取方法。

Method: 比较分析DINOv3和V-JEPA2两种自监督学习架构在视频动作识别任务上的表现。

Result: DINOv3在聚类性能和动作识别方面优于V-JEPA2，但在运动依赖行动上表现较差；V-JEPA2在各类动作上表现一致，兼具可靠性。

Conclusion: DINOv3和V-JEPA2在视频动作识别中的自监督学习架构具有各自的优缺点，适用于不同的任务需求和可靠性约束。

Abstract: This study presents a comprehensive comparative analysis of two prominent
self-supervised learning architectures for video action recognition: DINOv3,
which processes frames independently through spatial feature extraction, and
V-JEPA2, which employs joint temporal modeling across video sequences. We
evaluate both approaches on the UCF Sports dataset, examining feature quality
through multiple dimensions including classification accuracy, clustering
performance, intra-class consistency, and inter-class discrimination. Our
analysis reveals fundamental architectural trade-offs: DINOv3 achieves superior
clustering performance (Silhouette score: 0.31 vs 0.21) and demonstrates
exceptional discrimination capability (6.16x separation ratio) particularly for
pose-identifiable actions, while V-JEPA2 exhibits consistent reliability across
all action types with significantly lower performance variance (0.094 vs
0.288). Through action-specific evaluation, we identify that DINOv3's spatial
processing architecture excels at static pose recognition but shows degraded
performance on motion-dependent actions, whereas V-JEPA2's temporal modeling
provides balanced representation quality across diverse action categories.
These findings contribute to the understanding of architectural design choices
in video analysis systems and provide empirical guidance for selecting
appropriate feature extraction methods based on task requirements and
reliability constraints.

</details>


### [43] [VLCE: A Knowledge-Enhanced Framework for Image Description in Disaster Assessment](https://arxiv.org/abs/2509.21609)
*Md. Mahfuzur Rahman,Kishor Datta Gupta,Marufa Kamal,Fahad Rahman,Sunzida Siddique,Ahmed Rafi Hasan,Mohd Ariful Haque,Roy George*

Main category: cs.CV

TL;DR: VLCE是一种新型的多模态系统，能从自然灾害图像中自动生成详尽描述，显著提高了损害评估的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 在自然灾害后，传统评估手段慢且危险，而现有计算机视觉方法所提供的信息有限，因此需要开发能够更全面理解灾害图像的系统。

Method: 使用双重架构的方法，包括基于ResNet50的CNN-LSTM模型和预训练的Vision Transformer (ViT)模型，结合外部语义知识库提升描述准确性。

Result: VLCE在灾害图像语义描述的准确性和信息丰富性上显著优于领先的视觉-语言模型，最大InfoMetIC得分为95.33%。

Conclusion: VLCE系统在灾害损害评估方面具有显著潜力，能够从卫星和无人机照片中自动生成可操作且信息丰富的描述。

Abstract: Immediate damage assessment is essential after natural catastrophes; yet,
conventional hand evaluation techniques are sluggish and perilous. Although
satellite and unmanned aerial vehicle (UAV) photos offer extensive perspectives
of impacted regions, current computer vision methodologies generally yield just
classification labels or segmentation masks, so constraining their capacity to
deliver a thorough situational comprehension. We introduce the Vision Language
Caption Enhancer (VLCE), a multimodal system designed to produce comprehensive,
contextually-informed explanations of disaster imagery. VLCE employs a
dual-architecture approach: a CNN-LSTM model with a ResNet50 backbone
pretrained on EuroSat satellite imagery for the xBD dataset, and a Vision
Transformer (ViT) model pretrained on UAV pictures for the RescueNet dataset.
Both systems utilize external semantic knowledge from ConceptNet and WordNet to
expand vocabulary coverage and improve description accuracy. We assess VLCE in
comparison to leading vision-language models (LLaVA and QwenVL) utilizing
CLIPScore for semantic alignment and InfoMetIC for caption informativeness.
Experimental findings indicate that VLCE markedly surpasses baseline models,
attaining a maximum of 95.33% on InfoMetIC while preserving competitive
semantic alignment. Our dual-architecture system demonstrates significant
potential for improving disaster damage assessment by automating the production
of actionable, information-dense descriptions from satellite and drone photos.

</details>


### [44] [A Data-driven Typology of Vision Models from Integrated Representational Metrics](https://arxiv.org/abs/2509.21628)
*Jialin Wu,Shreya Saha,Yiqing Bo,Meenakshi Khosla*

Main category: cs.CV

TL;DR: 通过表示相似度度量和相似性网络融合的方法，作者对不同视觉模型的特征进行了深入分析，发现它们的表示结构受架构和训练目标共同影响。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉模型在架构和训练范式上的异同，寻找有效的方法来识别其表示的共同特征与独特计算策略。

Method: 利用一套表示相似度度量标准和相似性网络融合（SNF）方法，评估视觉模型之间的差异和特征。

Result: 这种方法在分离模型家族方面表现优秀，揭示了几种模型的群体归属和意外的模式。

Conclusion: 本文提出了一种生物启发的方法，对不同视觉模型的表示进行了分类，显示建筑和训练目标共同影响表示结构。

Abstract: Large vision models differ widely in architecture and training paradigm, yet
we lack principled methods to determine which aspects of their representations
are shared across families and which reflect distinctive computational
strategies. We leverage a suite of representational similarity metrics, each
capturing a different facet-geometry, unit tuning, or linear decodability-and
assess family separability using multiple complementary measures. Metrics
preserving geometry or tuning (e.g., RSA, Soft Matching) yield strong family
discrimination, whereas flexible mappings such as Linear Predictivity show
weaker separation. These findings indicate that geometry and tuning carry
family-specific signatures, while linearly decodable information is more
broadly shared. To integrate these complementary facets, we adapt Similarity
Network Fusion (SNF), a method inspired by multi-omics integration. SNF
achieves substantially sharper family separation than any individual metric and
produces robust composite signatures. Clustering of the fused similarity matrix
recovers both expected and surprising patterns: supervised ResNets and ViTs
form distinct clusters, yet all self-supervised models group together across
architectural boundaries. Hybrid architectures (ConvNeXt, Swin) cluster with
masked autoencoders, suggesting convergence between architectural modernization
and reconstruction-based training. This biology-inspired framework provides a
principled typology of vision models, showing that emergent computational
strategies-shaped jointly by architecture and training objective-define
representational structure beyond surface design categories.

</details>


### [45] [FantasyWorld: Geometry-Consistent World Modeling via Unified Video and 3D Prediction](https://arxiv.org/abs/2509.21657)
*Yixiang Dai,Fan Jiang,Chiyu Wang,Mu Xu,Yonggang Qi*

Main category: cs.CV

TL;DR: 本文提出了FantasyWorld框架，通过几何增强技术提升视频基础模型的三维感知能力，展示了其在多视图一致性和风格一致性方面的优越性。


<details>
  <summary>Details</summary>
Motivation: 高质量的3D世界模型对具身智能和人工通用智能 (AGI) 至关重要，可以推动增强现实和虚拟现实内容创作及机器人导航等应用的发展。

Method: 提出FantasyWorld框架，通过可训练的几何分支增强冻结的视频基础模型，实现视频潜在表示与隐式3D场的联合建模。

Result: FantasyWorld在多视图一致性和风格一致性方面优于最新的几何一致基线，通过统一的主干和跨分支信息交换实现了这些性能提升。

Conclusion: FantasyWorld有效地桥接了视频想象与三维感知，优于最新的几何一致基线，在多视图一致性和风格一致性方面表现出色。

Abstract: High-quality 3D world models are pivotal for embodied intelligence and
Artificial General Intelligence (AGI), underpinning applications such as AR/VR
content creation and robotic navigation. Despite the established strong
imaginative priors, current video foundation models lack explicit 3D grounding
capabilities, thus being limited in both spatial consistency and their utility
for downstream 3D reasoning tasks. In this work, we present FantasyWorld, a
geometry-enhanced framework that augments frozen video foundation models with a
trainable geometric branch, enabling joint modeling of video latents and an
implicit 3D field in a single forward pass. Our approach introduces
cross-branch supervision, where geometry cues guide video generation and video
priors regularize 3D prediction, thus yielding consistent and generalizable
3D-aware video representations. Notably, the resulting latents from the
geometric branch can potentially serve as versatile representations for
downstream 3D tasks such as novel view synthesis and navigation, without
requiring per-scene optimization or fine-tuning. Extensive experiments show
that FantasyWorld effectively bridges video imagination and 3D perception,
outperforming recent geometry-consistent baselines in multi-view coherence and
style consistency. Ablation studies further confirm that these gains stem from
the unified backbone and cross-branch information exchange.

</details>


### [46] [MORPH: Shape-agnostic PDE Foundation Models](https://arxiv.org/abs/2509.21670)
*Mahindra Singh Rautela,Alexander Most,Siddharth Mansingh,Bradley C. Love,Ayan Biswas,Diane Oyen,Earl Lawrence*

Main category: cs.CV

TL;DR: MORPH是一个处理异构PDE数据的基础模型，展现出优于传统模型的零-shot和全-shot泛化能力。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够有效学习处理异构和多模态科学观测数据的基础模型，以应对复杂的偏微分方程任务。

Method: MORPH使用卷积视觉变换器骨干，通过组件卷积、跨场交叉注意力和轴向注意力来处理不同维度和分辨率的PDE数据。

Result: MORPH在多项下游预测任务中表现优秀，超越了从头训练的模型，具备零-shot和全-shot泛化能力。

Conclusion: MORPH展示了在处理异构时空数据集方面的强大能力，并在科学机器学习中提供了灵活而强大的基础模型。

Abstract: We introduce MORPH, a shape-agnostic, autoregressive foundation model for
partial differential equations (PDEs). MORPH is built on a convolutional vision
transformer backbone that seamlessly handles heterogeneous spatiotemporal
datasets of varying data dimensionality (1D--3D) at different resolutions,
multiple fields with mixed scalar and vector components. The architecture
combines (i) component-wise convolution, which jointly processes scalar and
vector channels to capture local interactions, (ii) inter-field
cross-attention, which models and selectively propagates information between
different physical fields, (iii) axial attentions, which factorizes full
spatiotemporal self-attention along individual spatial and temporal axes to
reduce computational burden while retaining expressivity. We pretrain multiple
model variants on a diverse collection of heterogeneous PDE datasets and
evaluate transfer to a range of downstream prediction tasks. Using both
full-model fine-tuning and parameter-efficient low-rank adapters (LoRA), MORPH
outperforms models trained from scratch in both zero-shot and full-shot
generalization. Across extensive evaluations, MORPH matches or surpasses strong
baselines and recent state-of-the-art models. Collectively, these capabilities
present a flexible and powerful backbone for learning from heterogeneous and
multimodal nature of scientific observations, charting a path toward scalable
and data-efficient scientific machine learning.

</details>


### [47] [MS-YOLO: Infrared Object Detection for Edge Deployment via MobileNetV4 and SlideLoss](https://arxiv.org/abs/2509.21696)
*Jiali Zhang,Thomas S. White,Haoliang Zhang,Wenqing Hu,Donald C. Wunsch II,Jian Liu*

Main category: cs.CV

TL;DR: 本研究提出MS-YOLO模型，通过采用MobileNetV4和SlideLoss，有效提升了红外成像在低光环境下的对象检测性能，同时减少了计算负担，适合于实时应用。


<details>
  <summary>Details</summary>
Motivation: 在低光和不良天气条件下，红外成像在城市对象检测中展现了出色的优势，但存在类别不平衡、热噪声和计算约束等挑战。

Method: 通过在FLIR ADAS V2数据集上评估多种YOLO变体，选择YOLOv8作为基准，并提出了基于MobileNetV4和SlideLoss的新模型MS-YOLO。

Result: 实验结果表明，MS-YOLO在准确性和效率上相比YOLOv8有显著提升，且通过新提出的SlideLoss增强了对欠表示和遮挡样本的检测。

Conclusion: MS-YOLO在低光和恶劣天气条件下测试实现了竞争力的mAP和卓越的精度，同时计算性能仅为6.7 GFLOPs，适合实时边缘部署。

Abstract: Infrared imaging has emerged as a robust solution for urban object detection
under low-light and adverse weather conditions, offering significant advantages
over traditional visible-light cameras. However, challenges such as class
imbalance, thermal noise, and computational constraints can significantly
hinder model performance in practical settings. To address these issues, we
evaluate multiple YOLO variants on the FLIR ADAS V2 dataset, ultimately
selecting YOLOv8 as our baseline due to its balanced accuracy and efficiency.
Building on this foundation, we present \texttt{MS-YOLO} (\textbf{M}obileNetv4
and \textbf{S}lideLoss based on YOLO), which replaces YOLOv8's CSPDarknet
backbone with the more efficient MobileNetV4, reducing computational overhead
by \textbf{1.5%} while sustaining high accuracy. In addition, we introduce
\emph{SlideLoss}, a novel loss function that dynamically emphasizes
under-represented and occluded samples, boosting precision without sacrificing
recall. Experiments on the FLIR ADAS V2 benchmark show that \texttt{MS-YOLO}
attains competitive mAP and superior precision while operating at only
\textbf{6.7 GFLOPs}. These results demonstrate that \texttt{MS-YOLO}
effectively addresses the dual challenge of maintaining high detection quality
while minimizing computational costs, making it well-suited for real-time edge
deployment in urban environments.

</details>


### [48] [Motion-Aware Transformer for Multi-Object Tracking](https://arxiv.org/abs/2509.21715)
*Xu Yang,Gady Agam*

Main category: cs.CV

TL;DR: MATR通过显式预测对象运动，提高多目标跟踪的准确性，取得了在多个标准数据集上的优异成绩。


<details>
  <summary>Details</summary>
Motivation: 解决现有DETR框架在处理检测与跟踪请求时产生的冲突问题，提升关联准确性。

Method: 提出一种新的MATR（Motion-Aware Transformer）框架，提前预测对象在帧之间的运动来更新跟踪查询，自显著减少查询冲突。

Result: 在DanceTrack、SportsMOT和BDD100k数据集上，MATR在标准指标上表现显著提升，尤其在DanceTrack上提高了9个点，并达到了71.3的新状态时刻。

Conclusion: MATR通过显式建模运动，显著提升多目标跟踪性能，是一种有效的解决方案。

Abstract: Multi-object tracking (MOT) in videos remains challenging due to complex
object motions and crowded scenes. Recent DETR-based frameworks offer
end-to-end solutions but typically process detection and tracking queries
jointly within a single Transformer Decoder layer, leading to conflicts and
degraded association accuracy. We introduce the Motion-Aware Transformer
(MATR), which explicitly predicts object movements across frames to update
track queries in advance. By reducing query collisions, MATR enables more
consistent training and improves both detection and association. Extensive
experiments on DanceTrack, SportsMOT, and BDD100k show that MATR delivers
significant gains across standard metrics. On DanceTrack, MATR improves HOTA by
more than 9 points over MOTR without additional data and reaches a new
state-of-the-art score of 71.3 with supplementary data. MATR also achieves
state-of-the-art results on SportsMOT (72.2 HOTA) and BDD100k (54.7 mTETA, 41.6
mHOTA) without relying on external datasets. These results demonstrate that
explicitly modeling motion within end-to-end Transformers offers a simple yet
highly effective approach to advancing multi-object tracking.

</details>


### [49] [DeLiVR: Differential Spatiotemporal Lie Bias for Efficient Video Deraining](https://arxiv.org/abs/2509.21719)
*Shuning Sun,Jialang Lu,Xiang Chen,Jichao Wang,Dianjie Lu,Guijuan Zhang,Guangwei Gao,Zhuoran Zheng*

Main category: cs.CV

TL;DR: DeLiVR是一种高效的视频去雨方法，通过注入时空Lie群偏差来增强视频建模的一致性，显著提高了去雨效果。


<details>
  <summary>Details</summary>
Motivation: 视频在拍摄过程中常受到雨 streaks、模糊和噪声的影响，现有方法依赖于光流或启发式对齐，计算成本高且鲁棒性差，因此有必要寻找更有效的方法。

Method: 通过在网络的注意力分数中直接注入时空Lie群微分偏差，结合旋转限制的Lie相对偏差和微分群位移来进行视频去雨。

Result: 实验结果表明，DeLiVR在公共基准测试中表现出色，具有卓越的性能。

Conclusion: DeLiVR方法在公共基准上展现出有效的视频去雨效果。

Abstract: Videos captured in the wild often suffer from rain streaks, blur, and noise.
In addition, even slight changes in camera pose can amplify cross-frame
mismatches and temporal artifacts. Existing methods rely on optical flow or
heuristic alignment, which are computationally expensive and less robust. To
address these challenges, Lie groups provide a principled way to represent
continuous geometric transformations, making them well-suited for enforcing
spatial and temporal consistency in video modeling. Building on this insight,
we propose DeLiVR, an efficient video deraining method that injects
spatiotemporal Lie-group differential biases directly into attention scores of
the network. Specifically, the method introduces two complementary components.
First, a rotation-bounded Lie relative bias predicts the in-plane angle of each
frame using a compact prediction module, where normalized coordinates are
rotated and compared with base coordinates to achieve geometry-consistent
alignment before feature aggregation. Second, a differential group displacement
computes angular differences between adjacent frames to estimate a velocity.
This bias computation combines temporal decay and attention masks to focus on
inter-frame relationships while precisely matching the direction of rain
streaks. Extensive experimental results demonstrate the effectiveness of our
method on publicly available benchmarks.

</details>


### [50] [On the Status of Foundation Models for SAR Imagery](https://arxiv.org/abs/2509.21722)
*Nathan Inkawhich*

Main category: cs.CV

TL;DR: 本研究探讨了自监督学习在合成孔径雷达目标识别中的应用，微调了多个视觉基础模型，取得显著成果，开辟了未来的发展方向。


<details>
  <summary>Details</summary>
Motivation: 受到自然图像领域中基础AI/ML模型的成功案例启发，旨在将这一技术应用于合成孔径雷达(SAR)目标识别任务。

Method: 通过自监督学习微调技术，对现有的视觉基础模型进行实验和分析，特别是针对SAR数据。

Result: 通过微调公共的自监督学习模型，成功训练了多个AFRL-DINOv2模型，设定了SAR基础模型的新状态，显著超越了当前最佳的SAR领域模型SARATR-X。

Conclusion: 本研究展示了自监督微调技术在SAR目标识别中的潜力，同时为未来的发展指明了方向。

Abstract: In this work we investigate the viability of foundational AI/ML models for
Synthetic Aperture Radar (SAR) object recognition tasks. We are inspired by the
tremendous progress being made in the wider community, particularly in the
natural image domain where frontier labs are training huge models on web-scale
datasets with unprecedented computing budgets. It has become clear that these
models, often trained with Self-Supervised Learning (SSL), will transform how
we develop AI/ML solutions for object recognition tasks - they can be adapted
downstream with very limited labeled data, they are more robust to many forms
of distribution shift, and their features are highly transferable
out-of-the-box. For these reasons and more, we are motivated to apply this
technology to the SAR domain. In our experiments we first run tests with
today's most powerful visual foundational models, including DINOv2, DINOv3 and
PE-Core and observe their shortcomings at extracting semantically-interesting
discriminative SAR target features when used off-the-shelf. We then show that
Self-Supervised finetuning of publicly available SSL models with SAR data is a
viable path forward by training several AFRL-DINOv2s and setting a new
state-of-the-art for SAR foundation models, significantly outperforming today's
best SAR-domain model SARATR-X. Our experiments further analyze the performance
trade-off of using different backbones with different downstream
task-adaptation recipes, and we monitor each model's ability to overcome
challenges within the downstream environments (e.g., extended operating
conditions and low amounts of labeled data). We hope this work will inform and
inspire future SAR foundation model builders, because despite our positive
results, we still have a long way to go.

</details>


### [51] [UISim: An Interactive Image-Based UI Simulator for Dynamic Mobile Environments](https://arxiv.org/abs/2509.21733)
*Jiannan Xiang,Yun Zhu,Lei Shu,Maria Wang,Lijun Yu,Gabriel Barcik,James Lyon,Srinivas Sunkara,Jindong Chen*

Main category: cs.CV

TL;DR: UISim是一种新颖的图像基础用户界面模拟器，旨在改善UI测试和AI代理的训练效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统UI测试和AI训练方法的局限性，提供一个更动态、交互的平台。

Method: 通过两阶段方法，预测下一个UI状态的抽象布局，并合成新的视觉一致性图像。

Result: UISim在生成现实一致的下一状态的UI方面超越了现有的基线方法，展示了其高保真度和潜力。

Conclusion: UISim能够有效模拟移动用户界面的动态过渡，提升UI测试和AI代理训练的效率。

Abstract: Developing and testing user interfaces (UIs) and training AI agents to
interact with them are challenging due to the dynamic and diverse nature of
real-world mobile environments. Existing methods often rely on cumbersome
physical devices or limited static analysis of screenshots, which hinders
scalable testing and the development of intelligent UI agents. We introduce
UISim, a novel image-based UI simulator that offers a dynamic and interactive
platform for exploring mobile phone environments purely from screen images. Our
system employs a two-stage method: given an initial phone screen image and a
user action, it first predicts the abstract layout of the next UI state, then
synthesizes a new, visually consistent image based on this predicted layout.
This approach enables the realistic simulation of UI transitions. UISim
provides immediate practical benefits for UI testing, rapid prototyping, and
synthetic data generation. Furthermore, its interactive capabilities pave the
way for advanced applications, such as UI navigation task planning for AI
agents. Our experimental results show that UISim outperforms end-to-end UI
generation baselines in generating realistic and coherent subsequent UI states,
highlighting its fidelity and potential to streamline UI development and
enhance AI agent training.

</details>


### [52] [LFA-Net: A Lightweight Network with LiteFusion Attention for Retinal Vessel Segmentation](https://arxiv.org/abs/2509.21738)
*Mehwish Mehmood,Ivor Spence,Muhammad Fahim*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的LFA-Net网络，改进了视网膜血管分割的效率和准确性，特别适合资源有限的临床应用。


<details>
  <summary>Details</summary>
Motivation: 轻量级视网膜血管分割对于早期诊断威胁视力和全身疾病至关重要，但当前模型在小血管分割和计算成本方面仍面临挑战。

Method: 提出了一种新型血管分割网络LFA-Net，使用了LiteFusion-Attention注意力模块，该模块结合了残差学习连接和调制注意力。

Result: 在DRIVE、STARE和CHASE_DB数据集上，LFA-Net取得了优秀的分割性能，Dice分数分别为83.28%、87.44%和84.50%，Jaccard指数为72.85%、79.31%和74.70%。

Conclusion: LFA-Net在小型设备上实现了高效的视网膜血管分割，性能优越且计算资源占用低，适用于资源受限的临床环境。

Abstract: Lightweight retinal vessel segmentation is important for the early diagnosis
of vision-threatening and systemic diseases, especially in a real-world
clinical environment with limited computational resources. Although
segmentation methods based on deep learning are improving, existing models are
still facing challenges of small vessel segmentation and high computational
costs. To address these challenges, we proposed a new vascular segmentation
network, LFA-Net, which incorporates a newly designed attention module,
LiteFusion-Attention. This attention module incorporates residual learning
connections, Vision Mamba-inspired dynamics, and modulation-based attention,
enabling the model to capture local and global context efficiently and in a
lightweight manner. LFA-Net offers high performance with 0.11 million
parameters, 0.42 MB memory size, and 4.46 GFLOPs, which make it ideal for
resource-constrained environments. We validated our proposed model on DRIVE,
STARE, and CHASE_DB with outstanding performance in terms of dice scores of
83.28, 87.44, and 84.50% and Jaccard indices of 72.85, 79.31, and 74.70%,
respectively. The code of LFA-Net is available online
https://github.com/Mehwish4593/LFA-Net.

</details>


### [53] [Incorporating Scene Context and Semantic Labels for Enhanced Group-level Emotion Recognition](https://arxiv.org/abs/2509.21747)
*Qing Zhu,Wangdong Guo,Qirong Mao,Xiaohua Huang,Xiuyan Shao,Wenming Zheng*

Main category: cs.CV

TL;DR: 该论文提出了一种新框架，通过整合视觉上下文和情感语义信息，以提高群体情感识别性能，并在多个数据集上取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法低估了视觉场景上下文信息和情感标签的语义信息在建模个体关系中的重要性，导致情感理解不全面。

Method: 通过结合视觉场景上下文和标签引导的语义信息，使用视觉上下文编码模块和情感语义编码模块，最后通过相似度感知交互来提升GER性能。

Result: 在三个广泛应用的GER数据集上进行实验，表明所提出的方法在与最先进方法的比较中具有竞争力。

Conclusion: 所提出的方法在三种广泛被采用的GER数据集上展现了与最先进方法相比的竞争表现，从而表明该方法的有效性。

Abstract: Group-level emotion recognition (GER) aims to identify holistic emotions
within a scene involving multiple individuals. Current existed methods
underestimate the importance of visual scene contextual information in modeling
individual relationships. Furthermore, they overlook the crucial role of
semantic information from emotional labels for complete understanding of
emotions. To address this limitation, we propose a novel framework that
incorporates visual scene context and label-guided semantic information to
improve GER performance. It involves the visual context encoding module that
leverages multi-scale scene information to diversely encode individual
relationships. Complementarily, the emotion semantic encoding module utilizes
group-level emotion labels to prompt a large language model to generate nuanced
emotion lexicons. These lexicons, in conjunction with the emotion labels, are
then subsequently refined into comprehensive semantic representations through
the utilization of a structured emotion tree. Finally, similarity-aware
interaction is proposed to align and integrate visual and semantic information,
thereby generating enhanced group-level emotion representations and
subsequently improving the performance of GER. Experiments on three widely
adopted GER datasets demonstrate that our proposed method achieves competitive
performance compared to state-of-the-art methods.

</details>


### [54] [KG-SAM: Injecting Anatomical Knowledge into Segment Anything Models via Conditional Random Fields](https://arxiv.org/abs/2509.21750)
*Yu Li,Da Chang,Xi Xiao*

Main category: cs.CV

TL;DR: KG-SAM是一个整合医学知识和不确定性估计的框架，显著提升了医疗图像分割的表现。


<details>
  <summary>Details</summary>
Motivation: 为了克服SAM在医疗成像应用中的局限性，比如模糊边界、解剖关系建模不足和缺乏不确定性量化。

Method: KG-SAM方法结合了医学知识图、能量基条件随机场（CRF）和不确定性感知融合模块。

Result: KG-SAM在多个医疗数据集上的实验结果显示，前列腺分割的平均Dice分数为82.69%，MRI和CT的腹部分割分别达到了78.05%和79.68%。

Conclusion: KG-SAM作为一种强大且具有广泛适用性的框架，可以有效提升医学图像分割的性能。

Abstract: While the Segment Anything Model (SAM) has achieved remarkable success in
image segmentation, its direct application to medical imaging remains hindered
by fundamental challenges, including ambiguous boundaries, insufficient
modeling of anatomical relationships, and the absence of uncertainty
quantification. To address these limitations, we introduce KG-SAM, a
knowledge-guided framework that synergistically integrates anatomical priors
with boundary refinement and uncertainty estimation. Specifically, KG-SAM
incorporates (i) a medical knowledge graph to encode fine-grained anatomical
relationships, (ii) an energy-based Conditional Random Field (CRF) to enforce
anatomically consistent predictions, and (iii) an uncertainty-aware fusion
module to enhance reliability in high-stakes clinical scenarios. Extensive
experiments across multi-center medical datasets demonstrate the effectiveness
of our approach: KG-SAM achieves an average Dice score of 82.69% on prostate
segmentation and delivers substantial gains in abdominal segmentation, reaching
78.05% on MRI and 79.68% on CT. These results establish KG-SAM as a robust and
generalizable framework for advancing medical image segmentation.

</details>


### [55] [UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models](https://arxiv.org/abs/2509.21760)
*Lan Chen,Yuchao Gu,Qi Mao*

Main category: cs.CV

TL;DR: 本论文提出了UniVid框架，通过微调视频生成模型，探索其在不同视觉任务间的适应性，证明其潜在的可扩展性和统一性。


<details>
  <summary>Details</summary>
Motivation: 通过研究预训练视频生成模型是否能够适应多样化的图像和视频任务，探讨更统一和可扩展的替代方案。

Method: 提出一种框架UniVid，通过微调视频扩散变换器，处理各种视觉任务，无需任务特定的修改。

Result: UniVid在跨模态推理和跨源任务中均表现出良好的泛化能力，且理解与生成任务之间的切换简单易行。

Conclusion: 预训练的视频生成模型可作为一个可扩展的统一基础框架，适用于多种视觉任务。

Abstract: Large language models, trained on extensive corpora, successfully unify
diverse linguistic tasks within a single generative framework. Inspired by
this, recent works like Large Vision Model (LVM) extend this paradigm to vision
by organizing tasks into sequential visual sentences, where visual prompts
serve as the context to guide outputs. However, such modeling requires
task-specific pre-training across modalities and sources, which is costly and
limits scalability to unseen tasks. Given that pre-trained video generation
models inherently capture temporal sequence dependencies, we explore a more
unified and scalable alternative: can a pre-trained video generation model
adapt to diverse image and video tasks? To answer this, we propose UniVid, a
framework that fine-tunes a video diffusion transformer to handle various
vision tasks without task-specific modifications. Tasks are represented as
visual sentences, where the context sequence defines both the task and the
expected output modality. We evaluate the generalization of UniVid from two
perspectives: (1) cross-modal inference with contexts composed of both images
and videos, extending beyond LVM's uni-modal setting; (2) cross-source tasks
from natural to annotated data, without multi-source pre-training. Despite
being trained solely on natural video data, UniVid generalizes well in both
settings. Notably, understanding and generation tasks can easily switch by
simply reversing the visual sentence order in this paradigm. These findings
highlight the potential of pre-trained video generation models to serve as a
scalable and unified foundation for vision modeling. Our code will be released
at https://github.com/CUC-MIPG/UniVid.

</details>


### [56] [CubistMerge: Spatial-Preserving Token Merging For Diverse ViT Backbones](https://arxiv.org/abs/2509.21764)
*Wenyi Gong,Mieszko Lis*

Main category: cs.CV

TL;DR: 本论文提出了一种有效的token合并方法，能在保持空间结构的同时提高视觉任务的性能，达到了最新的研究进展。


<details>
  <summary>Details</summary>
Motivation: 现代ViT架构在空间设计上面临挑战，现有方法未能有效保留空间结构。因此，需要一种新方法来解决token减少的问题。

Method: 提出了一种保持空间完整性的token合并方法，采用2D减小策略和空间感知合并算法。

Result: 在各种视觉任务上，方法在空间和非空间架构上均实现了最先进的结果，展示了显著的加速效果。

Conclusion: 我们的方法在各种视觉任务中表现出色，在保持空间架构的同时实现了显著的性能提升。

Abstract: Many modern ViT backbones adopt spatial architectural designs, such as window
attention, decomposed relative positional embeddings in SAM, and RoPE in
DINOv3. Such architectures impose new challenges on token reduction, as the
vast majority of existing methods fail to preserve the spatial structure these
architectures depend on. In this paper, we introduce a simple yet effective
token merging method that maintains spatial integrity, enabling seamless
compatibility with spatial architectures. We reconcile two seemingly
conflicting requirements: (i)exploiting the uneven information distribution
across the spatial layout while (ii)preserving the spatial structure
post-merging. Our approach employs (i)a 2D reduction strategy to enforce
structured token layouts, (ii)a spatial-aware merging algorithm that maintains
relative token positions, and (iii)a novel max-magnitude-per-dimension token
representation that preserves salient features. Our method demonstrates strong
performance both off-the-shelf and with fine-tuning, achieving state-of-the-art
results on spatial and non-spatial architectures across various vision tasks.
Specifically, we achieve 1.25x speedup on SAM-H with only 0.7% mIOU drop
evaluated on COCO off-the-shelf, and 1.15x speedup on DeiT-B with no top-1
accuracy drop on ImageNet within just one epoch of fine-tuning.

</details>


### [57] [Training-Free Multimodal Deepfake Detection via Graph Reasoning](https://arxiv.org/abs/2509.21774)
*Yuxin Liu,Fei Wang,Kun Li,Yiqi Nie,Junjie Chen,Yanyan Wei,Zhangling Duan,Zhaohong Jia*

Main category: cs.CV

TL;DR: GASP-ICL是一种无训练的多模态深伪检测框架，能够提升视觉语言模型的性能，解决了多模态推理中的多个挑战。


<details>
  <summary>Details</summary>
Motivation: 解决大视觉语言模型在多模态深伪检测中存在的捕捉细微伪造线索、跨模态不一致性以及任务对齐检索等挑战。

Method: 提出了一种名为GASP-ICL的无训练框架，通过图结构的自适应评分器捕捉跨样本关系，并增强视觉语言模型的多模态推理能力。

Result: 经过四种伪造类型的实验，GASP-ICL显著优于多个强基线，证明其在保持语义相关性的同时可以注入任务意识知识。

Conclusion: GASP-ICL在多模态深伪检测方面优于传统基线，即使在不进行大模型微调的情况下也能实现性能提升。

Abstract: Multimodal deepfake detection (MDD) aims to uncover manipulations across
visual, textual, and auditory modalities, thereby reinforcing the reliability
of modern information systems. Although large vision-language models (LVLMs)
exhibit strong multimodal reasoning, their effectiveness in MDD is limited by
challenges in capturing subtle forgery cues, resolving cross-modal
inconsistencies, and performing task-aligned retrieval. To this end, we propose
Guided Adaptive Scorer and Propagation In-Context Learning (GASP-ICL), a
training-free framework for MDD. GASP-ICL employs a pipeline to preserve
semantic relevance while injecting task-aware knowledge into LVLMs. We leverage
an MDD-adapted feature extractor to retrieve aligned image-text pairs and build
a candidate set. We further design the Graph-Structured Taylor Adaptive Scorer
(GSTAS) to capture cross-sample relations and propagate query-aligned signals,
producing discriminative exemplars. This enables precise selection of
semantically aligned, task-relevant demonstrations, enhancing LVLMs for robust
MDD. Experiments on four forgery types show that GASP-ICL surpasses strong
baselines, delivering gains without LVLM fine-tuning.

</details>


### [58] [Prompt-guided Representation Disentanglement for Action Recognition](https://arxiv.org/abs/2509.21783)
*Tianci Wu,Guangming Zhu,Jiang Lu,Siyuan Wang,Ning Wang,Nuoye Xiong,Zhang Liang*

Main category: cs.CV

TL;DR: ProDA是一种新型框架，通过解耦复杂场景中的特定动作，结合时空场景图和动态提示模块，提升了视频行动识别的效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多动作场景时难以建模不同对象间互作，因此我们探索解耦特定动作作为一种有效的解决方案。

Method: 通过利用时空场景图和动态提示模块引导图解析神经网络生成特定动作的表示，结合与视频适应的图解析神经网络来聚合信息。

Result: 与最先进的方法相比，我们的方法在视频行动识别实验中展现了有效性。

Conclusion: ProDA框架在多动作场景的行动识别中展现了优越的性能，成功地解耦特定动作，并有效地建模了不同行动之间的交互。

Abstract: Action recognition is a fundamental task in video understanding. Existing
methods typically extract unified features to process all actions in one video,
which makes it challenging to model the interactions between different objects
in multi-action scenarios. To alleviate this issue, we explore disentangling
any specified actions from complex scenes as an effective solution. In this
paper, we propose Prompt-guided Disentangled Representation for Action
Recognition (ProDA), a novel framework that disentangles any specified actions
from a multi-action scene. ProDA leverages Spatio-temporal Scene Graphs (SSGs)
and introduces Dynamic Prompt Module (DPM) to guide a Graph Parsing Neural
Network (GPNN) in generating action-specific representations. Furthermore, we
design a video-adapted GPNN that aggregates information using dynamic weights.
Experiments in video action recognition demonstrate the effectiveness of our
approach when compared with the state-of-the-art methods. Our code can be found
in https://github.com/iamsnaping/ProDA.git

</details>


### [59] [DeHate: A Stable Diffusion-based Multimodal Approach to Mitigate Hate Speech in Images](https://arxiv.org/abs/2509.21787)
*Dwip Dalal,Gautam Vashishtha,Anku Ranui,Aishwarya Reganti,Parth Patwa,Mohd Sarique,Chandan Gupta,Keshav Nath,Viswanatha Reddy,Vinija Jain,Aman Chadha,Amitava Das,Amit Sheth,Asif Ekbal*

Main category: cs.CV

TL;DR: 提出了一种新的多模态数据集和DeHater模型，以提高图像仇恨内容检测能力，促进社交媒体上更具伦理的AI应用。


<details>
  <summary>Details</summary>
Motivation: 应对在线有害内容的增加，以保持健康的数字环境，促进更加伦理的社交媒体应用。

Method: 利用水印的稳定增强稳定扩散技术和数字注意力分析模块（DAAM），生成仇恨注意力图并模糊图像中的仇恨元素。

Result: 开发并发布了一个用于识别数字内容中仇恨的多模态数据集，介绍了新模型DeHater，并为去仇恨共享任务提供了详细信息。

Conclusion: 本研究提出了一种新的多模态数据集及相应的模型DeHater，旨在提高图像中仇恨内容的检测和处理能力，有助于实现更具伦理的社交媒体应用。

Abstract: The rise in harmful online content not only distorts public discourse but
also poses significant challenges to maintaining a healthy digital environment.
In response to this, we introduce a multimodal dataset uniquely crafted for
identifying hate in digital content. Central to our methodology is the
innovative application of watermarked, stability-enhanced, stable diffusion
techniques combined with the Digital Attention Analysis Module (DAAM). This
combination is instrumental in pinpointing the hateful elements within images,
thereby generating detailed hate attention maps, which are used to blur these
regions from the image, thereby removing the hateful sections of the image. We
release this data set as a part of the dehate shared task. This paper also
describes the details of the shared task. Furthermore, we present DeHater, a
vision-language model designed for multimodal dehatification tasks. Our
approach sets a new standard in AI-driven image hate detection given textual
prompts, contributing to the development of more ethical AI applications in
social media.

</details>


### [60] [MIRG-RL: Multi-Image Reasoning and Grounding with Reinforcement Learning](https://arxiv.org/abs/2509.21788)
*Lihao Zheng,Jiawei Chen,Xintian Shen,Hao Ma,Tao Wei*

Main category: cs.CV

TL;DR: 本文提出了一种新的框架MIRG-RL，解决了LVLMs在跨图像推理和奖励建模方面的挑战，并在多个基准测试中取得优秀成果。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型面临缺乏跨图像推理能力和不充分的跨图像参考奖励建模等挑战。

Method: 采用两阶段的训练范式，结合监督微调和图像感知的强化学习优化，逐步提升多图像推理能力。

Result: 实验验证了MIRG-RL在多图像基础基准测试中实现了最先进的表现，特别是在跨图像推理任务中超越了前述最佳方法。

Conclusion: MIRG-RL在多图像基础的基准测试中达到尖端性能，实现了跨图像推理任务的64.82%，优于之前的方法。

Abstract: Multi-image reasoning and grounding require understanding complex cross-image
relationships at both object levels and image levels. Current Large Visual
Language Models (LVLMs) face two critical challenges: the lack of cross-image
reasoning capabilities and insufficient cross-image reference reward modeling.
To address these issues, we propose a unified framework - Multi-Image Reasoning
and Grounding with Reinforcement Learning (MIRG-RL). Specifically, our
two-stage training paradigm combines supervised fine-tuning with annotated
trajectories and image-aware reinforcement learning optimization, progressively
developing multi-image reasoning capabilities. Furthermore, we innovatively
propose a method for constructing the trajectory data, which integrates
object-level and image-level annotation information, and use this method to
generate a lightweight reasoning-enhanced dataset. To effectively resolve
cross-image ambiguities, we design an image-aware RL policy with dual reward
functions for objects and images. Experiments demonstrate that MIRG-RL achieves
state-of-the-art (SOTA) performance in multi-image grounding benchmarks,
attaining 64.82% on cross-image reasoning tasks - exceeding the previous best
method by 1%. The code and dataset have been released at
https://github.com/ZEUS2035/MIRG-RL.

</details>


### [61] [LongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE](https://arxiv.org/abs/2509.21790)
*Yu Shang,Lei Jin,Yiding Ma,Xin Zhang,Chen Gao,Wei Wu,Yong Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为LongScape的混合框架，旨在解决现有视频生成方法在长距离生成中的不稳定性，实验结果显示其在生成质量和一致性上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成技术在长时间序列生成中存在时间一致性和视觉细节不足的问题，需要一种更有效的方法来生成高质量的操作数据。

Method: 提出了一种混合框架，通过行动引导的可变长度分块机制和上下文感知的专家组合框架进行视频生成。

Result: 经过大量实验，LongScape方法在生成稳定性和一致性方面表现出色，并在生成长时间序列视频时具有较高的视觉质量。

Conclusion: LongScape方法实现了稳定且一致的长距离视频生成，克服了传统方法在生成过程中的问题。

Abstract: Video-based world models hold significant potential for generating
high-quality embodied manipulation data. However, current video generation
methods struggle to achieve stable long-horizon generation: classical
diffusion-based approaches often suffer from temporal inconsistency and visual
drift over multiple rollouts, while autoregressive methods tend to compromise
on visual detail. To solve this, we introduce LongScape, a hybrid framework
that adaptively combines intra-chunk diffusion denoising with inter-chunk
autoregressive causal generation. Our core innovation is an action-guided,
variable-length chunking mechanism that partitions video based on the semantic
context of robotic actions. This ensures each chunk represents a complete,
coherent action, enabling the model to flexibly generate diverse dynamics. We
further introduce a Context-aware Mixture-of-Experts (CMoE) framework that
adaptively activates specialized experts for each chunk during generation,
guaranteeing high visual quality and seamless chunk transitions. Extensive
experimental results demonstrate that our method achieves stable and consistent
long-horizon generation over extended rollouts. Our code is available at:
https://github.com/tsinghua-fib-lab/Longscape.

</details>


### [62] [MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation](https://arxiv.org/abs/2509.21797)
*Yu Shang,Yangcheng Yu,Xin Zhang,Xin Jin,Haisheng Su,Wei Wu,Yong Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的混合世界模型框架MoWM，结合运动感知和视觉特征，以解决机器人行动规划中的关键挑战，从而在任务成功率和泛化能力上实现了突破。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成世界模型在生成精确的行动方面受限于冗余视觉信息，而潜在世界模型则忽略了对精细细节的关注，这些细节对于精确操作至关重要。

Method: 提出了一种混合世界模型框架MoWM，通过将运动感知表示与像素空间模型的精细视觉特征结合，进行体现行动规划。

Result: 在CALVIN基准测试上的广泛评估表明，MoWM取得了领先的任务成功率和优越的泛化性能。

Conclusion: MoWM框架在任务成功率和泛化能力方面达到了最先进的水平，并提供了对特征空间优势的全面分析。

Abstract: Embodied action planning is a core challenge in robotics, requiring models to
generate precise actions from visual observations and language instructions.
While video generation world models are promising, their reliance on
pixel-level reconstruction often introduces visual redundancies that hinder
action decoding and generalization. Latent world models offer a compact,
motion-aware representation, but overlook the fine-grained details critical for
precise manipulation. To overcome these limitations, we propose MoWM, a
mixture-of-world-model framework that fuses representations from hybrid world
models for embodied action planning. Our approach uses motion-aware
representations from a latent model as a high-level prior, which guides the
extraction of fine-grained visual features from the pixel space model. This
design allows MoWM to highlight the informative visual details needed for
action decoding. Extensive evaluations on the CALVIN benchmark demonstrate that
our method achieves state-of-the-art task success rates and superior
generalization. We also provide a comprehensive analysis of the strengths of
each feature space, offering valuable insights for future research in embodied
planning. The code is available at: https://github.com/tsinghua-fib-lab/MoWM.

</details>


### [63] [DiTraj: training-free trajectory control for video diffusion transformer](https://arxiv.org/abs/2509.21839)
*Cheng Lei,Jiayu Zhang,Yue Ma,Xinyu Wang,Long Chen,Liang Tang,Yiqiang Yan,Fei Su,Zhicheng Zhao*

Main category: cs.CV

TL;DR: DiTraj是针对DiT的视频生成中的轨迹控制新方法，结合前景背景分离引导和STD-RoPE优化，显著提升了视频质量和控制能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在可控视频生成中训练资源需求高或不适用于DiT的问题，推动轨迹控制的易用性。

Method: 提出了一种简单有效的训练无关的轨迹控制框架，利用LLM进行前景与背景提示的分离，并引入STD-RoPE优化位置嵌入。

Result: 实验结果表明，DiTraj在视频质量和轨迹可控性上显著优于先前方法。

Conclusion: DiTraj在视频生成中实现了优越的轨迹控制，超越了现有方法的表现。

Abstract: Diffusion Transformers (DiT)-based video generation models with 3D full
attention exhibit strong generative capabilities. Trajectory control represents
a user-friendly task in the field of controllable video generation. However,
existing methods either require substantial training resources or are
specifically designed for U-Net, do not take advantage of the superior
performance of DiT. To address these issues, we propose DiTraj, a simple but
effective training-free framework for trajectory control in text-to-video
generation, tailored for DiT. Specifically, first, to inject the object's
trajectory, we propose foreground-background separation guidance: we use the
Large Language Model (LLM) to convert user-provided prompts into foreground and
background prompts, which respectively guide the generation of foreground and
background regions in the video. Then, we analyze 3D full attention and explore
the tight correlation between inter-token attention scores and position
embedding. Based on this, we propose inter-frame Spatial-Temporal Decoupled
3D-RoPE (STD-RoPE). By modifying only foreground tokens' position embedding,
STD-RoPE eliminates their cross-frame spatial discrepancies, strengthening
cross-frame attention among them and thus enhancing trajectory control.
Additionally, we achieve 3D-aware trajectory control by regulating the density
of position embedding. Extensive experiments demonstrate that our method
outperforms previous methods in both video quality and trajectory
controllability.

</details>


### [64] [A Comprehensive Evaluation of Transformer-Based Question Answering Models and RAG-Enhanced Design](https://arxiv.org/abs/2509.21845)
*Zichen Zhang,Kunlong Zhang,Hongwei Ruan,Yiming Luo*

Main category: cs.CV

TL;DR: 本研究提出了一种以混合检索为基础的增强生成方法，用于提升多跳问答的效果，显示出显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的模型在问答领域取得了进展，但多跳推理仍然面临挑战，因此需要更有效的检索策略。

Method: 通过综合评估不同的检索策略，包括余弦相似度、最大边际相关性和一种将稠密嵌入与词汇重叠及重排序相结合的混合方法，评估多跳问答的检索效果。

Result: 在HotpotQA数据集上的实验表明，混合方法在精准匹配和F1分数上相较于余弦相似度分别提高了50%和47%。错误分析显示混合检索在实体召回和证据互补性方面有显著改善，但在处理干扰项和时间推理上仍然有限。

Conclusion: 混合检索增强生成的方法为多跳问答提供了一种有效的零样本解决方案，兼顾了准确性、效率和可解释性。

Abstract: Transformer-based models have advanced the field of question answering, but
multi-hop reasoning, where answers require combining evidence across multiple
passages, remains difficult. This paper presents a comprehensive evaluation of
retrieval strategies for multi-hop question answering within a
retrieval-augmented generation framework. We compare cosine similarity, maximal
marginal relevance, and a hybrid method that integrates dense embeddings with
lexical overlap and re-ranking. To further improve retrieval, we adapt the
EfficientRAG pipeline for query optimization, introducing token labeling and
iterative refinement while maintaining efficiency. Experiments on the HotpotQA
dataset show that the hybrid approach substantially outperforms baseline
methods, achieving a relative improvement of 50 percent in exact match and 47
percent in F1 score compared to cosine similarity. Error analysis reveals that
hybrid retrieval improves entity recall and evidence complementarity, while
remaining limited in handling distractors and temporal reasoning. Overall, the
results suggest that hybrid retrieval-augmented generation provides a practical
zero-shot solution for multi-hop question answering, balancing accuracy,
efficiency, and interpretability.

</details>


### [65] [Dynamic Novel View Synthesis in High Dynamic Range](https://arxiv.org/abs/2509.21853)
*Kaixuan Zhang,Zhipeng Xiong,Minxian Li,Mingwu Ren,Jiankang Deng,Xiatian Zhu*

Main category: cs.CV

TL;DR: 本研究提出HDR-4DGS，解决了动态场景下的HDR新视图合成问题，显著提高了生成图像的质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 由于现实场景中的动态元素常常存在，现有静态场景的方法无法有效处理这些挑战，因此提出HDR DNVS的研究问题。

Method: 引入基于Gaussian Splatting的架构及创新的动态色调映射模块，能够维持时间辐射的一致性，进行HDR和LDR域之间的转换。

Result: 实验结果表明，HDR-4DGS在定量性能和视觉保真度上均超越了当前的最先进方法。

Conclusion: HDR-4DGS模型在HDR动态新视图合成任务中具有更好的性能，解决了静态场景下无法处理动态元素的问题。

Abstract: High Dynamic Range Novel View Synthesis (HDR NVS) seeks to learn an HDR 3D
model from Low Dynamic Range (LDR) training images captured under conventional
imaging conditions. Current methods primarily focus on static scenes,
implicitly assuming all scene elements remain stationary and non-living.
However, real-world scenarios frequently feature dynamic elements, such as
moving objects, varying lighting conditions, and other temporal events, thereby
presenting a significantly more challenging scenario. To address this gap, we
propose a more realistic problem named HDR Dynamic Novel View Synthesis (HDR
DNVS), where the additional dimension ``Dynamic'' emphasizes the necessity of
jointly modeling temporal radiance variations alongside sophisticated 3D
translation between LDR and HDR. To tackle this complex, intertwined challenge,
we introduce HDR-4DGS, a Gaussian Splatting-based architecture featured with an
innovative dynamic tone-mapping module that explicitly connects HDR and LDR
domains, maintaining temporal radiance coherence by dynamically adapting
tone-mapping functions according to the evolving radiance distributions across
the temporal dimension. As a result, HDR-4DGS achieves both temporal radiance
consistency and spatially accurate color translation, enabling photorealistic
HDR renderings from arbitrary viewpoints and time instances. Extensive
experiments demonstrate that HDR-4DGS surpasses existing state-of-the-art
methods in both quantitative performance and visual fidelity. Source code will
be released.

</details>


### [66] [SRHand: Super-Resolving Hand Images and 3D Shapes via View/Pose-aware Neural Image Representations and Explicit 3D Meshes](https://arxiv.org/abs/2509.21859)
*Minje Kim,Tae-Kyun Kim*

Main category: cs.CV

TL;DR: SRHand是一种新方法，能够从低分辨率图像重建高质量手部3D几何和纹理，超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖高分辨率图像捕获手部几何特征，对低分辨率图像泛化能力差，因此需要一种新方法进行有效重建。

Method: SRHand结合隐式图像表示和显式手部网格，引入几何感知隐式图像函数，优化隐式图像和显式3D手形状，保持多视图一致性。

Result: 在InterHand2.6M和Goliath数据集上进行的实验表明，SRHand方法在图像上采样和3D手重建方面，定量和定性上均显著超越当前最先进的技术。

Conclusion: SRHand方法在3D手部重建中表现优越，能够从低分辨率图像中重建详细的手部几何结构和纹理，表现超越现有技术。

Abstract: Reconstructing detailed hand avatars plays a crucial role in various
applications. While prior works have focused on capturing high-fidelity hand
geometry, they heavily rely on high-resolution multi-view image inputs and
struggle to generalize on low-resolution images. Multi-view image
super-resolution methods have been proposed to enforce 3D view consistency.
These methods, however, are limited to static objects/scenes with fixed
resolutions and are not applicable to articulated deformable hands. In this
paper, we propose SRHand (Super-Resolution Hand), the method for reconstructing
detailed 3D geometry as well as textured images of hands from low-resolution
images. SRHand leverages the advantages of implicit image representation with
explicit hand meshes. Specifically, we introduce a geometric-aware implicit
image function (GIIF) that learns detailed hand prior by upsampling the coarse
input images. By jointly optimizing the implicit image function and explicit 3D
hand shapes, our method preserves multi-view and pose consistency among
upsampled hand images, and achieves fine-detailed 3D reconstruction (wrinkles,
nails). In experiments using the InterHand2.6M and Goliath datasets, our method
significantly outperforms state-of-the-art image upsampling methods adapted to
hand datasets, and 3D hand reconstruction methods, quantitatively and
qualitatively. Project page: https://yunminjin2.github.io/projects/srhand

</details>


### [67] [Deepfakes: we need to re-think the concept of "real" images](https://arxiv.org/abs/2509.21864)
*Janis Keuper,Margret Keuper*

Main category: cs.CV

TL;DR: 现代图像生成技术带来的社会问题促使对'假'图像检测的研究，但对'真实'图像定义的缺乏亟待解决，呼吁重新审视图像定义并建立新数据集。


<details>
  <summary>Details</summary>
Motivation: 鉴于现代图像生成模型的普及和低易用性引发的社会担忧，关注'假'图像的检测和'真实'图像的定义显得尤为必要。

Method: 分析现有的'假'图像检测方法及其依赖的低分辨率'真实'图像数据集。

Result: 指出现有方法依赖过时的'真实'图像数据集，且需要更新对'真实'图像的定义。

Conclusion: 当前对于'真实'图像的定义存在不足，需要重新考虑以及建立新的基准数据集。

Abstract: The wide availability and low usability barrier of modern image generation
models has triggered the reasonable fear of criminal misconduct and negative
social implications. The machine learning community has been engaging this
problem with an extensive series of publications proposing algorithmic
solutions for the detection of "fake", e.g. entirely generated or partially
manipulated images. While there is undoubtedly some progress towards technical
solutions of the problem, we argue that current and prior work is focusing too
much on generative algorithms and "fake" data-samples, neglecting a clear
definition and data collection of "real" images. The fundamental question "what
is a real image?" might appear to be quite philosophical, but our analysis
shows that the development and evaluation of basically all current
"fake"-detection methods is relying on only a few, quite old low-resolution
datasets of "real" images like ImageNet. However, the technology for the
acquisition of "real" images, aka taking photos, has drastically evolved over
the last decade: Today, over 90% of all photographs are produced by smartphones
which typically use algorithms to compute an image from multiple inputs (over
time) from multiple sensors. Based on the fact that these image formation
algorithms are typically neural network architectures which are closely related
to "fake"-image generators, we state the position that today, we need to
re-think the concept of "real" images. The purpose of this position paper is to
raise the awareness of the current shortcomings in this active field of
research and to trigger an open discussion whether the detection of "fake"
images is a sound objective at all. At the very least, we need a clear
technical definition of "real" images and new benchmark datasets.

</details>


### [68] [Unlocking the Essence of Beauty: Advanced Aesthetic Reasoning with Relative-Absolute Policy Optimization](https://arxiv.org/abs/2509.21871)
*Boyang Liu,Yifan Hu,Senjie Jin,Shihan Dou,Gonglei Shi,Jie Shao,Tao Gui,Xuanjing Huang*

Main category: cs.CV

TL;DR: 本研究提出的Aes-R1框架通过强化学习和链式思维数据的应用，显著改善了多模态大型语言模型在图像美学评估中的表现。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏多模态美学推理数据以及美学判断的主观性，现有的多模态大型语言模型难以生成准确且具有可解释性的美学判断。

Method: 本研究提出了一种名为Aes-R1的美学推理框架，通过加强学习优化图像的绝对评分和相对排名，并结合链式思维数据的构建与筛选。

Result: 通过Aes-R1框架，模型在图像的评分准确性及跨图像的偏好判断上有显著提升，PLCC/SRCC平均提高了47.9%和34.8%。

Conclusion: Aes-R1显著提升了多模态大型语言模型在图像美学评估中的表现，提供了有根据的解释和可信的评分。

Abstract: Multimodal large language models (MLLMs) are well suited to image aesthetic
assessment, as they can capture high-level aesthetic features leveraging their
cross-modal understanding capacity. However, the scarcity of multimodal
aesthetic reasoning data and the inherently subjective nature of aesthetic
judgment make it difficult for MLLMs to generate accurate aesthetic judgments
with interpretable rationales. To this end, we propose Aes-R1, a comprehensive
aesthetic reasoning framework with reinforcement learning (RL). Concretely,
Aes-R1 integrates a pipeline, AesCoT, to construct and filter high-quality
chain-of-thought aesthetic reasoning data used for cold-start. After teaching
the model to generate structured explanations prior to scoring, we then employ
the Relative-Absolute Policy Optimization (RAPO), a novel RL algorithm that
jointly optimizes absolute score regression and relative ranking order,
improving both per-image accuracy and cross-image preference judgments. Aes-R1
enables MLLMs to generate grounded explanations alongside faithful scores,
thereby enhancing aesthetic scoring and reasoning in a unified framework.
Extensive experiments demonstrate that Aes-R1 improves the backbone's average
PLCC/SRCC by 47.9%/34.8%, surpassing state-of-the-art baselines of similar
size. More ablation studies validate Aes-R1's robust generalization under
limited supervision and in out-of-distribution scenarios.

</details>


### [69] [StableDub: Taming Diffusion Prior for Generalized and Efficient Visual Dubbing](https://arxiv.org/abs/2509.21887)
*Liyang Chen,Tianze Zhou,Xu He,Boshi Tang,Zhiyong Wu,Yang Huang,Yang Wu,Zhongqian Sun,Wei Yang,Helen Meng*

Main category: cs.CV

TL;DR: StableDub是一种新框架，通过结合唇习惯感知和遮挡鲁棒合成解决现有可视配音技术的不足，在多项指标上优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 当前音频驱动的可视配音任务存在模型无法捕捉发言者特定唇习惯和遮挡处理容易引发视觉伪影的两个主要缺陷，限制了实际应用。

Method: 提出了一种集成唇习惯感知建模与遮挡鲁棒合成的新框架，结合了唇习惯调制机制和遮挡感知训练策略，利用混合Mamba-Transformer架构优化高效的训练过程。

Result: 通过大规模实验结果验证，StableDub在处理唇部运动生成和遮挡重建上实现了优越的性能，有效提高了唇部同步与视觉质量。

Conclusion: StableDub在唇部习惯相似度和遮挡鲁棒性方面表现优越，能够在音频唇同步、视频质量和分辨率一致性上超越其他方法。

Abstract: The visual dubbing task aims to generate mouth movements synchronized with
the driving audio, which has seen significant progress in recent years.
However, two critical deficiencies hinder their wide application: (1)
Audio-only driving paradigms inadequately capture speaker-specific lip habits,
which fail to generate lip movements similar to the target avatar; (2)
Conventional blind-inpainting approaches frequently produce visual artifacts
when handling obstructions (e.g., microphones, hands), limiting practical
deployment. In this paper, we propose StableDub, a novel and concise framework
integrating lip-habit-aware modeling with occlusion-robust synthesis.
Specifically, building upon the Stable-Diffusion backbone, we develop a
lip-habit-modulated mechanism that jointly models phonemic audio-visual
synchronization and speaker-specific orofacial dynamics. To achieve plausible
lip geometries and object appearances under occlusion, we introduce the
occlusion-aware training strategy by explicitly exposing the occlusion objects
to the inpainting process. By incorporating the proposed designs, the model
eliminates the necessity for cost-intensive priors in previous methods, thereby
exhibiting superior training efficiency on the computationally intensive
diffusion-based backbone. To further optimize training efficiency from the
perspective of model architecture, we introduce a hybrid Mamba-Transformer
architecture, which demonstrates the enhanced applicability in low-resource
research scenarios. Extensive experimental results demonstrate that StableDub
achieves superior performance in lip habit resemblance and occlusion
robustness. Our method also surpasses other methods in audio-lip sync, video
quality, and resolution consistency. We expand the applicability of visual
dubbing methods from comprehensive aspects, and demo videos can be found at
https://stabledub.github.io.

</details>


### [70] [Drag4D: Align Your Motion with Text-Driven 3D Scene Generation](https://arxiv.org/abs/2509.21888)
*Minjun Kang,Inkyu Shin,Taeyeop Lee,In So Kweon,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: Drag4D是一个互动框架，实现文本驱动的3D场景生成，并通过三阶段流程有效控制对象运动。


<details>
  <summary>Details</summary>
Motivation: 旨在通过互动框架实现文本驱动的3D场景生成，并提高对象运动控制的效率与效果。

Method: 该框架包括3D背景生成、3D对象提取与合成及用户定义的3D轨迹下的时间动画处理。

Result: 通过针对每个阶段的评估，展示了用户控制的对象运动与3D背景之间的和谐对齐。

Conclusion: Drag4D通过三阶段流程有效地实现了用户控制的对象运动与高质量3D背景的协调对齐。

Abstract: We introduce Drag4D, an interactive framework that integrates object motion
control within text-driven 3D scene generation. This framework enables users to
define 3D trajectories for the 3D objects generated from a single image,
seamlessly integrating them into a high-quality 3D background. Our Drag4D
pipeline consists of three stages. First, we enhance text-to-3D background
generation by applying 2D Gaussian Splatting with panoramic images and
inpainted novel views, resulting in dense and visually complete 3D
reconstructions. In the second stage, given a reference image of the target
object, we introduce a 3D copy-and-paste approach: the target instance is
extracted in a full 3D mesh using an off-the-shelf image-to-3D model and
seamlessly composited into the generated 3D scene. The object mesh is then
positioned within the 3D scene via our physics-aware object position learning,
ensuring precise spatial alignment. Lastly, the spatially aligned object is
temporally animated along a user-defined 3D trajectory. To mitigate motion
hallucination and ensure view-consistent temporal alignment, we develop a
part-augmented, motion-conditioned video diffusion model that processes
multiview image pairs together with their projected 2D trajectories. We
demonstrate the effectiveness of our unified architecture through evaluations
at each stage and in the final results, showcasing the harmonized alignment of
user-controlled object motion within a high-quality 3D background.

</details>


### [71] [Syncphony: Synchronized Audio-to-Video Generation with Diffusion Transformers](https://arxiv.org/abs/2509.21893)
*Jibin Song,Mingi Kwon,Jaeseok Jeong,Youngjung Uh*

Main category: cs.CV

TL;DR: Syncphony是一种新的视频生成模型，利用音频提示提高视频与音频的同步，展示了在同步精度和视觉质量上的显著提升。


<details>
  <summary>Details</summary>
Motivation: 提高音频到视频生成中的时间控制能力，以解决现有模型在精细同步上的限制。

Method: 使用基于预训练视频骨干网的方法，结合运动感知损失和音频同步指导来改善视频与音频的同步。

Result: 在AVSync15和The Greatest Hits数据集上，Syncphony在同步精度和视觉质量方面均表现优于现有方法。

Conclusion: Syncphony在同步精度和视觉质量上超越了现有方法，展示了音频提示在视频生成中的潜力。

Abstract: Text-to-video and image-to-video generation have made rapid progress in
visual quality, but they remain limited in controlling the precise timing of
motion. In contrast, audio provides temporal cues aligned with video motion,
making it a promising condition for temporally controlled video generation.
However, existing audio-to-video (A2V) models struggle with fine-grained
synchronization due to indirect conditioning mechanisms or limited temporal
modeling capacity. We present Syncphony, which generates 380x640 resolution,
24fps videos synchronized with diverse audio inputs. Our approach builds upon a
pre-trained video backbone and incorporates two key components to improve
synchronization: (1) Motion-aware Loss, which emphasizes learning at
high-motion regions; (2) Audio Sync Guidance, which guides the full model using
a visually aligned off-sync model without audio layers to better exploit audio
cues at inference while maintaining visual quality. To evaluate
synchronization, we propose CycleSync, a video-to-audio-based metric that
measures the amount of motion cues in the generated video to reconstruct the
original audio. Experiments on AVSync15 and The Greatest Hits datasets
demonstrate that Syncphony outperforms existing methods in both synchronization
accuracy and visual quality. Project page is available at:
https://jibin86.github.io/syncphony_project_page

</details>


### [72] [LG-CD: Enhancing Language-Guided Change Detection through SAM2 Adaptation](https://arxiv.org/abs/2509.21894)
*Yixiao Liu,Yizhou Yang,Jinwen Li,Jun Tao,Ruoyu Li,Xiangkun Wang,Min Zhu,Junlong Cheng*

Main category: cs.CV

TL;DR: LG-CD模型通过结合语言提示和视觉特征提取，实现了遥感变化检测的突破，显著提高了检测精度。


<details>
  <summary>Details</summary>
Motivation: 旨在克服现有深度学习方法对单一视觉信息的依赖，挖掘多模态数据（如文本）所提供的丰富语义信息。

Method: 提出了一种语言引导的变化检测模型LG-CD，利用自然语言提示结合视觉基础模型和多层适配器。

Result: LG-CD在LEVIR-CD、WHU-CD和SYSU-CD三个数据集上的实验显示了其在变化检测任务上优于现有方法的表现。

Conclusion: LG-CD模型显著提升了遥感变化检测的准确性和鲁棒性，能够有效整合多模态信息。

Abstract: Remote Sensing Change Detection (RSCD) typically identifies changes in land
cover or surface conditions by analyzing multi-temporal images. Currently, most
deep learning-based methods primarily focus on learning unimodal visual
information, while neglecting the rich semantic information provided by
multimodal data such as text. To address this limitation, we propose a novel
Language-Guided Change Detection model (LG-CD). This model leverages natural
language prompts to direct the network's attention to regions of interest,
significantly improving the accuracy and robustness of change detection.
Specifically, LG-CD utilizes a visual foundational model (SAM2) as a feature
extractor to capture multi-scale pyramid features from high-resolution to
low-resolution across bi-temporal remote sensing images. Subsequently,
multi-layer adapters are employed to fine-tune the model for downstream tasks,
ensuring its effectiveness in remote sensing change detection. Additionally, we
design a Text Fusion Attention Module (TFAM) to align visual and textual
information, enabling the model to focus on target change regions using text
prompts. Finally, a Vision-Semantic Fusion Decoder (V-SFD) is implemented,
which deeply integrates visual and semantic information through a
cross-attention mechanism to produce highly accurate change detection masks.
Our experiments on three datasets (LEVIR-CD, WHU-CD, and SYSU-CD) demonstrate
that LG-CD consistently outperforms state-of-the-art change detection methods.
Furthermore, our approach provides new insights into achieving generalized
change detection by leveraging multimodal information.

</details>


### [73] [TDEdit: A Unified Diffusion Framework for Text-Drag Guided Image Manipulation](https://arxiv.org/abs/2509.21905)
*Qihang Wang,Yaxiong Wang,Lechao Cheng,Zhun Zhong*

Main category: cs.CV

TL;DR: 本论文提出了一种新颖的扩散基础框架，实现了文本与拖动联合控制的图像编辑，克服了现有方法的局限，展现出极高的灵活性和编辑质量。


<details>
  <summary>Details</summary>
Motivation: 解决文本驱动和拖动驱动编辑方法在纹理和形状控制方面的互补局限性，提供一种更高效的图像编辑解决方案。

Method: 采用扩散基础框架，结合点云确定性拖动和拖动文本引导去噪技术，提升了图像编辑的灵活性和精确性。

Result: 实验证明，该方法在联合文本和拖动编辑方面表现出色，并且在各自模式下的性能与专门的文本或拖动方法相媲美。

Conclusion: 本研究提出了一种统一的扩散基础框架，支持文本和拖动交互联合控制的图像编辑，该框架实现了高保真度的联合编辑效果，并在多种编辑模式下表现优异。

Abstract: This paper explores image editing under the joint control of text and drag
interactions. While recent advances in text-driven and drag-driven editing have
achieved remarkable progress, they suffer from complementary limitations:
text-driven methods excel in texture manipulation but lack precise spatial
control, whereas drag-driven approaches primarily modify shape and structure
without fine-grained texture guidance. To address these limitations, we propose
a unified diffusion-based framework for joint drag-text image editing,
integrating the strengths of both paradigms. Our framework introduces two key
innovations: (1) Point-Cloud Deterministic Drag, which enhances latent-space
layout control through 3D feature mapping, and (2) Drag-Text Guided Denoising,
dynamically balancing the influence of drag and text conditions during
denoising. Notably, our model supports flexible editing modes - operating with
text-only, drag-only, or combined conditions - while maintaining strong
performance in each setting. Extensive quantitative and qualitative experiments
demonstrate that our method not only achieves high-fidelity joint editing but
also matches or surpasses the performance of specialized text-only or drag-only
approaches, establishing a versatile and generalizable solution for
controllable image manipulation. Code will be made publicly available to
reproduce all results presented in this work.

</details>


### [74] [Enhancing Vehicle Detection under Adverse Weather Conditions with Contrastive Learning](https://arxiv.org/abs/2509.21916)
*Boying Li,Chang Liu,Petter Kyösti,Mattias Öhman,Devashish Singha Roy,Sofia Plazzi,Hamam Mokayed,Olle Hagner*

Main category: cs.CV

TL;DR: 该研究提出了一种利用未标注数据改善车辆检测的框架，显著提高了检测精度。


<details>
  <summary>Details</summary>
Motivation: 在北欧地区，UAV图像中车辆检测面临可见性挑战和领域转移，同时标注数据昂贵，未标注数据易于获得。

Method: 通过对未标注数据进行对比学习训练CNN表示提取器，然后在微调阶段将其加载到冻结的YOLO11n骨干网络中。

Result: 在NVD数据集上，提出的模型在mAP50方面提高了3.8%到9.5%。

Conclusion: 提出的sideload-CL-adaptation框架显著提高了车辆检测的性能，尤其是在多雪覆盖的北欧地区.

Abstract: Aside from common challenges in remote sensing like small, sparse targets and
computation cost limitations, detecting vehicles from UAV images in the Nordic
regions faces strong visibility challenges and domain shifts caused by diverse
levels of snow coverage. Although annotated data are expensive, unannotated
data is cheaper to obtain by simply flying the drones. In this work, we
proposed a sideload-CL-adaptation framework that enables the use of unannotated
data to improve vehicle detection using lightweight models. Specifically, we
propose to train a CNN-based representation extractor through contrastive
learning on the unannotated data in the pretraining stage, and then sideload it
to a frozen YOLO11n backbone in the fine-tuning stage. To find a robust
sideload-CL-adaptation, we conducted extensive experiments to compare various
fusion methods and granularity. Our proposed sideload-CL-adaptation model
improves the detection performance by 3.8% to 9.5% in terms of mAP50 on the NVD
dataset.

</details>


### [75] [Taming Flow-based I2V Models for Creative Video Editing](https://arxiv.org/abs/2509.21917)
*Xianghao Kong,Hansheng Chen,Yuwei Guo,Lvmin Zhang,Gordon Wetzstein,Maneesh Agrawala,Anyi Rao*

Main category: cs.CV

TL;DR: 我们提出了一种先进的视频编辑方法IF-V2V，克服了传统方法的局限，实现了高质量的编辑效果。


<details>
  <summary>Details</summary>
Motivation: 尽管图像编辑技术已有显著进展，但视频编辑仍然是一个新兴挑战。

Method: 提出了一种无需反演的方法，结合了样本偏差的矢量场校正和保持结构与运动初始化，以实现视频编辑。

Result: 通过评估，我们的方法展示了卓越的编辑质量和一致性。

Conclusion: 我们的方法在编辑质量和一致性方面优于现有技术，提供了一种轻量级的即插即用解决方案以实现视觉创意。

Abstract: Although image editing techniques have advanced significantly, video editing,
which aims to manipulate videos according to user intent, remains an emerging
challenge. Most existing image-conditioned video editing methods either require
inversion with model-specific design or need extensive optimization, limiting
their capability of leveraging up-to-date image-to-video (I2V) models to
transfer the editing capability of image editing models to the video domain. To
this end, we propose IF-V2V, an Inversion-Free method that can adapt
off-the-shelf flow-matching-based I2V models for video editing without
significant computational overhead. To circumvent inversion, we devise Vector
Field Rectification with Sample Deviation to incorporate information from the
source video into the denoising process by introducing a deviation term into
the denoising vector field. To further ensure consistency with the source video
in a model-agnostic way, we introduce Structure-and-Motion-Preserving
Initialization to generate motion-aware temporally correlated noise with
structural information embedded. We also present a Deviation Caching mechanism
to minimize the additional computational cost for denoising vector
rectification without significantly impacting editing quality. Evaluations
demonstrate that our method achieves superior editing quality and consistency
over existing approaches, offering a lightweight plug-and-play solution to
realize visual creativity.

</details>


### [76] [Multi-View Crowd Counting With Self-Supervised Learning](https://arxiv.org/abs/2509.21918)
*Hong Mo,Xiong Zhang,Tengfei Shi,Zhongbo Wu*

Main category: cs.CV

TL;DR: SSLCounter是一种自监督学习的多视角计数框架，通过神经体积渲染降低了对大规模标注数据的依赖，并展现出在数据效率和性能上的优势。


<details>
  <summary>Details</summary>
Motivation: 传统的多视角计数方法主要基于完全监督学习，通常需要大量标注数据，这限制了其应用。

Method: 提出了一种新的自监督学习框架SSLCounter，结合神经体积渲染，以减少对大规模标注数据集的依赖。

Result: SSLCounter通过学习场景的隐含表示，实现了几何形状的连续重建及其2D投影的复杂视角依赖外观，并在多个MVC基准上展示了最先进的性能。

Conclusion: SSLCounter在多视角计数任务中表现优越，且以70%的训练数据达到了竞争力的效果，展现出卓越的数据效率。

Abstract: Multi-view counting (MVC) methods have attracted significant research
attention and stimulated remarkable progress in recent years. Despite their
success, most MVC methods have focused on improving performance by following
the fully supervised learning (FSL) paradigm, which often requires large
amounts of annotated data. In this work, we propose SSLCounter, a novel
self-supervised learning (SSL) framework for MVC that leverages neural
volumetric rendering to alleviate the reliance on large-scale annotated
datasets. SSLCounter learns an implicit representation w.r.t. the scene,
enabling the reconstruction of continuous geometry shape and the complex,
view-dependent appearance of their 2D projections via differential neural
rendering. Owing to its inherent flexibility, the key idea of our method can be
seamlessly integrated into exsiting frameworks. Notably, extensive experiments
demonstrate that SSLCounter not only demonstrates state-of-the-art performances
but also delivers competitive performance with only using 70% proportion of
training data, showcasing its superior data efficiency across multiple MVC
benchmarks.

</details>


### [77] [Spatial Reasoning in Foundation Models: Benchmarking Object-Centric Spatial Understanding](https://arxiv.org/abs/2509.21922)
*Vahid Mirjalili,Ramin Giahi,Sriram Kollipara,Akshay Kekuda,Kehui Yao,Kai Zhao,Jianpeng Xu,Kaushiki Nag,Sinduja Subramaniam,Topojoy Biswas,Evren Korpeoglu,Kannan Achan*

Main category: cs.CV

TL;DR: 本研究通过评估现有视觉和视觉语言模型，揭示了空间定位与真正空间理解之间的差距，并呼吁开发更具空间感知能力的基础模型。


<details>
  <summary>Details</summary>
Motivation: 空间理解是视觉基础模型的重要能力，目前的评估基准多集中于定位准确性，而忽视了对物体关系和位置的推理需求。

Method: 通过使用受控的合成数据集，对多种先进视觉模型和大型视觉语言模型进行评估，涵盖空间定位、空间推理和下游检索任务。

Result: 检测器通常提供精确的边界框但在关系推理上的能力有限，而视觉语言模型则在粗略布局和流畅描述上表现较好，但难以处理细致的空间上下文。

Conclusion: 在目标中心空间推理方面，当前的视觉基础模型尚存在局限，因此需要更加关注空间理解能力的模型开发。

Abstract: Spatial understanding is a critical capability for vision foundation models.
While recent advances in large vision models or vision-language models (VLMs)
have expanded recognition capabilities, most benchmarks emphasize localization
accuracy rather than whether models capture how objects are arranged and
related within a scene. This gap is consequential; effective scene
understanding requires not only identifying objects, but reasoning about their
relative positions, groupings, and depth. In this paper, we present a
systematic benchmark for object-centric spatial reasoning in foundation models.
Using a controlled synthetic dataset, we evaluate state-of-the-art vision
models (e.g., GroundingDINO, Florence-2, OWLv2) and large VLMs (e.g., InternVL,
LLaVA, GPT-4o) across three tasks: spatial localization, spatial reasoning, and
downstream retrieval tasks. We find a stable trade-off: detectors such as
GroundingDINO and OWLv2 deliver precise boxes with limited relational
reasoning, while VLMs like SmolVLM and GPT-4o provide coarse layout cues and
fluent captions but struggle with fine-grained spatial context. Our study
highlights the gap between localization and true spatial understanding, and
pointing toward the need for spatially-aware foundation models in the
community.

</details>


### [78] [PANICL: Mitigating Over-Reliance on Single Prompt in Visual In-Context Learning](https://arxiv.org/abs/2509.21926)
*Jiahao Zhang,Bowen Wang,Hong Liu,Yuta Nakashima,Hajime Nagahara*

Main category: cs.CV

TL;DR: 本论文提出了一种新框架PANICL，通过利用多个上下文对，改善了视频上下文学习的偏见和不稳定性，展示出在多个视觉任务上的强大性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: VICL面临过度依赖单一上下文对的问题，导致预测结果的偏见和不稳定性，亟需一种新方法来解决这一缺陷。

Method: PANICL是一种基于补丁的k最近邻视觉上下文学习框架，采用多种上下文对进行评分平滑，从而降低偏差，无需额外的训练。

Result: PANICL在前景分割、单目标检测、色彩化、多目标分割和关键点检测等多种任务上表现出持续的改进，并且对域迁移具有很强的鲁棒性。

Conclusion: PANICL通过利用多个上下文对缓解了VICL中过度依赖单一上下文对的问题，从而提高了视觉任务的预测准确性和稳定性。

Abstract: Visual In-Context Learning (VICL) uses input-output image pairs, referred to
as in-context pairs (or examples), as prompts alongside query images to guide
models in performing diverse vision tasks. However, VICL often suffers from
over-reliance on a single in-context pair, which can lead to biased and
unstable predictions. We introduce PAtch-based $k$-Nearest neighbor visual
In-Context Learning (PANICL), a general training-free framework that mitigates
this issue by leveraging multiple in-context pairs. PANICL smooths assignment
scores across pairs, reducing bias without requiring additional training.
Extensive experiments on a variety of tasks, including foreground segmentation,
single object detection, colorization, multi-object segmentation, and keypoint
detection, demonstrate consistent improvements over strong baselines. Moreover,
PANICL exhibits strong robustness to domain shifts, including dataset-level
shift (e.g., from COCO to Pascal) and label-space shift (e.g., FSS-1000), and
generalizes well to other VICL models such as SegGPT, Painter, and LVM,
highlighting its versatility and broad applicability.

</details>


### [79] [SingRef6D: Monocular Novel Object Pose Estimation with a Single RGB Reference](https://arxiv.org/abs/2509.21927)
*Jiahui Wang,Haiyue Zhu,Haoren Guo,Abdullah Al Mamun,Cheng Xiang,Tong Heng Lee*

Main category: cs.CV

TL;DR: SingRef6D是一个高效的6D姿态估计方法，仅需单个RGB图像，克服了低光和反射表面下的挑战，超越了现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前的6D姿态估计方法在传感器深度和低光照环境下依赖性强，因此提出了一种不需要深度传感器和多视图图像采集的方法。

Method: 提出了一种轻量级管道，仅需单个RGB图像作为参考，包含基于token-scaler的微调机制和深度感知匹配过程。

Result: 在REAL275数据集上，深度预测的14.41%提升以及在多个数据集上的6.1%平均召回率的改进。

Conclusion: SingRef6D在多个数据集上超越了最新的方法，显著提高了6D姿态估计的性能。

Abstract: Recent 6D pose estimation methods demonstrate notable performance but still
face some practical limitations. For instance, many of them rely heavily on
sensor depth, which may fail with challenging surface conditions, such as
transparent or highly reflective materials. In the meantime, RGB-based
solutions provide less robust matching performance in low-light and
texture-less scenes due to the lack of geometry information. Motivated by
these, we propose SingRef6D, a lightweight pipeline requiring only a single RGB
image as a reference, eliminating the need for costly depth sensors, multi-view
image acquisition, or training view synthesis models and neural fields. This
enables SingRef6D to remain robust and capable even under resource-limited
settings where depth or dense templates are unavailable. Our framework
incorporates two key innovations. First, we propose a token-scaler-based
fine-tuning mechanism with a novel optimization loss on top of Depth-Anything
v2 to enhance its ability to predict accurate depth, even for challenging
surfaces. Our results show a 14.41% improvement (in $\delta_{1.05}$) on REAL275
depth prediction compared to Depth-Anything v2 (with fine-tuned head). Second,
benefiting from depth availability, we introduce a depth-aware matching process
that effectively integrates spatial relationships within LoFTR, enabling our
system to handle matching for challenging materials and lighting conditions.
Evaluations of pose estimation on the REAL275, ClearPose, and Toyota-Light
datasets show that our approach surpasses state-of-the-art methods, achieving a
6.1% improvement in average recall.

</details>


### [80] [DynaNav: Dynamic Feature and Layer Selection for Efficient Visual Navigation](https://arxiv.org/abs/2509.21930)
*Jiahui Wang,Changhao Chen*

Main category: cs.CV

TL;DR: DynaNav是一个动态视觉导航框架，通过优化特征选择和计算资源使用，在资源紧张场景中显著提高了效率和导航性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有基础模型在计算开销和可解释性方面的问题，以支持在资源紧张的场景中视觉导航。

Method: 提出DynaNav框架，通过训练的硬特征选择器和早期退出机制结合贝叶斯优化来优化特征选择和计算资源使用。

Result: DynaNav较ViNT在FLOPs、推理时间和内存使用上分别减少了2.26倍、42.3%和32.8%，同时在导航性能上有所改善。

Conclusion: DynaNav通过动态特征和层选择显著提高了视觉导航的效率和解释性，并减少了计算成本，同时在多个公共数据集上提升了导航性能。

Abstract: Visual navigation is essential for robotics and embodied AI. However,
existing foundation models, particularly those with transformer decoders,
suffer from high computational overhead and lack interpretability, limiting
their deployment in resource-tight scenarios. To address this, we propose
DynaNav, a Dynamic Visual Navigation framework that adapts feature and layer
selection based on scene complexity. It employs a trainable hard feature
selector for sparse operations, enhancing efficiency and interpretability.
Additionally, we integrate feature selection into an early-exit mechanism, with
Bayesian Optimization determining optimal exit thresholds to reduce
computational cost. Extensive experiments in real-world-based datasets and
simulated environments demonstrate the effectiveness of DynaNav. Compared to
ViNT, DynaNav achieves a 2.26x reduction in FLOPs, 42.3% lower inference time,
and 32.8% lower memory usage, while improving navigation performance across
four public datasets.

</details>


### [81] [SemanticControl: A Training-Free Approach for Handling Loosely Aligned Visual Conditions in ControlNet](https://arxiv.org/abs/2509.21938)
*Woosung Joung,Daewon Chae,Jinkyu Kim*

Main category: cs.CV

TL;DR: SemanticControl提出了一种无需训练的方法，能有效利用不完全对齐的视觉条件，提升文本到图像生成的质量。


<details>
  <summary>Details</summary>
Motivation: 在现有控制模型无法处理不完全对齐的视觉条件下，本研究旨在改善文本到图像生成的效果，尤其是对于不寻常或富有想象力的场景。

Method: 提出了一种自适应的方法，通过辅助去噪过程来提取信息掩模，提高文本引导效应。

Result: 实验结果显示，SemanticControl在多个条件下优于现有基准，特别是在处理深度图、边缘图和人类骨架时的表现。

Conclusion: SemanticControl能有效利用不完全对齐但语义相关的视觉条件，改善文本到图像生成模型的性能。

Abstract: ControlNet has enabled detailed spatial control in text-to-image diffusion
models by incorporating additional visual conditions such as depth or edge
maps. However, its effectiveness heavily depends on the availability of visual
conditions that are precisely aligned with the generation goal specified by
text prompt-a requirement that often fails in practice, especially for uncommon
or imaginative scenes. For example, generating an image of a cat cooking in a
specific pose may be infeasible due to the lack of suitable visual conditions.
In contrast, structurally similar cues can often be found in more common
settings-for instance, poses of humans cooking are widely available and can
serve as rough visual guides. Unfortunately, existing ControlNet models
struggle to use such loosely aligned visual conditions, often resulting in low
text fidelity or visual artifacts. To address this limitation, we propose
SemanticControl, a training-free method for effectively leveraging misaligned
but semantically relevant visual conditions. Our approach adaptively suppresses
the influence of the visual condition where it conflicts with the prompt, while
strengthening guidance from the text. The key idea is to first run an auxiliary
denoising process using a surrogate prompt aligned with the visual condition
(e.g., "a human playing guitar" for a human pose condition) to extract
informative attention masks, and then utilize these masks during the denoising
of the actual target prompt (e.g., cat playing guitar). Experimental results
demonstrate that our method improves performance under loosely aligned
conditions across various conditions, including depth maps, edge maps, and
human skeletons, outperforming existing baselines. Our code is available at
https://mung3477.github.io/semantic-control.

</details>


### [82] [Customizing Visual Emotion Evaluation for MLLMs: An Open-vocabulary, Multifaceted, and Scalable Approach](https://arxiv.org/abs/2509.21950)
*Daiqing Wu,Dongbao Yang,Sicheng Zhao,Can Ma,Yu Zhou*

Main category: cs.CV

TL;DR: 本研究提出了一种情感陈述判断任务，旨在改善多模态大语言模型（MLLMs）在情感理解评估中的不足，并展示了现有模型与人类在情感理解上的性能差距。


<details>
  <summary>Details</summary>
Motivation: 探讨现有多模态大语言模型在图像情感感知中的表现及其评估方法的局限性。

Method: 提出了一项情感陈述判断任务，并开发了一个自动化流程来高效构建情感中心的陈述。

Result: 系统评估结果显示，现有模型在情感解释和基于情境的情感判断方面表现较强，但在理解感知主观性方面存在相对局限，且相比人类仍存在显著的性能差距。

Conclusion: 本研究开发了一个基础评估框架，并对现有的多模态大语言模型进行了系统评估，揭示了情感理解中的关键差距，为未来的改进指明了方向。

Abstract: Recently, Multimodal Large Language Models (MLLMs) have achieved exceptional
performance across diverse tasks, continually surpassing previous expectations
regarding their capabilities. Nevertheless, their proficiency in perceiving
emotions from images remains debated, with studies yielding divergent results
in zero-shot scenarios. We argue that this inconsistency stems partly from
constraints in existing evaluation methods, including the oversight of
plausible responses, limited emotional taxonomies, neglect of contextual
factors, and labor-intensive annotations. To facilitate customized visual
emotion evaluation for MLLMs, we propose an Emotion Statement Judgment task
that overcomes these constraints. Complementing this task, we devise an
automated pipeline that efficiently constructs emotion-centric statements with
minimal human effort. Through systematically evaluating prevailing MLLMs, our
study showcases their stronger performance in emotion interpretation and
context-based emotion judgment, while revealing relative limitations in
comprehending perception subjectivity. When compared to humans, even
top-performing MLLMs like GPT4o demonstrate remarkable performance gaps,
underscoring key areas for future improvement. By developing a fundamental
evaluation framework and conducting a comprehensive MLLM assessment, we hope
this work contributes to advancing emotional intelligence in MLLMs. Project
page: https://github.com/wdqqdw/MVEI.

</details>


### [83] [MultiCrafter: High-Fidelity Multi-Subject Generation via Spatially Disentangled Attention and Identity-Aware Reinforcement Learning](https://arxiv.org/abs/2509.21953)
*Tao Wu,Yibo Jiang,Yehao Lu,Zhizhong Wang,Zeyi Huang,Zequn Qin,Xi Li*

Main category: cs.CV

TL;DR: 提出了一种名为MultiCrafter的框架，以解决多主题图像生成中的属性泄漏和人类偏好对齐问题，通过显式位置监督和在线强化学习显著提升生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于上下文学习的方法由于依赖简单的重建目标，导致属性泄漏和与人类偏好的不对齐。

Method: 我们提出了MultiCrafter框架，通过引入显式位置监督减轻属性泄漏，采用专家混合架构提升模型能力，并设计新的在线强化学习框架与人类偏好对齐。

Result: 实验证明，多线程生成框架在主题保真度和与人类偏好的对齐方面有显著提升。

Conclusion: 我们的框架显著提高了主题的保真度，并更好地与人类偏好对齐。

Abstract: Multi-subject image generation aims to synthesize user-provided subjects in a
single image while preserving subject fidelity, ensuring prompt consistency,
and aligning with human aesthetic preferences. However, existing methods,
particularly those built on the In-Context-Learning paradigm, are limited by
their reliance on simple reconstruction-based objectives, leading to both
severe attribute leakage that compromises subject fidelity and failing to align
with nuanced human preferences. To address this, we propose MultiCrafter, a
framework that ensures high-fidelity, preference-aligned generation. First, we
find that the root cause of attribute leakage is a significant entanglement of
attention between different subjects during the generation process. Therefore,
we introduce explicit positional supervision to explicitly separate attention
regions for each subject, effectively mitigating attribute leakage. To enable
the model to accurately plan the attention region of different subjects in
diverse scenarios, we employ a Mixture-of-Experts architecture to enhance the
model's capacity, allowing different experts to focus on different scenarios.
Finally, we design a novel online reinforcement learning framework to align the
model with human preferences, featuring a scoring mechanism to accurately
assess multi-subject fidelity and a more stable training strategy tailored for
the MoE architecture. Experiments validate that our framework significantly
improves subject fidelity while aligning with human preferences better.

</details>


### [84] [PartSAM: A Scalable Promptable Part Segmentation Model Trained on Native 3D Data](https://arxiv.org/abs/2509.21965)
*Zhe Zhu,Le Wan,Rui Xu,Yiheng Zhang,Honghua Chen,Zhiyang Dou,Cheng Lin,Yuan Liu,Mingqiang Wei*

Main category: cs.CV

TL;DR: PartSAM是一个新颖的3D部件分割模型，通过直接训练于大规模3D数据，提升了部件识别和形状分解的准确性，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 旨在克服传统2D模型在3D物体分割中的局限性，特别是在捕捉内在几何和实现开放世界能力方面。

Method: 采用编码器-解码器架构，并结合三平面双分支编码器生成空间结构化的标记，支持可扩展的部件感知表示学习。

Result: PartSAM通过大规模3D数据和模型引导注释管道，提供了精细的标签，并在部件识别和形状分解上取得了优秀的结果。

Conclusion: PartSAM在3D物体部件分割任务中展示了显著的性能提升，超越了当前最先进的方法，标志着3D部件理解基础模型的重要进展。

Abstract: Segmenting 3D objects into parts is a long-standing challenge in computer
vision. To overcome taxonomy constraints and generalize to unseen 3D objects,
recent works turn to open-world part segmentation. These approaches typically
transfer supervision from 2D foundation models, such as SAM, by lifting
multi-view masks into 3D. However, this indirect paradigm fails to capture
intrinsic geometry, leading to surface-only understanding, uncontrolled
decomposition, and limited generalization. We present PartSAM, the first
promptable part segmentation model trained natively on large-scale 3D data.
Following the design philosophy of SAM, PartSAM employs an encoder-decoder
architecture in which a triplane-based dual-branch encoder produces spatially
structured tokens for scalable part-aware representation learning. To enable
large-scale supervision, we further introduce a model-in-the-loop annotation
pipeline that curates over five million 3D shape-part pairs from online assets,
providing diverse and fine-grained labels. This combination of scalable
architecture and diverse 3D data yields emergent open-world capabilities: with
a single prompt, PartSAM achieves highly accurate part identification, and in a
Segment-Every-Part mode, it automatically decomposes shapes into both surface
and internal structures. Extensive experiments show that PartSAM outperforms
state-of-the-art methods by large margins across multiple benchmarks, marking a
decisive step toward foundation models for 3D part understanding. Our code and
model will be released soon.

</details>


### [85] [No-Reference Image Contrast Assessment with Customized EfficientNet-B0](https://arxiv.org/abs/2509.21967)
*Javad Hassannataj Joloudari,Bita Mesbahzadeh,Omid Zare,Emrah Arslan,Roohallah Alizadehsani,Hossein Moosaei*

Main category: cs.CV

TL;DR: 本研究提出了一种改进的无参考对比度质量评估方法，EfficientNet B0模型在捕获感知对比度失真能力上表现优于传统方法，适用于资源受限的实时应用。


<details>
  <summary>Details</summary>
Motivation: 现有的无参考图像质量评估模型在真实场景下评估对比度失真时能力有限，因此亟需开发更有效的评估方法。

Method: 通过定制和微调三种预训练架构（EfficientNet B0、ResNet18、MobileNetV2）和一个基于西门子网络的附加模型，采用对比度感知回归头和针对性数据增强进行端到端训练。

Result: 在CID2013和CCID2014基准数据集上，定制的EfficientNet B0模型表现出色，超越了传统方法和其他深度基线，体现了模型在捕获感知对比度失真方面的强大能力。

Conclusion: 该研究提供了一种基于深度学习的无参考对比度质量评估框架，定制的EfficientNet B0模型在对比度失真捕获方面表现优异，适合实时和资源受限的应用。

Abstract: Image contrast was a fundamental factor in visual perception and played a
vital role in overall image quality. However, most no reference image quality
assessment NR IQA models struggled to accurately evaluate contrast distortions
under diverse real world conditions. In this study, we proposed a deep learning
based framework for blind contrast quality assessment by customizing and
fine-tuning three pre trained architectures, EfficientNet B0, ResNet18, and
MobileNetV2, for perceptual Mean Opinion Score, along with an additional model
built on a Siamese network, which indicated a limited ability to capture
perceptual contrast distortions. Each model is modified with a contrast-aware
regression head and trained end to end using targeted data augmentations on two
benchmark datasets, CID2013 and CCID2014, containing synthetic and authentic
contrast distortions. Performance is evaluated using Pearson Linear Correlation
Coefficient and Spearman Rank Order Correlation Coefficient, which assess the
alignment between predicted and human rated scores. Among these three models,
our customized EfficientNet B0 model achieved state-of-the-art performance with
PLCC = 0.9286 and SRCC = 0.9178 on CCID2014 and PLCC = 0.9581 and SRCC = 0.9369
on CID2013, surpassing traditional methods and outperforming other deep
baselines. These results highlighted the models robustness and effectiveness in
capturing perceptual contrast distortion. Overall, the proposed method
demonstrated that contrast aware adaptation of lightweight pre trained networks
can yield a high performing, scalable solution for no reference contrast
quality assessment suitable for real time and resource constrained
applications.

</details>


### [86] [Geo-R1: Improving Few-Shot Geospatial Referring Expression Understanding with Reinforcement Fine-Tuning](https://arxiv.org/abs/2509.21976)
*Zilun Zhang,Zian Guan,Tiancheng Zhao,Haozhan Shen,Tianyu Li,Yuxiang Cai,Zhonggen Su,Zhaojun Liu,Jianwei Yin,Xiang Li*

Main category: cs.CV

TL;DR: 本文提出Geo-R1，一种适用于少样本地理指称理解的推理中心强化微调方法，显著提升模型的推理能力和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺的情境下，现有的多模态大语言模型通过监督微调难以有效泛化。

Method: 提出了一种推理中心的强化微调（RFT）范式，首先生成可解释的推理链，然后利用这些推理来定位目标对象。

Result: Geo-R1在三个专门设计的少样本地理指称基准上显著超越了监督微调基线，并展示了较强的跨数据集泛化能力。

Conclusion: Geo-R1在少样本遥感指称理解任务中表现优异，且具有较强的跨数据集泛化能力。

Abstract: Referring expression understanding in remote sensing poses unique challenges,
as it requires reasoning over complex object-context relationships. While
supervised fine-tuning (SFT) on multimodal large language models achieves
strong performance with massive labeled datasets, they struggle in data-scarce
scenarios, leading to poor generalization. To address this limitation, we
propose Geo-R1, a reasoning-centric reinforcement fine-tuning (RFT) paradigm
for few-shot geospatial referring. Geo-R1 enforces the model to first generate
explicit, interpretable reasoning chains that decompose referring expressions,
and then leverage these rationales to localize target objects. This "reason
first, then act" process enables the model to make more effective use of
limited annotations, enhances generalization, and provides interpretability. We
validate Geo-R1 on three carefully designed few-shot geospatial referring
benchmarks, where our model consistently and substantially outperforms SFT
baselines. It also demonstrates strong cross-dataset generalization,
highlighting its robustness. Code and data will be released at
http://geo-r1.github.io.

</details>


### [87] [Benchmarking and Mitigate Psychological Sycophancy in Medical Vision-Language Models](https://arxiv.org/abs/2509.21979)
*Zikun Guo,Xinyue Xu,Pei Xiang,Shu Yang,Xin Han,Di Wang,Lijie Hu*

Main category: cs.CV

TL;DR: 本研究通过一个新基准评估医疗视觉语言模型中的阿谀奉承行为，提出一种减轻策略（VIPER），强调需要以证据为基础的防御。


<details>
  <summary>Details</summary>
Motivation: 通过一个新的临床基准评估医疗视觉问答中的阿谀奉承行为，以应对视觉语言模型在医学工作流程中的偏见和弱点。

Method: 通过使用心理学动机的压力模板进行对抗实验，评估了医疗视觉问答中的临床阿谀奉承行为。

Result: 我们的实验发现，这些模型通常表现出显著的脆弱性，阿谀奉承反应的发生具有显著变化，与模型的准确性或规模之间相关性较弱。提出的VIPER框架通过过滤非证据内容，减少了阿谀奉承的发生，表现超过基线，同时保持可解释性。

Conclusion: 我们的基准分析和减轻框架为医疗视觉语言模型在现实临床交互中的稳健部署奠定了基础，强调了以证据为基础的防御的必要性。

Abstract: Vision language models(VLMs) are increasingly integrated into clinical
workflows, but they often exhibit sycophantic behavior prioritizing alignment
with user phrasing social cues or perceived authority over evidence based
reasoning. This study evaluate clinical sycophancy in medical visual question
answering through a novel clinically grounded benchmark. We propose a medical
sycophancy dataset construct from PathVQA, SLAKE, and VQA-RAD stratified by
different type organ system and modality. Using psychologically motivated
pressure templates including various sycophancy. In our adversarial experiments
on various VLMs, we found that these models are generally vulnerable,
exhibiting significant variations in the occurrence of adversarial responses,
with weak correlations to the model accuracy or size. Imitation and expert
provided corrections were found to be the most effective triggers, suggesting
that the models possess a bias mechanism independent of visual evidence. To
address this, we propose Visual Information Purification for Evidence based
Response (VIPER) a lightweight mitigation strategy that filters non evidentiary
content for example social pressures and then generates constrained evidence
first answers. This framework reduces sycophancy by an average amount
outperforming baselines while maintaining interpretability. Our benchmark
analysis and mitigation framework lay the groundwork for robust deployment of
medical VLMs in real world clinician interactions emphasizing the need for
evidence anchored defenses.

</details>


### [88] [Resolving Ambiguity in Gaze-Facilitated Visual Assistant Interaction Paradigm](https://arxiv.org/abs/2509.21980)
*Zeyu Wang,Baiyu Chen,Kun Yan,Hongjing Piao,Hao Xue,Flora D. Salim,Yuanchun Shi,Yuntao Wang*

Main category: cs.CV

TL;DR: GLARIFY利用时空注视信息提升多模态查询的准确性，显著改善视觉助理的交互体验。


<details>
  <summary>Details</summary>
Motivation: 随着智能眼镜的普及，集成用户注意力以优化多模态查询，但用户的注视模式存在歧义和噪声。

Method: 提出GLARIFY方法，分析用户注视数据的噪声，设计GLARIFY-Ambi数据集并构建热图模块融入注意力信息。

Result: 实验结果显示GLARIFY在有效性上显著优于基线模型。

Conclusion: GLARIFY显著优于基线，能够更有效地与用户注意力对齐，推动视觉助理的可用性与直观交互。

Abstract: With the rise in popularity of smart glasses, users' attention has been
integrated into Vision-Language Models (VLMs) to streamline multi-modal
querying in daily scenarios. However, leveraging gaze data to model users'
attention may introduce ambiguity challenges: (1) users' verbal questions
become ambiguous by using pronouns or skipping context, (2) humans' gaze
patterns can be noisy and exhibit complex spatiotemporal relationships with
their spoken questions. Previous works only consider single image as visual
modality input, failing to capture the dynamic nature of the user's attention.
In this work, we introduce GLARIFY, a novel method to leverage spatiotemporal
gaze information to enhance the model's effectiveness in real-world
applications. Initially, we analyzed hundreds of querying samples with the gaze
modality to demonstrate the noisy nature of users' gaze patterns. We then
utilized GPT-4o to design an automatic data synthesis pipeline to generate the
GLARIFY-Ambi dataset, which includes a dedicated chain-of-thought (CoT) process
to handle noisy gaze patterns. Finally, we designed a heatmap module to
incorporate gaze information into cutting-edge VLMs while preserving their
pretrained knowledge. We evaluated GLARIFY using a hold-out test set.
Experiments demonstrate that GLARIFY significantly outperforms baselines. By
robustly aligning VLMs with human attention, GLARIFY paves the way for a usable
and intuitive interaction paradigm with a visual assistant.

</details>


### [89] [From Bias to Balance: Exploring and Mitigating Spatial Bias in LVLMs](https://arxiv.org/abs/2509.21984)
*Yingjie Zhu,Xuefeng Bai,Kehai Chen,Yang Xiang,Weili Guan,Jun Yu,Min Zhang*

Main category: cs.CV

TL;DR: 本文系统研究了大型视觉语言模型(LVLMs)在空间偏差下的表现，并提出平衡位置分配（BaPA）机制来增强其空间鲁棒性和整体性能。


<details>
  <summary>Details</summary>
Motivation: 研究大型视觉语言模型(LVLMs)在空间变化下的表现，揭示其空间-语义理解的基本局限性。

Method: 提出了平衡位置分配（BaPA）机制，该机制为所有图像标记分配相同的位置嵌入，从而促进视觉信息的更平衡整合。

Result: BaPA机制能够在不重新训练的情况下增强LVLMs的空间鲁棒性，并在轻量级微调后进一步提高其在多模态基准测试上的性能。

Conclusion: 通过引入平衡位置分配（BaPA）机制，可以增强大型视觉语言模型(LVLMs)在空间上的鲁棒性，并提高其在多模态基准测试中的表现。

Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable success across
a wide range of multimodal tasks, yet their robustness to spatial variations
remains insufficiently understood. In this work, we present a systematic study
of the spatial bias of LVLMs, focusing on how models respond when identical key
visual information is placed at different locations within an image. Through a
carefully designed probing dataset, we demonstrate that current LVLMs often
produce inconsistent outputs under such spatial shifts, revealing a fundamental
limitation in their spatial-semantic understanding. Further analysis shows that
this phenomenon originates not from the vision encoder, which reliably
perceives and interprets visual content across positions, but from the
unbalanced design of position embeddings in the language model component. In
particular, the widely adopted position embedding strategies, such as RoPE,
introduce imbalance during cross-modal interaction, leading image tokens at
different positions to exert unequal influence on semantic understanding. To
mitigate this issue, we introduce Balanced Position Assignment (BaPA), a simple
yet effective mechanism that assigns identical position embeddings to all image
tokens, promoting a more balanced integration of visual information. Extensive
experiments show that BaPA enhances the spatial robustness of LVLMs without
retraining and further boosts their performance across diverse multimodal
benchmarks when combined with lightweight fine-tuning. Further analysis of
information flow reveals that BaPA yields balanced attention, enabling more
holistic visual understanding.

</details>


### [90] [Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation](https://arxiv.org/abs/2509.21989)
*Abdelrahman Eldesokey,Aleksandar Cvejic,Bernard Ghanem,Peter Wonka*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，解耦预训练扩散模型的视觉和语义特征，并引入了视觉语义匹配度量，首次实现了对图像生成中的不一致性的定量和定位。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏注释数据集，隔离扩散模型中视觉特征的挑战促使我们开发一种自动化流程，用于基于现有生成数据集构建图像对，从而实现语义和视觉对应的注释。

Method: 引入了一种对比架构，将视觉和语义特征从预训练扩散模型的基础结构中解耦，并提出了一种新的度量方法，视觉语义匹配（VSM），用于量化视觉不一致性。

Result: 通过实证结果表明，我们的方法在定量视觉不一致性方面优于CLIP、DINO和视觉-语言模型，同时能够实现不一致区域的空间定位。

Conclusion: 我们的方法在主导图像生成中首次实现了一致性定量与定位的支撑，提供了促进该任务的有价值工具。

Abstract: We propose a novel approach for disentangling visual and semantic features
from the backbones of pre-trained diffusion models, enabling visual
correspondence in a manner analogous to the well-established semantic
correspondence. While diffusion model backbones are known to encode
semantically rich features, they must also contain visual features to support
their image synthesis capabilities. However, isolating these visual features is
challenging due to the absence of annotated datasets. To address this, we
introduce an automated pipeline that constructs image pairs with annotated
semantic and visual correspondences based on existing subject-driven image
generation datasets, and design a contrastive architecture to separate the two
feature types. Leveraging the disentangled representations, we propose a new
metric, Visual Semantic Matching (VSM), that quantifies visual inconsistencies
in subject-driven image generation. Empirical results show that our approach
outperforms global feature-based metrics such as CLIP, DINO, and
vision--language models in quantifying visual inconsistencies while also
enabling spatial localization of inconsistent regions. To our knowledge, this
is the first method that supports both quantification and localization of
inconsistencies in subject-driven generation, offering a valuable tool for
advancing this task. Project
Page:https://abdo-eldesokey.github.io/mind-the-glitch/

</details>


### [91] [ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models](https://arxiv.org/abs/2509.21991)
*Jewon Lee,Wooksu Shin,Seungmin Yang,Ki-Ung Song,DongUk Lim,Jaeyeon Kim,Tae-Ho Kim,Bo-Kyeong Kim*

Main category: cs.CV

TL;DR: ERGO是一种高效的视觉语言模型，通过粗到细的推理流程和新颖的奖励机制，实现了在减少计算成本的同时提高推理准确率。


<details>
  <summary>Details</summary>
Motivation: 高分辨率图像在现实视觉语言应用中的高效处理具有重要意义，而现有大规模视觉语言模型因视觉标记数量多而面临计算开销巨大的问题。

Method: 采用两阶段的粗到细推理管道，将下采样图像分析与全分辨率裁剪相结合，并在强化学习框架中开发有效的奖励组件。

Result: ERGO在多个数据集上的准确率超越了原始模型及其他竞争方法，例如在V*基准上优于Qwen2.5-VL-7B 4.7分，且仅使用23%的视觉标记，实现了三倍的推理加速。

Conclusion: ERGO通过有效的粗到细推理流程和奖励机制，在多个数据集上实现了更高的准确率和效率。

Abstract: Efficient processing of high-resolution images is crucial for real-world
vision-language applications. However, existing Large Vision-Language Models
(LVLMs) incur substantial computational overhead due to the large number of
vision tokens. With the advent of "thinking with images" models, reasoning now
extends beyond text to the visual domain. This capability motivates our
two-stage "coarse-to-fine" reasoning pipeline: first, a downsampled image is
analyzed to identify task-relevant regions; then, only these regions are
cropped at full resolution and processed in a subsequent reasoning stage. This
approach reduces computational cost while preserving fine-grained visual
details where necessary. A major challenge lies in inferring which regions are
truly relevant to a given query. Recent related methods often fail in the first
stage after input-image downsampling, due to perception-driven reasoning, where
clear visual information is required for effective reasoning. To address this
issue, we propose ERGO (Efficient Reasoning & Guided Observation) that performs
reasoning-driven perception-leveraging multimodal context to determine where to
focus. Our model can account for perceptual uncertainty, expanding the cropped
region to cover visually ambiguous areas for answering questions. To this end,
we develop simple yet effective reward components in a reinforcement learning
framework for coarse-to-fine perception. Across multiple datasets, our approach
delivers higher accuracy than the original model and competitive methods, with
greater efficiency. For instance, ERGO surpasses Qwen2.5-VL-7B on the V*
benchmark by 4.7 points while using only 23% of the vision tokens, achieving a
3x inference speedup. The code and models can be found at:
https://github.com/nota-github/ERGO.

</details>


### [92] [DualFocus: Depth from Focus with Spatio-Focal Dual Variational Constraints](https://arxiv.org/abs/2509.21992)
*Sungmin Woo,Sangyoun Lee*

Main category: cs.CV

TL;DR: DualFocus是一种新颖的深度从聚焦框架，通过联合建模聚焦变化，提升了复杂场景中的深度估计准确性和稳健性。


<details>
  <summary>Details</summary>
Motivation: 针对现有学习方法在复杂场景中遇到的聚焦提示模糊或误导的问题，提出了一种新的解决方案。

Method: 提出了一种新颖的深度从聚焦框架DualFocus，采用双重约束的变分形式，联合建模空间和焦距维度的焦距变化。

Result: 在四个公共数据集上的全面实验表明，DualFocus在深度准确性和感知质量上始终优于现有方法。

Conclusion: DualFocus在深度准确性和感知质量上超越了最先进的方法。

Abstract: Depth-from-Focus (DFF) enables precise depth estimation by analyzing focus
cues across a stack of images captured at varying focal lengths. While recent
learning-based approaches have advanced this field, they often struggle in
complex scenes with fine textures or abrupt depth changes, where focus cues may
become ambiguous or misleading. We present DualFocus, a novel DFF framework
that leverages the focal stack's unique gradient patterns induced by focus
variation, jointly modeling focus changes over spatial and focal dimensions.
Our approach introduces a variational formulation with dual constraints
tailored to DFF: spatial constraints exploit gradient pattern changes across
focus levels to distinguish true depth edges from texture artifacts, while
focal constraints enforce unimodal, monotonic focus probabilities aligned with
physical focus behavior. These inductive biases improve robustness and accuracy
in challenging regions. Comprehensive experiments on four public datasets
demonstrate that DualFocus consistently outperforms state-of-the-art methods in
both depth accuracy and perceptual quality.

</details>


### [93] [Rate-Distortion Optimized Communication for Collaborative Perception](https://arxiv.org/abs/2509.21994)
*Genjia Liu,Anning Hu,Yue Hu,Wenjun Zhang,Siheng Chen*

Main category: cs.CV

TL;DR: 本研究提出了RDcomm框架，通过优化通信策略，提高了多智能体协作感知的任务性能和效率。


<details>
  <summary>Details</summary>
Motivation: 旨在填补任务性能与通信量之间理论基础的空白，优化多智能体协作场景下的通信策略。

Method: 采用信息论中的实用率失真理论，分析多智能体系统中的性能与通信量的权衡。

Result: RDcomm在3D物体检测和BEV分割任务上达到了最佳准确性，同时将通信量减少了最多108倍。

Conclusion: 通过引入RDcomm框架，优化了多智能体协作感知中的通信效率，同时提高了任务准确性。

Abstract: Collaborative perception emphasizes enhancing environmental understanding by
enabling multiple agents to share visual information with limited bandwidth
resources. While prior work has explored the empirical trade-off between task
performance and communication volume, a significant gap remains in the
theoretical foundation. To fill this gap, we draw on information theory and
introduce a pragmatic rate-distortion theory for multi-agent collaboration,
specifically formulated to analyze performance-communication trade-off in
goal-oriented multi-agent systems. This theory concretizes two key conditions
for designing optimal communication strategies: supplying pragmatically
relevant information and transmitting redundancy-less messages. Guided by these
two conditions, we propose RDcomm, a communication-efficient collaborative
perception framework that introduces two key innovations: i) task entropy
discrete coding, which assigns features with task-relevant codeword-lengths to
maximize the efficiency in supplying pragmatic information; ii)
mutual-information-driven message selection, which utilizes mutual information
neural estimation to approach the optimal redundancy-less condition.
Experiments on 3D object detection and BEV segmentation demonstrate that RDcomm
achieves state-of-the-art accuracy on DAIR-V2X and OPV2V, while reducing
communication volume by up to 108 times. The code will be released.

</details>


### [94] [FailureAtlas:Mapping the Failure Landscape of T2I Models via Active Exploration](https://arxiv.org/abs/2509.21995)
*Muxi Chen,Zhaohua Zhang,Chenchen Zhao,Mingyang Chen,Wenyu Jiang,Tianwen Jiang,Jianhuan Zhuo,Yu Tang,Qiuyong Xiao,Jihong Zhang,Qiang Xu*

Main category: cs.CV

TL;DR: FailureAtlas 是一个新框架，用于主动探索 Text-to-Image 模型的失败，发现大量未知故障，并与数据稀缺性相关联。


<details>
  <summary>Details</summary>
Motivation: 静态基准测试在比较 T2I 模型方面有价值，但其局限性在于难以发现系统性故障的完整景观。

Method: 通过引入 FailureAtlas 框架，实现在 T2I 模型中自动探索和定位故障的最小概念，并应用了新的加速技术以解决计算复杂性问题。

Result: 应用 FailureAtlas 后，发现了超过 247,000 个在 SD1.5 版本中的未知错误切片，并首次大规模证实了这些故障与训练集中的数据稀缺性之间的联系。

Conclusion: FailureAtlas 提供了一种新的方法来主动探索和映射 T2I 模型的失败空间，从而促进更稳健的生成 AI 的发展。

Abstract: Static benchmarks have provided a valuable foundation for comparing
Text-to-Image (T2I) models. However, their passive design offers limited
diagnostic power, struggling to uncover the full landscape of systematic
failures or isolate their root causes. We argue for a complementary paradigm:
active exploration. We introduce FailureAtlas, the first framework designed to
autonomously explore and map the vast failure landscape of T2I models at scale.
FailureAtlas frames error discovery as a structured search for minimal,
failure-inducing concepts. While it is a computationally explosive problem, we
make it tractable with novel acceleration techniques. When applied to Stable
Diffusion models, our method uncovers hundreds of thousands of previously
unknown error slices (over 247,000 in SD1.5 alone) and provides the first
large-scale evidence linking these failures to data scarcity in the training
set. By providing a principled and scalable engine for deep model auditing,
FailureAtlas establishes a new, diagnostic-first methodology to guide the
development of more robust generative AI. The code is available at
https://github.com/cure-lab/FailureAtlas

</details>


### [95] [Exposing Hallucinations To Suppress Them: VLMs Representation Editing With Generative Anchors](https://arxiv.org/abs/2509.21997)
*Youxu Shi,Suorong Yang,Dong Liu*

Main category: cs.CV

TL;DR: 该研究提出了一种有效的自监督方法，能在不额外训练的情况下显著减轻多模态大语言模型的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在视觉任务中幻觉问题，这些模型即使在更大规模下仍表现出脆弱性，现有方法通常需要额外的微调或妥协。

Method: 通过一种新颖的幻觉放大机制，将文本投影到视觉空间中，以发现隐式幻觉信号，从而作为负锚点，同时使用原始图像作为正锚点，编辑解码器隐藏状态。

Result: 在多个基准上的广泛实验表明，所提方法在不同模型上显著减少了幻觉，同时几乎不引入副作用。

Conclusion: 提出了一种训练无关的自监督方法来减轻幻觉，显著降低了对象、属性和关系层面的幻觉，同时保持了较好的信息量和丰富性。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable success
across diverse vision-language tasks, yet they remain highly susceptible to
hallucinations, producing content that is fluent but inconsistent with visual
evidence. Such hallucinations, spanning objects, attributes, and relations,
persist even in larger models, while existing mitigation approaches often
require additional finetuning, handcrafted priors, or trade-offs that
compromise informativeness and scalability. To address this limitation, we
propose a training-free, self-supervised method for hallucination mitigation.
Our approach introduces a novel hallucination amplification mechanism: a
caption is projected into the visual space via a text-to-image model to reveal
implicit hallucination signals, serving as a negative anchor, while the
original image provides a positive anchor. Leveraging these dual anchors, we
edit decoder hidden states by pulling representations toward faithful semantics
and pushing them away from hallucination directions. This correction requires
no human priors or additional training costs, ensuring both effectiveness and
efficiency. Extensive experiments across multiple benchmarks show that our
method significantly reduces hallucinations at the object, attribute, and
relation levels while largely preserving recall and caption richness, e.g.,
achieving a hallucination reduction by over 5% using LLaVA-v1.5-7B on CHAIR.
Furthermore, results on diverse architectures, including LLaVA-NEXT-7B,
Cambrian-8B, and InstructBLIP-7B, validate strong cross-architecture
generalization. More importantly, when applied to hallucination-free captions,
our method introduces almost no side effects, underscoring its robustness and
practical plug-and-play applicability. The implementation will be publicly
available.

</details>


### [96] [CoFFT: Chain of Foresight-Focus Thought for Visual Language Models](https://arxiv.org/abs/2509.22010)
*Xinyu Zhang,Yuxuan Dong,Lingling Zhang,Chengyou Jia,Zhuohang Dang,Basura Fernando,Jun Liu,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 提出CoFFT，一个训练自由的方法，通过模拟人类视觉认知，改善视觉模型在复杂视觉输入下的推理能力，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: VLM在处理复杂和冗余视觉输入时容易受到干扰，导致无关推理过程或幻觉，需提高其在推理过程中的视觉聚焦能力。

Method: CoFFT方法由三个阶段组成：多样样本生成、双重前瞻解码和视觉聚焦调整。这些阶段循环进行，相互依赖。

Result: 在多个基准测试中，使用Qwen2.5-VL、InternVL-2.5和Llava-Next，CoFFT方法提高了3.1-5.8%的性能，计算开销可控增加。

Conclusion: 提出的CoFFT方法通过模拟人类视觉认知来增强VLM的视觉推理，能有效提升推理准确性和处理复杂视觉输入的能力。

Abstract: Despite significant advances in Vision Language Models (VLMs), they remain
constrained by the complexity and redundancy of visual input. When images
contain large amounts of irrelevant information, VLMs are susceptible to
interference, thus generating excessive task-irrelevant reasoning processes or
even hallucinations. This limitation stems from their inability to discover and
process the required regions during reasoning precisely. To address this
limitation, we present the Chain of Foresight-Focus Thought (CoFFT), a novel
training-free approach that enhances VLMs' visual reasoning by emulating human
visual cognition. Each Foresight-Focus Thought consists of three stages: (1)
Diverse Sample Generation: generates diverse reasoning samples to explore
potential reasoning paths, where each sample contains several reasoning steps;
(2) Dual Foresight Decoding: rigorously evaluates these samples based on both
visual focus and reasoning progression, adding the first step of optimal sample
to the reasoning process; (3) Visual Focus Adjustment: precisely adjust visual
focus toward regions most beneficial for future reasoning, before returning to
stage (1) to generate subsequent reasoning samples until reaching the final
answer. These stages function iteratively, creating an interdependent cycle
where reasoning guides visual focus and visual focus informs subsequent
reasoning. Empirical results across multiple benchmarks using Qwen2.5-VL,
InternVL-2.5, and Llava-Next demonstrate consistent performance improvements of
3.1-5.8\% with controllable increasing computational overhead.

</details>


### [97] [Lightweight Structured Multimodal Reasoning for Clinical Scene Understanding in Robotics](https://arxiv.org/abs/2509.22014)
*Saurav Jha,Stefan K. Ehrlich*

Main category: cs.CV

TL;DR: 提出了一种轻量级的多模态框架，用于视频场景理解，增强了医疗机器人在临床环境中的推理和适应能力。


<details>
  <summary>Details</summary>
Motivation: 医疗机器人需要强大的多模态感知和推理能力，以确保在动态临床环境中的安全性。

Method: 结合Qwen2.5-VL-3B-Instruct模型与SmolAgent基础的协调层，支持思维链推理、语音-视觉融合和动态工具调用。

Result: 在Video-MME基准和自定义临床数据集上的评估显示出竞争性准确性和相较于现有VLMs的更强鲁棒性。

Conclusion: 该框架在机器人辅助手术、患者监测和决策支持等应用中显示出潜力。

Abstract: Healthcare robotics requires robust multimodal perception and reasoning to
ensure safety in dynamic clinical environments. Current Vision-Language Models
(VLMs) demonstrate strong general-purpose capabilities but remain limited in
temporal reasoning, uncertainty estimation, and structured outputs needed for
robotic planning. We present a lightweight agentic multimodal framework for
video-based scene understanding. Combining the Qwen2.5-VL-3B-Instruct model
with a SmolAgent-based orchestration layer, it supports chain-of-thought
reasoning, speech-vision fusion, and dynamic tool invocation. The framework
generates structured scene graphs and leverages a hybrid retrieval module for
interpretable and adaptive reasoning. Evaluations on the Video-MME benchmark
and a custom clinical dataset show competitive accuracy and improved robustness
compared to state-of-the-art VLMs, demonstrating its potential for applications
in robot-assisted surgery, patient monitoring, and decision support.

</details>


### [98] [EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking](https://arxiv.org/abs/2509.22019)
*Yuki Sakai,Ryosuke Furuta,Juichun Yen,Yoichi Sato*

Main category: cs.CV

TL;DR: 本研究构建了一个新的视频数据集，以分析面授教学中的互动，并通过多模态大型语言模型评估其理解能力，结果表明这些模型在综合理解教学互动方面表现优越。


<details>
  <summary>Details</summary>
Motivation: 针对面授教学场景的缺乏系统性研究，主要是由于数据集不足和分析技术有限。

Method: 使用新的第一人称视频数据集，进行程序步骤分割和对话状态分类任务的实验评估。

Result: 多模态大型语言模型能有效整合图像、音频和文本，从而更好地理解面授教学互动场景，实验结果显示其在无任务特定微调情况下也能超越专业基线。

Conclusion: 多模态大型语言模型在课堂互动理解方面表现出色，超越了传统任务特定模型，显示了其在综合理解教学互动中的潜力。

Abstract: Analyzing instructional interactions between an instructor and a learner who
are co-present in the same physical space is a critical problem for educational
support and skill transfer. Yet such face-to-face instructional scenes have not
been systematically studied in computer vision. We identify two key reasons: i)
the lack of suitable datasets and ii) limited analytical techniques. To address
this gap, we present a new egocentric video dataset of face-to-face instruction
and provide ground-truth annotations for two fundamental tasks that serve as a
first step toward a comprehensive understanding of instructional interactions:
procedural step segmentation and conversation-state classification. Using this
dataset, we benchmark multimodal large language models (MLLMs) against
conventional task-specific models. Since face-to-face instruction involves
multiple modalities (speech content and prosody, gaze and body motion, and
visual context), effective understanding requires methods that handle verbal
and nonverbal communication in an integrated manner. Accordingly, we evaluate
recently introduced MLLMs that jointly process images, audio, and text. This
evaluation quantifies the extent to which current machine learning models
understand face-to-face instructional scenes. In experiments, MLLMs outperform
specialized baselines even without task-specific fine-tuning, suggesting their
promise for holistic understanding of instructional interactions.

</details>


### [99] [SpecXNet: A Dual-Domain Convolutional Network for Robust Deepfake Detection](https://arxiv.org/abs/2509.22070)
*Inzamamul Alam,Md Tanvir Islam,Simon S. Woo*

Main category: cs.CV

TL;DR: SpecXNet是一种双域架构，通过结合空间和频谱特征，实现了鲁棒的深伪检测，特别是在未见的伪造情况下表现出色，并且保持实时性。


<details>
  <summary>Details</summary>
Motivation: 深伪检测面临的挑战是生成内容的真实性越来越高，而现有方法仅关注空间或频谱特征，这限制了其对未见伪造的泛化能力。

Method: 提出了Spectral Cross-Attentional Network (SpecXNet)，结合了Dual-Domain Feature Coupler (DDFC)和Dual Fourier Attention (DFA)模块，采用双域架构进行深伪检测。

Result: 在多个深伪基准上的广泛实验表明，SpecXNet在准确性上达到最先进水平，尤其是在跨数据集和未见操作场景下表现优异。

Conclusion: SpecXNet展现了在跨数据集和未见操作场景下，使用统一的空间-频谱学习实现鲁棒且可泛化的深伪检测的有效性，并且保持了实时性。

Abstract: The increasing realism of content generated by GANs and diffusion models has
made deepfake detection significantly more challenging. Existing approaches
often focus solely on spatial or frequency-domain features, limiting their
generalization to unseen manipulations. We propose the Spectral
Cross-Attentional Network (SpecXNet), a dual-domain architecture for robust
deepfake detection. The core \textbf{Dual-Domain Feature Coupler (DDFC)}
decomposes features into a local spatial branch for capturing texture-level
anomalies and a global spectral branch that employs Fast Fourier Transform to
model periodic inconsistencies. This dual-domain formulation allows SpecXNet to
jointly exploit localized detail and global structural coherence, which are
critical for distinguishing authentic from manipulated images. We also
introduce the \textbf{Dual Fourier Attention (DFA)} module, which dynamically
fuses spatial and spectral features in a content-aware manner. Built atop a
modified XceptionNet backbone, we embed the DDFC and DFA modules within a
separable convolution block. Extensive experiments on multiple deepfake
benchmarks show that SpecXNet achieves state-of-the-art accuracy, particularly
under cross-dataset and unseen manipulation scenarios, while maintaining
real-time feasibility. Our results highlight the effectiveness of unified
spatial-spectral learning for robust and generalizable deepfake detection. To
ensure reproducibility, we released the full code on
\href{https://github.com/inzamamulDU/SpecXNet}{\textcolor{blue}{\textbf{GitHub}}}.

</details>


### [100] [Large Material Gaussian Model for Relightable 3D Generation](https://arxiv.org/abs/2509.22112)
*Jingrui Ye,Lingting Zhu,Runze Zhang,Zeyu Hu,Yingda Yin,Lanjiong Li,Lequan Yu,Qingmin Liao*

Main category: cs.CV

TL;DR: 本文提出了Large Material Gaussian Model (MGM)，旨在生成高质量的3D内容，同时支持PBR材料属性以改善渲染效果。


<details>
  <summary>Details</summary>
Motivation: 随着各行业对3D资产需求的增加，亟需高效且自动化的3D内容创建方法，现有模型难以产生真实渲染所需的材料属性。

Method: 通过微调新的多视角材料扩散模型，并利用生成的多视角PBR图像，探索高斯材料表示来建模PBR材料的各个通道。

Result: 本方法生成的材料在视觉上比基准方法更具吸引力，并增强了材料建模，支持动态重光照。

Conclusion: 本文提出的Large Material Gaussian Model (MGM)能够生成高质量的3D内容，并为PBR材料提供支持，从而提高了材料建模和渲染效果。

Abstract: The increasing demand for 3D assets across various industries necessitates
efficient and automated methods for 3D content creation. Leveraging 3D Gaussian
Splatting, recent large reconstruction models (LRMs) have demonstrated the
ability to efficiently achieve high-quality 3D rendering by integrating
multiview diffusion for generation and scalable transformers for
reconstruction. However, existing models fail to produce the material
properties of assets, which is crucial for realistic rendering in diverse
lighting environments. In this paper, we introduce the Large Material Gaussian
Model (MGM), a novel framework designed to generate high-quality 3D content
with Physically Based Rendering (PBR) materials, ie, albedo, roughness, and
metallic properties, rather than merely producing RGB textures with
uncontrolled light baking. Specifically, we first fine-tune a new multiview
material diffusion model conditioned on input depth and normal maps. Utilizing
the generated multiview PBR images, we explore a Gaussian material
representation that not only aligns with 2D Gaussian Splatting but also models
each channel of the PBR materials. The reconstructed point clouds can then be
rendered to acquire PBR attributes, enabling dynamic relighting by applying
various ambient light maps. Extensive experiments demonstrate that the
materials produced by our method not only exhibit greater visual appeal
compared to baseline methods but also enhance material modeling, thereby
enabling practical downstream rendering applications.

</details>


### [101] [Self-Supervised Point Cloud Completion based on Multi-View Augmentations of Single Partial Point Cloud](https://arxiv.org/abs/2509.22132)
*Jingjing Lu,Huilong Pi,Yunchuan Qin,Zhuo Tang,Ruihui Li*

Main category: cs.CV

TL;DR: 本研究提出了一种新颖的自监督点云补全方法，通过多视角增强生成自监督信号，结合Mamba提升模型学习能力，克服了现有方法的局限性，展示了在各种数据集上优异的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的点云补全方法在实际应用中受限于对真实数据集的泛化能力，存在合成与真实领域差距的问题，因此有必要提出一种新的自监督方法来解决这些挑战。

Method: 提出了一种新颖的自监督点云补全方法，设计了一套基于单个部分点云的多视角增强的新自监督信号，并首次将Mamba引入到自监督点云补全任务中，以提高模型的学习能力。

Result: 实验结果表明，该方法在合成和真实世界数据集上均实现了最先进的结果。

Conclusion: 该方法在合成和真实世界数据集上均实现了最先进的结果。

Abstract: Point cloud completion aims to reconstruct complete shapes from partial
observations. Although current methods have achieved remarkable performance,
they still have some limitations: Supervised methods heavily rely on ground
truth, which limits their generalization to real-world datasets due to the
synthetic-to-real domain gap. Unsupervised methods require complete point
clouds to compose unpaired training data, and weakly-supervised methods need
multi-view observations of the object. Existing self-supervised methods
frequently produce unsatisfactory predictions due to the limited capabilities
of their self-supervised signals. To overcome these challenges, we propose a
novel self-supervised point cloud completion method. We design a set of novel
self-supervised signals based on multi-view augmentations of the single partial
point cloud. Additionally, to enhance the model's learning ability, we first
incorporate Mamba into self-supervised point cloud completion task, encouraging
the model to generate point clouds with better quality. Experiments on
synthetic and real-world datasets demonstrate that our method achieves
state-of-the-art results.

</details>


### [102] [REFINE-CONTROL: A Semi-supervised Distillation Method For Conditional Image Generation](https://arxiv.org/abs/2509.22139)
*Yicheng Jiang,Jin Yuan,Hua Yuan,Yao Zhang,Yong Rui*

Main category: cs.CV

TL;DR: 提出Refine-Control模型，通过半监督蒸馏和知识融合技术，降低了条件图像生成模型的资源需求，提升了其在边缘设备上的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 高资源需求和数据注释稀缺使得条件图像生成模型难以在边缘设备上部署，从而引发高成本和隐私问题。

Method: 提出了一种半监督蒸馏框架，使用三层知识融合损失转移不同水平的知识，并利用有标签和无标签数据进行半监督学习。

Result: Refine-Control显著降低了计算成本和延迟，同时通过比较指标维持了高保真生成能力与可控性。

Conclusion: Refine-Control模型在保持高保真图像生成能力和可控性的同时，实现了显著降低计算成本和延迟的效果。

Abstract: Conditional image generation models have achieved remarkable results by
leveraging text-based control to generate customized images. However, the high
resource demands of these models and the scarcity of well-annotated data have
hindered their deployment on edge devices, leading to enormous costs and
privacy concerns, especially when user data is sent to a third party. To
overcome these challenges, we propose Refine-Control, a semi-supervised
distillation framework. Specifically, we improve the performance of the student
model by introducing a tri-level knowledge fusion loss to transfer different
levels of knowledge. To enhance generalization and alleviate dataset scarcity,
we introduce a semi-supervised distillation method utilizing both labeled and
unlabeled data. Our experiments reveal that Refine-Control achieves significant
reductions in computational cost and latency, while maintaining high-fidelity
generation capabilities and controllability, as quantified by comparative
metrics.

</details>


### [103] [Joint graph entropy knowledge distillation for point cloud classification and robustness against corruptions](https://arxiv.org/abs/2509.22150)
*Zhiqiang Tian,Weigang Li,Junwei Hu,Chunhua Deng*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D点云分类策略JGEKD，利用联合图熵进行知识蒸馏，解决类之间的相关性问题，同时提高模型对数据污染的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在3D点云分类任务中，传统的假设类事件是独立同分布的，从而破坏了类之间的相关性，因此需要一种适合非独立同分布数据的分类策略。

Method: 本文提出的JGEKD策略通过构建基于联合图熵的损失函数，进行知识蒸馏来实现类关联性的知识传递，采用Siamese结构处理空间变换不变的3D点云数据。

Result: 在ScanObject、ModelNet40、ScanntV2_cls和ModelNet-C等数据集上的广泛实验表明，提出的JGEKD策略在类别关联性知识转移以及对模型的污染鲁棒性方面表现优异。

Conclusion: 提出的JGEKD策略在3D点云分类任务中，能够有效地转移类之间的关联性，并提高模型对数据污染的鲁棒性，实验证明其在多个数据集上取得了竞争性结果。

Abstract: Classification tasks in 3D point clouds often assume that class events
\replaced{are }{follow }independent and identically distributed (IID), although
this assumption destroys the correlation between classes. This \replaced{study
}{paper }proposes a classification strategy, \textbf{J}oint \textbf{G}raph
\textbf{E}ntropy \textbf{K}nowledge \textbf{D}istillation (JGEKD), suitable for
non-independent and identically distributed 3D point cloud data,
\replaced{which }{the strategy } achieves knowledge transfer of class
correlations through knowledge distillation by constructing a loss function
based on joint graph entropy. First\deleted{ly}, we employ joint graphs to
capture add{the }hidden relationships between classes\replaced{ and}{,}
implement knowledge distillation to train our model by calculating the entropy
of add{add }graph.\replaced{ Subsequently}{ Then}, to handle 3D point clouds
\deleted{that is }invariant to spatial transformations, we construct
\replaced{S}{s}iamese structures and develop two frameworks, self-knowledge
distillation and teacher-knowledge distillation, to facilitate information
transfer between different transformation forms of the same data. \replaced{In
addition}{ Additionally}, we use the above framework to achieve knowledge
transfer between point clouds and their corrupted forms, and increase the
robustness against corruption of model. Extensive experiments on ScanObject,
ModelNet40, ScanntV2\_cls and ModelNet-C demonstrate that the proposed strategy
can achieve competitive results.

</details>


### [104] [MultiMat: Multimodal Program Synthesis for Procedural Materials using Large Multimodal Models](https://arxiv.org/abs/2509.22151)
*Jonas Belouadi,Tamy Boubekeur,Adrien Kaiser*

Main category: cs.CV

TL;DR: 本文提出MultiMat框架，通过结合视觉与文本图表示，提升程序材料图的生成效率与质量，解决了现有方法的局限，达到新状态的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的神经程序合成方法仅将图表示为文本程序，无法捕捉节点图的视觉-空间特性，限制了可接触性及易用性。

Method: 采用大规模多模态模型处理视觉和文本图表示，并结合受限树搜索推理算法，确保语法有效性，同时有效导航程序空间。

Result: 实验结果表明，多模态程序合成方法在图合成的效率和视觉质量上优于传统文本方法。

Conclusion: MultiMat框架在无条件和条件图合成中表现出更高的效率和视觉质量，超越了仅使用文本基线的方法，确立了新的性能标杆。

Abstract: Material node graphs are programs that generate the 2D channels of procedural
materials, including geometry such as roughness and displacement maps, and
reflectance such as albedo and conductivity maps. They are essential in
computer graphics for representing the appearance of virtual 3D objects
parametrically and at arbitrary resolution. In particular, their directed
acyclic graph structures and intermediate states provide an intuitive
understanding and workflow for interactive appearance modeling. Creating such
graphs is a challenging task and typically requires professional training.
While recent neural program synthesis approaches attempt to simplify this
process, they solely represent graphs as textual programs, failing to capture
the inherently visual-spatial nature of node graphs that makes them accessible
to humans. To address this gap, we present MultiMat, a multimodal program
synthesis framework that leverages large multimodal models to process both
visual and textual graph representations for improved generation of procedural
material graphs. We train our models on a new dataset of production-quality
procedural materials and combine them with a constrained tree search inference
algorithm that ensures syntactic validity while efficiently navigating the
program space. Our experimental results show that our multimodal program
synthesis method is more efficient in both unconditional and conditional graph
synthesis with higher visual quality and fidelity than text-only baselines,
establishing new state-of-the-art performance.

</details>


### [105] [DragGANSpace: Latent Space Exploration and Control for GANs](https://arxiv.org/abs/2509.22169)
*Kirsten Odendaal,Neela Kaushik,Spencer Halverson*

Main category: cs.CV

TL;DR: 本研究通过结合PCA与DragGAN，提升了图像生成与编辑的效率和可控性，尤其是在浅层潜在空间中表现突出。


<details>
  <summary>Details</summary>
Motivation: 旨在提高GAN生成图像的潜在空间的效率与可控性，以便进行更直观的图像操作与解释。

Method: 集成了StyleGAN、DragGAN和主成分分析（PCA）的方法来增强GAN生成图像的潜在空间效率和可控制性。

Result: 通过在AFHQ数据集上应用我们的方法，发现集成PCA和DragGAN能保持性能，同时提升优化效率，并改善生成图像的结构相似性指数（SSIM）。

Conclusion: 本研究表明，结合PCA和DragGAN框架可以有效提升图像生成和编辑的潜力，尤其是在浅层潜在空间中。

Abstract: This work integrates StyleGAN, DragGAN and Principal Component Analysis (PCA)
to enhance the latent space efficiency and controllability of GAN-generated
images. Style-GAN provides a structured latent space, DragGAN enables intuitive
image manipulation, and PCA reduces dimensionality and facilitates cross-model
alignment for more streamlined and interpretable exploration of latent spaces.
We apply our techniques to the Animal Faces High Quality (AFHQ) dataset, and
find that our approach of integrating PCA-based dimensionality reduction with
the Drag-GAN framework for image manipulation retains performance while
improving optimization efficiency. Notably, introducing PCA into the latent W+
layers of DragGAN can consistently reduce the total optimization time while
maintaining good visual quality and even boosting the Structural Similarity
Index Measure (SSIM) of the optimized image, particularly in shallower latent
spaces (W+ layers = 3). We also demonstrate capability for aligning images
generated by two StyleGAN models trained on similar but distinct data domains
(AFHQ-Dog and AFHQ-Cat), and show that we can control the latent space of these
aligned images to manipulate the images in an intuitive and interpretable
manner. Our findings highlight the possibility for efficient and interpretable
latent space control for a wide range of image synthesis and editing
applications.

</details>


### [106] [MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing](https://arxiv.org/abs/2509.22186)
*Junbo Niu,Zheng Liu,Zhuangcheng Gu,Bin Wang,Linke Ouyang,Zhiyuan Zhao,Tao Chu,Tianyao He,Fan Wu,Qintong Zhang,Zhenjiang Jin,Guang Liang,Rui Zhang,Wenzheng Zhang,Yuan Qu,Zhifei Ren,Yuefeng Sun,Yuanhong Zheng,Dongsheng Ma,Zirui Tang,Boyu Niu,Ziyang Miao,Hejun Dong,Siyi Qian,Junyuan Zhang,Jingzhou Chen,Fangdong Wang,Xiaomeng Zhao,Liqun Wei,Wei Li,Shasha Wang,Ruiliang Xu,Yuanyuan Cao,Lu Chen,Qianqian Wu,Huaiyu Gu,Lindong Lu,Keming Wang,Dechen Lin,Guanlin Shen,Xuanhe Zhou,Linfeng Zhang,Yuhang Zang,Xiaoyi Dong,Jiaqi Wang,Bo Zhang,Lei Bai,Pei Chu,Weijia Li,Jiang Wu,Lijun Wu,Zhenxiang Li,Guangyu Wang,Zhongying Tu,Chao Xu,Kai Chen,Yu Qiao,Bowen Zhou,Dahua Lin,Wentao Zhang,Conghui He*

Main category: cs.CV

TL;DR: MinerU2.5是一种高效的文档解析视觉语言模型，具有先进的识别精度和低计算开销。


<details>
  <summary>Details</summary>
Motivation: 目标是提高文档解析的识别准确性，同时保持卓越的计算效率。

Method: 采用粗到细的两阶段解析策略，第一阶段进行高效的布局分析，第二阶段在原始图像中针对性地进行内容识别。

Result: MinerU2.5在多个基准测试中实现了先进的性能，尤其在识别复杂文本、公式和表格时保持了细粒度的细节。

Conclusion: MinerU2.5展示了强大的文档解析能力，在多个基准测试中表现出色，超越了通用和特定领域的模型，且计算开销显著降低。

Abstract: We introduce MinerU2.5, a 1.2B-parameter document parsing vision-language
model that achieves state-of-the-art recognition accuracy while maintaining
exceptional computational efficiency. Our approach employs a coarse-to-fine,
two-stage parsing strategy that decouples global layout analysis from local
content recognition. In the first stage, the model performs efficient layout
analysis on downsampled images to identify structural elements, circumventing
the computational overhead of processing high-resolution inputs. In the second
stage, guided by the global layout, it performs targeted content recognition on
native-resolution crops extracted from the original image, preserving
fine-grained details in dense text, complex formulas, and tables. To support
this strategy, we developed a comprehensive data engine that generates diverse,
large-scale training corpora for both pretraining and fine-tuning. Ultimately,
MinerU2.5 demonstrates strong document parsing ability, achieving
state-of-the-art performance on multiple benchmarks, surpassing both
general-purpose and domain-specific models across various recognition tasks,
while maintaining significantly lower computational overhead.

</details>


### [107] [Towards Faithful Reasoning in Remote Sensing: A Perceptually-Grounded GeoSpatial Chain-of-Thought for Vision-Language Models](https://arxiv.org/abs/2509.22221)
*Jiaqi Liu,Lang Sun,Ronghao Fu,Bo Yang*

Main category: cs.CV

TL;DR: 提出Geo-CoT框架，提升遥感分析的多步骤可验证性，开发RSThinker模型，对比现有模型表现优越。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉语言模型在复杂分析任务中的局限性，尤其是在推理步骤的可验证性方面。

Method: 引入阶段性对齐策略，采用有监督微调(SFT)和群体奖励策略优化(GRPO)，通过Geo-CoT380k数据集进行训练。

Result: RSThinker输出最终答案及其可验证的分析轨迹，显著超越当前最先进模型的性能。

Conclusion: RSThinker模型在多个任务中表现优越，带来了从不透明感知到结构化、可验证推理的具体路径。

Abstract: Vision-Language Models (VLMs) in remote sensing often fail at complex
analytical tasks, a limitation stemming from their end-to-end training paradigm
that bypasses crucial reasoning steps and leads to unverifiable outputs. To
address this limitation, we introduce the Perceptually-Grounded Geospatial
Chain-of-Thought (Geo-CoT), a framework that models remote sensing analysis as
a verifiable, multi-step process. We instill this analytical process through a
two-stage alignment strategy, leveraging Geo-CoT380k, the first large-scale
dataset of structured Geo-CoT rationales. This strategy first employs
supervised fine-tuning (SFT) to instill the foundational cognitive
architecture, then leverages Group Reward Policy Optimization (GRPO) to refine
the model's reasoning policy towards factual correctness. The resulting model,
RSThinker, outputs both a final answer and its justifying, verifiable
analytical trace. This capability yields dominant performance, significantly
outperforming state-of-the-art models across a comprehensive range of tasks.
The public release of our Geo-CoT380k dataset and RSThinker model upon
publication serves as a concrete pathway from opaque perception towards
structured, verifiable reasoning for Earth Observation.

</details>


### [108] [Polysemous Language Gaussian Splatting via Matching-based Mask Lifting](https://arxiv.org/abs/2509.22225)
*Jiayu Ding,Xinpeng Liu,Zhiyi Pan,Shiqiang Long,Ge Li*

Main category: cs.CV

TL;DR: MUSplat是一个无训练框架，通过2D分割模型生成3D对象组，优化边界以实现快速的开放词汇查询，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 提升2D开放词汇理解到3D场景存在关键挑战，现有方法存在训练成本高、语义表示单一和视图不一致性问题。

Method: 一个无训练的框架，利用预训练的2D分割模型生成和提升多粒度2D掩码到3D，优化初始对象组的模糊边界。

Result: MUSplat减少了场景适应时间，从小时缩短至几分钟，同时通过语义匹配实现开放词汇查询。

Conclusion: MUSplat在开放词汇3D对象选择和语义分割的基准任务上超越了传统训练框架，并解决了它们的单义性局限性。

Abstract: Lifting 2D open-vocabulary understanding into 3D Gaussian Splatting (3DGS)
scenes is a critical challenge. However, mainstream methods suffer from three
key flaws: (i) their reliance on costly per-scene retraining prevents
plug-and-play application; (ii) their restrictive monosemous design fails to
represent complex, multi-concept semantics; and (iii) their vulnerability to
cross-view semantic inconsistencies corrupts the final semantic representation.
To overcome these limitations, we introduce MUSplat, a training-free framework
that abandons feature optimization entirely. Leveraging a pre-trained 2D
segmentation model, our pipeline generates and lifts multi-granularity 2D masks
into 3D, where we estimate a foreground probability for each Gaussian point to
form initial object groups. We then optimize the ambiguous boundaries of these
initial groups using semantic entropy and geometric opacity. Subsequently, by
interpreting the object's appearance across its most representative viewpoints,
a Vision-Language Model (VLM) distills robust textual features that reconciles
visual inconsistencies, enabling open-vocabulary querying via semantic
matching. By eliminating the costly per-scene training process, MUSplat reduces
scene adaptation time from hours to mere minutes. On benchmark tasks for
open-vocabulary 3D object selection and semantic segmentation, MUSplat
outperforms established training-based frameworks while simultaneously
addressing their monosemous limitations.

</details>


### [109] [UrbanFeel: A Comprehensive Benchmark for Temporal and Perceptual Understanding of City Scenes through Human Perspective](https://arxiv.org/abs/2509.22228)
*Jun He,Yi Lin,Zilong Huang,Jiacong Yin,Junyan Ye,Yuchuan Zhou,Weijia Li,Xiang Zhang*

Main category: cs.CV

TL;DR: UrbanFeel基准测试评估了多模态大语言模型在城市发展理解中的表现，尽管它们在场景理解上表现良好，但在时间推理和主观评价任务上仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 考虑到城市发展对全球人口的影响，需对其结构与感知变化进行人本理解，以支持可持续发展。

Method: 通过构建多维度的视觉问题和收集全球11个城市的多时态街景图像，结合空间聚类、规则生成、模型辅助提示和人工标注等方法开发UrbanFeel基准。

Result: 20种最新的多模态大语言模型经过评估，Gemini-2.5 Pro表现最佳，其准确率接近人类专家水平，但在城市发展时间推理方面表现不佳，部分模型在主观感知任务中超越人类评审。

Conclusion: UrbanFeel基准测试揭示了多模态大语言模型在城市发展理解和环境感知中的优势与不足，特别是在时间推理和主观评价方面的表现仍需改进。

Abstract: Urban development impacts over half of the global population, making
human-centered understanding of its structural and perceptual changes essential
for sustainable development. While Multimodal Large Language Models (MLLMs)
have shown remarkable capabilities across various domains, existing benchmarks
that explore their performance in urban environments remain limited, lacking
systematic exploration of temporal evolution and subjective perception of urban
environment that aligns with human perception. To address these limitations, we
propose UrbanFeel, a comprehensive benchmark designed to evaluate the
performance of MLLMs in urban development understanding and subjective
environmental perception. UrbanFeel comprises 14.3K carefully constructed
visual questions spanning three cognitively progressive dimensions: Static
Scene Perception, Temporal Change Understanding, and Subjective Environmental
Perception. We collect multi-temporal single-view and panoramic street-view
images from 11 representative cities worldwide, and generate high-quality
question-answer pairs through a hybrid pipeline of spatial clustering,
rule-based generation, model-assisted prompting, and manual annotation. Through
extensive evaluation of 20 state-of-the-art MLLMs, we observe that Gemini-2.5
Pro achieves the best overall performance, with its accuracy approaching human
expert levels and narrowing the average gap to just 1.5\%. Most models perform
well on tasks grounded in scene understanding. In particular, some models even
surpass human annotators in pixel-level change detection. However, performance
drops notably in tasks requiring temporal reasoning over urban development.
Additionally, in the subjective perception dimension, several models reach
human-level or even higher consistency in evaluating dimension such as
beautiful and safety.

</details>


### [110] [A Tale of Two Experts: Cooperative Learning for Source-Free Unsupervised Domain Adaptation](https://arxiv.org/abs/2509.22229)
*Jiaping Yu,Muli Yang,Jiapeng Ji,Jiexi Yan,Cheng Deng*

Main category: cs.CV

TL;DR: 本文提出EXCL方法，通过双专家框架和检索增强交互流程，实现源无访问的优秀领域适应，并在多个数据集上展示了其优越性。


<details>
  <summary>Details</summary>
Motivation: 针对现有方法忽视目标数据潜在结构及互补性洞察的问题，旨在提升源无访问的领域适应能力。

Method: 使用双专家框架和检索增强交互优化流程，通过无监督条件下的共同学习和模型交互。

Result: 在四个基准数据集上进行了广泛实验，结果表明我们的方法达到了最先进的性能。

Conclusion: 提出的EXCL方法在源无访问情况下实现了出色的领域适应，超越了现有的最先进水平。

Abstract: Source-Free Unsupervised Domain Adaptation (SFUDA) addresses the realistic
challenge of adapting a source-trained model to a target domain without access
to the source data, driven by concerns over privacy and cost. Existing SFUDA
methods either exploit only the source model's predictions or fine-tune large
multimodal models, yet both neglect complementary insights and the latent
structure of target data. In this paper, we propose the Experts Cooperative
Learning (EXCL). EXCL contains the Dual Experts framework and
Retrieval-Augmentation-Interaction optimization pipeline. The Dual Experts
framework places a frozen source-domain model (augmented with Conv-Adapter) and
a pretrained vision-language model (with a trainable text prompt) on equal
footing to mine consensus knowledge from unlabeled target samples. To
effectively train these plug-in modules under purely unsupervised conditions,
we introduce Retrieval-Augmented-Interaction(RAIN), a three-stage pipeline that
(1) collaboratively retrieves pseudo-source and complex target samples, (2)
separately fine-tunes each expert on its respective sample set, and (3)
enforces learning object consistency via a shared learning result. Extensive
experiments on four benchmark datasets demonstrate that our approach matches
state-of-the-art performance.

</details>


### [111] [FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image Editing](https://arxiv.org/abs/2509.22244)
*Junyi Wu,Zhiteng Li,Haotong Qin,Xiaohong Liu,Linghe Kong,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: FlashEdit 是一个高效的实时图像编辑框架，使图像编辑速度提高了 150 倍，同时确保背景一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本引导图像编辑方法在质量上表现优秀，但速度缓慢，限制了其在实际应用中的效果。

Method: FlashEdit 引入了三项创新技术：一是 OSIE 管道，二是 BG-Shield 技术，三是 SSCA 机制。

Result: FlashEdit 在编辑时间上小于 0.2 秒，比传统多步骤方法快超过 150 倍，同时保持优越的背景一致性和结构完整性。

Conclusion: FlashEdit 显著提高了图像编辑的速度和背景一致性，适用于实时高保真图像编辑。

Abstract: Text-guided image editing with diffusion models has achieved remarkable
quality but suffers from prohibitive latency, hindering real-world
applications. We introduce FlashEdit, a novel framework designed to enable
high-fidelity, real-time image editing. Its efficiency stems from three key
innovations: (1) a One-Step Inversion-and-Editing (OSIE) pipeline that bypasses
costly iterative processes; (2) a Background Shield (BG-Shield) technique that
guarantees background preservation by selectively modifying features only
within the edit region; and (3) a Sparsified Spatial Cross-Attention (SSCA)
mechanism that ensures precise, localized edits by suppressing semantic leakage
to the background. Extensive experiments demonstrate that FlashEdit maintains
superior background consistency and structural integrity, while performing
edits in under 0.2 seconds, which is an over 150$\times$ speedup compared to
prior multi-step methods. Our code will be made publicly available at
https://github.com/JunyiWuCode/FlashEdit.

</details>


### [112] [Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper Reasoning Benchmarks](https://arxiv.org/abs/2509.22258)
*Miao Jing,Mengting Jia,Junling Lin,Zhongxia Shen,Lijun Wang,Yuanyuan Peng,Huan Gao,Mingkun Xu,Shangyang Li*

Main category: cs.CV

TL;DR: Neural-MedBench是一个专为神经学临床推理设计的基准，揭示了现有视觉语言模型在推理方面的不足，强调了评估框架的多样性。


<details>
  <summary>Details</summary>
Motivation: 虽然现有模型在标准基准上成绩优异，但真实的临床推理能力尚不清晰，现有数据集更多关注分类准确性，忽视了推理能力的评估。

Method: 开发了一个混合评分管道，通过整合LLM评分、临床验证和语义相似度度量对模型进行评估。

Result: 通过评估最新的视觉语言模型，发现其在Neural-MedBench上的表现显著下降，主要是推理失败而非感知错误导致了模型性能不足。

Conclusion: Neural-MedBench揭示了现有视觉语言模型在高风险诊断推理中的不足，并呼吁建立以推理能力为核心的评估框架。

Abstract: Recent advances in vision-language models (VLMs) have achieved remarkable
performance on standard medical benchmarks, yet their true clinical reasoning
ability remains unclear. Existing datasets predominantly emphasize
classification accuracy, creating an evaluation illusion in which models appear
proficient while still failing at high-stakes diagnostic reasoning. We
introduce Neural-MedBench, a compact yet reasoning-intensive benchmark
specifically designed to probe the limits of multimodal clinical reasoning in
neurology. Neural-MedBench integrates multi-sequence MRI scans, structured
electronic health records, and clinical notes, and encompasses three core task
families: differential diagnosis, lesion recognition, and rationale generation.
To ensure reliable evaluation, we develop a hybrid scoring pipeline that
combines LLM-based graders, clinician validation, and semantic similarity
metrics. Through systematic evaluation of state-of-the-art VLMs, including
GPT-4o, Claude-4, and MedGemma, we observe a sharp performance drop compared to
conventional datasets. Error analysis shows that reasoning failures, rather
than perceptual errors, dominate model shortcomings. Our findings highlight the
necessity of a Two-Axis Evaluation Framework: breadth-oriented large datasets
for statistical generalization, and depth-oriented, compact benchmarks such as
Neural-MedBench for reasoning fidelity. We release Neural-MedBench at
https://neuromedbench.github.io/ as an open and extensible diagnostic testbed,
which guides the expansion of future benchmarks and enables rigorous yet
cost-effective assessment of clinically trustworthy AI.

</details>


### [113] [UniMapGen: A Generative Framework for Large-Scale Map Construction from Multi-modal Data](https://arxiv.org/abs/2509.22262)
*Yujian Yuan,Changjie Wu,Xinyuan Chang,Sijin Wang,Hang Zhang,Shiyi Liang,Shuang Zeng,Mu Xu*

Main category: cs.CV

TL;DR: 本文介绍了一种新颖的生成框架UniMapGen，旨在高效准确地构建大规模地图，克服传统方法的缺陷，并在OpenSatMap数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法的高成本和效率低下问题，同时克服卫星数据的局限性，提高地图构建的精确度和一致性。

Method: 基于生成框架UniMapGen，通过离散序列表示车道线、支持多模态输入并发展状态更新策略。

Result: 在OpenSatMap数据集上表现出色，生成更完整平滑的地图向量，并可以推测其他缺失信息。

Conclusion: UniMapGen在大规模地图构建方面实现了先进的性能，能够推断被遮挡的道路和预测数据集中缺失的道路。

Abstract: Large-scale map construction is foundational for critical applications such
as autonomous driving and navigation systems. Traditional large-scale map
construction approaches mainly rely on costly and inefficient special data
collection vehicles and labor-intensive annotation processes. While existing
satellite-based methods have demonstrated promising potential in enhancing the
efficiency and coverage of map construction, they exhibit two major
limitations: (1) inherent drawbacks of satellite data (e.g., occlusions,
outdatedness) and (2) inefficient vectorization from perception-based methods,
resulting in discontinuous and rough roads that require extensive
post-processing. This paper presents a novel generative framework, UniMapGen,
for large-scale map construction, offering three key innovations: (1)
representing lane lines as \textbf{discrete sequence} and establishing an
iterative strategy to generate more complete and smooth map vectors than
traditional perception-based methods. (2) proposing a flexible architecture
that supports \textbf{multi-modal} inputs, enabling dynamic selection among
BEV, PV, and text prompt, to overcome the drawbacks of satellite data. (3)
developing a \textbf{state update} strategy for global continuity and
consistency of the constructed large-scale map. UniMapGen achieves
state-of-the-art performance on the OpenSatMap dataset. Furthermore, UniMapGen
can infer occluded roads and predict roads missing from dataset annotations.
Our code will be released.

</details>


### [114] [GS-2M: Gaussian Splatting for Joint Mesh Reconstruction and Material Decomposition](https://arxiv.org/abs/2509.22276)
*Dinh Minh Nguyen,Malte Avenhaus,Thomas Lindemeier*

Main category: cs.CV

TL;DR: 提出一种基于3D高斯溅射的统一解决方案GS-2M，实现网格重建和材料分解，改善传统方法在重建高反射表面时的效果。


<details>
  <summary>Details</summary>
Motivation: 以往的研究通常分开处理网格重建和材料分解，且在重建高反射表面时面临挑战，因此我们提出一种统一解决方案。

Method: 联合优化与深度和法线质量相关的属性，同时保持几何细节，并对反射表面具有较强的鲁棒性。

Result: 该方法产生的重建结果与当前最先进的方法相媲美，能有效克服现有方法的局限性。

Conclusion: 我们的统一框架在重建结果上能与最先进的方法媲美，能够提供三角网格及其相关的材料成分，适用于后续任务。

Abstract: We propose a unified solution for mesh reconstruction and material
decomposition from multi-view images based on 3D Gaussian Splatting, referred
to as GS-2M. Previous works handle these tasks separately and struggle to
reconstruct highly reflective surfaces, often relying on priors from external
models to enhance the decomposition results. Conversely, our method addresses
these two problems by jointly optimizing attributes relevant to the quality of
rendered depth and normals, maintaining geometric details while being resilient
to reflective surfaces. Although contemporary works effectively solve these
tasks together, they often employ sophisticated neural components to learn
scene properties, which hinders their performance at scale. To further
eliminate these neural components, we propose a novel roughness supervision
strategy based on multi-view photometric variation. When combined with a
carefully designed loss and optimization process, our unified framework
produces reconstruction results comparable to state-of-the-art methods,
delivering triangle meshes and their associated material components for
downstream tasks. We validate the effectiveness of our approach with widely
used datasets from previous works and qualitative comparisons with
state-of-the-art surface reconstruction methods.

</details>


### [115] [MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning](https://arxiv.org/abs/2509.22281)
*Jinkun Hao,Naifu Liang,Zhen Luo,Xudong Xu,Weipeng Zhong,Ran Yi,Yichen Jin,Zhaoyang Lyu,Feng Zheng,Lizhuang Ma,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 本论文提出了一种新的任务导向桌面场景生成方法，并引入MesaTask-10K数据集，展示了其在生成合适桌面场景方面的优越性。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在创建任务相关桌面场景时的低效性和不一致性，满足机器人的训练需求。

Method: 提出了一种基于空间推理链的生成方法，结合了对象推理、空间关系推理和场景图构建，并使用LLM框架和DPO算法进行增强。

Result: 引入了MesaTask-10K数据集，包含约10,700个合成桌面场景，并在生成过程中实现了任务与场景之间的有效衔接。

Conclusion: 实验结果表明，MesaTask在生成符合任务的桌面场景和具有现实布局方面优于基线模型。

Abstract: The ability of robots to interpret human instructions and execute
manipulation tasks necessitates the availability of task-relevant tabletop
scenes for training. However, traditional methods for creating these scenes
rely on time-consuming manual layout design or purely randomized layouts, which
are limited in terms of plausibility or alignment with the tasks. In this
paper, we formulate a novel task, namely task-oriented tabletop scene
generation, which poses significant challenges due to the substantial gap
between high-level task instructions and the tabletop scenes. To support
research on such a challenging task, we introduce MesaTask-10K, a large-scale
dataset comprising approximately 10,700 synthetic tabletop scenes with manually
crafted layouts that ensure realistic layouts and intricate inter-object
relations. To bridge the gap between tasks and scenes, we propose a Spatial
Reasoning Chain that decomposes the generation process into object inference,
spatial interrelation reasoning, and scene graph construction for the final 3D
layout. We present MesaTask, an LLM-based framework that utilizes this
reasoning chain and is further enhanced with DPO algorithms to generate
physically plausible tabletop scenes that align well with given task
descriptions. Exhaustive experiments demonstrate the superior performance of
MesaTask compared to baselines in generating task-conforming tabletop scenes
with realistic layouts. Project page is at https://mesatask.github.io/

</details>


### [116] [Rule-Based Reinforcement Learning for Document Image Classification with Vision Language Models](https://arxiv.org/abs/2509.22283)
*Michael Jungo,Andreas Fischer*

Main category: cs.CV

TL;DR: 本研究探索了基于规则的强化学习在文档图像分类中的应用，发现其在处理复杂场景时具有更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管在文档分析领域，强化学习的应用不如其他领域普遍，但其增强的推理能力可能对许多下游任务有益。

Method: 本研究利用基于规则的强化学习，专注于文档图像分类这一常见的下游任务。

Result: 在不同场景下（如分布外图像、未见类别和不同模态），强化学习展示了更好的泛化能力。

Conclusion: 基于规则的强化学习在文档图像分类任务中表现出更好的泛化能力，特别是在处理分布外数据时。

Abstract: Rule-based reinforcement learning has been gaining popularity ever since
DeepSeek-R1 has demonstrated its success through simple verifiable rewards. In
the domain of document analysis, reinforcement learning is not as prevalent,
even though many downstream tasks may benefit from the emerging properties of
reinforcement learning, particularly the enhanced reason capabilities. We study
the effects of rule-based reinforcement learning with the task of Document
Image Classification which is one of the most commonly studied downstream tasks
in document analysis. We find that reinforcement learning tends to have better
generalisation capabilities to out-of-distritbution data, which we examine in
three different scenarios, namely out-of-distribution images, unseen classes
and different modalities. Our code is available at
https://github.com/jungomi/vision-finetune.

</details>


### [117] [Vision-Language Alignment from Compressed Image Representations using 2D Gaussian Splatting](https://arxiv.org/abs/2509.22615)
*Yasmine Omri,Connor Ding,Tsachy Weissman,Thierry Tambe*

Main category: cs.CV

TL;DR: 本文介绍了一种基于2D高斯点云（2DGS）的新方法，旨在提高视频语言处理的效率，降低传输成本，同时保持语义信息的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的RGB图像编码器存在能源密集和序列长度爆炸的结构性低效，推动对更高效视图表示的探索。

Method: 开发了一个可扩展的2DGS管道，包括结构化初始化、亮度意识修剪和批处理CUDA内核。

Result: GS编码器在大数据集上取得了有意义的零-shot ImageNet-1K表现，同时相对于像素压缩输入3至20倍。

Conclusion: 2DGS作为一种多模态基础能够有效压缩输入并在某些任务中展示出有意义的零-shot性能，为边缘云学习提供了既语义强大又传输高效的表示方式。

Abstract: Modern vision language pipelines are driven by RGB vision encoders trained on
massive image text corpora. While these pipelines have enabled impressive zero
shot capabilities and strong transfer across tasks, they still inherit two
structural inefficiencies from the pixel domain: (i) transmitting dense RGB
images from edge devices to the cloud is energy intensive and costly, and (ii)
patch based tokenization explodes sequence length, stressing attention budgets
and context limits. We explore 2D Gaussian Splatting (2DGS) as an alternative
visual substrate for alignment: a compact, spatially adaptive representation
that parameterizes images by a set of colored anisotropic Gaussians. We develop
a scalable 2DGS pipeline with structured initialization, luminance aware
pruning, and batched CUDA kernels, achieving over 90x faster fitting and about
97% GPU utilization compared to prior implementations. We further adapt
contrastive language image pretraining (CLIP) to 2DGS by reusing a frozen
RGB-based transformer backbone with a lightweight splat aware input stem and a
perceiver resampler, training only about 7% of the total parameters. On large
DataComp subsets, GS encoders yield meaningful zero shot ImageNet-1K
performance while compressing inputs 3 to 20x relative to pixels. While
accuracy currently trails RGB encoders, our results establish 2DGS as a viable
multimodal substrate, pinpoint architectural bottlenecks, and open a path
toward representations that are both semantically powerful and transmission
efficient for edge cloud learning.

</details>


### [118] [Jailbreaking on Text-to-Video Models via Scene Splitting Strategy](https://arxiv.org/abs/2509.22292)
*Wonjun Lee,Haon Park,Doehyeon Lee,Bumsub Ham,Suhyun Kim*

Main category: cs.CV

TL;DR: 随着T2V模型的发展，安全风险增加。本文提出SceneSplit攻击方法，通过分解叙事，成功增加生成有害视频的概率，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着T2V模型的快速发展，安全风险问题日益严重，尤其是缺乏针对T2V模型的安全研究。

Method: 提出了一种新的黑箱攻击方法SceneSplit，通过将有害叙事分解为多个单独的场景来操纵生成输出空间。

Result: 在11个安全类别上，SceneSplit在Luma Ray2、Hailuo和Veo2模型上取得了高达77.2%、84.1%和78.2%的攻击成功率，显著超过了现有基线。

Conclusion: 目前的T2V安全机制易受到利用叙事结构的攻击，提供了新的见解，以理解和改善T2V模型的安全性。

Abstract: Along with the rapid advancement of numerous Text-to-Video (T2V) models,
growing concerns have emerged regarding their safety risks. While recent
studies have explored vulnerabilities in models like LLMs, VLMs, and
Text-to-Image (T2I) models through jailbreak attacks, T2V models remain largely
unexplored, leaving a significant safety gap. To address this gap, we introduce
SceneSplit, a novel black-box jailbreak method that works by fragmenting a
harmful narrative into multiple scenes, each individually benign. This approach
manipulates the generative output space, the abstract set of all potential
video outputs for a given prompt, using the combination of scenes as a powerful
constraint to guide the final outcome. While each scene individually
corresponds to a wide and safe space where most outcomes are benign, their
sequential combination collectively restricts this space, narrowing it to an
unsafe region and significantly increasing the likelihood of generating a
harmful video. This core mechanism is further enhanced through iterative scene
manipulation, which bypasses the safety filter within this constrained unsafe
region. Additionally, a strategy library that reuses successful attack patterns
further improves the attack's overall effectiveness and robustness. To validate
our method, we evaluate SceneSplit across 11 safety categories on T2V models.
Our results show that it achieves a high average Attack Success Rate (ASR) of
77.2% on Luma Ray2, 84.1% on Hailuo, and 78.2% on Veo2, significantly
outperforming the existing baseline. Through this work, we demonstrate that
current T2V safety mechanisms are vulnerable to attacks that exploit narrative
structure, providing new insights for understanding and improving the safety of
T2V models.

</details>


### [119] [LABELING COPILOT: A Deep Research Agent for Automated Data Curation in Computer Vision](https://arxiv.org/abs/2509.22631)
*Debargha Ganguly,Sumit Kumar,Ishwar Balappanawar,Weicong Chen,Shashank Kambhatla,Srinivasan Iyengar,Shivkumar Kalyanaraman,Ponnurangam Kumaraguru,Vipin Chaudhary*

Main category: cs.CV

TL;DR: 引入Labeling Copilot，作为计算机视觉领域的数据策划深度研究代理，通过多步骤推理实现数据发现、合成和标注，展示了其对工业级数据集策划的有效性。


<details>
  <summary>Details</summary>
Motivation: 有效策划高质量的特定领域数据集，以克服部署强大视觉系统时在数据质量、多样性和成本之间面临的复杂权衡。

Method: Labeling Copilot 利用大型多模态语言模型，通过多步骤推理执行三个核心功能：标定发现、可控合成和共识注释。

Result: Labeling Copilot 在 COCO 和 Open Images 数据集上表现出色，平均每张图像产生14.2个候选提议，并成功发现903个新的边界框类别，而且其标定发现工具在样本效率上比替代方案高出至多40倍。

Conclusion: Labeling Copilot的组件经过大规模验证，证明了其在数据标注和发现方面的有效性，能够为工业级数据集的策划奠定坚实基础。

Abstract: Curating high-quality, domain-specific datasets is a major bottleneck for
deploying robust vision systems, requiring complex trade-offs between data
quality, diversity, and cost when researching vast, unlabeled data lakes. We
introduce Labeling Copilot, the first data curation deep research agent for
computer vision. A central orchestrator agent, powered by a large multimodal
language model, uses multi-step reasoning to execute specialized tools across
three core capabilities: (1) Calibrated Discovery sources relevant,
in-distribution data from large repositories; (2) Controllable Synthesis
generates novel data for rare scenarios with robust filtering; and (3)
Consensus Annotation produces accurate labels by orchestrating multiple
foundation models via a novel consensus mechanism incorporating non-maximum
suppression and voting. Our large-scale validation proves the effectiveness of
Labeling Copilot's components. The Consensus Annotation module excels at object
discovery: on the dense COCO dataset, it averages 14.2 candidate proposals per
image-nearly double the 7.4 ground-truth objects-achieving a final annotation
mAP of 37.1%. On the web-scale Open Images dataset, it navigated extreme class
imbalance to discover 903 new bounding box categories, expanding its capability
to over 1500 total. Concurrently, our Calibrated Discovery tool, tested at a
10-million sample scale, features an active learning strategy that is up to 40x
more computationally efficient than alternatives with equivalent sample
efficiency. These experiments validate that an agentic workflow with optimized,
scalable tools provides a robust foundation for curating industrial-scale
datasets.

</details>


### [120] [HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion Models](https://arxiv.org/abs/2509.22300)
*Seyedmorteza Sadat,Farnood Salehi,Romann M. Weber*

Main category: cs.CV

TL;DR: HiGS通过历史预测集成，提高了扩散模型的图像生成质量及效率，且不需要额外的计算或训练。


<details>
  <summary>Details</summary>
Motivation: 针对扩散模型生成图像时输出不够逼真且细节缺乏的问题，尤其是在使用较少的神经函数评估（NFE）或较低的引导规模时。

Method: 提出了一种基于动量的采样技术，即历史指导采样（HiGS），通过将最近的模型预测整合到每个推理步骤中来增强扩散采样的质量和效率。

Result: 使用HiGS，相比标准的250次采样步骤，在仅用30次采样步骤的情况下，预训练的SiT模型在256×256尺寸下对无引导的ImageNet生成实现了1.61的新的最先进的FID。

Conclusion: HiGS是一种即插即用的增强技术，能够提升标准扩散采样的速度和图像质量。

Abstract: While diffusion models have made remarkable progress in image generation,
their outputs can still appear unrealistic and lack fine details, especially
when using fewer number of neural function evaluations (NFEs) or lower guidance
scales. To address this issue, we propose a novel momentum-based sampling
technique, termed history-guided sampling (HiGS), which enhances quality and
efficiency of diffusion sampling by integrating recent model predictions into
each inference step. Specifically, HiGS leverages the difference between the
current prediction and a weighted average of past predictions to steer the
sampling process toward more realistic outputs with better details and
structure. Our approach introduces practically no additional computation and
integrates seamlessly into existing diffusion frameworks, requiring neither
extra training nor fine-tuning. Extensive experiments show that HiGS
consistently improves image quality across diverse models and architectures and
under varying sampling budgets and guidance scales. Moreover, using a
pretrained SiT model, HiGS achieves a new state-of-the-art FID of 1.61 for
unguided ImageNet generation at 256$\times$256 with only 30 sampling steps
(instead of the standard 250). We thus present HiGS as a plug-and-play
enhancement to standard diffusion sampling that enables faster generation with
higher fidelity.

</details>


### [121] [Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs](https://arxiv.org/abs/2509.22646)
*Xingyu Fu,Siyi Liu,Yinuo Xu,Pan Lu,Guangqiuse Hu,Tianbo Yang,Taran Anantasagar,Christopher Shen,Yikai Mao,Yuanzhe Liu,Keyush Shah,Chung Un Lee,Yejin Choi,James Zou,Dan Roth,Chris Callison-Burch*

Main category: cs.CV

TL;DR: 本论文提供了DeeptraceReward数据集，以帮助识别AI生成视频中的伪造迹象，并训练模型模拟人类判断，旨在提高视频生成的可信度。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在填补人类在识别AI生成视频中深度伪造迹象的能力与视频生成模型快速发展的能力之间的空白。

Method: 通过构建DeeptraceReward数据集，进行了对视频生成中的深度伪造迹象的注释，并训练了多模态语言模型进行人类判断的模拟。

Result: 研究表明，使用DeeptraceReward的数据集，7B奖励模型在假线索识别、定位及解释上比GPT-5平均提高了34.7%。

Conclusion: DeeptraceReward数据集提供了一个系统的方法来识别和理解AI生成视频中的虚假迹象，促进了更可信的视频生成技术。

Abstract: Can humans identify AI-generated (fake) videos and provide grounded reasons?
While video generation models have advanced rapidly, a critical dimension --
whether humans can detect deepfake traces within a generated video, i.e.,
spatiotemporal grounded visual artifacts that reveal a video as machine
generated -- has been largely overlooked. We introduce DeeptraceReward, the
first fine-grained, spatially- and temporally- aware benchmark that annotates
human-perceived fake traces for video generation reward. The dataset comprises
4.3K detailed annotations across 3.3K high-quality generated videos. Each
annotation provides a natural-language explanation, pinpoints a bounding-box
region containing the perceived trace, and marks precise onset and offset
timestamps. We consolidate these annotations into 9 major categories of
deepfake traces that lead humans to identify a video as AI-generated, and train
multimodal language models (LMs) as reward models to mimic human judgments and
localizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by
34.7% on average across fake clue identification, grounding, and explanation.
Interestingly, we observe a consistent difficulty gradient: binary fake v.s.
real classification is substantially easier than fine-grained deepfake trace
detection; within the latter, performance degrades from natural language
explanations (easiest), to spatial grounding, to temporal labeling (hardest).
By foregrounding human-perceived deepfake traces, DeeptraceReward provides a
rigorous testbed and training signal for socially aware and trustworthy video
generation.

</details>


### [122] [Johnson-Lindenstrauss Lemma Guided Network for Efficient 3D Medical Segmentation](https://arxiv.org/abs/2509.22307)
*Jinpeng Lu,Linghan Cai,Yinda Chen,Guo Tang,Songhan Jiang,Haoyuan Shi,Zhiwei Xiong*

Main category: cs.CV

TL;DR: 本研究提出 VeloxSeg，一种高效的轻量级 3D 医学图像分割框架，通过双流 CNN-Transformer 架构和知识转移显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究轻量级 3D 医学图像分割中的效率和鲁棒性矛盾，尤其是在处理复杂解剖结构和异构模态时。

Method: 该方法采用双流 CNN-Transformer 架构，包括配对窗口注意力 (PWA) 和引导卷积 (JLC)，并通过空间解耦知识转移 (SDKT) 引入自监督网络的纹理先验。

Result: 该方法显著增强了模型在低计算预算下的操作能力，并有效建模异构模态。

Conclusion: VeloxSeg 在多模态基准测试中取得了 26% 的 Dice 改进，同时 GPU 吞吐量提高了 11 倍，CPU 吞吐量提高了 48 倍。

Abstract: Lightweight 3D medical image segmentation remains constrained by a
fundamental "efficiency / robustness conflict", particularly when processing
complex anatomical structures and heterogeneous modalities. In this paper, we
study how to redesign the framework based on the characteristics of
high-dimensional 3D images, and explore data synergy to overcome the fragile
representation of lightweight methods. Our approach, VeloxSeg, begins with a
deployable and extensible dual-stream CNN-Transformer architecture composed of
Paired Window Attention (PWA) and Johnson-Lindenstrauss lemma-guided
convolution (JLC). For each 3D image, we invoke a "glance-and-focus" principle,
where PWA rapidly retrieves multi-scale information, and JLC ensures robust
local feature extraction with minimal parameters, significantly enhancing the
model's ability to operate with low computational budget. Followed by an
extension of the dual-stream architecture that incorporates modal interaction
into the multi-scale image-retrieval process, VeloxSeg efficiently models
heterogeneous modalities. Finally, Spatially Decoupled Knowledge Transfer
(SDKT) via Gram matrices injects the texture prior extracted by a
self-supervised network into the segmentation network, yielding stronger
representations than baselines at no extra inference cost. Experimental results
on multimodal benchmarks show that VeloxSeg achieves a 26% Dice improvement,
alongside increasing GPU throughput by 11x and CPU by 48x. Codes are available
at https://github.com/JinPLu/VeloxSeg.

</details>


### [123] [CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning](https://arxiv.org/abs/2509.22647)
*Long Xing,Xiaoyi Dong,Yuhang Zang,Yuhang Cao,Jianze Liang,Qidong Huang,Jiaqi Wang,Feng Wu,Dahua Lin*

Main category: cs.CV

TL;DR: 提出了CapRL框架，通过强化学习方法优化图像字幕生成，获得了显著性能提高，同时解决了SFT方法中的数据注释问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有监督微调(SFT)方法的局限性，特别是由于人类注释数据的昂贵和不可扩展性，导致模型在生成描述时缺乏多样性和创造性。

Method: 引入Captioning Reinforcement Learning (CapRL)框架，以使用可验证奖励的强化学习方法来训练图像字幕生成模型。

Result: 在CapRL-5M数据集上进行预训练，在12个基准测试中取得了显著的性能提升，尤其是在Prism框架下，其性能与Qwen2.5-VL-72B相当，并平均超出基准8.4%。

Conclusion: CapRL在图像字幕生成任务中表现出色，显著提高了多项基准的性能。

Abstract: Image captioning is a fundamental task that bridges the visual and linguistic
domains, playing a critical role in pre-training Large Vision-Language Models
(LVLMs). Current state-of-the-art captioning models are typically trained with
Supervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable
data annotated by humans or proprietary models. This approach often leads to
models that memorize specific ground-truth answers, limiting their generality
and ability to generate diverse, creative descriptions. To overcome the
limitation of SFT, we propose applying the Reinforcement Learning with
Verifiable Rewards (RLVR) paradigm to the open-ended task of image captioning.
A primary challenge, however, is designing an objective reward function for the
inherently subjective nature of what constitutes a "good" caption. We introduce
Captioning Reinforcement Learning (CapRL), a novel training framework that
redefines caption quality through its utility: a high-quality caption should
enable a non-visual language model to accurately answer questions about the
corresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM
generates a caption, and the objective reward is derived from the accuracy of a
separate, vision-free LLM answering Multiple-Choice Questions based solely on
that caption. As the first study to apply RLVR to the subjective image
captioning task, we demonstrate that CapRL significantly enhances multiple
settings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B
results in substantial gains across 12 benchmarks. Moreover, within the Prism
Framework for caption quality evaluation, CapRL achieves performance comparable
to Qwen2.5-VL-72B, while exceeding the baseline by an average margin of 8.4%.
Code is available here: https://github.com/InternLM/CapRL.

</details>


### [124] [NIFTY: a Non-Local Image Flow Matching for Texture Synthesis](https://arxiv.org/abs/2509.22318)
*Pierrick Chatillon,Julien Rabin,David Tschumperlé*

Main category: cs.CV

TL;DR: 本论文提出了NIFTY，一个结合现代扩散模型和经典补丁技术的纹理合成框架，展示了其在减少视觉伪影和提高合成效果上的优势。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决基于示例的纹理合成中存在的初始化差差或视觉伪影等问题。

Method: NIFTY是一个非参数流匹配模型，采用非局部补丁匹配，避免了神经网络训练的需求。

Result: 实验结果表明，NIFTY在与文献中的代表性方法相比时，表现出了有效性。

Conclusion: NIFTY框架在纹理合成任务中表现出色，通过结合扩散模型和传统的基于补丁的优化技术，克服了许多常见的问题。

Abstract: This paper addresses the problem of exemplar-based texture synthesis. We
introduce NIFTY, a hybrid framework that combines recent insights on diffusion
models trained with convolutional neural networks, and classical patch-based
texture optimization techniques. NIFTY is a non-parametric flow-matching model
built on non-local patch matching, which avoids the need for neural network
training while alleviating common shortcomings of patch-based methods, such as
poor initialization or visual artifacts. Experimental results demonstrate the
effectiveness of the proposed approach compared to representative methods from
the literature. Code is available at https://github.com/PierrickCh/Nifty.git

</details>


### [125] [RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion Transformer](https://arxiv.org/abs/2509.22323)
*Wangbo Zhao,Yizeng Han,Zhiwei Tang,Jiasheng Tang,Pengfei Zhou,Kai Wang,Bohan Zhuang,Zhangyang Wang,Fan Wang,Yang You*

Main category: cs.CV

TL;DR: RAPID3是一种基于图像级别的加速框架，可以提高Diffusion Transformers的采样速度，几乎达到3倍，并保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的加速器通常依赖统一启发式方法，无法针对每幅图像进行性能优化，且动态神经网络的高微调成本限制了其广泛适用性。

Method: 引入RAPID3框架，包括三种轻量级策略头（Step-Skip、Cache-Reuse和Sparse-Attention），在每个时间步独立决策加速，并利用在线训练和对抗学习增强奖励信号。

Result: RAPID3在包括Stable Diffusion 3和FLUX的最新DiT骨干网络上，实现了近3倍的采样速度，同时维持了竞争力的生成质量。

Conclusion: RAPID3框架在不改变基础生成器的情况下，通过图像级加速实现了近3倍的采样速度，并维持了竞争性的生成质量。

Abstract: Diffusion Transformers (DiTs) excel at visual generation yet remain hampered
by slow sampling. Existing training-free accelerators - step reduction, feature
caching, and sparse attention - enhance inference speed but typically rely on a
uniform heuristic or a manually designed adaptive strategy for all images,
leaving quality on the table. Alternatively, dynamic neural networks offer
per-image adaptive acceleration, but their high fine-tuning costs limit broader
applicability. To address these limitations, we introduce RAPID3: Tri-Level
Reinforced Acceleration Policies for Diffusion Transformers, a framework that
delivers image-wise acceleration with zero updates to the base generator.
Specifically, three lightweight policy heads - Step-Skip, Cache-Reuse, and
Sparse-Attention - observe the current denoising state and independently decide
their corresponding speed-up at each timestep. All policy parameters are
trained online via Group Relative Policy Optimization (GRPO) while the
generator remains frozen. Meanwhile, an adversarially learned discriminator
augments the reward signal, discouraging reward hacking by boosting returns
only when generated samples stay close to the original model's distribution.
Across state-of-the-art DiT backbones, including Stable Diffusion 3 and FLUX,
RAPID3 achieves nearly 3x faster sampling with competitive generation quality.

</details>


### [126] [Pedestrian Attribute Recognition via Hierarchical Cross-Modality HyperGraph Learning](https://arxiv.org/abs/2509.22331)
*Xiao Wang,Shujuan Wu,Xiaoxia Cheng,Changwei Bi,Jin Tang,Bin Luo*

Main category: cs.CV

TL;DR: 本论文提出一种多模态知识图和跨模态超图学习框架，以提高行人属性识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 为了充分利用属性知识和上下文信息，提高行人属性识别的准确性。

Method: 构建多模态知识图，并利用知识图引导的跨模态超图学习框架进行行人属性识别。

Result: 在多个行人属性识别基准数据集上进行了全面实验，证明了所提知识图方法的有效性。

Conclusion: 本文提出的多模态知识图有效提升了行人属性识别的准确性，为后续的知识驱动识别奠定了基础。

Abstract: Current Pedestrian Attribute Recognition (PAR) algorithms typically focus on
mapping visual features to semantic labels or attempt to enhance learning by
fusing visual and attribute information. However, these methods fail to fully
exploit attribute knowledge and contextual information for more accurate
recognition. Although recent works have started to consider using attribute
text as additional input to enhance the association between visual and semantic
information, these methods are still in their infancy. To address the above
challenges, this paper proposes the construction of a multi-modal knowledge
graph, which is utilized to mine the relationships between local visual
features and text, as well as the relationships between attributes and
extensive visual context samples. Specifically, we propose an effective
multi-modal knowledge graph construction method that fully considers the
relationships among attributes and the relationships between attributes and
vision tokens. To effectively model these relationships, this paper introduces
a knowledge graph-guided cross-modal hypergraph learning framework to enhance
the standard pedestrian attribute recognition framework. Comprehensive
experiments on multiple PAR benchmark datasets have thoroughly demonstrated the
effectiveness of our proposed knowledge graph for the PAR task, establishing a
strong foundation for knowledge-guided pedestrian attribute recognition. The
source code of this paper will be released on
https://github.com/Event-AHU/OpenPAR

</details>


### [127] [CircuitSense: A Hierarchical Circuit System Benchmark Bridging Visual Comprehension and Symbolic Reasoning in Engineering Design Process](https://arxiv.org/abs/2509.22339)
*Arman Akbari,Jian Gao,Yifei Zou,Mei Yang,Jinru Duan,Dmitrii Torbunov,Yanzhi Wang,Yihui Ren,Xuan Zhang*

Main category: cs.CV

TL;DR: 本研究提出了CircuitSense基准，评估多模态语言模型在电路理解中的能力，发现符号推理是电路设计中的关键能力，目前模型在视觉到数学推理方面仍存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 探索多模态大型语言模型在从技术图表提取数学模型方面的潜力，以及评估电路理解的整体工程工作流程。

Method: 引入了一个分层的合成生成流水线，包括基于网格的原理图生成器和自动生成符号方程标签的框图生成器。

Result: 对六种最先进的多模态大型语言模型的全面评估揭示了视觉到数学推理的基本局限性，闭源模型在符号推导和分析推理上的表现低于19%。

Conclusion: 在电路综合中，符号推理是工程能力的关键指标，强调了数学理解在设计任务中的重要性。

Abstract: Engineering design operates through hierarchical abstraction from system
specifications to component implementations, requiring visual understanding
coupled with mathematical reasoning at each level. While Multi-modal Large
Language Models (MLLMs) excel at natural image tasks, their ability to extract
mathematical models from technical diagrams remains unexplored. We present
\textbf{CircuitSense}, a comprehensive benchmark evaluating circuit
understanding across this hierarchy through 8,006+ problems spanning
component-level schematics to system-level block diagrams. Our benchmark
uniquely examines the complete engineering workflow: Perception, Analysis, and
Design, with a particular emphasis on the critical but underexplored capability
of deriving symbolic equations from visual inputs. We introduce a hierarchical
synthetic generation pipeline consisting of a grid-based schematic generator
and a block diagram generator with auto-derived symbolic equation labels.
Comprehensive evaluation of six state-of-the-art MLLMs, including both
closed-source and open-source models, reveals fundamental limitations in
visual-to-mathematical reasoning. Closed-source models achieve over 85\%
accuracy on perception tasks involving component recognition and topology
identification, yet their performance on symbolic derivation and analytical
reasoning falls below 19\%, exposing a critical gap between visual parsing and
symbolic reasoning. Models with stronger symbolic reasoning capabilities
consistently achieve higher design task accuracy, confirming the fundamental
role of mathematical understanding in circuit synthesis and establishing
symbolic reasoning as the key metric for engineering competence.

</details>


### [128] [HierLight-YOLO: A Hierarchical and Lightweight Object Detection Network for UAV Photography](https://arxiv.org/abs/2509.22365)
*Defan Chen,Yaohua Hu,Luchan Zhang*

Main category: cs.CV

TL;DR: 本论文提出HierLight-YOLO，一种针对小物体检测优化的轻量模型，能在资源受限的平台上实现高效实时检测。


<details>
  <summary>Details</summary>
Motivation: 解决无人机拍摄中的小物体实时检测问题，提升YOLO系列在小物体检测中的应用性能。

Method: 提出Hierarchical Extended Path Aggregation Network (HEPAN)方法，结合了轻量化模块IRDCB和LDown，实现高效的小物体检测。

Result: 在VisDrone2019基准测试中，HierLight-YOLO展示了领先的检测性能和效率。

Conclusion: HierLight-YOLO在小物体检测上表现出色，具有较低的假阴性率和优良的实时性能，是资源受限平台上的有效解决方案。

Abstract: The real-time detection of small objects in complex scenes, such as the
unmanned aerial vehicle (UAV) photography captured by drones, has dual
challenges of detecting small targets (<32 pixels) and maintaining real-time
efficiency on resource-constrained platforms. While YOLO-series detectors have
achieved remarkable success in real-time large object detection, they suffer
from significantly higher false negative rates for drone-based detection where
small objects dominate, compared to large object scenarios. This paper proposes
HierLight-YOLO, a hierarchical feature fusion and lightweight model that
enhances the real-time detection of small objects, based on the YOLOv8
architecture. We propose the Hierarchical Extended Path Aggregation Network
(HEPAN), a multi-scale feature fusion method through hierarchical cross-level
connections, enhancing the small object detection accuracy. HierLight-YOLO
includes two innovative lightweight modules: Inverted Residual Depthwise
Convolution Block (IRDCB) and Lightweight Downsample (LDown) module, which
significantly reduce the model's parameters and computational complexity
without sacrificing detection capabilities. Small object detection head is
designed to further enhance spatial resolution and feature fusion to tackle the
tiny object (4 pixels) detection. Comparison experiments and ablation studies
on the VisDrone2019 benchmark demonstrate state-of-the-art performance of
HierLight-YOLO.

</details>


### [129] [Effectiveness of Large Multimodal Models in Detecting Disinformation: Experimental Results](https://arxiv.org/abs/2509.22377)
*Yasmina Kheddache,Marc Lalonde*

Main category: cs.CV

TL;DR: 本论文研究了大规模多模态模型在多模态虚假信息检测中的应用，提出了一系列方法和评估标准，展现了模型的强项与不足，为自动化分析提供了方法论框架。


<details>
  <summary>Details</summary>
Motivation: 随着数字平台上虚假信息的传播愈加严重，尤其是在文本和图像的多模态环境中，本研究旨在探索大规模多模态模型在检测和缓解虚假信息方面的潜力。

Method: 研究采用优化的提示工程技术，结合多模态分析结构框架，进行图像和文本的预处理，以满足模型的令牌限制，同时定义六个特定评价标准。

Result: 通过在多种异质数据集上进行模型性能分析，研究突出展示了GPT-4o在虚假信息检测中的优点及局限性，并探讨了通过重复测试预测变异性的稳定性和可靠性。

Conclusion: 本研究提供了一种针对多模态虚假信息自动分析的稳健和可重复的方法论框架，利用GPT-4o模型的优势进行准确检测和评估。

Abstract: The proliferation of disinformation, particularly in multimodal contexts
combining text and images, presents a significant challenge across digital
platforms. This study investigates the potential of large multimodal models
(LMMs) in detecting and mitigating false information. We propose to approach
multimodal disinformation detection by leveraging the advanced capabilities of
the GPT-4o model. Our contributions include: (1) the development of an
optimized prompt incorporating advanced prompt engineering techniques to ensure
precise and consistent evaluations; (2) the implementation of a structured
framework for multimodal analysis, including a preprocessing methodology for
images and text to comply with the model's token limitations; (3) the
definition of six specific evaluation criteria that enable a fine-grained
classification of content, complemented by a self-assessment mechanism based on
confidence levels; (4) a comprehensive performance analysis of the model across
multiple heterogeneous datasets Gossipcop, Politifact, Fakeddit, MMFakeBench,
and AMMEBA highlighting GPT-4o's strengths and limitations in disinformation
detection; (5) an investigation of prediction variability through repeated
testing, evaluating the stability and reliability of the model's
classifications; and (6) the introduction of confidence-level and
variability-based evaluation methods. These contributions provide a robust and
reproducible methodological framework for automated multimodal disinformation
analysis.

</details>


### [130] [GPT-4 for Occlusion Order Recovery](https://arxiv.org/abs/2509.22383)
*Kaziwa Saleh,Zhyar Rzgar K Rostam,Sándor Szénási,Zoltán Vámossy*

Main category: cs.CV

TL;DR: 本研究提出利用预训练的GPT-4模型，通过设计提示语来增强遮挡顺序关系的预测能力，且可以零样本推理，表现出比基线方法更高的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型在处理复杂且密集的真实世界图像时，在遮挡问题上表现不佳，因此本研究旨在通过引入GPT-4模型来改善遮挡关系的预测。

Method: 信息提示结合输入图像，通过GPT-4进行分析生成遮挡顺序预测，从而构建遮挡矩阵。

Result: 在COCOA和InstaOrder数据集的评估中，模型通过利用语义上下文、视觉模式和常识知识，能够提供更准确的遮挡顺序预测。

Conclusion: 该方法通过使用预训练的GPT-4模型，使得在不需要标注训练数据的情况下，高效处理遮挡顺序关系，并在实际应用中展现出更好的准确性。

Abstract: Occlusion remains a significant challenge for current vision models to
robustly interpret complex and dense real-world images and scenes. To address
this limitation and to enable accurate prediction of the occlusion order
relationship between objects, we propose leveraging the advanced capability of
a pre-trained GPT-4 model to deduce the order. By providing a specifically
designed prompt along with the input image, GPT-4 can analyze the image and
generate order predictions. The response can then be parsed to construct an
occlusion matrix which can be utilized in assisting with other occlusion
handling tasks and image understanding. We report the results of evaluating the
model on COCOA and InstaOrder datasets. The results show that by using semantic
context, visual patterns, and commonsense knowledge, the model can produce more
accurate order predictions. Unlike baseline methods, the model can reason about
occlusion relationships in a zero-shot fashion, which requires no annotated
training data and can easily be integrated into occlusion handling frameworks.

</details>


### [131] [Gradient-based multi-focus image fusion with focus-aware saliency enhancement](https://arxiv.org/abs/2509.22392)
*Haoyu Li,XiaoSong Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的多焦点图像融合方法，通过显著边界增强和基于梯度的模型，克服了现有方法的局限，实验结果显示其优于12种先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多焦点图像融合方法难以保持清晰的焦点-失焦边界，导致模糊过渡和细节丢失。

Method: 基于梯度域的模型，通过Tenengrad梯度检测提取显著特征，并开发了基于梯度和互补信息的聚焦度量进行边界精细化。

Result: 通过显著边界增强，我们的方法生成高质量的融合图像，同时有效检测焦点信息，保持边界细节。

Conclusion: 我们的多焦点图像融合方法在主观和客观评估中均优于12种先进方法。

Abstract: Multi-focus image fusion (MFIF) aims to yield an all-focused image from
multiple partially focused inputs, which is crucial in applications cover
sur-veillance, microscopy, and computational photography. However, existing
methods struggle to preserve sharp focus-defocus boundaries, often resulting in
blurred transitions and focused details loss. To solve this problem, we propose
a MFIF method based on significant boundary enhancement, which generates
high-quality fused boundaries while effectively detecting focus in-formation.
Particularly, we propose a gradient-domain-based model that can obtain initial
fusion results with complete boundaries and effectively pre-serve the boundary
details. Additionally, we introduce Tenengrad gradient detection to extract
salient features from both the source images and the ini-tial fused image,
generating the corresponding saliency maps. For boundary refinement, we develop
a focus metric based on gradient and complementary information, integrating the
salient features with the complementary infor-mation across images to emphasize
focused regions and produce a high-quality initial decision result. Extensive
experiments on four public datasets demonstrate that our method consistently
outperforms 12 state-of-the-art methods in both subjective and objective
evaluations. We have realized codes in https://github.com/Lihyua/GICI

</details>


### [132] [Text Adversarial Attacks with Dynamic Outputs](https://arxiv.org/abs/2509.22393)
*Wenqiang Wang,Siyuan Liang,Xiao Yan,Xiaochun Cao*

Main category: cs.CV

TL;DR: TDOA是一种有效的文本对抗攻击方法，能够在动态和静态输出场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有文本对抗攻击方法多针对静态场景，缺乏应对动态输出场景的有效工具。

Method: TDOA方法通过聚类基础的代理模型训练，将动态输出场景转化为静态单输出场景，并采用最远标签目标攻击策略，选择与模型粗粒度标签偏差最大的对抗向量。

Result: TDOA在四个数据集和八个受害模型上进行广泛评估，单查询下达成最大攻击成功率50.81%，在传统静态场景中达到82.68%的最高ASR，并在生成设置中超越之前结果。

Conclusion: TDOA方法展示了在动态输出场景中有效生成对抗样本的潜力，并在传统静态输出场景中表现优异。

Abstract: Text adversarial attack methods are typically designed for static scenarios
with fixed numbers of output labels and a predefined label space, relying on
extensive querying of the victim model (query-based attacks) or the surrogate
model (transfer-based attacks). To address this gap, we introduce the Textual
Dynamic Outputs Attack (TDOA) method, which employs a clustering-based
surrogate model training approach to convert the dynamic-output scenario into a
static single-output scenario. To improve attack effectiveness, we propose the
farthest-label targeted attack strategy, which selects adversarial vectors that
deviate most from the model's coarse-grained labels, thereby maximizing
disruption. We extensively evaluate TDOA on four datasets and eight victim
models (e.g., ChatGPT-4o, ChatGPT-4.1), showing its effectiveness in crafting
adversarial examples and its strong potential to compromise large language
models with limited access. With a single query per text, TDOA achieves a
maximum attack success rate of 50.81\%. Additionally, we find that TDOA also
achieves state-of-the-art performance in conventional static output scenarios,
reaching a maximum ASR of 82.68\%. Meanwhile, by conceptualizing translation
tasks as classification problems with unbounded output spaces, we extend the
TDOA framework to generative settings, surpassing prior results by up to 0.64
RDBLEU and 0.62 RDchrF.

</details>


### [133] [Integrating Background Knowledge in Medical Semantic Segmentation with Logic Tensor Networks](https://arxiv.org/abs/2509.22399)
*Luca Bergamin,Giovanna Maria Dimitri,Fabio Aiolli*

Main category: cs.CV

TL;DR: 本研究提出了一种结合医学知识和逻辑张量网络的语义分割方法，能够提升医学图像分析的效果，特别是在数据不足的情况下。


<details>
  <summary>Details</summary>
Motivation: 通过将常见医学知识融入分割模型的损失函数，提升分割性能，帮助放射科医生在医学图像中区分对象。

Method: 采用逻辑张量网络（LTNs）将医学背景知识编码为一阶逻辑规则，并结合SwinUNETR构建端到端框架进行语义分割。

Result: 实验结果表明，LTNs在海马体脑MRI扫描分割任务中显著提高了基线分割性能。

Conclusion: 神经符号方法能够有效提升医学图像语义分割的性能，尤其在训练数据稀缺时表现良好，并且具有广泛适应性。

Abstract: Semantic segmentation is a fundamental task in medical image analysis, aiding
medical decision-making by helping radiologists distinguish objects in an
image. Research in this field has been driven by deep learning applications,
which have the potential to scale these systems even in the presence of noise
and artifacts. However, these systems are not yet perfected. We argue that
performance can be improved by incorporating common medical knowledge into the
segmentation model's loss function. To this end, we introduce Logic Tensor
Networks (LTNs) to encode medical background knowledge using first-order logic
(FOL) rules. The encoded rules span from constraints on the shape of the
produced segmentation, to relationships between different segmented areas. We
apply LTNs in an end-to-end framework with a SwinUNETR for semantic
segmentation. We evaluate our method on the task of segmenting the hippocampus
in brain MRI scans. Our experiments show that LTNs improve the baseline
segmentation performance, especially when training data is scarce. Despite
being in its preliminary stages, we argue that neurosymbolic methods are
general enough to be adapted and applied to other medical semantic segmentation
tasks.

</details>


### [134] [Closing the Safety Gap: Surgical Concept Erasure in Visual Autoregressive Models](https://arxiv.org/abs/2509.22400)
*Xinhao Zhong,Yimin Zhou,Zhiqi Zhang,Junhao Li,Yi Sun,Bin Chen,Shu-Tao Xia,Ke Xu*

Main category: cs.CV

TL;DR: 本文提出了VARE框架及S-VARE概念抹除方法，针对VAR模型有效解决了安全性问题，实现了概念精准抹除和高质量生成。


<details>
  <summary>Details</summary>
Motivation: 鉴于现有的概念抹除技术在VAR模型中无法有效实施，本文旨在通过新的框架提供一种可靠的概念抹除解决方案。

Method: 提出了VARE框架和S-VARE方法，通过引入过滤的交叉熵损失和保持损失实现概念抹除。

Result: 大量实验表明，所提方法能够实现精确的概念抹除，同时保持生成质量，改善了自回归文本到图像生成的安全性。

Conclusion: VARE和S-VARE方法有效地实现了VAR模型中概念的精确抹除，同时保持了生成质量，从而减少了自回归文本到图像生成中的安全隐患。

Abstract: The rapid progress of visual autoregressive (VAR) models has brought new
opportunities for text-to-image generation, but also heightened safety
concerns. Existing concept erasure techniques, primarily designed for diffusion
models, fail to generalize to VARs due to their next-scale token prediction
paradigm. In this paper, we first propose a novel VAR Erasure framework VARE
that enables stable concept erasure in VAR models by leveraging auxiliary
visual tokens to reduce fine-tuning intensity. Building upon this, we introduce
S-VARE, a novel and effective concept erasure method designed for VAR, which
incorporates a filtered cross entropy loss to precisely identify and minimally
adjust unsafe visual tokens, along with a preservation loss to maintain
semantic fidelity, addressing the issues such as language drift and reduced
diversity introduce by na\"ive fine-tuning. Extensive experiments demonstrate
that our approach achieves surgical concept erasure while preserving generation
quality, thereby closing the safety gap in autoregressive text-to-image
generation by earlier methods.

</details>


### [135] [RAU: Reference-based Anatomical Understanding with Vision Language Models](https://arxiv.org/abs/2509.22404)
*Yiwei Li,Yikang Liu,Jiaqi Guo,Lin Zhao,Zheyuan Zhang,Xiao Chen,Boris Mailhe,Ankush Mukherjee,Terrence Chen,Shanhui Sun*

Main category: cs.CV

TL;DR: RAU是首个探索VLM在医学图像中进行解剖结构识别、定位和分割能力的框架，通过结合空间推理和细粒度分割，展示了在不同分布数据集上的优秀表现，标志着VLM驱动方法在医学应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于专家标注数据的稀缺，自动生成报告和器官定位面临挑战，因此需要利用带注释的参考图像指导未标记目标的解读。

Method: RAU框架通过视觉语言模型（VLM）进行相对空间推理，结合SAM2进行细粒度分割。

Result: 在多个数据集上，RAU框架超越了传统的SAM2微调基线，提供了更准确的分割和更可靠的定位。

Conclusion: RAU框架在医学图像中实现了基于参考的解剖理解，显示出优越的分割和定位能力，尤其是在处理不同分布的数据集时。

Abstract: Anatomical understanding through deep learning is critical for automatic
report generation, intra-operative navigation, and organ localization in
medical imaging; however, its progress is constrained by the scarcity of
expert-labeled data. A promising remedy is to leverage an annotated reference
image to guide the interpretation of an unlabeled target. Although recent
vision-language models (VLMs) exhibit non-trivial visual reasoning, their
reference-based understanding and fine-grained localization remain limited. We
introduce RAU, a framework for reference-based anatomical understanding with
VLMs. We first show that a VLM learns to identify anatomical regions through
relative spatial reasoning between reference and target images, trained on a
moderately sized dataset. We validate this capability through visual question
answering (VQA) and bounding box prediction. Next, we demonstrate that the
VLM-derived spatial cues can be seamlessly integrated with the fine-grained
segmentation capability of SAM2, enabling localization and pixel-level
segmentation of small anatomical regions, such as vessel segments. Across two
in-distribution and two out-of-distribution datasets, RAU consistently
outperforms a SAM2 fine-tuning baseline using the same memory setup, yielding
more accurate segmentations and more reliable localization. More importantly,
its strong generalization ability makes it scalable to out-of-distribution
datasets, a property crucial for medical image applications. To the best of our
knowledge, RAU is the first to explore the capability of VLMs for
reference-based identification, localization, and segmentation of anatomical
structures in medical images. Its promising performance highlights the
potential of VLM-driven approaches for anatomical understanding in automated
clinical workflows.

</details>


### [136] [FreqDebias: Towards Generalizable Deepfake Detection via Consistency-Driven Frequency Debiasing](https://arxiv.org/abs/2509.22412)
*Hossein Kashiani,Niloufar Alipour Talemi,Fatemeh Afghah*

Main category: cs.CV

TL;DR: 本文提出FreqDebias框架，通过频率去偏和多样化训练样本，提升深度伪造检测器的跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度伪造检测器因训练数据有限而难以对新伪造类型泛化，存在频域的模型偏倚。

Method: 提出了一种频率去偏框架FreqDebias，包括Forge Mixup增强和双一致性正则化策略。

Result: 通过引入动态多样化频率特征的Forge Mixup增强和基于类激活图与von Mises-Fisher分布的双一致性正则化，成功减轻了频域偏倚。

Conclusion: FreqDebias显著提升了跨域泛化能力，并在跨域和内域设置中优于现有最佳方法。

Abstract: Deepfake detectors often struggle to generalize to novel forgery types due to
biases learned from limited training data. In this paper, we identify a new
type of model bias in the frequency domain, termed spectral bias, where
detectors overly rely on specific frequency bands, restricting their ability to
generalize across unseen forgeries. To address this, we propose FreqDebias, a
frequency debiasing framework that mitigates spectral bias through two
complementary strategies. First, we introduce a novel Forgery Mixup (Fo-Mixup)
augmentation, which dynamically diversifies frequency characteristics of
training samples. Second, we incorporate a dual consistency regularization
(CR), which enforces both local consistency using class activation maps (CAMs)
and global consistency through a von Mises-Fisher (vMF) distribution on a
hyperspherical embedding space. This dual CR mitigates over-reliance on certain
frequency components by promoting consistent representation learning under both
local and global supervision. Extensive experiments show that FreqDebias
significantly enhances cross-domain generalization and outperforms
state-of-the-art methods in both cross-domain and in-domain settings.

</details>


### [137] [LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale Diffusion Transformer](https://arxiv.org/abs/2509.22414)
*Song Fei,Tian Ye,Lujia Wang,Lei Zhu*

Main category: cs.CV

TL;DR: LucidFlux是一个无文本提示的通用图像恢复框架，通过优化信号注入和调制策略，显著提升了图像恢复的鲁棒性和效果。


<details>
  <summary>Details</summary>
Motivation: 传统的图像恢复方法往往由于依赖文本提示而出现过平滑、幻想或漂移等问题，故需要一种无文本提示的解决方案。

Method: LucidFlux采用了一个轻量级的双分支调节器，并设计了适应性调制调度，以有效利用输入图像信号。

Result: 在合成数据和真实世界基准测试中，LucidFlux始终优于强大的开源和商业基准，并且消融研究验证了各组件的必要性。

Conclusion: LucidFlux在综合性图像恢复任务中表现优异，且通过无文本提示的方式实现了鲁棒性和高效性。

Abstract: Universal image restoration (UIR) aims to recover images degraded by unknown
mixtures while preserving semantics -- conditions under which discriminative
restorers and UNet-based diffusion priors often oversmooth, hallucinate, or
drift. We present LucidFlux, a caption-free UIR framework that adapts a large
diffusion transformer (Flux.1) without image captions. LucidFlux introduces a
lightweight dual-branch conditioner that injects signals from the degraded
input and a lightly restored proxy to respectively anchor geometry and suppress
artifacts. Then, a timestep- and layer-adaptive modulation schedule is designed
to route these cues across the backbone's hierarchy, in order to yield
coarse-to-fine and context-aware updates that protect the global structure
while recovering texture. After that, to avoid the latency and instability of
text prompts or MLLM captions, we enforce caption-free semantic alignment via
SigLIP features extracted from the proxy. A scalable curation pipeline further
filters large-scale data for structure-rich supervision. Across synthetic and
in-the-wild benchmarks, LucidFlux consistently outperforms strong open-source
and commercial baselines, and ablation studies verify the necessity of each
component. LucidFlux shows that, for large DiTs, when, where, and what to
condition on -- rather than adding parameters or relying on text prompts -- is
the governing lever for robust and caption-free universal image restoration in
the wild.

</details>


### [138] [Explaining multimodal LLMs via intra-modal token interactions](https://arxiv.org/abs/2509.22415)
*Jiawei Liang,Ruoyu Chen,Xianghao Jiao,Siyuan Liang,Shiming Liu,Qunli Zhang,Zheng Hu,Xiaochun Cao*

Main category: cs.CV

TL;DR: 本文提出通过多尺度解释聚合和激活排名关联来改善多模态大语言模型的可解释性，克服了现有方法中的不足。


<details>
  <summary>Details</summary>
Motivation: MLLM的内部决策机制尚未充分理解，现有方法忽略了模态内部的依赖关系。

Method: 采用多尺度解释聚合（MSEA）和激活排名关联（ARC）来提升对视觉和文本模态的可解释性。

Result: 我们的实验表明，所提出的方法在多种最先进的MLLM和基准数据集上，始终优于现有解释性方法。

Conclusion: 我们的方法在解释性方面优于现有方法，提供了更真实和更细致的模型行为解释。

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable success
across diverse vision-language tasks, yet their internal decision-making
mechanisms remain insufficiently understood. Existing interpretability research
has primarily focused on cross-modal attribution, identifying which image
regions the model attends to during output generation. However, these
approaches often overlook intra-modal dependencies. In the visual modality,
attributing importance to isolated image patches ignores spatial context due to
limited receptive fields, resulting in fragmented and noisy explanations. In
the textual modality, reliance on preceding tokens introduces spurious
activations. Failing to effectively mitigate these interference compromises
attribution fidelity. To address these limitations, we propose enhancing
interpretability by leveraging intra-modal interaction. For the visual branch,
we introduce \textit{Multi-Scale Explanation Aggregation} (MSEA), which
aggregates attributions over multi-scale inputs to dynamically adjust receptive
fields, producing more holistic and spatially coherent visual explanations. For
the textual branch, we propose \textit{Activation Ranking Correlation} (ARC),
which measures the relevance of contextual tokens to the current token via
alignment of their top-$k$ prediction rankings. ARC leverages this relevance to
suppress spurious activations from irrelevant contexts while preserving
semantically coherent ones. Extensive experiments across state-of-the-art MLLMs
and benchmark datasets demonstrate that our approach consistently outperforms
existing interpretability methods, yielding more faithful and fine-grained
explanations of model behavior.

</details>


### [139] [U-MAN: U-Net with Multi-scale Adaptive KAN Network for Medical Image Segmentation](https://arxiv.org/abs/2509.22444)
*Bohan Huang,Qianyun Bao,Haoyuan Ma*

Main category: cs.CV

TL;DR: 提出了U-MAN架构，通过PAGF和MAN模块，提高了医学图像分割的细节和边界表现。


<details>
  <summary>Details</summary>
Motivation: 解决传统U-Net在医学图像分割中存在的细节和边界保留能力不足的问题。

Method: 提出了U-Net与多尺度自适应KAN（U-MAN）架构，结合了PAGF和MAN两个模块。

Result: 在三个公共数据集（BUSI、GLAS和CVC）中，U-MAN在准确边界定义和细节保留上优于当前先进的方法。

Conclusion: U-MAN 结构在边界定义和细节保留方面优于现有方法，展示了其在医学图像分割中的有效性。

Abstract: Medical image segmentation faces significant challenges in preserving
fine-grained details and precise boundaries due to complex anatomical
structures and pathological regions. These challenges primarily stem from two
key limitations of conventional U-Net architectures: (1) their simple skip
connections ignore the encoder-decoder semantic gap between various features,
and (2) they lack the capability for multi-scale feature extraction in deep
layers. To address these challenges, we propose the U-Net with Multi-scale
Adaptive KAN (U-MAN), a novel architecture that enhances the emerging
Kolmogorov-Arnold Network (KAN) with two specialized modules: Progressive
Attention-Guided Feature Fusion (PAGF) and the Multi-scale Adaptive KAN (MAN).
Our PAGF module replaces the simple skip connection, using attention to fuse
features from the encoder and decoder. The MAN module enables the network to
adaptively process features at multiple scales, improving its ability to
segment objects of various sizes. Experiments on three public datasets (BUSI,
GLAS, and CVC) show that U-MAN outperforms state-of-the-art methods,
particularly in defining accurate boundaries and preserving fine details.

</details>


### [140] [$γ$-Quant: Towards Learnable Quantization for Low-bit Pattern Recognition](https://arxiv.org/abs/2509.22448)
*Mishal Fatima,Shashank Agnihotri,Marius Bock,Kanchana Vaishnavi Gandikota,Kristof Van Laerhoven,Michael Moeller,Margret Keuper*

Main category: cs.CV

TL;DR: 本研究提出$	ext{γ}$-Quant，展示了在低位深数据下也能实现高效的模式识别，与传统12位数据效果相当。


<details>
  <summary>Details</summary>
Motivation: 考虑到计算机视觉和人体活动识别任务中，传统的数据预处理方法在自动分析中的有效性不足，以及对数据传输和设备电池寿命的影响，提出了在资源受限环境下进行模式识别的新方法。

Method: 设计了一种针对低比特深度数据的非线性量化方法$	ext{γ}$-Quant，并在原始图像物体检测和可穿戴设备的人体活动识别领域进行实验。

Result: 通过使用仅4位数据的学习量化方法，$	ext{γ}$-Quant在原始图像物体检测和可穿戴的HAR任务中，显示出与使用12位原始数据相当的性能。

Conclusion: 本研究提出的$	ext{γ}$-Quant可以在低带宽和低能耗的设置中有效进行模式识别，证明了经过任务特定学习的非线性量化能与传统的高位深数据相媲美。

Abstract: Most pattern recognition models are developed on pre-proce\-ssed data. In
computer vision, for instance, RGB images processed through image signal
processing (ISP) pipelines designed to cater to human perception are the most
frequent input to image analysis networks. However, many modern vision tasks
operate without a human in the loop, raising the question of whether such
pre-processing is optimal for automated analysis. Similarly, human activity
recognition (HAR) on body-worn sensor data commonly takes normalized
floating-point data arising from a high-bit analog-to-digital converter (ADC)
as an input, despite such an approach being highly inefficient in terms of data
transmission, significantly affecting the battery life of wearable devices. In
this work, we target low-bandwidth and energy-constrained settings where
sensors are limited to low-bit-depth capture. We propose $\gamma$-Quant,
i.e.~the task-specific learning of a non-linear quantization for pattern
recognition. We exemplify our approach on raw-image object detection as well as
HAR of wearable data, and demonstrate that raw data with a learnable
quantization using as few as 4-bits can perform on par with the use of raw
12-bit data. All code to reproduce our experiments is publicly available via
https://github.com/Mishalfatima/Gamma-Quant

</details>


### [141] [SSVIF: Self-Supervised Segmentation-Oriented Visible and Infrared Image Fusion](https://arxiv.org/abs/2509.22450)
*Zixian Zhao,Xingchen Zhang*

Main category: cs.CV

TL;DR: 本研究提出了一种自监督训练框架，通过任务交叉分割一致性学习高层特征，有效地提高了可见与红外图像融合的性能，尤其在没有标注数据的情况下。


<details>
  <summary>Details</summary>
Motivation: 传统应用导向的可见和红外图像融合方法依赖于标注数据，这导致数据获取过程费时且费力，因此需要一种新的方法来解决这一问题。

Method: 提出一个自监督训练框架，通过任务间一致性学习高层语义特征，结合两阶段训练策略和动态权重调整方法。

Result: 实验结果表明，SSVIF在公共数据集上表现出色，在仅使用未标记的可见-红外图像对进行训练的情况下超过了传统的VIF方法，并且与监督的分割导向方法相当。

Conclusion: 提出的自监督训练框架SSVIF有效地实现了可见和红外图像融合，且在没有标签的情况下超过了传统方法的性能，表现出与监督方法相当的效果。

Abstract: Visible and infrared image fusion (VIF) has gained significant attention in
recent years due to its wide application in tasks such as scene segmentation
and object detection. VIF methods can be broadly classified into traditional
VIF methods and application-oriented VIF methods. Traditional methods focus
solely on improving the quality of fused images, while application-oriented VIF
methods additionally consider the performance of downstream tasks on fused
images by introducing task-specific loss terms during training. However,
compared to traditional methods, application-oriented VIF methods require
datasets labeled for downstream tasks (e.g., semantic segmentation or object
detection), making data acquisition labor-intensive and time-consuming. To
address this issue, we propose a self-supervised training framework for
segmentation-oriented VIF methods (SSVIF). Leveraging the consistency between
feature-level fusion-based segmentation and pixel-level fusion-based
segmentation, we introduce a novel self-supervised task-cross-segmentation
consistency-that enables the fusion model to learn high-level semantic features
without the supervision of segmentation labels. Additionally, we design a
two-stage training strategy and a dynamic weight adjustment method for
effective joint learning within our self-supervised framework. Extensive
experiments on public datasets demonstrate the effectiveness of our proposed
SSVIF. Remarkably, although trained only on unlabeled visible-infrared image
pairs, our SSVIF outperforms traditional VIF methods and rivals supervised
segmentation-oriented ones. Our code will be released upon acceptance.

</details>


### [142] [Bézier Meets Diffusion: Robust Generation Across Domains for Medical Image Segmentation](https://arxiv.org/abs/2509.22476)
*Chen Li,Meilong Xu,Xiaoling Hu,Weimin Lyu,Chao Chen*

Main category: cs.CV

TL;DR: 提出了Bézier Meets Diffusion框架，通过Bézier曲线风格转移和条件扩散模型结合，提高了医学成像领域中无监督域适应的表现。


<details>
  <summary>Details</summary>
Motivation: 不同医学成像模态之间的领域差异较大，使得训练稳健的学习算法变得具有挑战性，未标记数据的无监督域适应（UDA）提供了解决方案。

Method: 提出了一种统一框架，Bézier Meets Diffusion，结合了Bézier曲线的风格转移策略和条件扩散模型（CDM）来进行跨域图像生成和标记。

Result: 通过在目标域生成高质量的标记图像，增强了目标域的数据，并提高了基于这些图像的分割模型的性能。

Conclusion: 该方法生成了逼真的标记图像，显著增加了目标领域的样本量，提高了分割性能。

Abstract: Training robust learning algorithms across different medical imaging
modalities is challenging due to the large domain gap. Unsupervised domain
adaptation (UDA) mitigates this problem by using annotated images from the
source domain and unlabeled images from the target domain to train the deep
models. Existing approaches often rely on GAN-based style transfer, but these
methods struggle to capture cross-domain mappings in regions with high
variability. In this paper, we propose a unified framework, B\'ezier Meets
Diffusion, for cross-domain image generation. First, we introduce a
B\'ezier-curve-based style transfer strategy that effectively reduces the
domain gap between source and target domains. The transferred source images
enable the training of a more robust segmentation model across domains.
Thereafter, using pseudo-labels generated by this segmentation model on the
target domain, we train a conditional diffusion model (CDM) to synthesize
high-quality, labeled target-domain images. To mitigate the impact of noisy
pseudo-labels, we further develop an uncertainty-guided score matching method
that improves the robustness of CDM training. Extensive experiments on public
datasets demonstrate that our approach generates realistic labeled images,
significantly augmenting the target domain and improving segmentation
performance.

</details>


### [143] [PSTTS: A Plug-and-Play Token Selector for Efficient Event-based Spatio-temporal Representation Learning](https://arxiv.org/abs/2509.22481)
*Xiangmo Zhao,Nan Yang,Yang Wang,Zhanwen Liu*

Main category: cs.CV

TL;DR: PSTTS是一种新颖且高效的事件数据处理模块，通过去除冗余令牌显著提升了计算效率，适用于现有的多个神经网络架构。


<details>
  <summary>Details</summary>
Motivation: 针对现有事件驱动的时空表示学习方法忽视事件帧序列中的空间稀疏性和帧间运动冗余的问题，提出更为高效的处理方法。

Method: 提出了一种新颖的PSTTS模块，包括空间令牌净化和时间令牌选择两个阶段，利用原始事件数据中的时空分布特征来识别和去除冗余令牌。

Result: 在多个数据集上，PSTTS较现有方法减少了29-43.6%的FLOPs，提高了21.6-41.3%的FPS，同时保持了准确性。

Conclusion: PSTTS模块有效提升了事件数据处理的效率，减少了计算开销，同时保持了任务的准确性。

Abstract: Mainstream event-based spatio-temporal representation learning methods
typically process event streams by converting them into sequences of event
frames, achieving remarkable performance. However, they neglect the high
spatial sparsity and inter-frame motion redundancy inherent in event frame
sequences, leading to significant computational overhead. Existing token
sparsification methods for RGB videos rely on unreliable intermediate token
representations and neglect the influence of event noise, making them
ineffective for direct application to event data. In this paper, we propose
Progressive Spatio-Temporal Token Selection (PSTTS), a Plug-and-Play module for
event data without introducing any additional parameters. PSTTS exploits the
spatio-temporal distribution characteristics embedded in raw event data to
effectively identify and discard spatio-temporal redundant tokens, achieving an
optimal trade-off between accuracy and efficiency. Specifically, PSTTS consists
of two stages, Spatial Token Purification and Temporal Token Selection. Spatial
Token Purification discards noise and non-event regions by assessing the
spatio-temporal consistency of events within each event frame to prevent
interference with subsequent temporal redundancy evaluation. Temporal Token
Selection evaluates the motion pattern similarity between adjacent event
frames, precisely identifying and removing redundant temporal information. We
apply PSTTS to four representative backbones UniformerV2, VideoSwin, EVMamba,
and ExACT on the HARDVS, DailyDVS-200, and SeACT datasets. Experimental results
demonstrate that PSTTS achieves significant efficiency improvements.
Specifically, PSTTS reduces FLOPs by 29-43.6% and increases FPS by 21.6-41.3%
on the DailyDVS-200 dataset, while maintaining task accuracy. Our code will be
available.

</details>


### [144] [Group Critical-token Policy Optimization for Autoregressive Image Generation](https://arxiv.org/abs/2509.22485)
*Guohui Zhang,Hu Yu,Xiaoxiao Ma,JingHao Zhang,Yaning Pan,Mingde Yao,Jie Xiao,Linjiang Huang,Feng Zhao*

Main category: cs.CV

TL;DR: GCPO通过动态优化关键图像token，提高了AR视觉生成的性能。


<details>
  <summary>Details</summary>
Motivation: 探索不同图像token在RLVR训练中的贡献，解决如何识别和优化关键图像token的挑战。

Method: 提出了GCPO方法，通过动态token-wise优势权重对关键token进行有效的策略优化。

Result: GCPO在多个文本到图像基准测试中展示了优越的性能，尤其在关键token的动态优化方面。

Conclusion: GCPO在AR视觉生成中使用30%的图像token实现了优于使用所有token的GRPO的性能，证明了其有效性。

Abstract: Recent studies have extended Reinforcement Learning with Verifiable Rewards
(RLVR) to autoregressive (AR) visual generation and achieved promising
progress. However, existing methods typically apply uniform optimization across
all image tokens, while the varying contributions of different image tokens for
RLVR's training remain unexplored. In fact, the key obstacle lies in how to
identify more critical image tokens during AR generation and implement
effective token-wise optimization for them. To tackle this challenge, we
propose $\textbf{G}$roup $\textbf{C}$ritical-token $\textbf{P}$olicy
$\textbf{O}$ptimization ($\textbf{GCPO}$), which facilitates effective policy
optimization on critical tokens. We identify the critical tokens in RLVR-based
AR generation from three perspectives, specifically: $\textbf{(1)}$ Causal
dependency: early tokens fundamentally determine the later tokens and final
image effect due to unidirectional dependency; $\textbf{(2)}$ Entropy-induced
spatial structure: tokens with high entropy gradients correspond to image
structure and bridges distinct visual regions; $\textbf{(3)}$ RLVR-focused
token diversity: tokens with low visual similarity across a group of sampled
images contribute to richer token-level diversity. For these identified
critical tokens, we further introduce a dynamic token-wise advantage weight to
encourage exploration, based on confidence divergence between the policy model
and reference model. By leveraging 30\% of the image tokens, GCPO achieves
better performance than GRPO with full tokens. Extensive experiments on
multiple text-to-image benchmarks for both AR models and unified multimodal
models demonstrate the effectiveness of GCPO for AR visual generation.

</details>


### [145] [Where MLLMs Attend and What They Rely On: Explaining Autoregressive Token Generation](https://arxiv.org/abs/2509.22496)
*Ruoyu Chen,Xiaoqing Guo,Kangwei Liu,Siyuan Liang,Shiming Liu,Qunli Zhang,Hua Zhang,Xiaochun Cao*

Main category: cs.CV

TL;DR: EAGLE是一个轻量级框架，通过量化语言先验和感知证据的影响，提高了多模态大型语言模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大型语言模型生成的令牌与视觉模态的依赖性尚不清晰，需要一个可靠的解释框架来提高模型的可解释性和可靠性。

Method: EAGLE通过优化统一的目标函数，结合充分性和必要性评分，来解释自回归令牌生成过程。

Result: EAGLE在开源多模态大型语言模型上表现出色，在可信性、定位和幻觉诊断方面优于现有方法，同时显著减少GPU内存使用。

Conclusion: EAGLE框架有效提升了多模态大型语言模型的可解释性，超越了现有方法，且内存消耗显著降低。

Abstract: Multimodal large language models (MLLMs) have demonstrated remarkable
capabilities in aligning visual inputs with natural language outputs. Yet, the
extent to which generated tokens depend on visual modalities remains poorly
understood, limiting interpretability and reliability. In this work, we present
EAGLE, a lightweight black-box framework for explaining autoregressive token
generation in MLLMs. EAGLE attributes any selected tokens to compact perceptual
regions while quantifying the relative influence of language priors and
perceptual evidence. The framework introduces an objective function that
unifies sufficiency (insight score) and indispensability (necessity score),
optimized via greedy search over sparsified image regions for faithful and
efficient attribution. Beyond spatial attribution, EAGLE performs
modality-aware analysis that disentangles what tokens rely on, providing
fine-grained interpretability of model decisions. Extensive experiments across
open-source MLLMs show that EAGLE consistently outperforms existing methods in
faithfulness, localization, and hallucination diagnosis, while requiring
substantially less GPU memory. These results highlight its effectiveness and
practicality for advancing the interpretability of MLLMs. The code is available
at https://github.com/RuoyuChen10/EAGLE.

</details>


### [146] [Color Names in Vision-Language Models](https://arxiv.org/abs/2509.22524)
*Alexandra Gomez-Villa,Pablo Hernández-Cámara,Muhammad Atif Butt,Valero Laparra,Jesus Malo,Javier Vazquez-Corral*

Main category: cs.CV

TL;DR: 本研究系统评估了视觉语言模型的颜色命名能力，发现其在典型颜色上表现良好，但对非典型颜色的命名能力有限，并揭示了语言模型架构对颜色命名的影响。


<details>
  <summary>Details</summary>
Motivation: 评估视觉语言模型的颜色命名能力，以提高人机交互的有效性。

Method: 通过经典颜色命名方法对957种颜色样本进行系统评估，应用五种代表性模型。

Result: VLM在典型颜色的准确率高，但在扩展的非典型颜色集上性能显著下降，发现21个常见颜色术语以及模型间的训练不平衡。

Conclusion: 视觉语言模型在经典的颜色命名上表现良好，但在非典型颜色上性能较差，不同模型在颜色命名策略上存在显著差异。

Abstract: Color serves as a fundamental dimension of human visual perception and a
primary means of communicating about objects and scenes. As vision-language
models (VLMs) become increasingly prevalent, understanding whether they name
colors like humans is crucial for effective human-AI interaction. We present
the first systematic evaluation of color naming capabilities across VLMs,
replicating classic color naming methodologies using 957 color samples across
five representative models. Our results show that while VLMs achieve high
accuracy on prototypical colors from classical studies, performance drops
significantly on expanded, non-prototypical color sets. We identify 21 common
color terms that consistently emerge across all models, revealing two distinct
approaches: constrained models using predominantly basic terms versus expansive
models employing systematic lightness modifiers. Cross-linguistic analysis
across nine languages demonstrates severe training imbalances favoring English
and Chinese, with hue serving as the primary driver of color naming decisions.
Finally, ablation studies reveal that language model architecture significantly
influences color naming independent of visual processing capabilities.

</details>


### [147] [EfficientDepth: A Fast and Detail-Preserving Monocular Depth Estimation Model](https://arxiv.org/abs/2509.22527)
*Andrii Litvynchuk,Ivan Livinsky,Anand Ravi,Nima Kalantari,Andrii Tsarov*

Main category: cs.CV

TL;DR: 提出了一种新的单目深度估计系统EfficientDepth，结合变换器和轻量级解码器，解决了现有方法在细节、几何一致性等方面的不足，具有较好的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有单目深度估计方法在3D重建和视图合成中的几何一致性、细节、鲁棒性和效率等关键需求。

Method: 结合变换器架构与轻量级卷积解码器，并引入双模态密度头以估计详细的深度图。

Result: 实验结果表明，EfficientDepth在性能上与现有最先进模型相当或更好，计算资源消耗显著减少。

Conclusion: EfficientDepth在性能上与现有最先进模型相当或更好，同时显著减少了计算资源的使用。

Abstract: Monocular depth estimation (MDE) plays a pivotal role in various computer
vision applications, such as robotics, augmented reality, and autonomous
driving. Despite recent advancements, existing methods often fail to meet key
requirements for 3D reconstruction and view synthesis, including geometric
consistency, fine details, robustness to real-world challenges like reflective
surfaces, and efficiency for edge devices. To address these challenges, we
introduce a novel MDE system, called EfficientDepth, which combines a
transformer architecture with a lightweight convolutional decoder, as well as a
bimodal density head that allows the network to estimate detailed depth maps.
We train our model on a combination of labeled synthetic and real images, as
well as pseudo-labeled real images, generated using a high-performing MDE
method. Furthermore, we employ a multi-stage optimization strategy to improve
training efficiency and produce models that emphasize geometric consistency and
fine detail. Finally, in addition to commonly used objectives, we introduce a
loss function based on LPIPS to encourage the network to produce detailed depth
maps. Experimental results demonstrate that EfficientDepth achieves performance
comparable to or better than existing state-of-the-art models, with
significantly reduced computational resources.

</details>


### [148] [Category Discovery: An Open-World Perspective](https://arxiv.org/abs/2509.22542)
*Zhenqi He,Yuanpei Liu,Kai Han*

Main category: cs.CV

TL;DR: 本综述全面回顾了类别发现领域的文献，提出了分类体系，分析了不同方法并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 类别发现是一个新兴的开放世界学习任务，自动分类未标记数据中的未知类别具有重要应用价值。

Method: 综合分析不同的类别发现方法，构建了一个分类体系，并对每种方法的三个基本组成部分进行了详细分析。

Result: 编制了类别发现文献的系统综述，分析了多种方法，提供了关键见解，建议大规模预训练骨干网等方法对类别发现有益，同时指出了设计标签分配和类别数量估计等方面的挑战。

Conclusion: 研究总结了类别发现领域的现有文献，并提出了未来研究的方向。

Abstract: Category discovery (CD) is an emerging open-world learning task, which aims
at automatically categorizing unlabelled data containing instances from unseen
classes, given some labelled data from seen classes. This task has attracted
significant attention over the years and leads to a rich body of literature
trying to address the problem from different perspectives. In this survey, we
provide a comprehensive review of the literature, and offer detailed analysis
and in-depth discussion on different methods. Firstly, we introduce a taxonomy
for the literature by considering two base settings, namely novel category
discovery (NCD) and generalized category discovery (GCD), and several derived
settings that are designed to address the extra challenges in different
real-world application scenarios, including continual category discovery,
skewed data distribution, federated category discovery, etc. Secondly, for each
setting, we offer a detailed analysis of the methods encompassing three
fundamental components, representation learning, label assignment, and
estimation of class number. Thirdly, we benchmark all the methods and distill
key insights showing that large-scale pretrained backbones, hierarchical and
auxiliary cues, and curriculum-style training are all beneficial for category
discovery, while challenges remain in the design of label assignment, the
estimation of class numbers, and scaling to complex multi-object
scenarios.Finally, we discuss the key insights from the literature so far and
point out promising future research directions. We compile a living survey of
the category discovery literature at
\href{https://github.com/Visual-AI/Category-Discovery}{https://github.com/Visual-AI/Category-Discovery}.

</details>


### [149] [HyCoVAD: A Hybrid SSL-LLM Model for Complex Video Anomaly Detection](https://arxiv.org/abs/2509.22544)
*Mohammad Mahdi Hemmatyar,Mahdi Jafari,Mohammad Amin Yousefi,Mohammad Reza Nemati,Mobin Azadani,Hamid Reza Rastad,Amirmohammad Akbari*

Main category: cs.CV

TL;DR: 本论文提出了一种新的混合模型HyCoVAD，结合自监督学习和大型语言模型，提高了复杂视频异常检测的准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在复杂异常检测中困难，尤其是识别多实体间复杂关系和时序依赖性的问题。

Method: HyCoVAD结合了多任务自监督学习的时间分析器和大型语言模型的验证器，采用nnFormer作为骨干，进行视频帧的异常检测。

Result: HyCoVAD在ComplexVAD数据集上达到了72.5%的帧级AUC，比现有基线提高了12.5%。

Conclusion: HyCoVAD模型在复杂视频异常检测任务上表现优越，提供了一种高效的混合自监督学习和大型语言模型的方法，并降低了计算成本。

Abstract: Video anomaly detection (VAD) is crucial for intelligent surveillance, but a
significant challenge lies in identifying complex anomalies, which are events
defined by intricate relationships and temporal dependencies among multiple
entities rather than by isolated actions. While self-supervised learning (SSL)
methods effectively model low-level spatiotemporal patterns, they often
struggle to grasp the semantic meaning of these interactions. Conversely, large
language models (LLMs) offer powerful contextual reasoning but are
computationally expensive for frame-by-frame analysis and lack fine-grained
spatial localization. We introduce HyCoVAD, Hybrid Complex Video Anomaly
Detection, a hybrid SSL-LLM model that combines a multi-task SSL temporal
analyzer with LLM validator. The SSL module is built upon an nnFormer backbone
which is a transformer-based model for image segmentation. It is trained with
multiple proxy tasks, learns from video frames to identify those suspected of
anomaly. The selected frames are then forwarded to the LLM, which enriches the
analysis with semantic context by applying structured, rule-based reasoning to
validate the presence of anomalies. Experiments on the challenging ComplexVAD
dataset show that HyCoVAD achieves a 72.5% frame-level AUC, outperforming
existing baselines by 12.5% while reducing LLM computation. We release our
interaction anomaly taxonomy, adaptive thresholding protocol, and code to
facilitate future research in complex VAD scenarios.

</details>


### [150] [JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation](https://arxiv.org/abs/2509.22548)
*Shuang Zeng,Dekang Qi,Xinyuan Chang,Feng Xiong,Shichao Xie,Xiaolong Wu,Shiyi Liang,Mu Xu,Xing Wei*

Main category: cs.CV

TL;DR: 本研究提出JanusVLN，一个双隐式神经记忆的VLN框架，提升了导航效率并取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLN方法依赖显式语义记忆，导致空间信息损失和计算冗余，激发了对更有效记忆模型的探索。

Method: 提出了一种双隐式神经记忆的VLN框架JanusVLN，分别建模空间-几何和视觉-语义记忆。

Result: JanusVLN在成功率上相比其他方法提高了10.5-35.5，并在使用RGB输入的基础上加强了空间推理能力。

Conclusion: JanusVLN在Vision-and-Language Navigation任务中超越了20种近期方法，显示出其在未来研究中的潜力。

Abstract: Vision-and-Language Navigation requires an embodied agent to navigate through
unseen environments, guided by natural language instructions and a continuous
video stream. Recent advances in VLN have been driven by the powerful semantic
understanding of Multimodal Large Language Models. However, these methods
typically rely on explicit semantic memory, such as building textual cognitive
maps or storing historical visual frames. This type of method suffers from
spatial information loss, computational redundancy, and memory bloat, which
impede efficient navigation. Inspired by the implicit scene representation in
human navigation, analogous to the left brain's semantic understanding and the
right brain's spatial cognition, we propose JanusVLN, a novel VLN framework
featuring a dual implicit neural memory that models spatial-geometric and
visual-semantic memory as separate, compact, and fixed-size neural
representations. This framework first extends the MLLM to incorporate 3D prior
knowledge from the spatial-geometric encoder, thereby enhancing the spatial
reasoning capabilities of models based solely on RGB input. Then, the
historical key-value caches from the spatial-geometric and visual-semantic
encoders are constructed into a dual implicit memory. By retaining only the KVs
of tokens in the initial and sliding window, redundant computation is avoided,
enabling efficient incremental updates. Extensive experiments demonstrate that
JanusVLN outperforms over 20 recent methods to achieve SOTA performance. For
example, the success rate improves by 10.5-35.5 compared to methods using
multiple data types as input and by 3.6-10.8 compared to methods using more RGB
training data. This indicates that the proposed dual implicit neural memory, as
a novel paradigm, explores promising new directions for future VLN research.
Ours project page: https://miv-xjtu.github.io/JanusVLN.github.io/.

</details>


### [151] [SpikeMatch: Semi-Supervised Learning with Temporal Dynamics of Spiking Neural Networks](https://arxiv.org/abs/2509.22581)
*Jini Yang,Beomseok Oh,Seungryong Kim,Sunok Kim*

Main category: cs.CV

TL;DR: 本文介绍了一种新的半监督学习框架SpikeMatch，针对SNN模型生成伪标签，并展示了其优于现有方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管SNN在生物逻辑和能效方面引起了广泛关注，但基于SNN的半监督学习方法尚未得到充分探索。

Method: 引入了SpikeMatch框架，利用SNN的时间动态和泄漏因子，通过共同训练框架实现多样化的伪标签生成。

Result: 实验表明，SpikeMatch能够从弱增强的未标记样本中生成可靠的伪标签，用于在强增强的样本上进行训练，有效缓解确认偏差。

Conclusion: SpikeMatch框架在多个标准基准测试中优于现有的适应于SNN基础的SSL方法。

Abstract: Spiking neural networks (SNNs) have recently been attracting significant
attention for their biological plausibility and energy efficiency, but
semi-supervised learning (SSL) methods for SNN-based models remain
underexplored compared to those for artificial neural networks (ANNs). In this
paper, we introduce SpikeMatch, the first SSL framework for SNNs that leverages
the temporal dynamics through the leakage factor of SNNs for diverse
pseudo-labeling within a co-training framework. By utilizing agreement among
multiple predictions from a single SNN, SpikeMatch generates reliable
pseudo-labels from weakly-augmented unlabeled samples to train on
strongly-augmented ones, effectively mitigating confirmation bias by capturing
discriminative features with limited labels. Experiments show that SpikeMatch
outperforms existing SSL methods adapted to SNN backbones across various
standard benchmarks.

</details>


### [152] [LongLive: Real-time Interactive Long Video Generation](https://arxiv.org/abs/2509.22622)
*Shuai Yang,Wei Huang,Ruihang Chu,Yicheng Xiao,Yuyang Zhao,Xianbang Wang,Muyang Li,Enze Xie,Yingcong Chen,Yao Lu,Song Han,Yukang Chen*

Main category: cs.CV

TL;DR: LongLive是一种高效的长视频生成框架，采用创新的因果自回归模型，支持快速生成和动态内容创作。


<details>
  <summary>Details</summary>
Motivation: 解决传统长视频生成中效率低下和质量不高的问题，同时满足动态内容创作的交互需求。

Method: 采用因果自回归设计，集成KV重新缓存机制、流式长调优、短窗口注意力以及帧级注意力机制。

Result: LongLive在单个NVIDIA H100上以20.7 FPS的速度生成240秒的视频，且在VBench上表现优异。

Conclusion: LongLive是一种高效的长视频生成框架，能够实现快速生成并保持高质量，满足动态内容创作的需求。

Abstract: We present LongLive, a frame-level autoregressive (AR) framework for
real-time and interactive long video generation. Long video generation presents
challenges in both efficiency and quality. Diffusion and Diffusion-Forcing
models can produce high-quality videos but suffer from low efficiency due to
bidirectional attention. Causal attention AR models support KV caching for
faster inference, but often degrade in quality on long videos due to memory
challenges during long-video training. In addition, beyond static prompt-based
generation, interactive capabilities, such as streaming prompt inputs, are
critical for dynamic content creation, enabling users to guide narratives in
real time. This interactive requirement significantly increases complexity,
especially in ensuring visual consistency and semantic coherence during prompt
transitions. To address these challenges, LongLive adopts a causal, frame-level
AR design that integrates a KV-recache mechanism that refreshes cached states
with new prompts for smooth, adherent switches; streaming long tuning to enable
long video training and to align training and inference (train-long-test-long);
and short window attention paired with a frame-level attention sink, shorten as
frame sink, preserving long-range consistency while enabling faster generation.
With these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model
to minute-long generation in just 32 GPU-days. At inference, LongLive sustains
20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both
short and long videos. LongLive supports up to 240-second videos on a single
H100 GPU. LongLive further supports INT8-quantized inference with only marginal
quality loss.

</details>


### [153] [SPARK: Synergistic Policy And Reward Co-Evolving Framework](https://arxiv.org/abs/2509.22624)
*Ziyu Liu,Yuhang Zang,Shengyuan Ding,Yuhang Cao,Xiaoyi Dong,Haodong Duan,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: SPARK是一种新提出的框架，通过回收奖励信息同时训练生成奖励模型，从而在多个基准上显著提升LLM和LVLM的性能，克服了现有RLHF和RLVR方法的限制。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有方法（如RLHF和RLVR）中的高成本和潜在的奖励-策略不匹配问题，提出了一种新的框架。

Method: SPARK是一种高效、在线且稳定的方法，回收有价值的信息来同时训练生成奖励模型，消除了对单独奖励模型和人类偏好数据的需求。

Result: SPARK在7个推理基准上平均提高9.7%，在2个奖励基准上提高12.1%，在8个普通基准上提高1.5%。

Conclusion: SPARK框架在多个LLM和LVLM模型上表现出显著性能提升，证明了其稳健性和广泛的泛化能力。

Abstract: Recent Large Language Models (LLMs) and Large Vision-Language Models (LVLMs)
increasingly use Reinforcement Learning (RL) for post-pretraining, such as RL
with Verifiable Rewards (RLVR) for objective tasks and RL from Human Feedback
(RLHF) for subjective tasks. However, RLHF incurs high costs and potential
reward-policy mismatch due to reliance on human preferences, while RLVR still
wastes supervision by discarding rollouts and correctness signals after each
update. To address these challenges, we introduce the Synergistic Policy And
Reward Co-Evolving Framework (SPARK), an efficient, on-policy, and stable
method that builds on RLVR. Instead of discarding rollouts and correctness
data, SPARK recycles this valuable information to simultaneously train the
model itself as a generative reward model. This auxiliary training uses a mix
of objectives, such as pointwise reward score, pairwise comparison, and
evaluation conditioned on further-reflection responses, to teach the model to
evaluate and improve its own responses. Our process eliminates the need for a
separate reward model and costly human preference data. SPARK creates a
positive co-evolving feedback loop: improved reward accuracy yields better
policy gradients, which in turn produce higher-quality rollouts that further
refine the reward model. Our unified framework supports test-time scaling via
self-reflection without external reward models and their associated costs. We
show that SPARK achieves significant performance gains on multiple LLM and LVLM
models and multiple reasoning, reward models, and general benchmarks. For
example, SPARK-VL-7B achieves an average 9.7% gain on 7 reasoning benchmarks,
12.1% on 2 reward benchmarks, and 1.5% on 8 general benchmarks over the
baselines, demonstrating robustness and broad generalization.

</details>


### [154] [CCNeXt: An Effective Self-Supervised Stereo Depth Estimation Approach](https://arxiv.org/abs/2509.22627)
*Alexandre Lopes,Roberto Souza,Helio Pedrini*

Main category: cs.CV

TL;DR: 本论文提出了一种新颖自监督卷积架构CCNeXt，能在保持计算效率的同时超越现有深度估计技术，尤其在KITTI数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 在机器人、自主驾驶和增强现实等应用中，深度估计受到计算能力限制，因此需要一种有效的方法来进行深度估计，而自监督技术能够利用大量未标记的数据。

Method: 该方法提出了一种新颖的自监督卷积网络架构，结合了现代CNN特征提取器和窗口化的极线交叉注意模块，以及重新设计的深度估计解码器。

Result: CCNeXt在KITTI Eigen Split测试数据上具有竞争力的指标，并且相较于最佳模型速度提升了10.18倍，在各项指标上达到了最新的最优结果。

Conclusion: 提出的CCNeXt架构在各项度量指标上都优于现有技术，并且在计算效率上提供了显著的优势。

Abstract: Depth Estimation plays a crucial role in recent applications in robotics,
autonomous vehicles, and augmented reality. These scenarios commonly operate
under constraints imposed by computational power. Stereo image pairs offer an
effective solution for depth estimation since it only needs to estimate the
disparity of pixels in image pairs to determine the depth in a known rectified
system. Due to the difficulty in acquiring reliable ground-truth depth data
across diverse scenarios, self-supervised techniques emerge as a solution,
particularly when large unlabeled datasets are available. We propose a novel
self-supervised convolutional approach that outperforms existing
state-of-the-art Convolutional Neural Networks (CNNs) and Vision Transformers
(ViTs) while balancing computational cost. The proposed CCNeXt architecture
employs a modern CNN feature extractor with a novel windowed epipolar
cross-attention module in the encoder, complemented by a comprehensive redesign
of the depth estimation decoder. Our experiments demonstrate that CCNeXt
achieves competitive metrics on the KITTI Eigen Split test data while being
10.18$\times$ faster than the current best model and achieves state-of-the-art
results in all metrics in the KITTI Eigen Split Improved Ground Truth and
Driving Stereo datasets when compared to recently proposed techniques. To
ensure complete reproducibility, our project is accessible at
\href{https://github.com/alelopes/CCNext}{\texttt{https://github.com/alelopes/CCNext}}.

</details>


### [155] [UML-CoT: Structured Reasoning and Planning with Unified Modeling Language for Robotic Room Cleaning](https://arxiv.org/abs/2509.22628)
*Hongyu Chen,Guangrun Wang*

Main category: cs.CV

TL;DR: UML-CoT是一个基于UML的结构化推理框架，实现了比传统无结构CoT更高的可解释性和执行成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的CoT提示仅依赖于无结构文本，限制了在体现任务中的可解释性和可执行性，因此需要一种新的有结构的推理方法。

Method: 提出了一个利用统一建模语言（UML）的结构化推理和规划框架，生成符号化的CoT和可执行的行动计划，并通过三阶段的训练管道进行训练。

Result: 在新的拥挤室内清洁场景基准MRoom-30k上评估，UML-CoT在多个方面超越了无结构CoT。

Conclusion: UML-CoT表现出比无结构CoT更高的可解释性、规划一致性和执行成功率，表明UML作为一种更具表现力和可操作性的结构化推理形式。

Abstract: Chain-of-Thought (CoT) prompting improves reasoning in large language models
(LLMs), but its reliance on unstructured text limits interpretability and
executability in embodied tasks. Prior work has explored structured CoTs using
scene or logic graphs, yet these remain fundamentally limited: they model only
low-order relations, lack constructs like inheritance or behavioral
abstraction, and provide no standardized semantics for sequential or
conditional planning. We propose UML-CoT, a structured reasoning and planning
framework that leverages Unified Modeling Language (UML) to generate symbolic
CoTs and executable action plans. UML class diagrams capture compositional
object semantics, while activity diagrams model procedural control flow. Our
three-stage training pipeline combines supervised fine-tuning with Group
Relative Policy Optimization (GRPO), including reward learning from answer-only
data. We evaluate UML-CoT on MRoom-30k, a new benchmark of cluttered
room-cleaning scenarios. UML-CoT outperforms unstructured CoTs in
interpretability, planning coherence, and execution success, highlighting UML
as a more expressive and actionable structured reasoning formalism.

</details>


### [156] [Training-Free Synthetic Data Generation with Dual IP-Adapter Guidance](https://arxiv.org/abs/2509.22635)
*Luc Boudier,Loris Manganelli,Eleftherios Tsonis,Nicolas Dufour,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: DIPSY是一种无训练方式的合成图像生成方法，通过独立控制图像条件和类相似性采样，解决少样本图像分类问题。


<details>
  <summary>Details</summary>
Motivation: 解决少样本图像分类中标签实例稀缺的问题，利用文本到图像的扩散模型生成合成训练数据。

Method: 通过IP-Adapter进行图像到图像的转换生成合成图像，采用延伸的无分类器引导方案和基于类别相似性的采样策略。

Result: 在十个基准数据集上的实验表明，DIPSY实现了最佳或相当的性能，省去了生成模型的适配需求。

Conclusion: DIPSY在生成类别区分特征方面有效，尤其适用于细粒度分类任务，且不需要模型微调或外部工具。

Abstract: Few-shot image classification remains challenging due to the limited
availability of labeled examples. Recent approaches have explored generating
synthetic training data using text-to-image diffusion models, but often require
extensive model fine-tuning or external information sources. We present a novel
training-free approach, called DIPSY, that leverages IP-Adapter for
image-to-image translation to generate highly discriminative synthetic images
using only the available few-shot examples. DIPSY introduces three key
innovations: (1) an extended classifier-free guidance scheme that enables
independent control over positive and negative image conditioning; (2) a class
similarity-based sampling strategy that identifies effective contrastive
examples; and (3) a simple yet effective pipeline that requires no model
fine-tuning or external captioning and filtering. Experiments across ten
benchmark datasets demonstrate that our approach achieves state-of-the-art or
comparable performance, while eliminating the need for generative model
adaptation or reliance on external tools for caption generation and image
filtering. Our results highlight the effectiveness of leveraging dual image
prompting with positive-negative guidance for generating class-discriminative
features, particularly for fine-grained classification tasks.

</details>


### [157] [Scale-Wise VAR is Secretly Discrete Diffusion](https://arxiv.org/abs/2509.22636)
*Amandeep Kumar,Nithin Gopalakrishnan Nair,Vishal M. Patel*

Main category: cs.CV

TL;DR: 自回归变换器VAR与扩散模型结合，提出了一种新的生成模型SRDD，有效提升生成效率和质量。


<details>
  <summary>Details</summary>
Motivation: 探讨自回归变换器在视觉生成中的作为，并通过理论洞见提升生成模型的效率。

Method: 回顾Visual Autoregressive Generation (VAR)并与马尔可夫注意力机制结合，建立了与离散扩散模型的等价性。

Result: 在多个数据集上，采用扩散视角的VAR实现了更快的收敛、更低的推理成本，以及改善的零-shot重建。

Conclusion: 通过将变换器与扩散模型结合，VAR实现了更高的效率和更好的生成质量。

Abstract: Autoregressive (AR) transformers have emerged as a powerful paradigm for
visual generation, largely due to their scalability, computational efficiency
and unified architecture with language and vision. Among them, next scale
prediction Visual Autoregressive Generation (VAR) has recently demonstrated
remarkable performance, even surpassing diffusion-based models. In this work,
we revisit VAR and uncover a theoretical insight: when equipped with a
Markovian attention mask, VAR is mathematically equivalent to a discrete
diffusion. We term this reinterpretation as Scalable Visual Refinement with
Discrete Diffusion (SRDD), establishing a principled bridge between AR
transformers and diffusion models. Leveraging this new perspective, we show how
one can directly import the advantages of diffusion such as iterative
refinement and reduce architectural inefficiencies into VAR, yielding faster
convergence, lower inference cost, and improved zero-shot reconstruction.
Across multiple datasets, we show that the diffusion based perspective of VAR
leads to consistent gains in efficiency and generation.

</details>


### [158] [Hierarchical Representation Matching for CLIP-based Class-Incremental Learning](https://arxiv.org/abs/2509.22645)
*Zhen-Hao Wen,Yan Wang,Ji Feng,Han-Jia Ye,De-Chuan Zhan,Da-Wei Zhou*

Main category: cs.CV

TL;DR: HERMAN是一种新的CLIP类增量学习方法，通过层次化的文本描述改善了模型的表现，解决了知识遗忘问题并提高了分类精度。


<details>
  <summary>Details</summary>
Motivation: 现有的类增量学习方法往往忽视了视觉概念的层次结构，并对CLIP特征映射的使用较为简单，不能有效处理类的细微差别。

Method: HERMAN方法利用大规模语言模型生成递归性区分文本描述，并将其匹配到语义层次的不同级别，适应任务特定需求。

Result: 在多个基准测试中，HERMAN方法 consistently achieves state-of-the-art performance，验证了该方法在类增量学习中的有效性。

Conclusion: 本研究提出的HERMAN方法通过层次化的文本描述增强了CLIP的类增量学习能力，并在多个基准测试中取得了优秀的表现。

Abstract: Class-Incremental Learning (CIL) aims to endow models with the ability to
continuously adapt to evolving data streams. Recent advances in pre-trained
vision-language models (e.g., CLIP) provide a powerful foundation for this
task. However, existing approaches often rely on simplistic templates, such as
"a photo of a [CLASS]", which overlook the hierarchical nature of visual
concepts. For example, recognizing "cat" versus "car" depends on coarse-grained
cues, while distinguishing "cat" from "lion" requires fine-grained details.
Similarly, the current feature mapping in CLIP relies solely on the
representation from the last layer, neglecting the hierarchical information
contained in earlier layers. In this work, we introduce HiErarchical
Representation MAtchiNg (HERMAN) for CLIP-based CIL. Our approach leverages
LLMs to recursively generate discriminative textual descriptors, thereby
augmenting the semantic space with explicit hierarchical cues. These
descriptors are matched to different levels of the semantic hierarchy and
adaptively routed based on task-specific requirements, enabling precise
discrimination while alleviating catastrophic forgetting in incremental tasks.
Extensive experiments on multiple benchmarks demonstrate that our method
consistently achieves state-of-the-art performance.

</details>


### [159] [RefAM: Attention Magnets for Zero-Shot Referral Segmentation](https://arxiv.org/abs/2509.22650)
*Anna Kukleva,Enis Simsar,Alessio Tonioni,Muhammad Ferjad Naeem,Federico Tombari,Jan Eric Lenssen,Bernt Schiele*

Main category: cs.CV

TL;DR: 本研究提出了一种新的无训练框架RefAM，利用扩散变换器的特征和注意力机制，显著提升了参考分割的性能，无需额外的训练或架构调整。


<details>
  <summary>Details</summary>
Motivation: 现有的参考分割方法通常需要细调或组合多个预训练模型，导致训练成本高和架构复杂，期望找到一种更简便有效的方式。

Method: 通过利用扩散变换器的特征和注意力分数，建立了一种无需架构修改或额外训练的全新方法。

Result: 在零-shot的参考图像和视频分割基准上，RefAM方法始终超越了以往的技术，展现出了更高的性能。

Conclusion: RefAM框架在无额外训练或调优的情况下，优于现有的参考图像和视频分割方法，建立了新的性能标准。

Abstract: Most existing approaches to referring segmentation achieve strong performance
only through fine-tuning or by composing multiple pre-trained models, often at
the cost of additional training and architectural modifications. Meanwhile,
large-scale generative diffusion models encode rich semantic information,
making them attractive as general-purpose feature extractors. In this work, we
introduce a new method that directly exploits features, attention scores, from
diffusion transformers for downstream tasks, requiring neither architectural
modifications nor additional training. To systematically evaluate these
features, we extend benchmarks with vision-language grounding tasks spanning
both images and videos. Our key insight is that stop words act as attention
magnets: they accumulate surplus attention and can be filtered to reduce noise.
Moreover, we identify global attention sinks (GAS) emerging in deeper layers
and show that they can be safely suppressed or redirected onto auxiliary
tokens, leading to sharper and more accurate grounding maps. We further propose
an attention redistribution strategy, where appended stop words partition
background activations into smaller clusters, yielding sharper and more
localized heatmaps. Building on these findings, we develop RefAM, a simple
training-free grounding framework that combines cross-attention maps, GAS
handling, and redistribution. Across zero-shot referring image and video
segmentation benchmarks, our approach consistently outperforms prior methods,
establishing a new state of the art without fine-tuning or additional
components.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [160] [A Novel Differential Feature Learning for Effective Hallucination Detection and Classification](https://arxiv.org/abs/2509.21357)
*Wenkai Wang,Vincent Lee,Yizhen Zheng*

Main category: cs.CL

TL;DR: 本研究探讨了大语言模型中的幻觉信号的特征定位，提出了一种新颖的检测体系，通过双模型架构和特征学习机制大幅提升了模型在问答和对话任务中的性能，同时降低了特征维度的使用。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型的幻觉问题，改进检测方法，减少输出偏离事实准确性所带来的挑战。

Method: 提出了一种双模型架构，结合了自适应层间特征加权的Projected Fusion (PF)模块和通过计算来自相同输入的平行编码器学习互补表示之间的差异来识别判别特征的Differential Feature Learning (DFL)机制。

Result: 通过HaluEval的问答、对话和摘要数据集的系统实验，验证了幻觉信号集中在高度稀疏的特征子集中，在问答和对话任务上实现了显著的准确性提升。

Conclusion: 幻觉信号比之前假设的更为集中，这为开发计算高效的检测系统提供了路径，能够在保持准确性的同时降低推理成本。

Abstract: Large language model hallucination represents a critical challenge where
outputs deviate from factual accuracy due to distributional biases in training
data. While recent investigations establish that specific hidden layers exhibit
differences between hallucinatory and factual content, the precise localization
of hallucination signals within layers remains unclear, limiting the
development of efficient detection methods. We propose a dual-model
architecture integrating a Projected Fusion (PF) block for adaptive inter-layer
feature weighting and a Differential Feature Learning (DFL) mechanism that
identifies discriminative features by computing differences between parallel
encoders learning complementary representations from identical inputs. Through
systematic experiments across HaluEval's question answering, dialogue, and
summarization datasets, we demonstrate that hallucination signals concentrate
in highly sparse feature subsets, achieving significant accuracy improvements
on question answering and dialogue tasks. Notably, our analysis reveals a
hierarchical "funnel pattern" where shallow layers exhibit high feature
diversity while deep layers demonstrate concentrated usage, enabling detection
performance to be maintained with minimal degradation using only 1\% of feature
dimensions. These findings suggest that hallucination signals are more
concentrated than previously assumed, offering a pathway toward computationally
efficient detection systems that could reduce inference costs while maintaining
accuracy.

</details>


### [161] [Influence Guided Context Selection for Effective Retrieval-Augmented Generation](https://arxiv.org/abs/2509.21359)
*Jiale Deng,Yanyan Shen,Ziyuan Pei,Youmin Chen,Linpeng Huang*

Main category: cs.CL

TL;DR: 提出了一种新的上下文质量评估指标CI值，通过测量去除上下文对性能的影响来优化上下文选择，显著提升RAG的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于预定义上下文质量评估指标的选择方法效果有限，未能全面利用查询、上下文列表和生成器的信息进行质量评估。

Method: 提出了一种参数化的替代模型进行CI值预测，通过分层架构捕获局部查询-上下文相关性和全局上下文间交互，采用oracle CI值监督和端到端生成器反馈进行训练。

Result: 在8个自然语言处理任务和多个大型语言模型上的广泛实验表明，我们的上下文选择方法显著提高了性能。

Conclusion: 我们的方法显著优于现有的基线，能够有效过滤低质量上下文，同时保留关键信息。

Abstract: Retrieval-Augmented Generation (RAG) addresses large language model (LLM)
hallucinations by grounding responses in external knowledge, but its
effectiveness is compromised by poor-quality retrieved contexts containing
irrelevant or noisy information. While existing approaches attempt to improve
performance through context selection based on predefined context quality
assessment metrics, they show limited gains over standard RAG. We attribute
this limitation to their failure in holistically utilizing available
information (query, context list, and generator) for comprehensive quality
assessment. Inspired by recent advances in data selection, we reconceptualize
context quality assessment as an inference-time data valuation problem and
introduce the Contextual Influence Value (CI value). This novel metric
quantifies context quality by measuring the performance degradation when
removing each context from the list, effectively integrating query-aware
relevance, list-aware uniqueness, and generator-aware alignment. Moreover, CI
value eliminates complex selection hyperparameter tuning by simply retaining
contexts with positive CI values. To address practical challenges of label
dependency and computational overhead, we develop a parameterized surrogate
model for CI value prediction during inference. The model employs a
hierarchical architecture that captures both local query-context relevance and
global inter-context interactions, trained through oracle CI value supervision
and end-to-end generator feedback. Extensive experiments across 8 NLP tasks and
multiple LLMs demonstrate that our context selection method significantly
outperforms state-of-the-art baselines, effectively filtering poor-quality
contexts while preserving critical information. Code is available at
https://github.com/SJTU-DMTai/RAG-CSM.

</details>


### [162] [Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs](https://arxiv.org/abs/2509.21361)
*Norman Paulsen*

Main category: cs.CL

TL;DR: 本研究揭示了最大有效上下文窗口大幅低于报告的最大上下文窗口，并且该有效窗口值与问题类型相关，为提高模型准确性提供了重要见解。


<details>
  <summary>Details</summary>
Motivation: 测试上下文窗口在实际应用中的有效性，以探索当前大语言模型的性能边界。

Method: 定义最大有效上下文窗口的概念，制定测试方法以评估不同上下文窗口大小和问题类型的有效性，并创建标准化的模型效率比较方式。

Result: 收集了数十万个数据点，发现大多数模型在上下文数据达到1000个tokens时精确度显著下降，且多数模型的实际有效上下文窗口远低于其最大上下文窗口，最大下降幅度可达99%。

Conclusion: 最大有效上下文窗口（MECW）与报告的最大上下文窗口（MCW）之间存在显著差异，并且MECW会随着问题类型的变化而变化。

Abstract: Large language model (LLM) providers boast big numbers for maximum context
window sizes. To test the real world use of context windows, we 1) define a
concept of maximum effective context window, 2) formulate a testing method of a
context window's effectiveness over various sizes and problem types, and 3)
create a standardized way to compare model efficacy for increasingly larger
context window sizes to find the point of failure. We collected hundreds of
thousands of data points across several models and found significant
differences between reported Maximum Context Window (MCW) size and Maximum
Effective Context Window (MECW) size. Our findings show that the MECW is, not
only, drastically different from the MCW but also shifts based on the problem
type. A few top of the line models in our test group failed with as little as
100 tokens in context; most had severe degradation in accuracy by 1000 tokens
in context. All models fell far short of their Maximum Context Window by as
much as 99 percent. Our data reveals the Maximum Effective Context Window
shifts based on the type of problem provided, offering clear and actionable
insights into how to improve model accuracy and decrease model hallucination
rates.

</details>


### [163] [How Large Language Models Need Symbolism](https://arxiv.org/abs/2509.21404)
*Xiaotie Deng,Hanyu Li*

Main category: cs.CL

TL;DR: AI未来不仅仅依赖扩展，需要人类设计的符号来引导大型语言模型的直觉。


<details>
  <summary>Details</summary>
Motivation: 当前的AI发展主要依赖于规模的扩大，但缺乏有效的指导，可能导致发现的局限性。

Method: 通过提出一种新视角，即需要人类提供指导符号，以增强大型语言模型的能力。

Result: 大语言模型在未来的发展中，应整合人类符号，以促进真正的发现。

Conclusion: AI的未来需要人类设计的符号来引导其强大的直觉，而不仅仅是依靠扩展规模。

Abstract: We argue that AI's future requires more than scaling. To unlock genuine
discovery, large language models need a compass: human-crafted symbols to guide
their powerful but blind intuition.

</details>


### [164] [One Model, Many Morals: Uncovering Cross-Linguistic Misalignments in Computational Moral Reasoning](https://arxiv.org/abs/2509.21443)
*Sualeha Farid,Jayden Lin,Zean Chen,Shivani Kumar,David Jurgens*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型在多语言环境下的道德决策，发现其道德判断受文化因素影响显著，强调了提高文化意识的必要性。


<details>
  <summary>Details</summary>
Motivation: 探讨语言如何影响大型语言模型的道德决策，以应对其在多语言和多文化环境中的应用问题。

Method: 将两个成熟的道德推理基准翻译成五种具有文化和类型学多样性的语言，并进行多语言零-shot评估。

Result: 发现大型语言模型在不同语言中的道德判断存在显著不一致，主要反映文化不对齐。通过研究问题揭示了导致差异的根本因素，包括不同的推理策略。

Conclusion: 本研究揭示了大型语言模型在道德判断上的显著不一致，强调了对文化背景的关注以提高其伦理反应能力。

Abstract: Large Language Models (LLMs) are increasingly deployed in multilingual and
multicultural environments where moral reasoning is essential for generating
ethically appropriate responses. Yet, the dominant pretraining of LLMs on
English-language data raises critical concerns about their ability to
generalize judgments across diverse linguistic and cultural contexts. In this
work, we systematically investigate how language mediates moral decision-making
in LLMs. We translate two established moral reasoning benchmarks into five
culturally and typologically diverse languages, enabling multilingual zero-shot
evaluation. Our analysis reveals significant inconsistencies in LLMs' moral
judgments across languages, often reflecting cultural misalignment. Through a
combination of carefully constructed research questions, we uncover the
underlying drivers of these disparities, ranging from disagreements to
reasoning strategies employed by LLMs. Finally, through a case study, we link
the role of pretraining data in shaping an LLM's moral compass. Through this
work, we distill our insights into a structured typology of moral reasoning
errors that calls for more culturally-aware AI.

</details>


### [165] [LLM-Based Support for Diabetes Diagnosis: Opportunities, Scenarios, and Challenges with GPT-5](https://arxiv.org/abs/2509.21450)
*Gaurav Kumar Gupta,Nirajan Acharya,Pranal Pande*

Main category: cs.CL

TL;DR: 本研究评估GPT-5在糖尿病管理中的应用，显示其与ADA标准一致，可以帮助临床医生和患者。


<details>
  <summary>Details</summary>
Motivation: 应对糖尿病早期识别的挑战，利用大型语言模型改善决策支持。

Method: 使用基于ADA护理标准2025的模拟框架，测试GPT-5在五种代表性场景中的表现。

Result: GPT-5在案例分类、临床推理生成和患者解释方面与ADA标准高度一致。

Conclusion: GPT-5可以作为临床医生和患者的双重工具，需建立可重复的评估框架以负责任地评估医疗领域的LLM。

Abstract: Diabetes mellitus is a major global health challenge, affecting over half a
billion adults worldwide with prevalence projected to rise. Although the
American Diabetes Association (ADA) provides clear diagnostic thresholds, early
recognition remains difficult due to vague symptoms, borderline laboratory
values, gestational complexity, and the demands of long-term monitoring.
Advances in large language models (LLMs) offer opportunities to enhance
decision support through structured, interpretable, and patient-friendly
outputs. This study evaluates GPT-5, the latest generative pre-trained
transformer, using a simulation framework built entirely on synthetic cases
aligned with ADA Standards of Care 2025 and inspired by public datasets
including NHANES, Pima Indians, EyePACS, and MIMIC-IV. Five representative
scenarios were tested: symptom recognition, laboratory interpretation,
gestational diabetes screening, remote monitoring, and multimodal complication
detection. For each, GPT-5 classified cases, generated clinical rationales,
produced patient explanations, and output structured JSON summaries. Results
showed strong alignment with ADA-defined criteria, suggesting GPT-5 may
function as a dual-purpose tool for clinicians and patients, while underscoring
the importance of reproducible evaluation frameworks for responsibly assessing
LLMs in healthcare.

</details>


### [166] [Diagnosing the Performance Trade-off in Moral Alignment: A Case Study on Gender Stereotypes](https://arxiv.org/abs/2509.21456)
*Guangliang Liu,Bocheng Chen,Xitong Zhang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: 研究讨论预训练语言模型的道德对齐与性能权衡，指出当前公平目标的不足，强调遗忘与性能的复杂关系。


<details>
  <summary>Details</summary>
Motivation: 研究道德对齐方法的有效性，特别是在预训练语言模型的性能与公平性之间的权衡。

Method: 通过分析遗忘机制和公平目标，探讨影响之下的性能权衡。

Result: 我们的分析显示，遗忘水平、选择性遗忘和一般性解决方案对下游任务表现影响显著。

Conclusion: 当前的公平目标在实现性能权衡方面存在局限性，尤其是在减少性别刻板印象的背景下。

Abstract: Moral alignment has emerged as a widely adopted approach for regulating the
behavior of pretrained language models (PLMs), typically through fine-tuning or
model editing on curated datasets. However, this process often comes at the
cost of degraded downstream task performance. Prior studies commonly aim to
achieve a performance trade-off by encouraging PLMs to selectively forget
stereotypical knowledge through carefully designed fairness objectives, while
preserving their helpfulness. In this short paper, we investigate the
underlying mechanisms of the performance trade-off in the context of mitigating
gender stereotypes, through the lens of forgetting and the fairness objective.
Our analysis reveals the limitations of current fairness objective in achieving
trade-off by demonstrating that: (1) downstream task performance is primarily
driven by the overall forgetting level; (2) selective forgetting of stereotypes
tends to increase overall forgetting; and (3) general solutions for mitigating
forgetting are ineffective at reducing overall forgetting and fail to improve
downstream task performance.

</details>


### [167] [A State-of-the-Art SQL Reasoning Model using RLVR](https://arxiv.org/abs/2509.21459)
*Alnur Ali,Ashutosh Baheti,Jonathan Chang,Ta-Chung Chi,Brandon Cui,Andrew Drozdov,Jonathan Frankle,Abhay Gupta,Pallavi Koppol,Sean Kulinski,Jonathan Li,Dipendra Misra,Krista Opsahl-Ong,Jose Javier Gonzalez Ortiz,Matei Zaharia,Yue Zhang*

Main category: cs.CL

TL;DR: 本研究应用强化学习（RL）与可验证奖励（RLVR）相结合的方法，在BIRD基准上实现了状态-of-the-art的准确率，显示出在企业领域的广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 开发可定制的推理模型，通过强化学习结合特定组织的知识，以解决企业客户面临的问题。

Method: 使用了一种简单通用的训练方法，包括精心选择的提示和模型，结合离线RL方法TAO的热身阶段，然后进行严格的在线RLVR训练。

Result: 在BIRD基准上，我们首次提交达到73.56%（无自一致性）和75.68%（有自一致性）的最高准确率，同时生成次数少于第二好方法。

Conclusion: 该研究在BIRD基准上取得了先进的准确率，并展示了其广泛适用性。

Abstract: Developing custom reasoning models via Reinforcement Learning (RL) that can
incorporate organization-specific knowledge has great potential to address
problems faced by enterprise customers. In many of these problems, the reward
function is verifiable, a setting termed RL with Verifiable Rewards (RLVR). We
apply RLVR to a popular data science benchmark called BIRD that measures the
ability of an AI agent to convert a natural language query for a database to
SQL executions. We apply a simple and general-purpose training recipe involving
careful prompt and model selection, a warm-up stage using our offline RL
approach called TAO, followed by rigorous online RLVR training. With no
additional training data beyond the BIRD training set and no use of proprietary
models, our very first submission to the BIRD leaderboard reached
state-of-the-art accuracy on the private test set: 73.56% without
self-consistency and 75.68% with self-consistency. In the latter case, our
model also required fewer generations than the second-best approach. While BIRD
is only a proxy task, the simplicity of our framework makes it broadly
applicable to enterprise domains such as business intelligence, data science,
and coding.

</details>


### [168] [Learning to Reason with Mixture of Tokens](https://arxiv.org/abs/2509.21482)
*Adit Jain,Brendan Rappazzo*

Main category: cs.CL

TL;DR: 本研究提出了混合标记生成（MoT-G）方法，显著提高了大型语言模型的推理能力，在多项推理任务中表现优异，同时提升了训练效率。


<details>
  <summary>Details</summary>
Motivation: 当前的RLVR方法在推理过程中未能有效利用模型对候选标记的概率分布信息，限制了推理搜索空间。

Method: 研究混合标记生成（MoT-G）在强化学习可验证奖励中的应用，并将其与当前方法进行对比。

Result: MoT-G在Reasoning-Gym的推理任务中取得了5-35%的性能提升，相较于标准解码方法，能够用一半的轨迹达到相似的准确性。

Conclusion: MoT-G方法在推理任务中表现出明显的优势，能够提高训练效率并在推理过程中保持较高的隐藏状态熵。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a leading
approach for improving large language model (LLM) reasoning capabilities. Most
current methods follow variants of Group Relative Policy Optimization, which
samples multiple reasoning completions, scores them relative to each other, and
adjusts the policy accordingly. However, these approaches invariably sample
discrete tokens at each reasoning step, discarding the rich distributional
information in the model's probability distribution over candidate tokens.
While preserving and utilizing this distributional information has proven
beneficial in non-RL settings, current RLVR methods seem to be unnecessarily
constraining the reasoning search space by not using this information. To
address this limitation, we investigate mixture-of-token generation (MoT-G) in
RLVR. We present a unified framework that generalizes existing MoT-G
approaches, including existing training-free methods that construct mixture
embeddings as weighted sums over token embeddings, and extend RLVR to operate
directly in this continuous mixture space for generating chain-of-thought.
Evaluating two MoT-G variants on Reasoning-Gym, a suite of reasoning-intensive
language tasks, we find that MoT--G methods achieve substantial improvements
(5--35 \% gains on 7 out of 10 tasks) compared to standard decoding with the
Qwen2.5-1.5B model, while reaching comparable accuracy with half the number of
trajectories, suggesting improved training efficiency. Through comprehensive
hidden-state and token-level analyses, we provide evidence that MoT--G's
benefits may stem from its ability to maintain higher hidden-state entropy
throughout the reasoning process and promote exploration in token space.

</details>


### [169] [Dual-Head Reasoning Distillation: Improving Classifier Accuracy with Train-Time-Only Reasoning](https://arxiv.org/abs/2509.21487)
*Jillian Xu,Dylan Zhou,Vinay Shukla,Yang Yang,Junrui Ruan,Shuhuai Lin,Wenfei Zou,Yinxiao Liu,Karthik Lakshmanan*

Main category: cs.CL

TL;DR: DHRD方法通过引入双头推理蒸馏技术，在不牺牲推理吞吐量的情况下提高分类准确度。


<details>
  <summary>Details</summary>
Motivation: 解决Chain-of-Thought提示带来的分类准确度提升与推理吞吐量显著降低的折衷问题。

Method: 引入Dual-Head Reasoning Distillation (DHRD)方法，为解码器语言模型添加池化分类头和仅在训练中使用的推理头，通过加权的损失函数进行训练。

Result: 在七个SuperGLUE任务上，DHRD相较于池化基线获得0.65-5.47%的相对提升，尤其在蕴涵/因果任务上提升更为明显。

Conclusion: DHRD在推理时关闭推理头，推理吞吐量与池化分类器相当，并在QPS上超越相同模型的CoT解码96-142倍。

Abstract: Chain-of-Thought (CoT) prompting often improves classification accuracy, but
it introduces a significant throughput penalty with rationale generation (Wei
et al., 2022; Cheng and Van Durme, 2024). To resolve this trade-off, we
introduce Dual-Head Reasoning Distillation (DHRD), a simple training method for
decoder-only language models (LMs) that adds (i) a pooled classification head
used during training and inference and (ii) a reasoning head supervised by
teacher rationales used only in training. We train with a loss function that is
a weighted sum of label cross-entropy and token-level LM loss over
input-plus-rationale sequences. On seven SuperGLUE tasks, DHRD yields relative
gains of 0.65-5.47% over pooled baselines, with notably larger gains on
entailment/causal tasks. Since we disable the reasoning head at test time,
inference throughput matches pooled classifiers and exceeds CoT decoding on the
same backbones by 96-142 times in QPS.

</details>


### [170] [On Code-Induced Reasoning in LLMs](https://arxiv.org/abs/2509.21499)
*Abdul Waheed,Zhen Wu,Carolyn Rosé,Daphne Ippolito*

Main category: cs.CL

TL;DR: 研究不同代码特性如何影响大型语言模型的推理能力，发现结构扰动更具影响，适当抽象有效。


<details>
  <summary>Details</summary>
Motivation: 探究代码的哪些方面最能增强大型语言模型的推理能力。

Method: 构建了十种编程语言的平行指令数据集，并通过控制扰动来选择性破坏代码的结构或语义特性。

Result: LLMs对结构性扰动比语义性扰动更敏感，特别是在数学和代码任务上，且适当的抽象形式可以与代码同样有效。

Conclusion: 不同特性的代码对大型语言模型的推理能力有显著影响，适当的抽象可以提高性能。

Abstract: Code data has been shown to enhance the reasoning capabilities of large
language models (LLMs), but it remains unclear which aspects of code are most
responsible. We investigate this question with a systematic, data-centric
framework. We construct parallel instruction datasets in ten programming
languages and apply controlled perturbations that selectively disrupt
structural or semantic properties of code. We then finetune LLMs from five
model families and eight scales on each variant and evaluate their performance
on natural language, math, and code tasks. Across 3,331 experiments, our
results show that LLMs are more vulnerable to structural perturbations than
semantic ones, particularly on math and code tasks. Appropriate abstractions
like pseudocode and flowcharts can be as effective as code, while encoding the
same information with fewer tokens without adhering to original syntax can
often retain or even improve performance. Remarkably, even corrupted code with
misleading signals remains competitive when surface-level regularities persist.
Finally, syntactic styles also shape task-specific gains with Python favoring
natural language reasoning and lower-level languages such as Java and Rust
favoring math. Through our systematic framework, we aim to provide insight into
how different properties of code influence reasoning and inform the design of
training data for enhancing LLM reasoning capabilities.

</details>


### [171] [Agribot: agriculture-specific question answer system](https://arxiv.org/abs/2509.21535)
*Naman Jain,Pranjali Jain,Pratik Kayal,Jayakrishna Sahit,Soham Pachpande,Jayesh Choudhari*

Main category: cs.CL

TL;DR: 本研究构建了一个农业聊天机器人，通过数据处理显著提升了对农业相关问题的解答准确率，旨在改善印度农民的信息获取方式。


<details>
  <summary>Details</summary>
Motivation: 印度作为一个农业经济体，获取农业实践的准确信息是实现农业可持续增长和优化产出的重要关键。

Method: 通过Kisan Call Center的数据构建了一种基于句子嵌入模型的农业聊天机器人，最初准确率为56%，经过同义词消除和实体提取后提升至86%。

Result: 该系统能够全天候提供与天气、市场价格、植物保护和政府方案相关的查询解答，提高了农民的信息获取效率。

Conclusion: 该农业聊天机器人将在农业实践信息获取方面显著提高农民的效率，并减轻呼叫中心工作人员的负担。

Abstract: India is an agro-based economy and proper information about agricultural
practices is the key to optimal agricultural growth and output. In order to
answer the queries of the farmer, we have build an agricultural chatbot based
on the dataset from Kisan Call Center. This system is robust enough to answer
queries related to weather, market rates, plant protection and government
schemes. This system is available 24* 7, can be accessed through any electronic
device and the information is delivered with the ease of understanding. The
system is based on a sentence embedding model which gives an accuracy of 56%.
After eliminating synonyms and incorporating entity extraction, the accuracy
jumps to 86%. With such a system, farmers can progress towards easier
information about farming related practices and hence a better agricultural
output. The job of the Call Center workforce would be made easier and the hard
work of various such workers can be redirected to a better goal.

</details>


### [172] [Domain-Aware Speaker Diarization On African-Accented English](https://arxiv.org/abs/2509.21554)
*Chibuzor Okocha,Kelechi Ezema,Christan Grant*

Main category: cs.CL

TL;DR: 本研究聚焦于非洲口音英语的扬声器识别领域效应，通过多个系统的比较分析，发现临床与一般对话存在显著差异，并提出适应方案以降低错误率。


<details>
  <summary>Details</summary>
Motivation: 研究非洲口音英语的扬声器识别中领域效应的问题。

Method: 对不同领域的扬声器识别进行评估，使用严格的DER协议对重叠进行打分。

Result: 临床语音中的领域惩罚显著，与错误警报和漏检相关，并且通过轻量级领域适应减少错误，但仍未消除差距。

Conclusion: 结果显示，重叠识别和均衡的临床资源是实践中的下一步工作

Abstract: This study examines domain effects in speaker diarization for
African-accented English. We evaluate multiple production and open systems on
general and clinical dialogues under a strict DER protocol that scores overlap.
A consistent domain penalty appears for clinical speech and remains significant
across models. Error analysis attributes much of this penalty to false alarms
and missed detections, aligning with short turns and frequent overlap. We test
lightweight domain adaptation by fine-tuning a segmentation module on
accent-matched data; it reduces error but does not eliminate the gap. Our
contributions include a controlled benchmark across domains, a concise approach
to error decomposition and conversation-level profiling, and an adaptation
recipe that is easy to reproduce. Results point to overlap-aware segmentation
and balanced clinical resources as practical next steps.

</details>


### [173] [Generation-Time vs. Post-hoc Citation: A Holistic Evaluation of LLM Attribution](https://arxiv.org/abs/2509.21557)
*Yash Saxena,Raviteja Bommireddy,Ankur Padia,Manas Gaur*

Main category: cs.CL

TL;DR: 研究评估了两种大型语言模型的引用生成方法，建议在高风险领域采用以检索为中心的事后引用方法。


<details>
  <summary>Details</summary>
Motivation: 为了在医疗、法律、学术和金融等重要领域确保大型语言模型的可信性，必须引用可供人类验证的来源。

Method: 比较两种引用生成范式：生成时间引用(G-Cite)和事后引用(P-Cite)，并在多个数据集上进行全面评估。

Result: P-Cite方法在覆盖率和正确性上表现出竞争力，而G-Cite方法在精度上优先但在覆盖率和速度上有所牺牲。

Conclusion: 建议在高风险应用中采用以检索为中心的P-Cite优先方法，而在需要严格验证的情况下使用G-Cite。

Abstract: Trustworthy Large Language Models (LLMs) must cite human-verifiable sources
in high-stakes domains such as healthcare, law, academia, and finance, where
even small errors can have severe consequences. Practitioners and researchers
face a choice: let models generate citations during decoding, or let models
draft answers first and then attach appropriate citations. To clarify this
choice, we introduce two paradigms: Generation-Time Citation (G-Cite), which
produces the answer and citations in one pass, and Post-hoc Citation (P-Cite),
which adds or verifies citations after drafting. We conduct a comprehensive
evaluation from zero-shot to advanced retrieval-augmented methods across four
popular attribution datasets and provide evidence-based recommendations that
weigh trade-offs across use cases. Our results show a consistent trade-off
between coverage and citation correctness, with retrieval as the main driver of
attribution quality in both paradigms. P-Cite methods achieve high coverage
with competitive correctness and moderate latency, whereas G-Cite methods
prioritize precision at the cost of coverage and speed. We recommend a
retrieval-centric, P-Cite-first approach for high-stakes applications,
reserving G-Cite for precision-critical settings such as strict claim
verification. Our codes and human evaluation results are available at
https://anonymous.4open.science/r/Citation_Paradigms-BBB5/

</details>


### [174] [Comparative Personalization for Multi-document Summarization](https://arxiv.org/abs/2509.21562)
*Haoyuan Li,Snigdha Chaturvedi*

Main category: cs.CL

TL;DR: 提出了一种名为ComPSum的个性化多文档摘要框架，通过用户偏好的比较生成个性化摘要，并用构建的评估框架AuthorMap进行验证，显示出良好的性能。


<details>
  <summary>Details</summary>
Motivation: 识别用户偏好之间的细微差别对于有效个性化至关重要。

Method: 通过比较用户与其他用户的偏好，生成结构化的用户分析，进而指导个性化摘要的生成。

Result: ComPSum在PerMSum数据集上的评估结果显示其优于其他强基准模型。

Conclusion: ComPSum框架在个性化多文档摘要任务中表现优越，能够有效满足不同用户的写作风格和内容关注点的偏好。

Abstract: Personalized multi-document summarization (MDS) is essential for meeting
individual user preferences of writing style and content focus for summaries.
In this paper, we propose that for effective personalization, it is important
to identify fine-grained differences between users' preferences by comparing
the given user's preferences with other users' preferences.Motivated by this,
we propose ComPSum, a personalized MDS framework. It first generates a
structured analysis of a user by comparing their preferences with other users'
preferences. The generated structured analysis is then used to guide the
generation of personalized summaries. To evaluate the performance of ComPSum,
we propose AuthorMap, a fine-grained reference-free evaluation framework for
personalized MDS. It evaluates the personalization of a system based on the
authorship attribution between two personalized summaries generated for
different users. For robust evaluation of personalized MDS, we construct
PerMSum, a personalized MDS dataset in the review and news domain. We evaluate
the performance of ComPSum on PerMSum using AuthorMap, showing that it
outperforms strong baselines.

</details>


### [175] [Vision Language Models Cannot Plan, but Can They Formalize?](https://arxiv.org/abs/2509.21576)
*Muyu He,Yuxi Zheng,Yuchen Liu,Zijian An,Bill Cai,Jiani Huang,Lifeng Zhou,Feng Liu,Ziyang Li,Li Zhang*

Main category: cs.CL

TL;DR: 本研究提出了多模态环境中的VLM-as-formalizer，显著提高了长时间范围的计划能力，尽管视觉信息处理仍是瓶颈。


<details>
  <summary>Details</summary>
Motivation: 已知单一模态的长时间范围计划取得了显著进展，但多模态环境中的VLM作为形式化工具的研究依然不足。

Method: 提出了一套五个VLM-as-formalizer管道，处理一体化、开放词汇和多模态PDDL形式化，并在既有基准上评估。

Result: VLM-as-formalizer在真实、多视角和低质量图像下进行计划，测试显示其性能优于传统方法。

Conclusion: VLM-as-formalizer显著优于端到端的计划生成，但由于视觉信息捕捉不足，仍存在性能改进的空间。

Abstract: The advancement of vision language models (VLMs) has empowered embodied
agents to accomplish simple multimodal planning tasks, but not long-horizon
ones requiring long sequences of actions. In text-only simulations,
long-horizon planning has seen significant improvement brought by repositioning
the role of LLMs. Instead of directly generating action sequences, LLMs
translate the planning domain and problem into a formal planning language like
the Planning Domain Definition Language (PDDL), which can call a formal solver
to derive the plan in a verifiable manner. In multimodal environments, research
on VLM-as-formalizer remains scarce, usually involving gross simplifications
such as predefined object vocabulary or overly similar few-shot examples. In
this work, we present a suite of five VLM-as-formalizer pipelines that tackle
one-shot, open-vocabulary, and multimodal PDDL formalization. We evaluate those
on an existing benchmark while presenting another two that for the first time
account for planning with authentic, multi-view, and low-quality images. We
conclude that VLM-as-formalizer greatly outperforms end-to-end plan generation.
We reveal the bottleneck to be vision rather than language, as VLMs often fail
to capture an exhaustive set of necessary object relations. While generating
intermediate, textual representations such as captions or scene graphs
partially compensate for the performance, their inconsistent gain leaves
headroom for future research directions on multimodal planning formalization.

</details>


### [176] ["Be My Cheese?": Assessing Cultural Nuance in Multilingual LLM Translations](https://arxiv.org/abs/2509.21577)
*Madison Van Doren,Cory Holland*

Main category: cs.CL

TL;DR: 本研究评估多语言 AI 在翻译比喻语言的本地化能力，强调文化适切性对机器翻译质量的重要性，并呼吁扩大研究规模以改进实际应用。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨多语言 AI 模型在翻译英语的比喻性语言（如成语和双关语）时的本地化能力，关注文化适切性和本地化质量。

Method: 对87个 LLM 生成的电子商务市场营销邮件翻译进行评估，涵盖20种语言的24个地区方言，通过流利的人工审核员提供反馈。

Result: 尽管领先的模型通常生成语法正确的翻译，但文化细微之处仍需改进，大多数情况下需要大量人力调整。

Conclusion: 当前的多语言 AI 系统在现实本地化应用中存在局限性，文化适切性是多语言模型性能的重要因素。

Abstract: This pilot study explores the localisation capabilities of state-of-the-art
multilingual AI models when translating figurative language, such as idioms and
puns, from English into a diverse range of global languages. It expands on
existing LLM translation research and industry benchmarks, which emphasise
grammatical accuracy and token-level correctness, by focusing on cultural
appropriateness and overall localisation quality - critical factors for
real-world applications like marketing and e-commerce.
  To investigate these challenges, this project evaluated a sample of 87
LLM-generated translations of e-commerce marketing emails across 24 regional
dialects of 20 languages. Human reviewers fluent in each target language
provided quantitative ratings and qualitative feedback on faithfulness to the
original's tone, meaning, and intended audience. Findings suggest that, while
leading models generally produce grammatically correct translations, culturally
nuanced language remains a clear area for improvement, often requiring
substantial human refinement. Notably, even high-resource global languages,
despite topping industry benchmark leaderboards, frequently mistranslated
figurative expressions and wordplay.
  This work challenges the assumption that data volume is the most reliable
predictor of machine translation quality and introduces cultural
appropriateness as a key determinant of multilingual LLM performance - an area
currently underexplored in existing academic and industry benchmarks. As a
proof of concept, this pilot highlights limitations of current multilingual AI
systems for real-world localisation use cases. Results of this pilot support
the opportunity for expanded research at greater scale to deliver generalisable
insights and inform deployment of reliable machine translation workflows in
culturally diverse contexts.

</details>


### [177] [Multi-Objective Reinforcement Learning for Large Language Model Optimization: Visionary Perspective](https://arxiv.org/abs/2509.21613)
*Lingxiao Kong,Cong Yang,Oya Deniz Beyan,Zeyd Boukhers*

Main category: cs.CL

TL;DR: 探讨多目标强化学习在优化大型语言模型中的应用，提出了基准框架与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 关注多目标强化学习在优化大型语言模型中的挑战与机遇，强调了个性化功能及其复杂性。

Method: 介绍了MORL分类法，并探讨了不同MORL方法在LLM优化中的优势和局限性。

Result: 提出了一个MORL基准框架，并聚焦于未来的元策略MORL发展，旨在提升效率和灵活性。

Conclusion: 提出了一种MORL基准框架，为多目标强化学习在大语言模型中的应用提供了进一步研究方向。

Abstract: Multi-Objective Reinforcement Learning (MORL) presents significant challenges
and opportunities for optimizing multiple objectives in Large Language Models
(LLMs). We introduce a MORL taxonomy and examine the advantages and limitations
of various MORL methods when applied to LLM optimization, identifying the need
for efficient and flexible approaches that accommodate personalization
functionality and inherent complexities in LLMs and RL. We propose a vision for
a MORL benchmarking framework that addresses the effects of different methods
on diverse objective relationships. As future research directions, we focus on
meta-policy MORL development that can improve efficiency and flexibility
through its bi-level learning paradigm, highlighting key research questions and
potential solutions for improving LLM performance.

</details>


### [178] [OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's Rule](https://arxiv.org/abs/2509.21623)
*Yuxuan Zhu,David H. Yang,Mohammad Mohammadi Amiri,Keerthiram Murugesan,Tejaswini Pedapati,Pin-Yu Chen*

Main category: cs.CL

TL;DR: OjaKV通过在线适配子空间和混合存储政策，优化大语言模型的内存使用效率，实现高效的长上下文推理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的长上下文能力受到内存瓶颈的限制，尤其是自回归生成所需的键值缓存。

Method: 通过在线主成分分析的Oja算法结合混合存储策略，实现低秩压缩和动态的子空间适配。

Result: OjaKV在高压缩比下保持或提高了零-shot 精度，特别是在需要复杂推理的长上下文基准测试中表现出色。

Conclusion: OjaKV是一种实用的、即插即用的解决方案，能够在不需要模型微调的情况下实现内存高效的长上下文推理。

Abstract: The expanding long-context capabilities of large language models are
constrained by a significant memory bottleneck: the key-value (KV) cache
required for autoregressive generation. This bottleneck is substantial; for
instance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of
4 requires approximately 16GB for its KV cache, a size exceeding the model's
weights. While KV-cache compression via low-rank projection is a promising
direction, existing methods rely on a static, offline-learned subspace that
performs poorly under data distribution shifts. To overcome these limitations,
we introduce OjaKV, a novel framework that integrates a strategic hybrid
storage policy with online subspace adaptation. First, OjaKV recognizes that
not all tokens are equally important for compression; it preserves the crucial
first and most recent tokens in full-rank, maintaining high-fidelity anchors
for attention. Second, for the vast majority of intermediate tokens, it applies
low-rank compression by incrementally adapting the projection basis using Oja's
algorithm for online principal component analysis. This adaptation involves a
comprehensive update during prompt prefilling and lightweight periodic updates
during decoding, ensuring the subspace remains aligned with the evolving
context. Crucially, our framework is fully compatible with modern attention
modules like FlashAttention. Experiments demonstrate that OjaKV maintains or
even improves zero-shot accuracy at high compression ratios. In particular,
OjaKV achieves its strongest gains on very long-context benchmarks that require
complex reasoning, highlighting the importance of online subspace adaptation in
dynamically tracking context shifts. These results establish our hybrid
framework as a practical, plug-and-play solution for memory-efficient
long-context inference without requiring model fine-tuning.

</details>


### [179] [Towards Transparent AI: A Survey on Explainable Language Models](https://arxiv.org/abs/2509.21631)
*Avash Palikhe,Zichong Wang,Zhipeng Yin,Rui Guo,Qiang Duan,Jie Yang,Wenbin Zhang*

Main category: cs.CL

TL;DR: 本文综述了语言模型中的可解释人工智能技术，强调了模型架构对其可解释性的重要影响，并提出了未来研究的方向。


<details>
  <summary>Details</summary>
Motivation: 鉴于语言模型在各个领域的广泛应用及其黑箱性质导致的可解释性问题，有必要审视和评估其可解释人工智能的方法。

Method: 综述了可解释人工智能技术，按照语言模型的不同变换器架构进行组织和分析。

Result: 整理了针对不同类型的语言模型的可解释人工智能技术，分析了它们的优势和局限性，并提出未来研究的挑战与方向。

Conclusion: 本文总结了在语言模型中的可解释人工智能技术的现状，强调了不同架构模型的独特挑战，并提出未来研究的方向。

Abstract: Language Models (LMs) have significantly advanced natural language processing
and enabled remarkable progress across diverse domains, yet their black-box
nature raises critical concerns about the interpretability of their internal
mechanisms and decision-making processes. This lack of transparency is
particularly problematic for adoption in high-stakes domains, where
stakeholders need to understand the rationale behind model outputs to ensure
accountability. On the other hand, while explainable artificial intelligence
(XAI) methods have been well studied for non-LMs, they face many limitations
when applied to LMs due to their complex architectures, considerable training
corpora, and broad generalization abilities. Although various surveys have
examined XAI in the context of LMs, they often fail to capture the distinct
challenges arising from the architectural diversity and evolving capabilities
of these models. To bridge this gap, this survey presents a comprehensive
review of XAI techniques with a particular emphasis on LMs, organizing them
according to their underlying transformer architectures: encoder-only,
decoder-only, and encoder-decoder, and analyzing how methods are adapted to
each while assessing their respective strengths and limitations. Furthermore,
we evaluate these techniques through the dual lenses of plausibility and
faithfulness, offering a structured perspective on their effectiveness.
Finally, we identify open research challenges and outline promising future
directions, aiming to guide ongoing efforts toward the development of robust,
transparent, and interpretable XAI methods for LMs.

</details>


### [180] [Thinking with Sound: Audio Chain-of-Thought Enables Multimodal Reasoning in Large Audio-Language Models](https://arxiv.org/abs/2509.21749)
*Zhen Xiong,Yujun Cai,Zhecheng Li,Junsong Yuan,Yiwei Wang*

Main category: cs.CL

TL;DR: 本研究提出TwS框架，为大型音频语言模型引入音频推理，显著提升其在复杂情境下的表现，无需重新训练，展现了强大的鲁棒性和扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型音频语言模型在复杂音频情境中的推理任务存在显著局限，因此需要借助声学工具来解决这些问题。

Method: 引入Thinking-with-Sound (TwS)框架，通过结合语言推理和即时音频领域分析，改造大规模音频语言模型。

Result: 实验表明，现有最先进的LALMs在MELD-Hard1k基准测试中的准确率下降超过50%，而TwS在模型的鲁棒性方面取得了重大改进，小模型绝对准确率提升24.73%，大模型则提升至36.61%。

Conclusion: Audio CoT significantly提升了音频理解模型的鲁棒性，并且无需重新训练，指明了更强大的音频理解系统的未来发展方向。

Abstract: Recent Large Audio-Language Models (LALMs) have shown strong performance on
various audio understanding tasks such as speech translation and Audio Q\&A.
However, they exhibit significant limitations on challenging audio reasoning
tasks in complex acoustic scenarios. These situations would greatly benefit
from the use of acoustic tools like noise suppression, source separation, and
precise temporal alignment, but current LALMs lack access to such tools. To
address this limitation, we introduce Thinking-with-Sound (TwS), a framework
that equips LALMs with Audio CoT by combining linguistic reasoning with
on-the-fly audio-domain analysis. Unlike existing approaches that treat audio
as static input, TwS enables models to actively think with audio signals,
performing numerical analysis and digital manipulation through multimodal
reasoning. To evaluate this approach, we construct MELD-Hard1k, a new
robustness benchmark created by introducing various acoustic perturbations.
Experiments reveal that state-of-the-art LALMs suffer dramatic performance
degradation on MELD-Hard1k, with accuracy dropping by more than $50\%$ compared
to clean audio. TwS achieves substantial improvements in robustness,
demonstrating both effectiveness and scalability: small models gain $24.73\%$
absolute accuracy, with improvements scaling consistently up to $36.61\%$ for
larger models. Our findings demonstrate that Audio CoT can significantly
enhance robustness without retraining, opening new directions for developing
more robust audio understanding systems.

</details>


### [181] [ReviewScore: Misinformed Peer Review Detection with Large Language Models](https://arxiv.org/abs/2509.21679)
*Hyun Ryu,Doohyuk Jang,Hyemin S. Lee,Joonhyun Jeong,Gyeongman Kim,Donghyeon Cho,Gyouk Chu,Minyeong Hwang,Hyeongwon Jang,Changhun Kim,Haechan Kim,Jina Kim,Joowon Kim,Yoonjeon Kim,Kwanhyung Lee,Chanjae Park,Heecheol Yun,Gregor Betz,Eunho Yang*

Main category: cs.CL

TL;DR: 本文探讨了如何通过自动化技术检测学术评审中的低质量内容，并提出了一种新的评估方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI会议投稿数量激增，评审质量下降，因此需要可靠的方法来检测低质量评审。

Method: 提出一种自动化引擎来重构弱点评估中的每个显性和隐性前提，并通过人类专家标注的ReviewScore数据集来评估巨型语言模型的评估能力。

Result: 发现15.2%的弱点评估和26.4%的问题评估是误导性的，并引入了ReviewScore来表明评审点是否误导。

Conclusion: 评估前提层面的真实性显著高于整体弱点评估，并且完全自动化的ReviewScore评估具有潜力。

Abstract: Peer review serves as a backbone of academic research, but in most AI
conferences, the review quality is degrading as the number of submissions
explodes. To reliably detect low-quality reviews, we define misinformed review
points as either "weaknesses" in a review that contain incorrect premises, or
"questions" in a review that can be already answered by the paper. We verify
that 15.2% of weaknesses and 26.4% of questions are misinformed and introduce
ReviewScore indicating if a review point is misinformed. To evaluate the
factuality of each premise of weaknesses, we propose an automated engine that
reconstructs every explicit and implicit premise from a weakness. We build a
human expert-annotated ReviewScore dataset to check the ability of LLMs to
automate ReviewScore evaluation. Then, we measure human-model agreements on
ReviewScore using eight current state-of-the-art LLMs and verify moderate
agreements. We also prove that evaluating premise-level factuality shows
significantly higher agreements than evaluating weakness-level factuality. A
thorough disagreement analysis further supports a potential of fully automated
ReviewScore evaluation.

</details>


### [182] [GRAB: A Risk Taxonomy--Grounded Benchmark for Unsupervised Topic Discovery in Financial Disclosures](https://arxiv.org/abs/2509.21698)
*Ying Li,Tiejun Ma*

Main category: cs.CL

TL;DR: GRAB是一个针对金融风险分类的基准，包含1.61M句子和无人工注释的标签，促进对多种主题模型在金融披露上进行标准比较。


<details>
  <summary>Details</summary>
Motivation: 在10-K风险披露中的风险分类对监督和投资具有重要意义，但目前缺乏公开的基准来评估无监督主题模型的能力。

Method: 使用FinBERT的token关注、YAKE关键词信号和基于分类法的搭配匹配生成未经过人工注释的句子标签，结合风险分类法对193个术语进行映射。

Result: GRAB结合了来自8247份文件的161万句子，并支持在金融披露上进行多种主题模型的评估固定数据集拆分及稳健指标评估。

Conclusion: GRAB为金融风险分类提供了一个统一的评估基准，使得不同主题模型能够在金融披露领域进行标准化比较与再现性评估。

Abstract: Risk categorization in 10-K risk disclosures matters for oversight and
investment, yet no public benchmark evaluates unsupervised topic models for
this task. We present GRAB, a finance-specific benchmark with 1.61M sentences
from 8,247 filings and span-grounded sentence labels produced without manual
annotation by combining FinBERT token attention, YAKE keyphrase signals, and
taxonomy-aware collocation matching. Labels are anchored in a risk taxonomy
mapping 193 terms to 21 fine-grained types nested under five macro classes; the
21 types guide weak supervision, while evaluation is reported at the macro
level. GRAB unifies evaluation with fixed dataset splits and robust
metrics--Accuracy, Macro-F1, Topic BERTScore, and the entropy-based Effective
Number of Topics. The dataset, labels, and code enable reproducible,
standardized comparison across classical, embedding-based, neural, and hybrid
topic models on financial disclosures.

</details>


### [183] [Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval](https://arxiv.org/abs/2509.21710)
*Xiaojun Wu,Cehao Yang,Xueyuan Lin,Chengjin Xu,Xuhui Jiang,Yuanliang Sun,Hui Xiong,Jia Li,Jian Guo*

Main category: cs.CL

TL;DR: 本文提出ToG-3框架，通过动态图构建与多代理机制克服传统图基RAG的局限，显著提升了轻量级LLMs的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的图基RAG方法在质量与实用性之间存在权衡，而如何有效解决手动构建和自动提取知识图中的局限性是研究的动力。

Method: 本文提出了Think-on-Graph 3.0 (ToG-3)框架，采用多代理上下文演进和检索机制（MACER），通过动态构建和细化Chunk-Triplets-Community异构图索引，结合双重演进机制进行精确证据检索。

Result: ToG-3在深度和广度推理基准测试中优于比较基线，消融实验验证了MACER框架各组成部分的有效性。

Conclusion: ToG-3通过动态构建和细化异构图索引，并采用多代理系统，成功克服了静态图结构的固有限制，提升了对知识的有效检索与推理能力。

Abstract: Retrieval-Augmented Generation (RAG) and Graph-based RAG has become the
important paradigm for enhancing Large Language Models (LLMs) with external
knowledge. However, existing approaches face a fundamental trade-off. While
graph-based methods are inherently dependent on high-quality graph structures,
they face significant practical constraints: manually constructed knowledge
graphs are prohibitively expensive to scale, while automatically extracted
graphs from corpora are limited by the performance of the underlying LLM
extractors, especially when using smaller, local-deployed models. This paper
presents Think-on-Graph 3.0 (ToG-3), a novel framework that introduces
Multi-Agent Context Evolution and Retrieval (MACER) mechanism to overcome these
limitations. Our core innovation is the dynamic construction and refinement of
a Chunk-Triplets-Community heterogeneous graph index, which pioneeringly
incorporates a dual-evolution mechanism of Evolving Query and Evolving
Sub-Graph for precise evidence retrieval. This approach addresses a critical
limitation of prior Graph-based RAG methods, which typically construct a static
graph index in a single pass without adapting to the actual query. A
multi-agent system, comprising Constructor, Retriever, Reflector, and Responser
agents, collaboratively engages in an iterative process of evidence retrieval,
answer generation, sufficiency reflection, and, crucially, evolving query and
subgraph. This dual-evolving multi-agent system allows ToG-3 to adaptively
build a targeted graph index during reasoning, mitigating the inherent
drawbacks of static, one-time graph construction and enabling deep, precise
reasoning even with lightweight LLMs. Extensive experiments demonstrate that
ToG-3 outperforms compared baselines on both deep and broad reasoning
benchmarks, and ablation studies confirm the efficacy of the components of
MACER framework.

</details>


### [184] [VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing](https://arxiv.org/abs/2509.22651)
*Ke Wang,Houxing Ren,Zimu Lu,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: 本文提出了VoiceAssistant-Eval，一个旨在全面评估AI助手能力的基准，涵盖听、说、视三大领域，评估多种模型表现并指出其优劣和改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有基准难以全面评估AI助手的能力，因此需要一个综合性评估工具。

Method: 通过引入VoiceAssistant-Eval基准，涵盖听觉、说话和视觉三方面任务，评估21个开源模型和GPT-4o-Audio。

Result: 评估结果显示，专有模型不一定优于开源模型，多数模型在说话任务上表现良好，但音频理解能力不足，且小型模型在某些方面可以与大型模型竞争。

Conclusion: VoiceAssistant-Eval为评估和指导下一代AI助手的发展建立了严格的框架，识别出音频理解和模型鲁棒性方面的显著差距。

Abstract: The growing capabilities of large language models and multimodal systems have
spurred interest in voice-first AI assistants, yet existing benchmarks are
inadequate for evaluating the full range of these systems' capabilities. We
introduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI
assistants across listening, speaking, and viewing. VoiceAssistant-Eval
comprises 10,497 curated examples spanning 13 task categories. These tasks
include natural sounds, music, and spoken dialogue for listening; multi-turn
dialogue, role-play imitation, and various scenarios for speaking; and highly
heterogeneous images for viewing. To demonstrate its utility, we evaluate 21
open-source models and GPT-4o-Audio, measuring the quality of the response
content and speech, as well as their consistency. The results reveal three key
findings: (1) proprietary models do not universally outperform open-source
models; (2) most models excel at speaking tasks but lag in audio understanding;
and (3) well-designed smaller models can rival much larger ones. Notably, the
mid-sized Step-Audio-2-mini (7B) achieves more than double the listening
accuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal
(audio plus visual) input and role-play voice imitation tasks are difficult for
current models, and significant gaps persist in robustness and safety
alignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous
framework for evaluating and guiding the development of next-generation AI
assistants. Code and data will be released at
https://mathllm.github.io/VoiceAssistantEval/ .

</details>


### [185] [ProPerSim: Developing Proactive and Personalized AI Assistants through User-Assistant Simulation](https://arxiv.org/abs/2509.21730)
*Jiho Kim,Junseong Choi,Woosog Chay,Daeun Kyung,Yeonsu Kwon,Yohan Jo,Edward Choi*

Main category: cs.CL

TL;DR: 本文提出了一个新的任务和模拟框架ProPerSim，结合主动性和个性化来发展AI助手，并展示了ProPerAssistant在不同个性下基于用户反馈的学习和适应能力。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型日益融入日常生活，对既主动又个性化的AI助手的需求不断增长，但二者的结合尚未得到充分探索。

Method: 在ProPerSim模拟环境中，用户代理与助手互动，通过对建议的评分来评估助手的表现，从而引导助手学习和适应。

Result: 在32个不同个性的实验中，ProPerAssistant展示了其适应策略的能力，并持续提高用户满意度。

Conclusion: ProPerAssistant通过不断学习和适应用户反馈，展示了将主动性与个性化结合的潜力，显著提高了用户满意度。

Abstract: As large language models (LLMs) become increasingly integrated into daily
life, there is growing demand for AI assistants that are not only reactive but
also proactive and personalized. While recent advances have pushed forward
proactivity and personalization individually, their combination remains
underexplored. To bridge this gap, we introduce ProPerSim, a new task and
simulation framework for developing assistants capable of making timely,
personalized recommendations in realistic home scenarios. In our simulation
environment, a user agent with a rich persona interacts with the assistant,
providing ratings on how well each suggestion aligns with its preferences and
context. The assistant's goal is to use these ratings to learn and adapt to
achieve higher scores over time. Built on ProPerSim, we propose
ProPerAssistant, a retrieval-augmented, preference-aligned assistant that
continually learns and adapts through user feedback. Experiments across 32
diverse personas show that ProPerAssistant adapts its strategy and steadily
improves user satisfaction, highlighting the promise of uniting proactivity and
personalization.

</details>


### [186] [How Accurate Are LLMs at Multi-Question Answering on Conversational Transcripts?](https://arxiv.org/abs/2509.21732)
*Xiliang Zhu,Shi Zong,David Rossouw*

Main category: cs.CL

TL;DR: 本研究探讨了大语言模型在长文本上下文中回答多个问题的能力，发现经过微调的公共模型在准确性上超越强大的私有模型，具有成本效益和透明性。


<details>
  <summary>Details</summary>
Motivation: 解决在长文本上下文中使用大语言模型进行问答时的高计算成本和延迟问题，特别是在同一上下文下回答多个问题时的挑战。

Method: 进行了广泛的实验，对多种专有和公共大语言模型在基于相同对话上下文回答多个问题的能力进行了基准测试。

Result: 实验结果显示，尽管像GPT-4o这样的强大私有语言模型表现最佳，但经过微调的公共模型（最多8亿参数）在准确性方面可以超越GPT-4o。

Conclusion: 经过实验证明，经过微调的公共大语言模型在特定任务上能够超越强大的私有模型，并且具有较低的成本和更高的透明度，适合实际应用。

Abstract: Deploying Large Language Models (LLMs) for question answering (QA) over
lengthy contexts is a significant challenge. In industrial settings, this
process is often hindered by high computational costs and latency, especially
when multiple questions must be answered based on the same context. In this
work, we explore the capabilities of LLMs to answer multiple questions based on
the same conversational context. We conduct extensive experiments and benchmark
a range of both proprietary and public models on this challenging task. Our
findings highlight that while strong proprietary LLMs like GPT-4o achieve the
best overall performance, fine-tuned public LLMs with up to 8 billion
parameters can surpass GPT-4o in accuracy, which demonstrates their potential
for transparent and cost-effective deployment in real-world applications.

</details>


### [187] [Self-Speculative Biased Decoding for Faster Live Translation](https://arxiv.org/abs/2509.21740)
*Linxiao Zeng,Haoyun Deng,Kangyuan Shu,Shizhen Wang*

Main category: cs.CL

TL;DR: 本研究提出了一种新颖的自我推测偏向解码策略，通过使用最新的输出草稿来加速实时翻译，实现了显著的速度提升和减少用户干扰。


<details>
  <summary>Details</summary>
Motivation: 在流媒体应用中使用大型语言模型进行文本生成时，如何在确保合理计算成本的前提下，实现输出的持续更新是一个挑战。

Method: 提出自我推测偏向解码的方法，在验证阶段输出偏向于草稿 token，利用草稿提高接受率，并在分歧点之后继续常规解码。

Result: 实验结果表明，该方法相比于常规自回归重翻译实现了最高1.7倍的速度提升，同时质量保持不变，且通过使用显示-only mask-k 技术显著减少了80%的干扰。

Conclusion: 该研究提出了一种新型推理范式，自我推测偏向解码，通过使用最新输出作为当前输入上下文的草稿来加速实时翻译，同时避免了重复生成输出的需求，从而提高了速度并减少了用户干扰。

Abstract: Large Language Models (LLMs) have recently demonstrated impressive
capabilities in various text generation tasks. However, it remains challenging
to use them off-the-shelf in streaming applications (such as live translation),
where the output must continually update as the input context expands, while
still maintaining a reasonable computational cost to meet the latency
requirement.
  In this work, we reexamine the re-translation approach to simultaneous
translation and propose Self-Speculative Biased Decoding, a novel inference
paradigm designed to avoid repeatedly generating output from scratch for a
consistently growing input stream. We propose using the most recent output as a
draft for the current growing input context. During the verification stage, the
output will be biased towards the draft token for a higher draft acceptance
rate. This strategy not only minimizes flickering that might distract users but
also leads to higher speedups. Conventional decoding may take charge from the
point of divergence after draft verification and continue until the end
condition is met.
  Unlike existing speculative decoding strategies, our approach eliminates the
need for draft computations, making it a model-agnostic and plug-and-play
solution for accelerating latency-sensitive streaming applications.
Experimental results on simultaneous text-to-text re-translation demonstrate
that our approach achieves up to 1.7x speedup compared to conventional
auto-regressive re-translation without compromising quality. Additionally, it
significantly reduces flickering by 80% by incorporating the display-only
mask-k technique.

</details>


### [188] [SynerGen: Contextualized Generative Recommender for Unified Search and Recommendation](https://arxiv.org/abs/2509.21777)
*Vianne R. Gao,Chen Xue,Marc Versage,Xie Zhou,Zhongruo Wang,Chao Li,Yeon Seonwoo,Nan Chen,Zhen Ge,Gourab Kundu,Weiqi Zhang,Tian Wang,Qingjun Cui,Trishul Chilimbi*

Main category: cs.CL

TL;DR: SynerGen模型整合个性化搜索与推荐，采用新型Transformer架构和联合优化方法，在性能上显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 针对现有推荐系统在检索和排序阶段的分裂及不同优化目标导致的精度不佳和工程负担，提出统一个性化搜索与推荐的模型需求。

Method: 采用基于Transformer的解码器，有效结合InfoNCE和混合的点对点-对比损失进行联合优化。

Result: SynerGen在主流推荐和搜索基准上表现出色，优于强大的生成推荐和联合搜索与推荐基线。

Conclusion: SynerGen在推荐和搜索基准测试中显著提高了性能，证明了单一生成基础模型在工业规模统一信息访问中的可行性。

Abstract: The dominant retrieve-then-rank pipeline in large-scale recommender systems
suffers from mis-calibration and engineering overhead due to its architectural
split and differing optimization objectives. While recent generative sequence
models have shown promise in unifying retrieval and ranking by
auto-regressively generating ranked items, existing solutions typically address
either personalized search or query-free recommendation, often exhibiting
performance trade-offs when attempting to unify both. We introduce
\textit{SynerGen}, a novel generative recommender model that bridges this
critical gap by providing a single generative backbone for both personalized
search and recommendation, while simultaneously excelling at retrieval and
ranking tasks. Trained on behavioral sequences, our decoder-only Transformer
leverages joint optimization with InfoNCE for retrieval and a hybrid
pointwise-pairwise loss for ranking, allowing semantic signals from search to
improve recommendation and vice versa. We also propose a novel time-aware
rotary positional embedding to effectively incorporate time information into
the attention mechanism. \textit{SynerGen} achieves significant improvements on
widely adopted recommendation and search benchmarks compared to strong
generative recommender and joint search and recommendation baselines. This work
demonstrates the viability of a single generative foundation model for
industrial-scale unified information access.

</details>


### [189] [Navigating the Impact of Structured Output Format on Large Language Models through the Compass of Causal Inference](https://arxiv.org/abs/2509.21791)
*Han Yuan,Yue Zhao,Li Zhang,Wuqiong Luo,Zheng Ma*

Main category: cs.CL

TL;DR: 本研究使用因果推断分析，发现结构化输出对大语言模型的生成影响在大多数情况下没有因果关系，只有在有限的场景中表现出复杂的因果结构。


<details>
  <summary>Details</summary>
Motivation: 以往研究对于结构化输出的影响存在片面性，缺乏全面和精确的分析，故此研究旨在提供更深入的理解。

Method: 采用因果推断分析，基于一假设和两个保证的约束条件，推导出五种潜在因果结构，评估结构化输出对生成质量的影响。

Result: 通过七个公共和一个开发的推理任务，粗略评估指标显示对GPT-4o生成质量的影响有正面、负面或中立的效果，然而因果推断结果显示在48种场景中，有43种不存在因果影响，仅有5种情况下展示了复杂的因果结构。

Conclusion: 在大多数情况下，结构化输出对大语言模型（LLMs）生成的影响不具因果关系，只有在少数情况下显示出具体指令影响的复杂因果结构。

Abstract: Structured output from large language models (LLMs) has enhanced efficiency
in processing generated information and is increasingly adopted in industrial
applications. Prior studies have investigated the impact of structured output
on LLMs' generation quality, often presenting one-way findings. Some suggest
that structured format enhances completeness and factual accuracy, while others
argue that it restricts the reasoning capacity of LLMs and leads to reductions
in standard evaluation metrics. Potential limitations of these assessments
include restricted testing scenarios, weakly controlled comparative settings,
and reliance on coarse metrics. In this work, we present a refined analysis
using causal inference. Based on one assumed and two guaranteed constraints, we
derive five potential causal structures characterizing the influence of
structured output on LLMs' generation: (1) collider without m-bias, (2)
collider with m-bias, (3) single cause from instruction, (4) single cause from
output format, and (5) independence. Across seven public and one developed
reasoning tasks, we find that coarse metrics report positive, negative, or
neutral effects of structured output on GPT-4o's generation. However, causal
inference reveals no causal impact in 43 out of 48 scenarios. In the remaining
5, 3 involve multifaceted causal structures influenced by concrete
instructions.

</details>


### [190] [Evaluating and Improving Cultural Awareness of Reward Models for LLM Alignment](https://arxiv.org/abs/2509.21798)
*Hongbin Zhang,Kehai Chen,Xuefeng Bai,Yang Xiang,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出文化意识奖励建模基准CARB，评估当前奖励模型在文化意识方面的缺陷，并通过Think-as-Locals方法提升文化理解，验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型的文化意识，以实现全球协调，但现有的奖励模型评估缺乏文化相关的数据集。

Method: 采用强化学习从可验证奖励（RLVR）中引导生成模型进行更深入的文化推理，并设计奖励以确保准确的偏好判断和高质量的结构化评估标准生成。

Result: 对最先进的奖励模型的评估显示它们在文化意识建模方面的不足，并且CARB在多语言文化协调任务中的表现与之呈正相关。

Conclusion: 提议的Think-as-Locals方法有效缓解了表面特征干扰，推动了文化意识奖励建模的进展。

Abstract: Reward models (RMs) are crucial for aligning large language models (LLMs)
with diverse cultures. Consequently, evaluating their cultural awareness is
essential for further advancing global alignment of LLMs. However, existing RM
evaluations fall short in assessing cultural awareness due to the scarcity of
culturally relevant evaluation datasets. To fill this gap, we propose Cultural
Awareness Reward modeling Benchmark (CARB), covering 10 distinct cultures
across 4 cultural domains. Our extensive evaluation of state-of-the-art RMs
reveals their deficiencies in modeling cultural awareness and demonstrates a
positive correlation between performance on CARB and downstream multilingual
cultural alignment tasks. Further analysis identifies the spurious correlations
within culture-aware reward modeling, wherein RM's scoring relies predominantly
on surface-level features rather than authentic cultural nuance understanding.
To address these, we propose Think-as-Locals to elicit deeper culturally
grounded reasoning from generative RMs via reinforcement learning from
verifiable rewards (RLVR) and employ well-designed rewards to ensure accurate
preference judgments and high-quality structured evaluation criteria
generation. Experimental results validate its efficacy in mitigating spurious
features interference and advancing culture-aware reward modeling.

</details>


### [191] [Redefining Machine Simultaneous Interpretation: From Incremental Translation to Human-Like Strategies](https://arxiv.org/abs/2509.21801)
*Qianen Zhang,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 通过扩展SiMT动作空间，引入新的自适应翻译动作，显著提升了翻译质量和处理速度。


<details>
  <summary>Details</summary>
Motivation: 提高同时机器翻译在实时性和语义准确性上的表现，克服传统编码-解码策略的限制。

Method: 在解码器-only的大型语言模型框架中实现新动作，并通过行动感知召唤构建训练参考。

Result: 在ACL60/60的英中和英德基准测试中，相较于参考翻译及基于salami的基线，提升了语义指标并降低了延迟。

Conclusion: 扩展SiMT的动作空间能够有效提高翻译质量和降低延迟，尤其是在结合DROP和SENTENCE_CUT时，表现优异。

Abstract: Simultaneous Machine Translation (SiMT) requires high-quality translations
under strict real-time constraints, which traditional encoder-decoder policies
with only READ/WRITE actions cannot fully address. We extend the action space
of SiMT with four adaptive actions: SENTENCE_CUT, DROP, PARTIAL_SUMMARIZATION
and PRONOMINALIZATION, which enable real-time restructuring, omission, and
simplification while preserving semantic fidelity. We implement these actions
in a decoder-only large language model (LLM) framework and construct training
references through action-aware prompting. To evaluate both quality and
latency, we further develop a latency-aware TTS pipeline that maps textual
outputs to speech with realistic timing. Experiments on the ACL60/60
English-Chinese and English-German benchmarks show that our framework
consistently improves semantic metrics (e.g., COMET-KIWI) and achieves lower
delay (measured by Average Lagging) compared to reference translations and
salami-based baselines. Notably, combining DROP and SENTENCE_CUT yields the
best overall balance between fluency and latency. These results demonstrate
that enriching the action space of LLM-based SiMT provides a promising
direction for bridging the gap between human and machine interpretation.

</details>


### [192] [Towards Minimal Causal Representations for Human Multimodal Language Understanding](https://arxiv.org/abs/2509.21805)
*Menghua Jiang,Yuncheng Jiang,Haifeng Hu,Sijie Mai*

Main category: cs.CL

TL;DR: 本文提出了一种新的Causal Multimodal Information Bottleneck (CaMIB)模型，旨在增强多模态语言理解中的因果特征提取，克服传统方法在面对数据集偏差和OOD泛化时的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决现有多模态学习方法中由于数据集偏差导致的模型泛化能力下降问题。

Method: 引入因果多模态信息瓶颈（CaMIB）模型，通过信息瓶颈过滤输入，应用参数化掩码生成器解耦模态表示，结合工具变量约束和后门调整以稳定因果估计。

Result: 在多模态情感分析、幽默检测和讽刺检测等领域的广泛实验中，证明了CaMIB模型的有效性和可解释性。

Conclusion: CaMIB模型有效地提高了多模态语言理解中的因果特征提取能力，并在处理数据集偏差和OOD泛化问题上显示出优势。

Abstract: Human Multimodal Language Understanding (MLU) aims to infer human intentions
by integrating related cues from heterogeneous modalities. Existing works
predominantly follow a ``learning to attend" paradigm, which maximizes mutual
information between data and labels to enhance predictive performance. However,
such methods are vulnerable to unintended dataset biases, causing models to
conflate statistical shortcuts with genuine causal features and resulting in
degraded out-of-distribution (OOD) generalization. To alleviate this issue, we
introduce a Causal Multimodal Information Bottleneck (CaMIB) model that
leverages causal principles rather than traditional likelihood. Concretely, we
first applies the information bottleneck to filter unimodal inputs, removing
task-irrelevant noise. A parameterized mask generator then disentangles the
fused multimodal representation into causal and shortcut subrepresentations. To
ensure global consistency of causal features, we incorporate an instrumental
variable constraint, and further adopt backdoor adjustment by randomly
recombining causal and shortcut features to stabilize causal estimation.
Extensive experiments on multimodal sentiment analysis, humor detection, and
sarcasm detection, along with OOD test sets, demonstrate the effectiveness of
CaMIB. Theoretical and empirical analyses further highlight its
interpretability and soundness.

</details>


### [193] [Can LLMs Solve and Generate Linguistic Olympiad Puzzles?](https://arxiv.org/abs/2509.21820)
*Neh Majmudar,Elena Filatova*

Main category: cs.CL

TL;DR: 本文探讨了语言难题的解决与生成，发现LLMs在解题方面优于人类，并提出生成难题对普及语言学的重要性。


<details>
  <summary>Details</summary>
Motivation: 旨在通过语言难题增强人们对语言学的兴趣，特别是针对少数语言和未充分研究的语言。

Method: 利用大型语言模型（LLMs）解决语言难题并进行性能分析，同时探讨语言难题的生成。

Result: 大型语言模型在绝大多数语言难题上表现超过人类，尤其是在少数语言的写作系统难题上表现不佳。

Conclusion: 该研究表明，大型语言模型在解决大多数语言难题时优于人类，并提出了自动生成语言难题的重要性，以促进语言学的广泛传播。

Abstract: In this paper, we introduce a combination of novel and exciting tasks: the
solution and generation of linguistic puzzles. We focus on puzzles used in
Linguistic Olympiads for high school students. We first extend the existing
benchmark for the task of solving linguistic puzzles. We explore the use of
Large Language Models (LLMs), including recent state-of-the-art models such as
OpenAI's o1, for solving linguistic puzzles, analyzing their performance across
various linguistic topics. We demonstrate that LLMs outperform humans on most
puzzles types, except for those centered on writing systems, and for the
understudied languages. We use the insights from puzzle-solving experiments to
direct the novel task of puzzle generation. We believe that automating puzzle
generation, even for relatively simple puzzles, holds promise for expanding
interest in linguistics and introducing the field to a broader audience. This
finding highlights the importance of linguistic puzzle generation as a research
task: such puzzles can not only promote linguistics but also support the
dissemination of knowledge about rare and understudied languages.

</details>


### [194] [ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models](https://arxiv.org/abs/2509.21826)
*Zihan Lin,Xiaohan Wang,Jie Cao,Jiajun Chai,Guojun Yin,Wei Lin,Ran He*

Main category: cs.CL

TL;DR: 本研究提出了一种新的工具使用任务训练方法ResT，通过熵感知的政策重加权增强训练稳定性并提升性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决工具使用任务中因稀疏奖励导致的政策梯度方差和训练效率低下的问题。

Method: 采用基于熵的信息重加权策略来调整政策梯度，优化工具使用任务的训练过程。

Result: ResT在BFCL和API-Bank的评估中表现突出，超越了之前的方法，特别是在单轮和多轮任务中表现优异。

Conclusion: ResT在多轮工具使用任务中实现了更高的稳定性和性能，超越了之前的方法。

Abstract: Large language models (LLMs) transcend passive generation and act as
goal-directed agents by invoking external tools. Reinforcement learning (RL)
offers a principled framework for optimizing these emergent tool-use policies,
yet the prevailing paradigm relies exclusively on sparse outcome rewards and
lacks consideration of the particularity of tool-use tasks, inflating
policy-gradient variance and resulting in inefficient training. To better
understand and address these challenges, we first establish a theoretical link
between policy entropy and training stability of tool-use tasks, which reveals
that structured, low-entropy tokens are primary determinants of rewards.
Motivated by this insight, we propose \textbf{Res}haped \textbf{T}oken-level
policy gradients (\textbf{ResT}) for tool-use tasks. ResT reshapes the policy
gradient through entropy-informed token reweighting, progressively upweighting
reasoning tokens as training proceeds. This entropy-aware scheme enables a
smooth shift from structural correctness to semantic reasoning and stabilizes
convergence in multi-turn tool-use tasks. Evaluation on BFCL and API-Bank shows
that ResT achieves state-of-the-art results, outperforming prior methods by up
to $8.76\%$. When fine-tuned on a 4B base LLM, ResT further surpasses GPT-4o by
$4.11\%$ on single-turn tasks and $1.50\%$ on multi-turn base tasks.

</details>


### [195] [Semantic Agreement Enables Efficient Open-Ended LLM Cascades](https://arxiv.org/abs/2509.21837)
*Duncan Soiffer,Steven Kolawole,Virginia Smith*

Main category: cs.CL

TL;DR: 本研究提出语义一致性作为评估LLM输出可靠性的有效信号，展示了其在降低成本和延迟方面的优势。


<details>
  <summary>Details</summary>
Motivation: 在开放式文本生成中，如何评估生成输出的可靠性和质量的连续性是一个基本挑战。

Method: 提出采用语义一致性作为可靠性信号，以支持小模型和大模型间的请求转发。

Result: 使用500M到70B参数的模型进行评估，发现语义级联在40%的成本下质量匹配或超越目标模型，同时延迟减少高达60%。

Conclusion: 语义级联在成本效益和延迟方面显著优于目标模型，同时保持或超过其质量，是一种有效的LLM部署基线。

Abstract: Cascade systems route computational requests to smaller models when possible
and defer to larger models only when necessary, offering a promising approach
to balance cost and quality in LLM deployment. However, they face a fundamental
challenge in open-ended text generation: determining output reliability when
generation quality lies on a continuous spectrum, often with multiple valid
responses. To address this, we propose semantic agreement -- meaning-level
consensus between ensemble outputs -- as a training-free signal for reliable
deferral. We show that when diverse model outputs agree semantically, their
consensus is a stronger reliability signal than token-level confidence.
Evaluated from 500M to 70B-parameter models, we find that semantic cascades
match or surpass target-model quality at 40% of the cost and reduce latency by
up to 60%. Our method requires no model internals, works across black-box APIs,
and remains robust to model updates, making it a practical baseline for
real-world LLM deployment.

</details>


### [196] [Following the TRACE: A Structured Path to Empathetic Response Generation with Multi-Agent Models](https://arxiv.org/abs/2509.21849)
*Ziqi Liu,Ziyang Zhou,Yilin Li,Haiyang Zhang,Yangbin Chen*

Main category: cs.CL

TL;DR: TRACE是一个新框架，通过任务分解的方式增强了同理心应答生成的能力，实验结果显示其在多项评估中超越了传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前方法在专业模型的分析深度与大型语言模型的生成流畅性之间存在权衡，而TRACE框架试图解决这一问题。

Method: TRACE框架将同理心建模为一个结构化的认知过程，分解任务为分析和合成的管道，以建立全面理解后再进行生成。

Result: 实验结果显示，TRACE框架在自动评估和基于LLM的评估中显著优于强基线，验证了其结构化分解的有效性。

Conclusion: TRACE框架通过任务分解实现了对同理心的建模，显著提升了应答生成效果，并表明这一结构化分解方法在创建更具能力和可解释性的同理心代理方面具有潜力。

Abstract: Empathetic response generation is a crucial task for creating more human-like
and supportive conversational agents. However, existing methods face a core
trade-off between the analytical depth of specialized models and the generative
fluency of Large Language Models (LLMs). To address this, we propose TRACE,
Task-decomposed Reasoning for Affective Communication and Empathy, a novel
framework that models empathy as a structured cognitive process by decomposing
the task into a pipeline for analysis and synthesis. By building a
comprehensive understanding before generation, TRACE unites deep analysis with
expressive generation. Experimental results show that our framework
significantly outperforms strong baselines in both automatic and LLM-based
evaluations, confirming that our structured decomposition is a promising
paradigm for creating more capable and interpretable empathetic agents. Our
code is available at https://anonymous.4open.science/r/TRACE-18EF/README.md.

</details>


### [197] [KnowMT-Bench: Benchmarking Knowledge-Intensive Long-Form Question Answering in Multi-Turn Dialogues](https://arxiv.org/abs/2509.21856)
*Junhao Chen,Yu Huang,Siyuan Li,Rui Yao,Hanqian Li,Hanyu Zhang,Jungang Li,Jian Chen,Bowen Wang,Xuming Hu*

Main category: cs.CL

TL;DR: 本文介绍了KnowMT-Bench，这是一个新基准，专门评估大型语言模型在多轮长篇问答中的知识传递能力，并发现通过检索增强生成可以改善模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准局限于单轮对话，而多轮基准通常评估其他不相关能力，因此需要一个系统的基准来评估MT-LFQA在知识密集领域的表现。

Method: 通过动态评估设置，模型生成自我多轮对话历史，并使用人工验证的自动化流程评估最终回合回答的事实能力和信息交付效率。

Result: 实验结果表明，多轮上下文会导致性能下降，但检索增强生成（RAG）可以有效缓解这一事实退化现象。

Conclusion: KnowMT-Bench是第一个针对多轮长篇问答（MT-LFQA）的基准，强调了在知识密集型应用中评估大型语言模型的必要性，并提出了有效的缓解策略来改善模型性能。

Abstract: Multi-Turn Long-Form Question Answering (MT-LFQA) is a key application
paradigm of Large Language Models (LLMs) in knowledge-intensive domains.
However, existing benchmarks are limited to single-turn dialogue, while
multi-turn dialogue benchmarks typically assess other orthogonal capabilities
rather than knowledge-intensive factuality. To bridge this critical gap, we
introduce \textbf{KnowMT-Bench}, the \textit{first-ever} benchmark designed to
systematically evaluate MT-LFQA for LLMs across knowledge-intensive fields,
including medicine, finance, and law. To faithfully assess the model's
real-world performance, KnowMT-Bench employs a dynamic evaluation setting where
models generate their own multi-turn dialogue histories given logically
progressive question sequences. The factual capability and information delivery
efficiency of the \textit{final-turn} answer are then evaluated using a
human-validated automated pipeline. Our experiments reveal that multi-turn
contexts degrade performance: factual capability declines due to the contextual
noise from self-generated histories, while information efficiency drops as
models become more verbose with increasing dialogue length. We then investigate
mitigation strategies, demonstrating that retrieval-augmented generation (RAG)
can effectively alleviate and even reverse this factual degradation. These
findings underscore the importance of our benchmark in evaluating and enhancing
the conversational factual capabilities of LLMs in real-world
knowledge-intensive applications. Code is available at
\href{https://github.com/hardenyu21/KnowMT-Bench}{\textcolor{cyan}{\texttt{KnowMT-Bench}}}.

</details>


### [198] [Enhancing Low-Rank Adaptation with Structured Nonlinear Transformations](https://arxiv.org/abs/2509.21870)
*Guanzhi Deng,Mingyang Liu,Dapeng Wu,Yinqiao Li,Linqi Song*

Main category: cs.CL

TL;DR: LoRAN是一种改进的LoRA方法，通过Sinter激活函数提升低秩调优性能，展示了激活函数设计对模型性能的重要性。


<details>
  <summary>Details</summary>
Motivation: 针对LoRA的线性限制，寻求增强表达能力的有效方法。

Method: 提出了LoRAN，一种非线性LoRA扩展，通过轻量级变换处理低秩更新，并引入Sinter激活函数。

Result: 实验结果表明，LoRAN在摘要和分类任务中持续超越QLoRA，Sinter表现优于Sigmoid、ReLU和Tanh等标准激活函数。

Conclusion: LoRAN显著改善了传统LoRA，并且Sinter激活函数在低秩调优中优于标准激活函数，展示了激活设计的重要性。

Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient
fine-tuning method for large language models. However, its linear nature limits
expressiveness. We propose LoRAN, a non-linear extension of LoRA that applies
lightweight transformations to the low-rank updates. We further introduce
Sinter, a sine-based activation that adds structured perturbations without
increasing parameter count. Experiments across summarization and classification
tasks show that LoRAN consistently improves over QLoRA. Ablation studies reveal
that Sinter outperforms standard activations such as Sigmoid, ReLU, and Tanh,
highlighting the importance of activation design in lowrank tuning.

</details>


### [199] [LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge Signals](https://arxiv.org/abs/2509.21875)
*Min-Hsuan Yeh,Yixuan Li,Tanwi Mallick*

Main category: cs.CL

TL;DR: LUMINA框架通过上下文-知识信号有效检测RAG系统中的幻觉，超越了以往的方法。


<details>
  <summary>Details</summary>
Motivation: 随着对RAG模型中幻觉现象的关注增加，了解外部上下文和内部知识如何影响模型的生成至关重要。

Method: 通过上下文-知识信号量化外部上下文利用和内部知识利用，使用分布距离和变换器层的预测标记演变进行测量。

Result: 在常见的RAG幻觉基准测试和四个开源LLM上，LUMINA在AUROC和AUPRC得分上表现出色，最高超出之前的方法13%。

Conclusion: LUMINA是一种有效且实用的框架，能够在RAG系统中检测幻觉，表现优于现有方法。

Abstract: Retrieval-Augmented Generation (RAG) aims to mitigate hallucinations in large
language models (LLMs) by grounding responses in retrieved documents. Yet,
RAG-based LLMs still hallucinate even when provided with correct and sufficient
context. A growing line of work suggests that this stems from an imbalance
between how models use external context and their internal knowledge, and
several approaches have attempted to quantify these signals for hallucination
detection. However, existing methods require extensive hyperparameter tuning,
limiting their generalizability. We propose LUMINA, a novel framework that
detects hallucinations in RAG systems through context-knowledge signals:
external context utilization is quantified via distributional distance, while
internal knowledge utilization is measured by tracking how predicted tokens
evolve across transformer layers. We further introduce a framework for
statistically validating these measurements. Experiments on common RAG
hallucination benchmarks and four open-source LLMs show that LUMINA achieves
consistently high AUROC and AUPRC scores, outperforming prior utilization-based
methods by up to +13% AUROC on HalluRAG. Moreover, LUMINA remains robust under
relaxed assumptions about retrieval quality and model matching, offering both
effectiveness and practicality.

</details>


### [200] [No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping](https://arxiv.org/abs/2509.21880)
*Thanh-Long V. Le,Myeongho Jeon,Kim Vu,Viet Lai,Eunho Yang*

Main category: cs.CL

TL;DR: RL-ZVP算法利用零方差提示优化策略，展示了其在大语言模型推理能力提升中的潜力。


<details>
  <summary>Details</summary>
Motivation: 探讨如何有效利用零方差提示来优化策略，填补现有方法的空白。

Method: 提出了RL-ZVP算法，从零方差提示中提取学习信号，直接奖励正确性并惩罚错误。

Result: RL-ZVP在准确率上提高了最高8.61个百分点，在通过率上提高了7.77个百分点，超越了GRPO和其他基线。

Conclusion: RL-ZVP算法在六个数学推理基准上显著提高了准确率和通过率，展示了零方差提示在强化学习中的潜力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework
for improving the reasoning abilities of Large Language Models (LLMs). However,
current methods such as GRPO rely only on problems where the model responses to
the same input differ in correctness, while ignoring those where all responses
receive the same reward - so-called zero-variance prompts. In this work, we
argue that such prompts are not useless but can, in fact, provide meaningful
feedback for policy optimization. To this end, we introduce RL with
Zero-Variance Prompts (RL-ZVP), a novel algorithm that extract learning signals
from zero-variance prompts. RL-ZVP directly rewards correctness and penalizes
errors even without contrasting responses, modulating feedback with token-level
characteristics to preserve informative, nuanced signals. Across six math
reasoning benchmarks, RL-ZVP achieves significant improvements of up to 8.61
points in accuracy and 7.77 points in pass rate over GRPO, while consistently
outperforming other baselines that filter out zero-variance prompts. These
results highlight the untapped potential of learning from zero-variance prompts
in RLVR.

</details>


### [201] [QoNext: Towards Next-generation QoE for Foundation Models](https://arxiv.org/abs/2509.21889)
*Yijin Guo,Ye Shen,Farong Wen,Junying Wang,Zicheng Zhang,Qi Jia,Guangtao Zhai*

Main category: cs.CL

TL;DR: QoNext是一个新框架，通过引入QoE原则，更全面地评估基础模型，提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 现有的基础模型评估方法忽视了用户交互过程中用户满意度的重要性，无法反映用户体验的真实机制。

Method: 通过构建QoE导向数据库和训练预测模型，我们在受控实验中收集人类评分，并评估用户体验。

Result: QoNext能够实现主动的、细致入微的评估，并对基础模型优化提供可行的指导。

Conclusion: QoNext提供了一种新的评估基础模型的方法，通过引入QoE原则，能够更好地捕捉用户体验，并为优化产品化服务提供实用建议。

Abstract: Existing evaluations of foundation models, including recent human-centric
approaches, fail to capture what truly matters: user's experience during
interaction. Current methods treat evaluation as a matter of output correctness
alone, overlooking that user satisfaction emerges from the interplay between
response quality and interaction, which limits their ability to account for the
mechanisms underlying user experience. To address this gap, we introduce
QoNext, the first framework that adapts Quality of Experience (QoE) principles
from networking and multimedia to the assessment of foundation models. QoNext
identifies experiential factors that shape user experience and incorporates
them into controlled experiments, where human ratings are collected under
varied configurations. From these studies we construct a QoE-oriented database
and train predictive models that estimate perceived user experience from
measurable system parameters. Our results demonstrate that QoNext not only
enables proactive and fine-grained evaluation but also provides actionable
guidance for productized services of optimizing foundation models in practice.

</details>


### [202] [Elastic MoE: Unlocking the Inference-Time Scalability of Mixture-of-Experts](https://arxiv.org/abs/2509.21892)
*Naibin Gu,Zhenyu Zhang,Yuchen Feng,Yilong Chen,Peng Fu,Zheng Lin,Shuohuan Wang,Yu Sun,Hua Wu,Weiping Wang,Haifeng Wang*

Main category: cs.CL

TL;DR: 提出了一种新的EMoE框架，改善了MoE模型在推理时的专家激活数量，以提高性能，结果表明该方法显著提升了模型的有效性能范围和峰值性能。


<details>
  <summary>Details</summary>
Motivation: 传统的MoE模型在训练和推理时激活的专家数量是固定的，推理时增加激活专家数量的直觉上能够提高性能，但实际上效果有限。

Method: 提出了一种新的训练框架EMoE，使得MoE模型在推理时能够灵活调整激活专家的数量，同时促进专家之间的协作。

Result: 实验表明，EMoE能将有效性能扩展范围提升至训练时数量的2-3倍，并提高模型的峰值性能。

Conclusion: EMoE能显著扩展有效性能扩展范围，且在相同条件下提高模型的峰值性能。

Abstract: Mixture-of-Experts (MoE) models typically fix the number of activated experts
$k$ at both training and inference. Intuitively, activating more experts at
inference $k'$ (where $k'> k$) means engaging a larger set of model parameters
for the computation and thus is expected to improve performance. However,
contrary to this intuition, we find the scaling range to be so narrow that
performance begins to degrade rapidly after only a slight increase in the
number of experts. Further investigation reveals that this degradation stems
from a lack of learned collaboration among experts. To address this, we
introduce Elastic Mixture-of-Experts (EMoE), a novel training framework that
enables MoE models to scale the number of activated experts at inference
without incurring additional training overhead. By simultaneously training
experts to collaborate in diverse combinations and encouraging the router for
high-quality selections, EMoE ensures robust performance across computational
budgets at inference. We conduct extensive experiments on various MoE settings.
Our results show that EMoE significantly expands the effective
performance-scaling range, extending it to as much as 2-3$\times$ the
training-time $k$, while also pushing the model's peak performance to a higher
level.

</details>


### [203] [A Large-Scale Dataset and Citation Intent Classification in Turkish with LLMs](https://arxiv.org/abs/2509.21907)
*Kemal Sami Karaca,Bahaeddin Eravcı*

Main category: cs.CL

TL;DR: 本研究提出了一种针对土耳其语引用意图的分类方法，建立了新的数据集并实现了91.3%准确率，促进了未来的定性研究。


<details>
  <summary>Details</summary>
Motivation: 理解引用的定性意图对于全面评估学术研究至关重要，但在诸如土耳其语等粘性语言中会面临独特的挑战。

Method: 本研究首先介绍了一个新的公共可用土耳其引用意图数据集，并评估了使用大型语言模型的标准上下文学习（ICL）的表现。随后，提出了建立在DSPy框架上的可编程分类管道，以系统地自动优化提示，并使用堆叠泛化集成方法进行最终分类。

Result: 通过引入自动化优化的提示和堆叠泛化集成，该研究达到了91.3%的最佳准确率，从而提供了更稳定和可靠的预测。

Conclusion: 本研究为土耳其自然语言处理社区和更广泛的学术界提供了基础数据集和强大的分类框架，为未来的定性引用研究铺平了道路。

Abstract: Understanding the qualitative intent of citations is essential for a
comprehensive assessment of academic research, a task that poses unique
challenges for agglutinative languages like Turkish. This paper introduces a
systematic methodology and a foundational dataset to address this problem. We
first present a new, publicly available dataset of Turkish citation intents,
created with a purpose-built annotation tool. We then evaluate the performance
of standard In-Context Learning (ICL) with Large Language Models (LLMs),
demonstrating that its effectiveness is limited by inconsistent results caused
by manually designed prompts. To address this core limitation, we introduce a
programmable classification pipeline built on the DSPy framework, which
automates prompt optimization systematically. For final classification, we
employ a stacked generalization ensemble to aggregate outputs from multiple
optimized models, ensuring stable and reliable predictions. This ensemble, with
an XGBoost meta-model, achieves a state-of-the-art accuracy of 91.3\%.
Ultimately, this study provides the Turkish NLP community and the broader
academic circles with a foundational dataset and a robust classification
framework paving the way for future qualitative citation studies.

</details>


### [204] [AutoSCORE: Enhancing Automated Scoring with Multi-Agent Large Language Models via Structured Component Recognition](https://arxiv.org/abs/2509.21910)
*Yun Wang,Zhaojun Ding,Xuansheng Wu,Siyue Sun,Ninghao Liu,Xiaoming Zhai*

Main category: cs.CL

TL;DR: AutoSCORE是一个多代理LLM框架，通过结构化组件识别提升自动评分的准确性和可解释性，显著改善人机一致性。


<details>
  <summary>Details</summary>
Motivation: 自动评分在教育中发挥重要作用，但当前大语言模型在准确性、可解释性及评分标准对齐方面存在挑战，限制了其应用。

Method: 提出了AutoSCORE，一个多代理LLM框架，首先从学生回复中提取与评分标准相关的组件，并将其编码为结构化表示，随后用于分配最终分数。

Result: 在ASAP基准的四个数据集上评估AutoSCORE，结果显示其在评分准确性、人机一致性和错误度量上均优于单代理基线，尤其在复杂的多维评分标准上表现尤为突出。

Conclusion: AutoSCORE通过结构化组件识别和多代理设计，为自动评分提供了一种可扩展、可靠和可解释的解决方案，实现评分准确性和人机一致性的显著提高。

Abstract: Automated scoring plays a crucial role in education by reducing the reliance
on human raters, offering scalable and immediate evaluation of student work.
While large language models (LLMs) have shown strong potential in this task,
their use as end-to-end raters faces challenges such as low accuracy, prompt
sensitivity, limited interpretability, and rubric misalignment. These issues
hinder the implementation of LLM-based automated scoring in assessment
practice. To address the limitations, we propose AutoSCORE, a multi-agent LLM
framework enhancing automated scoring via rubric-aligned Structured COmponent
REcognition. With two agents, AutoSCORE first extracts rubric-relevant
components from student responses and encodes them into a structured
representation (i.e., Scoring Rubric Component Extraction Agent), which is then
used to assign final scores (i.e., Scoring Agent). This design ensures that
model reasoning follows a human-like grading process, enhancing
interpretability and robustness. We evaluate AutoSCORE on four benchmark
datasets from the ASAP benchmark, using both proprietary and open-source LLMs
(GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B). Across diverse tasks and rubrics,
AutoSCORE consistently improves scoring accuracy, human-machine agreement (QWK,
correlations), and error metrics (MAE, RMSE) compared to single-agent
baselines, with particularly strong benefits on complex, multi-dimensional
rubrics, and especially large relative gains on smaller LLMs. These results
demonstrate that structured component recognition combined with multi-agent
design offers a scalable, reliable, and interpretable solution for automated
scoring.

</details>


### [205] [SimulSense: Sense-Driven Interpreting for Efficient Simultaneous Speech Translation](https://arxiv.org/abs/2509.21932)
*Haotian Tan,Hiroki Ouchi,Sakriani Sakti*

Main category: cs.CL

TL;DR: SimulSense框架通过模拟人类翻译者的决策过程，在SimulST中实现了 superior的质量-延迟权衡和显著提高的实时效率。


<details>
  <summary>Details</summary>
Motivation: 现有SimulST系统依赖昂贵的LLM推理和专门的交叉训练数据，使得实时翻译的质量和效率受限。

Method: 提出了一种新的SimulST框架SimulSense，模拟人类翻译者根据感知的新意义单位进行连续决策。

Result: 实验结果表明，SimulSense相比现有的两个最先进基线系统，在质量和延迟表现上优越，且实时决策效率大幅提升。

Conclusion: SimulSense框架在质量和延迟之间实现了更优的平衡，并显著提高了实时效率，其决策速度比现有基线系统快9.6倍。

Abstract: How to make human-interpreter-like read/write decisions for simultaneous
speech translation (SimulST) systems? Current state-of-the-art systems
formulate SimulST as a multi-turn dialogue task, requiring specialized
interleaved training data and relying on computationally expensive large
language model (LLM) inference for decision-making. In this paper, we propose
SimulSense, a novel framework for SimulST that mimics human interpreters by
continuously reading input speech and triggering write decisions to produce
translation when a new sense unit is perceived. Experiments against two
state-of-the-art baseline systems demonstrate that our proposed method achieves
a superior quality-latency tradeoff and substantially improved real-time
efficiency, where its decision-making is up to 9.6x faster than the baselines.

</details>


### [206] [Why Chain of Thought Fails in Clinical Text Understanding](https://arxiv.org/abs/2509.21933)
*Jiageng Wu,Kevin Xie,Bowen Gu,Nils Krüger,Kueiyu Joshua Lin,Jie Yang*

Main category: cs.CL

TL;DR: 本文是对大型语言模型在临床文本理解中的Chain-of-thought提示的首次大规模系统研究，发现其在临床任务中的有效性存在问题。


<details>
  <summary>Details</summary>
Motivation: 探索Chain-of-thought（CoT）提示在临床文本理解中的有效性，尤其是在电子健康记录（EHR）环境中。

Method: 对95个先进大型语言模型进行评估，涵盖87个真实临床文本任务，涵盖9种语言和8种任务类型。

Result: 86.3%的模型在CoT设置中表现出一致的性能下降，更强的模型相对稳健，而较弱的模型遭受显著下降。

Conclusion: CoT增强了解释性但可能削弱临床文本任务的可靠性。

Abstract: Large language models (LLMs) are increasingly being applied to clinical care,
a domain where both accuracy and transparent reasoning are critical for safe
and trustworthy deployment. Chain-of-thought (CoT) prompting, which elicits
step-by-step reasoning, has demonstrated improvements in performance and
interpretability across a wide range of tasks. However, its effectiveness in
clinical contexts remains largely unexplored, particularly in the context of
electronic health records (EHRs), the primary source of clinical documentation,
which are often lengthy, fragmented, and noisy. In this work, we present the
first large-scale systematic study of CoT for clinical text understanding. We
assess 95 advanced LLMs on 87 real-world clinical text tasks, covering 9
languages and 8 task types. Contrary to prior findings in other domains, we
observe that 86.3\% of models suffer consistent performance degradation in the
CoT setting. More capable models remain relatively robust, while weaker ones
suffer substantial declines. To better characterize these effects, we perform
fine-grained analyses of reasoning length, medical concept alignment, and error
profiles, leveraging both LLM-as-a-judge evaluation and clinical expert
evaluation. Our results uncover systematic patterns in when and why CoT fails
in clinical contexts, which highlight a critical paradox: CoT enhances
interpretability but may undermine reliability in clinical text tasks. This
work provides an empirical basis for clinical reasoning strategies of LLMs,
highlighting the need for transparent and trustworthy approaches.

</details>


### [207] [Debiasing Large Language Models in Thai Political Stance Detection via Counterfactual Calibration](https://arxiv.org/abs/2509.21946)
*Kasidit Sermsri,Teerapong Panboonyuen*

Main category: cs.CL

TL;DR: 本文提出了ThaiFACTUAL，一个轻量级模型无关的校准框架，用于减轻泰国政治语境中的大型语言模型的政治偏见，通过创新的数据增强和监督方法，实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 在低资源和文化复杂的环境中，政治立场检测面临重大挑战，尤其是在存在偏见时影响到公平性和可靠性。

Method: 采用反事实数据增强和基于理由的监督来降低政治偏见，且不需要微调。

Result: 发布了首个高质量的泰国政治立场数据集，并且实验结果表明ThaiFACTUAL显著改善了模型的表现。

Conclusion: ThaiFACTUAL大幅减少了虚假相关性，增强了零-shot 泛化能力，并提高了多种大型语言模型的公平性。

Abstract: Political stance detection in low-resource and culturally complex settings
poses a critical challenge for large language models (LLMs). In the Thai
political landscape - marked by indirect language, polarized figures, and
entangled sentiment and stance - LLMs often display systematic biases such as
sentiment leakage and favoritism toward entities. These biases undermine
fairness and reliability. We present ThaiFACTUAL, a lightweight, model-agnostic
calibration framework that mitigates political bias without requiring
fine-tuning. ThaiFACTUAL uses counterfactual data augmentation and
rationale-based supervision to disentangle sentiment from stance and reduce
bias. We also release the first high-quality Thai political stance dataset,
annotated with stance, sentiment, rationales, and bias markers across diverse
entities and events. Experimental results show that ThaiFACTUAL significantly
reduces spurious correlations, enhances zero-shot generalization, and improves
fairness across multiple LLMs. This work highlights the importance of
culturally grounded debiasing techniques for underrepresented languages.

</details>


### [208] [MotivGraph-SoIQ: Integrating Motivational Knowledge Graphs and Socratic Dialogue for Enhanced LLM Ideation](https://arxiv.org/abs/2509.21978)
*Xinping Lei,Tong Zhou,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 本文提出了MotivGraph-SoIQ框架，通过动机知识图和苏格拉底对话改善大型语言模型的创意生成，有效提高创意的质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型在创意生成中存在的接地性不足和确认偏误问题，提出改进方案。

Method: 通过整合动机知识图和苏格拉底对话，构建MotivGraph-SoIQ框架。

Result: 在ICLR25论文主题数据集上，MotivGraph-SoIQ在多个评价指标上明显优于现有最佳方法。

Conclusion: MotivGraph-SoIQ在LLM思想生成中表现出明显优势，能够有效缓解确认偏误并提高创意质量。

Abstract: Large Language Models (LLMs) hold substantial potential for accelerating
academic ideation but face critical challenges in grounding ideas and
mitigating confirmation bias for further refinement. We propose integrating
motivational knowledge graphs and socratic dialogue to address these
limitations in enhanced LLM ideation (MotivGraph-SoIQ). This novel framework
provides essential grounding and practical idea improvement steps for LLM
ideation by integrating a Motivational Knowledge Graph (MotivGraph) with a
Q-Driven Socratic Ideator. The MotivGraph structurally stores three key node
types(problem, challenge and solution) to offer motivation grounding for the
LLM ideation process. The Ideator is a dual-agent system utilizing Socratic
questioning, which facilitates a rigorous refinement process that mitigates
confirmation bias and improves idea quality across novelty, experimental rigor,
and motivational rationality dimensions. On the ICLR25 paper topics dataset,
MotivGraph-SoIQ exhibits clear advantages over existing state-of-the-art
approaches across LLM-based scoring, ELO ranking, and human evaluation metrics.

</details>


### [209] [Black-Box Hallucination Detection via Consistency Under the Uncertain Expression](https://arxiv.org/abs/2509.21999)
*Seongho Joo,Kyungmin Min,Jahyun Koo,Kyomin Jung*

Main category: cs.CL

TL;DR: 本文提出了一种新的黑箱幻觉检测指标，旨在解决大型语言模型生成不真实响应的问题，方法有效且无需外部资源。


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型（LLMs）生成不真实响应的问题，寻找无需依赖外部资源或模型内部状态的方法。

Method: 通过对大型语言模型在表达不确定性时的行为分析，设计出一种简单的黑箱幻觉检测指标。

Result: 实验结果表明，该黑箱检测指标比依赖于LLMs内部知识的基线方法更能有效预测模型响应的真实性。

Conclusion: 提出了一种新的黑箱幻觉检测指标，能够有效预测语言模型生成文本的真实性。

Abstract: Despite the great advancement of Language modeling in recent days, Large
Language Models (LLMs) such as GPT3 are notorious for generating non-factual
responses, so-called "hallucination" problems. Existing methods for detecting
and alleviating this hallucination problem require external resources or the
internal state of LLMs, such as the output probability of each token. Given the
LLM's restricted external API availability and the limited scope of external
resources, there is an urgent demand to establish the Black-Box approach as the
cornerstone for effective hallucination detection. In this work, we propose a
simple black-box hallucination detection metric after the investigation of the
behavior of LLMs under expression of uncertainty. Our comprehensive analysis
reveals that LLMs generate consistent responses when they present factual
responses while non-consistent responses vice versa. Based on the analysis, we
propose an efficient black-box hallucination detection metric with the
expression of uncertainty. The experiment demonstrates that our metric is more
predictive of the factuality in model responses than baselines that use
internal knowledge of LLMs.

</details>


### [210] [GraphSearch: An Agentic Deep Searching Workflow for Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2509.22009)
*Cehao Yang,Xiaojun Wu,Xueyuan Lin,Chengjin Xu,Xuhui Jiang,Yuanliang Sun,Jia Li,Hui Xiong,Jian Guo*

Main category: cs.CL

TL;DR: GraphSearch是一种改进GraphRAG的深度检索方法，通过双通道检索增强了推理能力，实验结果表明其效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的GraphRAG方法存在浅层检索和预构建结构图数据利用不充分的问题，影响复杂查询的有效推理。

Method: 提出了一种模块化框架的双通道检索工作流程，包括六个模块，支持多轮交互和迭代推理。

Result: 在六个多跳RAG基准上的实验结果显示，GraphSearch在答案准确性和生成质量上优于传统策略。

Conclusion: GraphSearch通过双通道检索显著提高了答案准确性和生成质量，是图检索增强生成的重要发展方向。

Abstract: Graph Retrieval-Augmented Generation (GraphRAG) enhances factual reasoning in
LLMs by structurally modeling knowledge through graph-based representations.
However, existing GraphRAG approaches face two core limitations: shallow
retrieval that fails to surface all critical evidence, and inefficient
utilization of pre-constructed structural graph data, which hinders effective
reasoning from complex queries. To address these challenges, we propose
\textsc{GraphSearch}, a novel agentic deep searching workflow with dual-channel
retrieval for GraphRAG. \textsc{GraphSearch} organizes the retrieval process
into a modular framework comprising six modules, enabling multi-turn
interactions and iterative reasoning. Furthermore, \textsc{GraphSearch} adopts
a dual-channel retrieval strategy that issues semantic queries over chunk-based
text data and relational queries over structural graph data, enabling
comprehensive utilization of both modalities and their complementary strengths.
Experimental results across six multi-hop RAG benchmarks demonstrate that
\textsc{GraphSearch} consistently improves answer accuracy and generation
quality over the traditional strategy, confirming \textsc{GraphSearch} as a
promising direction for advancing graph retrieval-augmented generation.

</details>


### [211] [From Outliers to Topics in Language Models: Anticipating Trends in News Corpora](https://arxiv.org/abs/2509.22030)
*Evangelia Zve,Benjamin Icard,Alice Breton,Lila Sainero,Gauvain Bourgne,Jean-Gabriel Ganascia*

Main category: cs.CL

TL;DR: 本文研究了异常值在动态新闻语料库中如何成为新兴话题的信号，并发现其在时间上演变为一致性话题的现象。


<details>
  <summary>Details</summary>
Motivation: 探索在主题建模中，异常值如何作为新兴话题的微弱信号，而非简单的噪声。

Method: 利用最先进的语言模型生成的向量嵌入和累积聚类方法，追踪异常值在新闻数据集中的演变过程。

Result: 异常值在企业社会责任和气候变化的法语和英语新闻数据集中，逐渐演变为连贯的话题。

Conclusion: 异常值在时间推移中会演变成一致的话题，且这一模式在不同模型和语言中都是一致的。

Abstract: This paper examines how outliers, often dismissed as noise in topic modeling,
can act as weak signals of emerging topics in dynamic news corpora. Using
vector embeddings from state-of-the-art language models and a cumulative
clustering approach, we track their evolution over time in French and English
news datasets focused on corporate social responsibility and climate change.
The results reveal a consistent pattern: outliers tend to evolve into coherent
topics over time across both models and languages.

</details>


### [212] [Taxonomy of Comprehensive Safety for Clinical Agents](https://arxiv.org/abs/2509.22041)
*Jean Seo,Hyunkyung Lee,Gibaeg Kim,Wooseok Han,Jaehyo Yoo,Seungseop Lim,Kihun Shin,Eunho Yang*

Main category: cs.CL

TL;DR: 本文提出了一种新的21类细分分类法TACOS，以增强临床聊天机器人的安全性，验证了其有效性并提供了有价值的见解。


<details>
  <summary>Details</summary>
Motivation: 当前的方法在临床领域的复杂需求上往往不足以保证安全，因此需要一个更全面的解决方案。

Method: 通过开发一个21类的细分分类法，将安全过滤和工具选择集成到用户意图分类步骤中，并进行大量实验验证。

Result: 实验结果表明TACOS在临床代理设置中具有重要的应用价值，并揭示了训练数据分布和基础模型的预训练知识的有用见解。

Conclusion: TACOS提供了一种新的细化分类法，专门针对临床聊天机器人设置的安全性问题，并且验证了其有效性。

Abstract: Safety is a paramount concern in clinical chatbot applications, where
inaccurate or harmful responses can lead to serious consequences. Existing
methods--such as guardrails and tool calling--often fall short in addressing
the nuanced demands of the clinical domain. In this paper, we introduce TACOS
(TAxonomy of COmprehensive Safety for Clinical Agents), a fine-grained,
21-class taxonomy that integrates safety filtering and tool selection into a
single user intent classification step. TACOS is a taxonomy that can cover a
wide spectrum of clinical and non-clinical queries, explicitly modeling varying
safety thresholds and external tool dependencies. To validate our framework, we
curate a TACOS-annotated dataset and perform extensive experiments. Our results
demonstrate the value of a new taxonomy specialized for clinical agent
settings, and reveal useful insights about train data distribution and
pretrained knowledge of base models.

</details>


### [213] [Fuzzy Reasoning Chain (FRC): An Innovative Reasoning Framework from Fuzziness to Clarity](https://arxiv.org/abs/2509.22054)
*Ping Chen,Xiang Liu,Zhaoxiang Liu,Zezhou Chen,Xingpeng Zhang,Huan Hu,Zipeng Wang,Kai Wang,Shuming Shi,Shiguo Lian*

Main category: cs.CL

TL;DR: FRC框架将模糊推理与概率推理结合，处理模糊文本，提升了解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在自然语言处理上取得了显著进展，但在处理含糊、多义性或不确定性文本时仍面临重大挑战。

Method: 提出Fuzzy Reasoning Chain (FRC)框架，将LLM语义先验与连续模糊隶属度相结合。

Result: 在情感分析任务中进行了验证，理论分析和实证结果表明FRC确保了稳定的推理，并促进了不同模型规模之间的知识迁移。

Conclusion: FRC提供了一种有效管理细微和模糊表达的机制，增强了解释性和鲁棒性。

Abstract: With the rapid advancement of large language models (LLMs), natural language
processing (NLP) has achieved remarkable progress. Nonetheless, significant
challenges remain in handling texts with ambiguity, polysemy, or uncertainty.
We introduce the Fuzzy Reasoning Chain (FRC) framework, which integrates LLM
semantic priors with continuous fuzzy membership degrees, creating an explicit
interaction between probability-based reasoning and fuzzy membership reasoning.
This transition allows ambiguous inputs to be gradually transformed into clear
and interpretable decisions while capturing conflicting or uncertain signals
that traditional probability-based methods cannot. We validate FRC on sentiment
analysis tasks, where both theoretical analysis and empirical results show that
it ensures stable reasoning and facilitates knowledge transfer across different
model scales. These findings indicate that FRC provides a general mechanism for
managing subtle and ambiguous expressions with improved interpretability and
robustness.

</details>


### [214] [RedNote-Vibe: A Dataset for Capturing Temporal Dynamics of AI-Generated Text in Social Media](https://arxiv.org/abs/2509.22055)
*Yudong Li,Yufei Sun,Yuhan Yao,Peiru Yang,Wanyue Li,Jiajun Zou,Yongfeng Huang,Linlin Shen*

Main category: cs.CL

TL;DR: 本文提出了RedNote-Vibe数据集及PLAD框架，解析了社交媒体中的AI生成文本及其用户互动的动态特征。


<details>
  <summary>Details</summary>
Motivation: 针对社交媒体上广泛的AI生成文本，现有的数据集和检测方法主要局限于静态分析，缺乏动态用户互动的研究。

Method: 提出了一种利用心理语言学特征的可解释AIGT检测框架（PLAD）。

Result: RedNote-Vibe是首个五年期的社交媒体AIGT分析数据集，涵盖用户互动指标及时间戳，验证了PLAD框架的有效性。

Conclusion: PLAD在社交媒体环境中展示了优越的AIGT检测性能，并揭示了语言特征与用户互动之间的复杂关系。

Abstract: The proliferation of Large Language Models (LLMs) has led to widespread
AI-Generated Text (AIGT) on social media platforms, creating unique challenges
where content dynamics are driven by user engagement and evolve over time.
However, existing datasets mainly depict static AIGT detection. In this work,
we introduce RedNote-Vibe, the first longitudinal (5-years) dataset for social
media AIGT analysis. This dataset is sourced from Xiaohongshu platform,
containing user engagement metrics (e.g., likes, comments) and timestamps
spanning from the pre-LLM period to July 2025, which enables research into the
temporal dynamics and user interaction patterns of AIGT. Furthermore, to detect
AIGT in the context of social media, we propose PsychoLinguistic AIGT Detection
Framework (PLAD), an interpretable approach that leverages psycholinguistic
features. Our experiments show that PLAD achieves superior detection
performance and provides insights into the signatures distinguishing human and
AI-generated content. More importantly, it reveals the complex relationship
between these linguistic features and social media engagement. The dataset is
available at https://github.com/testuser03158/RedNote-Vibe.

</details>


### [215] [The QCET Taxonomy of Standard Quality Criterion Names and Definitions for the Evaluation of NLP Systems](https://arxiv.org/abs/2509.22064)
*Anya Belz,Simon Mille,Craig Thomson*

Main category: cs.CL

TL;DR: QCET通过建立标准化的质量评估标准，解决了NLP领域中不同评估之间可比性的问题，推动了科学进展。


<details>
  <summary>Details</summary>
Motivation: 由于相同质量标准名称的不同评估可能评估不同的质量方面，导致NLP领域的科学进步受阻，因此需要建立统一的质量标准。

Method: QCET质量标准评估分类法通过对NLP中报告的评估进行三次调查，提炼出标准质量标准名称和定义，并构建出层次结构。

Result: QCET为现有评估建立了可比性，指导新评估的设计，及评估监管合规性。

Conclusion: QCET为NLP领域提供了一个标准的质量标准名称和定义集合，促进了评估的可比性和新评估设计的指导。

Abstract: Prior work has shown that two NLP evaluation experiments that report results
for the same quality criterion name (e.g. Fluency) do not necessarily evaluate
the same aspect of quality, and the comparability implied by the name can be
misleading. Not knowing when two evaluations are comparable in this sense means
we currently lack the ability to draw reliable conclusions about system quality
on the basis of multiple, independently conducted evaluations. This in turn
hampers the ability of the field to progress scientifically as a whole, a
pervasive issue in NLP since its beginning (Sparck Jones, 1981). It is hard to
see how the issue of unclear comparability can be fully addressed other than by
the creation of a standard set of quality criterion names and definitions that
the several hundred quality criterion names actually in use in the field can be
mapped to, and grounded in. Taking a strictly descriptive approach, the QCET
Quality Criteria for Evaluation Taxonomy derives a standard set of quality
criterion names and definitions from three surveys of evaluations reported in
NLP, and structures them into a hierarchy where each parent node captures
common aspects of its child nodes. We present QCET and the resources it
consists of, and discuss its three main uses in (i) establishing comparability
of existing evaluations, (ii) guiding the design of new evaluations, and (iii)
assessing regulatory compliance.

</details>


### [216] [Fine-tuning Done Right in Model Editing](https://arxiv.org/abs/2509.22072)
*Wanli Yang,Fei Sun,Rui Tang,Hongyu Zang,Du Su,Qi Cao,Jingang Wang,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 这篇论文提出了一种新颖的模型编辑微调方法LocFT-BF，恢复了广度优先的优化策略，显著提升了编辑效果，尤其在处理大型语言模型时。


<details>
  <summary>Details</summary>
Motivation: 挑战传统认为微调无法有效用于模型编辑的理念，探索更有效的微调策略。

Method: 采用广度优先的批量优化方法，并引入了新的局部调优策略LocFT-BF进行实验。

Result: 在多种大型语言模型和数据集上，LocFT-BF的表现大幅优于现有最先进方法，特别是在处理100K次编辑和72B参数模型时。

Conclusion: 通过恢复标准的广度优先优化方法，结合局部调优策略LocFT-BF，我们显著提升了大语言模型的编辑效果，开创了模型编辑的新方法。

Abstract: Fine-tuning, a foundational method for adapting large language models, has
long been considered ineffective for model editing. Here, we challenge this
belief, arguing that the reported failure arises not from the inherent
limitation of fine-tuning itself, but from adapting it to the sequential nature
of the editing task, a single-pass depth-first pipeline that optimizes each
sample to convergence before moving on. While intuitive, this depth-first
pipeline coupled with sample-wise updating over-optimizes each edit and induces
interference across edits. Our controlled experiments reveal that simply
restoring fine-tuning to the standard breadth-first (i.e., epoch-based)
pipeline with mini-batch optimization substantially improves its effectiveness
for model editing. Moreover, fine-tuning in editing also suffers from
suboptimal tuning parameter locations inherited from prior methods. Through
systematic analysis of tuning locations, we derive LocFT-BF, a simple and
effective localized editing method built on the restored fine-tuning framework.
Extensive experiments across diverse LLMs and datasets demonstrate that
LocFT-BF outperforms state-of-the-art methods by large margins. Notably, to our
knowledge, it is the first to sustain 100K edits and 72B-parameter models,10 x
beyond prior practice, without sacrificing general capabilities. By clarifying
a long-standing misconception and introducing a principled localized tuning
strategy, we advance fine-tuning from an underestimated baseline to a leading
method for model editing, establishing a solid foundation for future research.

</details>


### [217] [COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning](https://arxiv.org/abs/2509.22075)
*Dmitriy Shopkhoev,Denis Makhov,Magauiya Zhussip,Ammar Ali,Stamatios Lefkimmiatis*

Main category: cs.CL

TL;DR: CoSpaDi是一种新颖的压缩框架，通过结构化稀疏字典学习替代低秩分解，实现高效且准确的LLM压缩，具备优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统的低秩权重近似方法在模型压缩中存在结构约束过于严格，导致准确性下降的问题，因此需要一种更灵活且高效的压缩策略。

Method: 通过采用结构化稀疏因式分解，结合稠密字典和列稀疏系数矩阵，实现了更灵活的模型压缩。

Result: CoSpaDi在20-50%的压缩比下，在多种Llama和Qwen模型的测试中表现出比现有的低秩方法更优的准确性和困惑度。

Conclusion: CoSpaDi作为一种高效的LLM压缩方法，展示了其在准确性和困惑度上的优势，为传统低秩方法提供了强有力的替代方案。

Abstract: Post-training compression of large language models (LLMs) largely relies on
low-rank weight approximation, which represents each column of a weight matrix
in a shared low-dimensional subspace. While this is a computationally efficient
strategy, the imposed structural constraint is rigid and can lead to a
noticeable model accuracy drop. In this work, we propose CoSpaDi (Compression
via Sparse Dictionary Learning), a novel training-free compression framework
that replaces low-rank decomposition with a more flexible structured sparse
factorization in which each weight matrix is represented with a dense
dictionary and a column-sparse coefficient matrix. This formulation enables a
union-of-subspaces representation: different columns of the original weight
matrix are approximated in distinct subspaces spanned by adaptively selected
dictionary atoms, offering greater expressiveness than a single invariant
basis. Crucially, CoSpaDi leverages a small calibration dataset to optimize the
factorization such that the output activations of compressed projection layers
closely match those of the original ones, thereby minimizing functional
reconstruction error rather than mere weight approximation. This data-aware
strategy preserves better model fidelity without any fine-tuning under
reasonable compression ratios. Moreover, the resulting structured sparsity
allows efficient sparse-dense matrix multiplication and is compatible with
post-training quantization for further memory and latency gains. We evaluate
CoSpaDi across multiple Llama and Qwen models under per-layer and per-group
settings at 20-50\% compression ratios, demonstrating consistent superiority
over state-of-the-art data-aware low-rank methods both in accuracy and
perplexity. Our results establish structured sparse dictionary learning as a
powerful alternative to conventional low-rank approaches for efficient LLM
deployment.

</details>


### [218] [Multilingual Dialogue Generation and Localization with Dialogue Act Scripting](https://arxiv.org/abs/2509.22086)
*Justin Vasselli,Eunike Andriani Kardinata,Yusuke Sakai,Taro Watanabe*

Main category: cs.CL

TL;DR: 该研究提出DAS框架，通过结构化对话行为表示生成多语言对话，克服传统翻译方法的不足，评估结果显示DAS生成的对话在多个文化维度上优于机器和人工翻译。


<details>
  <summary>Details</summary>
Motivation: 由于非英语对话数据集稀缺，现有模型常在英语对话的翻译上训练或评估，但这种方法会引入降低自然性和文化适当性的伪影。

Method: 该研究提出了对话行为脚本（DAS）框架，通过结构化的对话行为表示支持多语言对话的编码、定位和生成。

Result: 在人类评估中，DAS生成的意大利语、德语和中文对话在文化相关性、连贯性和情境适切性方面优于机器和人工翻译的对话。

Conclusion: DAS生成的对话在文化相关性、连贯性和情境适切性方面优于机器翻译和人工翻译生成的对话。

Abstract: Non-English dialogue datasets are scarce, and models are often trained or
evaluated on translations of English-language dialogues, an approach which can
introduce artifacts that reduce their naturalness and cultural appropriateness.
This work proposes Dialogue Act Script (DAS), a structured framework for
encoding, localizing, and generating multilingual dialogues from abstract
intent representations. Rather than translating dialogue utterances directly,
DAS enables the generation of new dialogues in the target language that are
culturally and contextually appropriate. By using structured dialogue act
representations, DAS supports flexible localization across languages,
mitigating translationese and enabling more fluent, naturalistic conversations.
Human evaluations across Italian, German, and Chinese show that DAS-generated
dialogues consistently outperform those produced by both machine and human
translators on measures of cultural relevance, coherence, and situational
appropriateness.

</details>


### [219] [S2J: Bridging the Gap Between Solving and Judging Ability in Generative Reward Models](https://arxiv.org/abs/2509.22099)
*Shaoning Sun,Jiachen Yu,Zongqi Wang,Xuewei Yang,Tianle Gu,Yujiu Yang*

Main category: cs.CL

TL;DR: 该论文提出S2J方法以缩小大语言模型中的解决能力与判断能力之间的差距，通过优化策略提升判断性能，且在使用较小训练集的情况下实现了领先性能。


<details>
  <summary>Details</summary>
Motivation: 识别到现有GRM在针对特定查询时，解决能力与判断能力之间存在显著差距。

Method: 提出S2J方法，通过同时利用GRM的解决和判断能力，对模型输出进行监督，优化模型。

Result: S2J方法有效减少了16.2%的解决-判断差距，提高了5.8%的判断性能，并且在同基模型上获得了最新的性能。

Conclusion: S2J方法显著缩小了GRM在解决问题与判断能力之间的差距，提升了判断性能，并在使用更小训练数据集的情况下达到了现有技术的领先表现。

Abstract: With the rapid development of large language models (LLMs), generative reward
models (GRMs) have been widely adopted for reward modeling and evaluation.
Previous studies have primarily focused on training specialized GRMs by
optimizing them on preference datasets with the judgment correctness as
supervision. While it's widely accepted that GRMs with stronger problem-solving
capabilities typically exhibit superior judgment abilities, we first identify a
significant solve-to-judge gap when examining individual queries. Specifically,
the solve-to-judge gap refers to the phenomenon where GRMs struggle to make
correct judgments on some queries (14%-37%), despite being fully capable of
solving them. In this paper, we propose the Solve-to-Judge (S2J) approach to
address this problem. Specifically, S2J simultaneously leverages both the
solving and judging capabilities on a single GRM's output for supervision,
explicitly linking the GRM's problem-solving and evaluation abilities during
model optimization, thereby narrowing the gap. Our comprehensive experiments
demonstrate that S2J effectively reduces the solve-to-judge gap by 16.2%,
thereby enhancing the model's judgment performance by 5.8%. Notably, S2J
achieves state-of-the-art (SOTA) performance among GRMs built on the same base
model while utilizing a significantly smaller training dataset. Moreover, S2J
accomplishes this through self-evolution without relying on more powerful
external models for distillation.

</details>


### [220] [Think Right, Not More: Test-Time Scaling for Numerical Claim Verification](https://arxiv.org/abs/2509.22101)
*Primakov Chungkham,V Venktesh,Vinay Setty,Avishek Anand*

Main category: cs.CL

TL;DR: 本研究通过引入适应性测试时间计算机制，提升了对复杂数字声明的事实检查性能和效率，有效解决了推理漂移问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在处理需要复合推理和数字推理的真实世界声明时的不足，特别是呈现的推理漂移问题。

Method: 训练一个验证模型VERIFIERFC来探索和选择多种推理路径，以解决复杂的事实检查任务。

Result: 使用可扩展的测试时间计算（TTS）机制，VERIFIERFC在复杂数字声明的事实检查性能上提升了18.8%。

Conclusion: 通过引入基于复杂性的自适应机制，VERIFIERFC显著提高了对复杂数字声明的事实检查效率和准确性。

Abstract: Fact-checking real-world claims, particularly numerical claims, is inherently
complex that require multistep reasoning and numerical reasoning for verifying
diverse aspects of the claim. Although large language models (LLMs) including
reasoning models have made tremendous advances, they still fall short on
fact-checking real-world claims that require a combination of compositional and
numerical reasoning. They are unable to understand nuance of numerical aspects,
and are also susceptible to the reasoning drift issue, where the model is
unable to contextualize diverse information resulting in misinterpretation and
backtracking of reasoning process. In this work, we systematically explore
scaling test-time compute (TTS) for LLMs on the task of fact-checking complex
numerical claims, which entails eliciting multiple reasoning paths from an LLM.
We train a verifier model (VERIFIERFC) to navigate this space of possible
reasoning paths and select one that could lead to the correct verdict. We
observe that TTS helps mitigate the reasoning drift issue, leading to
significant performance gains for fact-checking numerical claims. To improve
compute efficiency in TTS, we introduce an adaptive mechanism that performs TTS
selectively based on the perceived complexity of the claim. This approach
achieves 1.8x higher efficiency than standard TTS, while delivering a notable
18.8% performance improvement over single-shot claim verification methods. Our
code and data can be found at https://github.com/VenkteshV/VerifierFC

</details>


### [221] [Universal Legal Article Prediction via Tight Collaboration between Supervised Classification Model and LLM](https://arxiv.org/abs/2509.22119)
*Xiao Chi,Wenlin Zhong,Yiquan Wu,Wei Wang,Kun Kuang,Fei Wu,Minghui Xiong*

Main category: cs.CL

TL;DR: 本文提出一种名为Uni-LAP的普遍法律条款预测框架，通过改进的监督分类模型和大型语言模型的结合，解决了当前法律条款预测中的复杂性和适用性问题。


<details>
  <summary>Details</summary>
Motivation: 旨在解决当前法律条款预测方法在复杂性和适用性上的局限，提供一种通用框架。

Method: 采用了一种新的Top-K损失函数增强监督分类模型，并通过类三段论推理来精炼最终预测。

Result: 实践结果表明，Uni-LAP在多个法域的数据集上表现优于现有方法。

Conclusion: Uni-LAP在多个法域的数据集上获得了优于现有基线模型的表现，展示了其有效性和通用性。

Abstract: Legal Article Prediction (LAP) is a critical task in legal text
classification, leveraging natural language processing (NLP) techniques to
automatically predict relevant legal articles based on the fact descriptions of
cases. As a foundational step in legal decision-making, LAP plays a pivotal
role in determining subsequent judgments, such as charges and penalties.
Despite its importance, existing methods face significant challenges in
addressing the complexities of LAP. Supervised classification models (SCMs),
such as CNN and BERT, struggle to fully capture intricate fact patterns due to
their inherent limitations. Conversely, large language models (LLMs), while
excelling in generative tasks, perform suboptimally in predictive scenarios due
to the abstract and ID-based nature of legal articles. Furthermore, the
diversity of legal systems across jurisdictions exacerbates the issue, as most
approaches are tailored to specific countries and lack broader applicability.
To address these limitations, we propose Uni-LAP, a universal framework for
legal article prediction that integrates the strengths of SCMs and LLMs through
tight collaboration. Specifically, in Uni-LAP, the SCM is enhanced with a novel
Top-K loss function to generate accurate candidate articles, while the LLM
employs syllogism-inspired reasoning to refine the final predictions. We
evaluated Uni-LAP on datasets from multiple jurisdictions, and empirical
results demonstrate that our approach consistently outperforms existing
baselines, showcasing its effectiveness and generalizability.

</details>


### [222] [Multilingual Vision-Language Models, A Survey](https://arxiv.org/abs/2509.22123)
*Andrei-Alexandru Manea,Jindřich Libovický*

Main category: cs.CL

TL;DR: 本研究调查了多语言视觉语言模型的中立性与文化适应性之间的矛盾，发现现有模型和评估方法在多样性和适应性上存在不足。


<details>
  <summary>Details</summary>
Motivation: 探讨多语言视觉语言模型在处理文本和图像时的表现，及其文化适应能力和中立性。

Method: 通过回顾31个模型和21个评估基准，比较它们在跨语言处理中的表现。

Result: 发现现有评估基准多以翻译为基础，强调语义一致性，而文化内容的融入仍有待提高，跨语言能力也存在差异。

Conclusion: 在多语言视觉语言模型中，存在语言中立性与文化意识之间的紧张关系，当前的评估基准和训练目标尚存在差距。

Abstract: This survey examines multilingual vision-language models that process text
and images across languages. We review 31 models and 21 benchmarks, spanning
encoder-only and generative architectures, and identify a key tension between
language neutrality (consistent cross-lingual representations) and cultural
awareness (adaptation to cultural contexts). Current training methods favor
neutrality through contrastive learning, while cultural awareness depends on
diverse data. Two-thirds of evaluation benchmarks use translation-based
approaches prioritizing semantic consistency, though recent work incorporates
culturally grounded content. We find discrepancies in cross-lingual
capabilities and gaps between training objectives and evaluation goals.

</details>


### [223] [FoodSEM: Large Language Model Specialized in Food Named-Entity Linking](https://arxiv.org/abs/2509.22125)
*Ana Gjorgjevikj,Matej Martinc,Gjorgjina Cenikj,Sašo Džeroski,Barbara Koroušić Seljak,Tome Eftimov*

Main category: cs.CL

TL;DR: 本论文提出了FoodSEM，一个针对食品领域的命名实体链接的开源大语言模型，展示了其优越的性能和实用性，并为未来的研究提供了基准。


<details>
  <summary>Details</summary>
Motivation: 食品领域的命名实体链接(NEL)任务无法被现有的通用大语言模型或定制的领域特定模型准确解决。

Method: 通过指令响应场景将文本中的食品相关实体链接到多个本体，包括FoodOn、SNOMED-CT和Hansard分类法。

Result: FoodSEM在一些本体和数据集上实现了最高可达98%的F1得分，优于现有相关模型和系统。

Conclusion: FoodSEM是一个先进的开源大语言模型，专注于食品相关实体链接，表现出卓越的性能和实用性。

Abstract: This paper introduces FoodSEM, a state-of-the-art fine-tuned open-source
large language model (LLM) for named-entity linking (NEL) to food-related
ontologies. To the best of our knowledge, food NEL is a task that cannot be
accurately solved by state-of-the-art general-purpose (large) language models
or custom domain-specific models/systems. Through an instruction-response (IR)
scenario, FoodSEM links food-related entities mentioned in a text to several
ontologies, including FoodOn, SNOMED-CT, and the Hansard taxonomy. The FoodSEM
model achieves state-of-the-art performance compared to related models/systems,
with F1 scores even reaching 98% on some ontologies and datasets. The presented
comparative analyses against zero-shot, one-shot, and few-shot LLM prompting
baselines further highlight FoodSEM's superior performance over its
non-fine-tuned version. By making FoodSEM and its related resources publicly
available, the main contributions of this article include (1) publishing a
food-annotated corpora into an IR format suitable for LLM
fine-tuning/evaluation, (2) publishing a robust model to advance the semantic
understanding of text in the food domain, and (3) providing a strong baseline
on food NEL for future benchmarking.

</details>


### [224] [R-Capsule: Compressing High-Level Plans for Efficient Large Language Model Reasoning](https://arxiv.org/abs/2509.22131)
*Hongyu Shan,Mingyang Song,Chang Dai,Di Liang,Han Chen*

Main category: cs.CL

TL;DR: R-Capsule框架旨在通过结合潜在推理的高效性与显式推理的透明性，提升大型语言模型在复杂推理任务中的表现，减少token使用并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管Chain-of-Thought提示能改善LLM的推理能力，但其冗长性可能导致延迟和内存问题，同时传播早期错误。

Method: R-Capsule框架通过压缩高层次计划为一组小的学习潜在token，并结合低容量瓶颈来鼓励最小性和充分性，同时引入主任务损失和辅助计划重建损失来实现目标。

Result: R-Capsule有效提高了推理效率与解释的透明性，在保持复杂任务准确性的同时，减少了推理过程中的token使用。

Conclusion: R-Capsule框架通过减少推理的可见token数量，同时保持或提高复杂基准的准确性，达成了效率、准确性和可解释性之间的平衡。

Abstract: Chain-of-Thought (CoT) prompting helps Large Language Models (LLMs) tackle
complex reasoning by eliciting explicit step-by-step rationales. However, CoT's
verbosity increases latency and memory usage and may propagate early errors
across long chains. We propose the Reasoning Capsule (R-Capsule), a framework
that aims to combine the efficiency of latent reasoning with the transparency
of explicit CoT. The core idea is to compress the high-level plan into a small
set of learned latent tokens (a Reasoning Capsule) while keeping execution
steps lightweight or explicit. This hybrid approach is inspired by the
Information Bottleneck (IB) principle, where we encourage the capsule to be
approximately minimal yet sufficient for the task. Minimality is encouraged via
a low-capacity bottleneck, which helps improve efficiency. Sufficiency is
encouraged via a dual objective: a primary task loss for answer accuracy and an
auxiliary plan-reconstruction loss that encourages the capsule to faithfully
represent the original textual plan. The reconstruction objective helps ground
the latent space, thereby improving interpretability and reducing the use of
uninformative shortcuts. Our framework strikes a balance between efficiency,
accuracy, and interpretability, thereby reducing the visible token footprint of
reasoning while maintaining or improving accuracy on complex benchmarks. Our
codes are available at:
https://anonymous.4open.science/r/Reasoning-Capsule-7BE0

</details>


### [225] [Bridging Draft Policy Misalignment: Group Tree Optimization for Speculative Decoding](https://arxiv.org/abs/2509.22134)
*Shijing Hu,Jingyang Li,Zhihui Lu,Pan Zhou*

Main category: cs.CL

TL;DR: 本论文提出了一种名为GTO的新方法，通过对齐训练与解码策略，提升了大型语言模型的推理速度和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的训练目标优化仅关注单一路径，而解码过程则采用树状策略，这种不匹配限制了推理速度的提升。

Method: 通过引入Draft Tree Reward和基于组的草拟策略训练，GTO优化了模型的训练过程，使其与解码时的树状策略对齐。

Result: GTO在MT-Bench、HumanEval、GSM8K等任务上提高了接收长度7.4%，并相较于现有最先进的EAGLE-3实现了额外7.7%的速度提升。

Conclusion: GTO提供了一种有效的、通用的解决方案来提高大型语言模型的推理效率。

Abstract: Speculative decoding accelerates large language model (LLM) inference by
letting a lightweight draft model propose multiple tokens that the target model
verifies in parallel. Yet existing training objectives optimize only a single
greedy draft path, while decoding follows a tree policy that re-ranks and
verifies multiple branches. This draft policy misalignment limits achievable
speedups. We introduce Group Tree Optimization (GTO), which aligns training
with the decoding-time tree policy through two components: (i) Draft Tree
Reward, a sampling-free objective equal to the expected acceptance length of
the draft tree under the target model, directly measuring decoding performance;
(ii) Group-based Draft Policy Training, a stable optimization scheme that
contrasts trees from the current and a frozen reference draft model, forming
debiased group-standardized advantages and applying a PPO-style surrogate along
the longest accepted sequence for robust updates. We further prove that
increasing our Draft Tree Reward provably improves acceptance length and
speedup. Across dialogue (MT-Bench), code (HumanEval), and math (GSM8K), and
multiple LLMs (e.g., LLaMA-3.1-8B, LLaMA-3.3-70B, Vicuna-1.3-13B,
DeepSeek-R1-Distill-LLaMA-8B), GTO increases acceptance length by 7.4% and
yields an additional 7.7% speedup over prior state-of-the-art EAGLE-3. By
bridging draft policy misalignment, GTO offers a practical, general solution
for efficient LLM inference.

</details>


### [226] [NFDI4DS Shared Tasks for Scholarly Document Processing](https://arxiv.org/abs/2509.22141)
*Raia Abu Ahmad,Rana Abdulla,Tilahun Abedissa Taffa,Soeren Auer,Hamed Babaei Giglou,Ekaterina Borisova,Zongxiong Chen,Stefan Dietze,Jennifer DSouza,Mayra Elwes,Genet-Asefa Gesese,Shufan Jiang,Ekaterina Kutafina,Philipp Mayr,Georg Rehm,Sameer Sadruddin,Sonja Schimmler,Daniel Schneider,Kanishka Silva,Sharmila Upadhyaya,Ricardo Usbeck*

Main category: cs.CL

TL;DR: 本文概述了德国国家研究数据基础设施下的十二个共享任务，推动了学术文献处理领域的研究和创新。


<details>
  <summary>Details</summary>
Motivation: 提升社区标准化评估，推动研究的可发现性、可获取性、互操作性和可重用性。

Method: 通过开发和托管共享任务，收集了多样的学术文献处理挑战。

Result: 提供开放获取的数据集、模型和工具，推动方法创新并整合进研究数据基础设施。

Conclusion: 共享任务促进了科研标准化评估，并推动了FAIR及可重复研究实践的发展。

Abstract: Shared tasks are powerful tools for advancing research through
community-based standardised evaluation. As such, they play a key role in
promoting findable, accessible, interoperable, and reusable (FAIR), as well as
transparent and reproducible research practices. This paper presents an updated
overview of twelve shared tasks developed and hosted under the German National
Research Data Infrastructure for Data Science and Artificial Intelligence
(NFDI4DS) consortium, covering a diverse set of challenges in scholarly
document processing. Hosted at leading venues, the tasks foster methodological
innovations and contribute open-access datasets, models, and tools for the
broader research community, which are integrated into the consortium's research
data infrastructure.

</details>


### [227] [From Long to Lean: Performance-aware and Adaptive Chain-of-Thought Compression via Multi-round Refinement](https://arxiv.org/abs/2509.22144)
*Jianzhi Yan,Le Liu,Youcheng Pan,Shiwei Chen,Zike Yuan,Yang Xiang,Buzhou Tang*

Main category: cs.CL

TL;DR: MACC是一种自适应链式思维压缩框架，旨在提高复杂任务的推理效率，通过优化压缩深度来减少推理延迟和长度，同时提高准确率。


<details>
  <summary>Details</summary>
Motivation: 链式思维（CoT）推理在复杂任务中提高性能，但由于冗长导致显著推理延迟，亟需改进方法。

Method: 提出了多轮自适应链式思维压缩（MACC）框架，通过多轮精炼逐步压缩链式思维，利用token弹性现象实现优化的压缩深度。

Result: 我们的MACC方法平均提高了5.6%的准确率，同时将CoT长度平均减少了47个token，显著降低了推理延迟。

Conclusion: 我们的MACC方法在提高准确率的同时有效地减少了CoT的长度和推理延迟，并且可以通过可解释特征进行有效的模型选择和预测。

Abstract: Chain-of-Thought (CoT) reasoning improves performance on complex tasks but
introduces significant inference latency due to verbosity. We propose
Multiround Adaptive Chain-of-Thought Compression (MACC), a framework that
leverages the token elasticity phenomenon--where overly small token budgets can
paradoxically increase output length--to progressively compress CoTs via
multiround refinement. This adaptive strategy allows MACC to determine the
optimal compression depth for each input. Our method achieves an average
accuracy improvement of 5.6 percent over state-of-the-art baselines, while also
reducing CoT length by an average of 47 tokens and significantly lowering
latency. Furthermore, we show that test-time performance--accuracy and token
length--can be reliably predicted using interpretable features like perplexity
and compression rate on the training set. Evaluated across different models,
our method enables efficient model selection and forecasting without repeated
fine-tuning, demonstrating that CoT compression is both effective and
predictable. Our code will be released in https://github.com/Leon221220/MACC.

</details>


### [228] [Mixture of Detectors: A Compact View of Machine-Generated Text Detection](https://arxiv.org/abs/2509.22147)
*Sai Teja Lekkala,Yadagiri Annepaka,Arun Kumar Challa,Samatha Reddy Machireddy,Partha Pakray,Chukhu Chunka*

Main category: cs.CL

TL;DR: 该研究探讨大型语言模型对人类创造力的影响，提出BMAS English数据集，并发展了机器生成文本检测的多种方法。


<details>
  <summary>Details</summary>
Motivation: 探讨人类工作真实性和创造力的保留，回应大型语言模型对人类创造力的挑战。

Method: 通过提出BMAS English数据集，进行文本的二分类和多分类，句子级别分割，以及针对机器生成文本的对抗攻击的检测。

Result: 引入BMAS English数据集，增强对机器生成文本的检测能力，并探讨了不同场景下的文本检测方法。

Conclusion: 本研究将为机器生成文本检测 (MGTD) 的相关工作提供更有意义的贡献。

Abstract: Large Language Models (LLMs) are gearing up to surpass human creativity. The
veracity of the statement needs careful consideration. In recent developments,
critical questions arise regarding the authenticity of human work and the
preservation of their creativity and innovative abilities. This paper
investigates such issues. This paper addresses machine-generated text detection
across several scenarios, including document-level binary and multiclass
classification or generator attribution, sentence-level segmentation to
differentiate between human-AI collaborative text, and adversarial attacks
aimed at reducing the detectability of machine-generated text. We introduce a
new work called BMAS English: an English language dataset for binary
classification of human and machine text, for multiclass classification, which
not only identifies machine-generated text but can also try to determine its
generator, and Adversarial attack addressing where it is a common act for the
mitigation of detection, and Sentence-level segmentation, for predicting the
boundaries between human and machine-generated text. We believe that this paper
will address previous work in Machine-Generated Text Detection (MGTD) in a more
meaningful way.

</details>


### [229] [Context Parametrization with Compositional Adapters](https://arxiv.org/abs/2509.22158)
*Josip Jukić,Martin Tutek,Jan Šnajder*

Main category: cs.CL

TL;DR: 本文提出CompAs，一个新颖的框架，通过将上下文信息转化为组合适配器参数，克服了现有方法在处理多示例时的效率和灵活性问题。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在处理多示例时的低效问题以及微调过程中的灵活性损失。

Method: 提出一种元学习框架CompAs，将上下文信息转化为具有组合结构的适配器参数。

Result: CompAs在多个选择题和抽取式问答任务中的表现超越了传统的ICL和生成器基方法，特别是在输入数量增加时。

Conclusion: CompAs作为一个可组合的适配器生成框架，在多任务学习中展示了其高效性和实用性，尤其是在处理长输入时具有明显优势。

Abstract: Large language models (LLMs) often seamlessly adapt to new tasks through
in-context learning (ICL) or supervised fine-tuning (SFT). However, both of
these approaches face key limitations: ICL is inefficient when handling many
demonstrations, and SFT incurs training overhead while sacrificing flexibility.
Mapping instructions or demonstrations from context directly into adapter
parameters offers an appealing alternative. While prior work explored
generating adapters based on a single input context, it has overlooked the need
to integrate multiple chunks of information. To address this gap, we introduce
CompAs, a meta-learning framework that translates context into adapter
parameters with a compositional structure. Adapters generated this way can be
merged algebraically, enabling instructions, demonstrations, or retrieved
passages to be seamlessly combined without reprocessing long prompts.
Critically, this approach yields three benefits: lower inference cost,
robustness to long-context instability, and establishes a principled solution
when input exceeds the model's context window. Furthermore, CompAs encodes
information into adapter parameters in a reversible manner, enabling recovery
of input context through a decoder, facilitating safety and security. Empirical
results on diverse multiple-choice and extractive question answering tasks show
that CompAs outperforms ICL and prior generator-based methods, especially when
scaling to more inputs. Our work establishes composable adapter generation as a
practical and efficient alternative for scaling LLM deployment.

</details>


### [230] [When Does Reasoning Matter? A Controlled Study of Reasoning's Contribution to Model Performance](https://arxiv.org/abs/2509.22193)
*Nicolas Boizard,Hippolyte Gisserot-Boukhlef,Kevin El-Haddad,Céline Hudelot,Pierre Colombo*

Main category: cs.CL

TL;DR: 本研究探讨了推理模型在各类任务中的有效性，发现其在解决推理密集型和开放式任务时，优于较大的IFT系统，尤其在模型规模扩大时更为显著。


<details>
  <summary>Details</summary>
Motivation: 探讨推理能力的有效性、任务和模型规模，以及训练和推理成本。

Method: 采用合成数据蒸馏框架，进行大规模监督研究，比较不同规模的指令微调和推理模型。

Result: 推理能力在多个任务上持续提高模型性能，通常可与显著更大的IFT系统相媲美或超越。

Conclusion: 在推理密集型和开放式任务上，推理模型随着模型规模的扩大，能够克服IFT的性能限制。

Abstract: Large Language Models (LLMs) with reasoning capabilities have achieved
state-of-the-art performance on a wide range of tasks. Despite its empirical
success, the tasks and model scales at which reasoning becomes effective, as
well as its training and inference costs, remain underexplored. In this work,
we rely on a synthetic data distillation framework to conduct a large-scale
supervised study. We compare Instruction Fine-Tuning (IFT) and reasoning models
of varying sizes, on a wide range of math-centric and general-purpose tasks,
evaluating both multiple-choice and open-ended formats. Our analysis reveals
that reasoning consistently improves model performance, often matching or
surpassing significantly larger IFT systems. Notably, while IFT remains
Pareto-optimal in training and inference costs, reasoning models become
increasingly valuable as model size scales, overcoming IFT performance limits
on reasoning-intensive and open-ended tasks.

</details>


### [231] [The Outputs of Large Language Models are Meaningless](https://arxiv.org/abs/2509.22206)
*Anandi Hattiangadi,Anders J. Schoubye*

Main category: cs.CL

TL;DR: 本文论证大语言模型的输出没有字面意义，因为它们缺乏必要的意图，但其输出仍然显得有意义。


<details>
  <summary>Details</summary>
Motivation: 探讨为何大语言模型的输出被认为是无意义的。

Method: 通过提出两个关键前提进行论证：一种意图是LLMs的输出具有字面意义所必需的，且LLMs无法 plausibly 的拥有正确的意图。

Result: 提出了一个简单的论据，认为大语言模型的输出并无字面意义，讨论了多种反应对此论证的辩护。

Conclusion: 虽然我们的论点是合理的，但大语言模型（LLMs）的输出似乎依然是有意义的，并且可以用来获取真实的信念和知识。

Abstract: In this paper, we offer a simple argument for the conclusion that the outputs
of large language models (LLMs) are meaningless. Our argument is based on two
key premises: (a) that certain kinds of intentions are needed in order for
LLMs' outputs to have literal meanings, and (b) that LLMs cannot plausibly have
the right kinds of intentions. We defend this argument from various types of
responses, for example, the semantic externalist argument that deference can be
assumed to take the place of intentions and the semantic internalist argument
that meanings can be defined purely in terms of intrinsic relations between
concepts, such as conceptual roles. We conclude the paper by discussing why,
even if our argument is sound, the outputs of LLMs nevertheless seem meaningful
and can be used to acquire true beliefs and even knowledge.

</details>


### [232] [Question-Driven Analysis and Synthesis: Building Interpretable Thematic Trees with LLMs for Text Clustering and Controllable Generation](https://arxiv.org/abs/2509.22211)
*Tiago Fernandes Tavares*

Main category: cs.CL

TL;DR: RTP是一种新颖的文本分析框架，利用大语言模型提高主题可解释性，并在下游任务中提供有效特征。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺领域，传统主题模型缺乏可解释性，难以有效分析文本数据。

Method: 引入递归主题划分（RTP），通过交互式构建二叉树，在每个节点上设置自然语言问题，实现数据的语义划分。

Result: RTP的问答驱动层次结构比强基线如BERTopic的关键词主题更具可解释性，且在下游分类任务中表现出色。

Conclusion: RTP框架提供了一种可解释的主题分析方法，利用大语言模型的能力，改进了文本语料的理解与可操作性。

Abstract: Unsupervised analysis of text corpora is challenging, especially in
data-scarce domains where traditional topic models struggle. While these models
offer a solution, they typically describe clusters with lists of keywords that
require significant manual effort to interpret and often lack semantic
coherence. To address this critical interpretability gap, we introduce
Recursive Thematic Partitioning (RTP), a novel framework that leverages Large
Language Models (LLMs) to interactively build a binary tree. Each node in the
tree is a natural language question that semantically partitions the data,
resulting in a fully interpretable taxonomy where the logic of each cluster is
explicit. Our experiments demonstrate that RTP's question-driven hierarchy is
more interpretable than the keyword-based topics from a strong baseline like
BERTopic. Furthermore, we establish the quantitative utility of these clusters
by showing they serve as powerful features in downstream classification tasks,
particularly when the data's underlying themes correlate with the task labels.
RTP introduces a new paradigm for data exploration, shifting the focus from
statistical pattern discovery to knowledge-driven thematic analysis.
Furthermore, we demonstrate that the thematic paths from the RTP tree can serve
as structured, controllable prompts for generative models. This transforms our
analytical framework into a powerful tool for synthesis, enabling the
consistent imitation of specific characteristics discovered in the source
corpus.

</details>


### [233] [StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient SpeechLLMs](https://arxiv.org/abs/2509.22220)
*Yuhan Song,Linhao Zhang,Chuhan Wu,Aiwei Liu,Wei Jia,Houfeng Wang,Xiao Zhou*

Main category: cs.CL

TL;DR: 提出StableToken，解决了现有tokenizer在噪音条件下的不稳定性，显著提升了语音模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的语义语音tokenizer对声学扰动的稳定性差，导致下游任务学习负担加重。

Method: 采用多分支架构并通过位投票机制合并处理表示，生成稳定的token序列。

Result: StableToken在不同噪音条件下显著降低了单位编辑距离(UED)，并提高了SpeechLLMs在多种任务上的鲁棒性。

Conclusion: StableToken通过共识驱动机制实现了token的稳定性，显著提高了语音模型的鲁棒性。

Abstract: Prevalent semantic speech tokenizers, designed to capture linguistic content,
are surprisingly fragile. We find they are not robust to meaning-irrelevant
acoustic perturbations; even at high Signal-to-Noise Ratios (SNRs) where speech
is perfectly intelligible, their output token sequences can change drastically,
increasing the learning burden for downstream LLMs. This instability stems from
two flaws: a brittle single-path quantization architecture and a distant
training signal indifferent to intermediate token stability. To address this,
we introduce StableToken, a tokenizer that achieves stability through a
consensus-driven mechanism. Its multi-branch architecture processes audio in
parallel, and these representations are merged via a powerful bit-wise voting
mechanism to form a single, stable token sequence. StableToken sets a new
state-of-the-art in token stability, drastically reducing Unit Edit Distance
(UED) under diverse noise conditions. This foundational stability translates
directly to downstream benefits, significantly improving the robustness of
SpeechLLMs on a variety of tasks.

</details>


### [234] [Thinking in Many Modes: How Composite Reasoning Elevates Large Language Model Performance with Limited Data](https://arxiv.org/abs/2509.22224)
*Zishan Ahmad,Saisubramaniam Gopalakrishnan*

Main category: cs.CL

TL;DR: 本文提出一种新方法复合推理，改善LLM在复杂问题上的推理能力，通过动态结合多种推理风格，显著提升科学和医学问题解决效率。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLM）在复杂问题上依赖单一推理范式的局限性，促进多样化认知策略的应用。

Method: 引入复合推理（CR）方法，使LLM能够动态探索和结合多种推理风格，如演绎推理、归纳推理和溯因推理，以实现更细致的问题解决。

Result: 在科学和医学问答基准测试中，我们的方法超越了现有的基准，如思维链（CoT）和深度搜索（SR）推理能力，展现出更高的样本效率和适当的令牌使用。

Conclusion: 通过培养内部推理风格的多样性，LLM能够获得更强大、自适应和高效的问题解决能力。

Abstract: Large Language Models (LLMs), despite their remarkable capabilities, rely on
singular, pre-dominant reasoning paradigms, hindering their performance on
intricate problems that demand diverse cognitive strategies. To address this,
we introduce Composite Reasoning (CR), a novel reasoning approach empowering
LLMs to dynamically explore and combine multiple reasoning styles like
deductive, inductive, and abductive for more nuanced problem-solving. Evaluated
on scientific and medical question-answering benchmarks, our approach
outperforms existing baselines like Chain-of-Thought (CoT) and also surpasses
the accuracy of DeepSeek-R1 style reasoning (SR) capabilities, while
demonstrating superior sample efficiency and adequate token usage. Notably, CR
adaptively emphasizes domain-appropriate reasoning styles. It prioritizes
abductive and deductive reasoning for medical question answering, but shifts to
causal, deductive, and inductive methods for scientific reasoning. Our findings
highlight that by cultivating internal reasoning style diversity, LLMs acquire
more robust, adaptive, and efficient problem-solving abilities.

</details>


### [235] [In Their Own Words: Reasoning Traces Tailored for Small Models Make Them Better Reasoners](https://arxiv.org/abs/2509.22230)
*Jaehoon Kim,Kwangwook Seo,Dongha Lee*

Main category: cs.CL

TL;DR: 本研究揭示了小型模型在推理能力转移中遭遇的分布不一致问题，并通过RSD机制成功改善了小型模型的性能。


<details>
  <summary>Details</summary>
Motivation: 研究发现，尽管有高质量教师示范，小模型在监督微调中常常出现性能下降，原因是分布不一致。

Method: 通过引入RSD机制，教师模型提出候选标记，小型学生模型根据自己的概率分布决定接受，过滤低概率标记。

Result: 采用RSD生成的推理轨迹，模型在多个主要推理基准测试中恢复了4.9%的性能，而直接蒸馏的性能则下降20.5%。

Conclusion: 逆向推测解码（RSD）机制显著提高了小型语言模型的推理能力转移，克服了低概率标记导致的学习障碍。

Abstract: Transferring reasoning capabilities from larger language models to smaller
ones through supervised fine-tuning often fails counterintuitively, with
performance degrading despite access to high-quality teacher demonstrations. We
identify that this failure stems from distributional misalignment: reasoning
traces from larger models contain tokens that are low probability under the
student's distribution, exceeding the internal representation capacity of
smaller architectures and creating learning barriers rather than helpful
guidance. We propose Reverse Speculative Decoding (RSD), a mechanism for
generating student-friendly reasoning traces in which the teacher model
proposes candidate tokens but the student model determines acceptance based on
its own probability distributions, filtering low probability tokens. When
applied to Qwen3-0.6B, direct distillation of s1K-1.1 reasoning trace data
degrades average performance across major reasoning benchmarks by 20.5\%, while
the same model trained on RSD-generated reasoning traces achieves meaningful
improvements of 4.9\%. Our analysis reveals that low probability tokens
constitute the critical bottleneck in reasoning ability transfer. However,
cross-model experiments demonstrate that RSD traces are model-specific rather
than universally applicable, indicating that distributional alignment must be
tailored for each student architecture's unique internal representation.

</details>


### [236] [FeatBench: Evaluating Coding Agents on Feature Implementation for Vibe Coding](https://arxiv.org/abs/2509.22237)
*Haorui Chen,Chengze Li,Jia Li*

Main category: cs.CL

TL;DR: 本论文提出了FeatBench，一个针对vibe编码中特性实现的新基准，以弥补现有评估标准的不足，并展示了这一领域的重大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的代码生成评估基准不足以评估vibe编码能力，特别是在特性实现的关键场景中。

Method: 通过纯自然语言提示，建立了一种多层过滤的数据收集流程，并评估多个先进的代理框架。

Result: 在FeatBench上评估四个领先的LLM，发现特性实现的成功率最高仅为29.94%。

Conclusion: FeatBench是一个新颖的基准测试，专注于在vibe编码范式下的特性实现，显示出该领域的显著挑战。

Abstract: The rapid advancement of Large Language Models (LLMs) has given rise to a
novel software development paradigm known as "vibe coding," where users
interact with coding agents through high-level natural language. However,
existing evaluation benchmarks for code generation inadequately assess an
agent's vibe coding capabilities. Existing benchmarks are misaligned, as they
either require code-level specifications or focus narrowly on issue-solving,
neglecting the critical scenario of feature implementation within the vibe
coding paradiam. To address this gap, we propose FeatBench, a novel benchmark
for vibe coding that focuses on feature implementation. Our benchmark is
distinguished by several key features: 1. Pure Natural Language Prompts. Task
inputs consist solely of abstract natural language descriptions, devoid of any
code or structural hints. 2. A Rigorous & Evolving Data Collection Process.
FeatBench is built on a multi-level filtering pipeline to ensure quality and a
fully automated pipeline to evolve the benchmark, mitigating data
contamination. 3. Comprehensive Test Cases. Each task includes Fail-to-Pass
(F2P) and Pass-to-Pass (P2P) tests to verify correctness and prevent
regressions. 4. Diverse Application Domains. The benchmark includes
repositories from diverse domains to ensure it reflects real-world scenarios.
We evaluate two state-of-the-art agent frameworks with four leading LLMs on
FeatBench. Our evaluation reveals that feature implementation within the vibe
coding paradigm is a significant challenge, with the highest success rate of
only 29.94%. Our analysis also reveals a tendency for "aggressive
implementation," a strategy that paradoxically leads to both critical failures
and superior software design. We release FeatBench, our automated collection
pipeline, and all experimental results to facilitate further community
research.

</details>


### [237] [FLEXI: Benchmarking Full-duplex Human-LLM Speech Interaction](https://arxiv.org/abs/2509.22243)
*Yuan Ge,Saihan Chen,Jingqi Xiao,Xiaoqian Liu,Tong Xiao,Yan Xiang,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: FLEXI基准为全双工人机语音交互提供评估框架，揭示了开源与商业模型之间的区别，并提出token-pair预测可能实现更自然的交互。


<details>
  <summary>Details</summary>
Motivation: 全双工语音对语音大语言模型是人机交互的基础，但在基准测试和建模上存在挑战。

Method: 通过六种不同的人类与LLM的交互场景系统评估延迟、质量和对话有效性。

Result: FLEXI是首个为全双工LLM与人类语音交互提供的基准，发现开源模型与商业模型在紧急意识、终止轮次和交互延迟等方面存在显著差距。

Conclusion: 下一步的token-pair预测为实现真正无缝的人类双工交互提供了有希望的路径。

Abstract: Full-Duplex Speech-to-Speech Large Language Models (LLMs) are foundational to
natural human-computer interaction, enabling real-time spoken dialogue systems.
However, benchmarking and modeling these models remains a fundamental
challenge. We introduce FLEXI, the first benchmark for full-duplex LLM-human
spoken interaction that explicitly incorporates model interruption in emergency
scenarios. FLEXI systematically evaluates the latency, quality, and
conversational effectiveness of real-time dialogue through six diverse
human-LLM interaction scenarios, revealing significant gaps between open source
and commercial models in emergency awareness, turn terminating, and interaction
latency. Finally, we suggest that next token-pair prediction offers a promising
path toward achieving truly seamless and human-like full-duplex interaction.

</details>


### [238] [Safety Compliance: Rethinking LLM Safety Reasoning through the Lens of Compliance](https://arxiv.org/abs/2509.22250)
*Wenbin Hu,Huihao Jing,Haochen Shi,Haoran Li,Yangqiu Song*

Main category: cs.CL

TL;DR: 本研究从法律合规的角度解决大型语言模型（LLM）的安全问题，提出Compliance Reasoner，通过新的安全合规基准验证其在安全性方面的有效性，显著提高了与法律标准的对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的安全方法依赖于临时分类，缺乏严谨的系统保护，无法确保现代大型语言模型系统的安全。

Method: 通过Group Policy Optimization（GRPO）对Qwen3-8B进行对齐，构建出Compliance Reasoner。

Result: Compliance Reasoner在新的安全合规基准上平均提升了+10.45%的EU AI法案和+11.85%的GDPR的表现。

Conclusion: Compliance Reasoner在新的安全合规基准上表现优越，有效地将大型语言模型与法律标准对齐，从而减轻安全风险。

Abstract: The proliferation of Large Language Models (LLMs) has demonstrated remarkable
capabilities, elevating the critical importance of LLM safety. However,
existing safety methods rely on ad-hoc taxonomy and lack a rigorous, systematic
protection, failing to ensure safety for the nuanced and complex behaviors of
modern LLM systems. To address this problem, we solve LLM safety from legal
compliance perspectives, named safety compliance. In this work, we posit
relevant established legal frameworks as safety standards for defining and
measuring safety compliance, including the EU AI Act and GDPR, which serve as
core legal frameworks for AI safety and data security in Europe. To bridge the
gap between LLM safety and legal compliance, we first develop a new benchmark
for safety compliance by generating realistic LLM safety scenarios seeded with
legal statutes. Subsequently, we align Qwen3-8B using Group Policy Optimization
(GRPO) to construct a safety reasoner, Compliance Reasoner, which effectively
aligns LLMs with legal standards to mitigate safety risks. Our comprehensive
experiments demonstrate that the Compliance Reasoner achieves superior
performance on the new benchmark, with average improvements of +10.45% for the
EU AI Act and +11.85% for GDPR.

</details>


### [239] [Beyond Textual Context: Structural Graph Encoding with Adaptive Space Alignment to alleviate the hallucination of LLMs](https://arxiv.org/abs/2509.22251)
*Yifang Zhang,Pengfei Duan,Yiwen Yang,Shengwu Xiong*

Main category: cs.CL

TL;DR: 本研究提出SSKG-LLM模型，通过整合知识图谱的结构与语义信息，提升了大语言模型的事实推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型对知识图谱结构信息利用不足和嵌入空间不匹配的问题。

Method: 提出SSKG-LLM模型架构，采用知识图谱检索、知识图谱编码和知识图谱适配模块，促进结构与语义信息的结合。

Result: SSKG-LLM的实验结果表明，整合知识图谱的结构信息后，LLMs的推理能力显著增强。

Conclusion: SSKG-LLM有效整合了知识图谱的结构和语义信息，提高了大语言模型的事实推理能力。

Abstract: Currently, the main approach for Large Language Models (LLMs) to tackle the
hallucination issue is incorporating Knowledge Graphs(KGs).However, LLMs
typically treat KGs as plain text, extracting only semantic information and
limiting their use of the crucial structural aspects of KGs. Another challenge
is the gap between the embedding spaces of KGs encoders and LLMs text
embeddings, which hinders the effective integration of structured knowledge. To
overcome these obstacles, we put forward the SSKG-LLM, an innovative model
architecture that is designed to efficiently integrate both the Structural and
Semantic information of KGs into the reasoning processes of LLMs. SSKG-LLM
incorporates the Knowledge Graph Retrieval (KGR) module and the Knowledge Graph
Encoding (KGE) module to preserve semantics while utilizing structure. Then,
the Knowledge Graph Adaptation (KGA) module is incorporated to enable LLMs to
understand KGs embeddings. We conduct extensive experiments and provide a
detailed analysis to explore how incorporating the structural information of
KGs can enhance the factual reasoning abilities of LLMs. Our code are available
at https://github.com/yfangZhang/SSKG-LLM.

</details>


### [240] [Bridging Fairness and Explainability: Can Input-Based Explanations Promote Fairness in Hate Speech Detection?](https://arxiv.org/abs/2509.22291)
*Yifan Wang,Mayank Jobanputra,Ji-Ung Lee,Soyoung Oh,Isabel Valera,Vera Demberg*

Main category: cs.CL

TL;DR: 本研究系统分析了可解释性与仇恨言论检测中的公平性关系，发现输入基础的解释能帮助检测和减少偏见，但不适合选择公平模型。


<details>
  <summary>Details</summary>
Motivation: 自然语言处理模型容易复制或放大训练数据中的社会偏见，这引发了对公平性的关注。

Method: 对仇恨言论检测中的可解释性与公平性关系进行系统研究，分析编码器和解码器模型。

Result: 发现输入基础的解释在检测偏见预测方面有效，并能作为训练中的有用监督，但在选择公平模型上存在不 reliability。

Conclusion: 输入基础的解释可以有效检测偏见预测，并在训练中减少偏见，但在选择公平模型时不够可靠。

Abstract: Natural language processing (NLP) models often replicate or amplify social
bias from training data, raising concerns about fairness. At the same time,
their black-box nature makes it difficult for users to recognize biased
predictions and for developers to effectively mitigate them. While some studies
suggest that input-based explanations can help detect and mitigate bias, others
question their reliability in ensuring fairness. Existing research on
explainability in fair NLP has been predominantly qualitative, with limited
large-scale quantitative analysis. In this work, we conduct the first
systematic study of the relationship between explainability and fairness in
hate speech detection, focusing on both encoder- and decoder-only models. We
examine three key dimensions: (1) identifying biased predictions, (2) selecting
fair models, and (3) mitigating bias during model training. Our findings show
that input-based explanations can effectively detect biased predictions and
serve as useful supervision for reducing bias during training, but they are
unreliable for selecting fair models among candidates.

</details>


### [241] [Advancing Natural Language Formalization to First Order Logic with Fine-tuned LLMs](https://arxiv.org/abs/2509.22338)
*Felix Vossel,Till Mossakowski,Björn Gehrke*

Main category: cs.CL

TL;DR: 本研究评估了微调的语言模型在自动翻译自然语言到一阶逻辑方面的表现，发现结构逻辑翻译效果良好，但谓词提取仍然是一个挑战。


<details>
  <summary>Details</summary>
Motivation: 自动化自然语言转换为一阶逻辑对于知识表示和形式方法至关重要，但仍然具有挑战性。

Method: 通过比较多种模型架构和训练策略，评估微调的语言模型在自然语言到一阶逻辑的翻译任务上的表现。

Result: 微调的Flan-T5-XXL在含有谓词列表的情况下达到了70%的准确率，优于GPT-4o和DeepSeek-R1-0528模型，且在FOLIO数据集上能泛化到未见的逻辑论证。

Conclusion: 结构逻辑翻译表现出色，但谓词提取是主要瓶颈。

Abstract: Automating the translation of natural language to first-order logic (FOL) is
crucial for knowledge representation and formal methods, yet remains
challenging. We present a systematic evaluation of fine-tuned LLMs for this
task, comparing architectures (encoder-decoder vs. decoder-only) and training
strategies. Using the MALLS and Willow datasets, we explore techniques like
vocabulary extension, predicate conditioning, and multilingual training,
introducing metrics for exact match, logical equivalence, and predicate
alignment. Our fine-tuned Flan-T5-XXL achieves 70% accuracy with predicate
lists, outperforming GPT-4o and even the DeepSeek-R1-0528 model with CoT
reasoning ability as well as symbolic systems like ccg2lambda. Key findings
show: (1) predicate availability boosts performance by 15-20%, (2) T5 models
surpass larger decoder-only LLMs, and (3) models generalize to unseen logical
arguments (FOLIO dataset) without specific training. While structural logic
translation proves robust, predicate extraction emerges as the main bottleneck.

</details>


### [242] [Transformers Can Learn Connectivity in Some Graphs but Not Others](https://arxiv.org/abs/2509.22343)
*Amit Roy,Abulhair Saparov*

Main category: cs.CL

TL;DR: 本研究探讨了变压器在推断传递关系中的表现，发现其在高维网格图中能力较弱，而增加模型规模有助于提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究变压器在推断传递关系方面的能力，以确保基于大语言模型的响应的事实正确性。

Method: 通过生成有向图训练不同规模的变压器模型，以评估其推断传递关系的能力。

Result: 变压器在高维网格图中推断连通性的能力较弱，而在低维网格图中表现更佳，模型规模的增加能够提高推断的泛化能力。

Conclusion: 变压器在推断传递关系的能力上存在差异，尤其是在稀疏图和高维网格图中表现不佳。

Abstract: Reasoning capability is essential to ensure the factual correctness of the
responses of transformer-based Large Language Models (LLMs), and robust
reasoning about transitive relations is instrumental in many settings, such as
causal inference. Hence, it is essential to investigate the capability of
transformers in the task of inferring transitive relations (e.g., knowing A
causes B and B causes C, then A causes C). The task of inferring transitive
relations is equivalent to the task of connectivity in directed graphs (e.g.,
knowing there is a path from A to B, and there is a path from B to C, then
there is a path from A to C). Past research focused on whether transformers can
learn to infer transitivity from in-context examples provided in the input
prompt. However, transformers' capability to infer transitive relations from
training examples and how scaling affects the ability is unexplored. In this
study, we seek to answer this question by generating directed graphs to train
transformer models of varying sizes and evaluate their ability to infer
transitive relations for various graph sizes. Our findings suggest that
transformers are capable of learning connectivity on "grid-like'' directed
graphs where each node can be embedded in a low-dimensional subspace, and
connectivity is easily inferable from the embeddings of the nodes. We find that
the dimensionality of the underlying grid graph is a strong predictor of
transformers' ability to learn the connectivity task, where higher-dimensional
grid graphs pose a greater challenge than low-dimensional grid graphs. In
addition, we observe that increasing the model scale leads to increasingly
better generalization to infer connectivity over grid graphs. However, if the
graph is not a grid graph and contains many disconnected components,
transformers struggle to learn the connectivity task, especially when the
number of components is large.

</details>


### [243] [The InviTE Corpus: Annotating Invectives in Tudor English Texts for Computational Modeling](https://arxiv.org/abs/2509.22345)
*Sophie Spliethoff,Sanne Hoeken,Silke Schwandt,Sina Zarrieß,Özge Alaçam*

Main category: cs.CL

TL;DR: 本论文利用自然语言处理技术研究16世纪英格兰的宗教攻击性语言，创建了InviTE语料库，并评估了不同模型的性能，发现微调模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 应用自然语言处理技术于历史研究，特别是研究英格兰都铎王朝时期的宗教攻击性语言。

Method: 通过从原始数据到预处理和数据选择，再到迭代注释过程，构建了InviTE语料库，并评估了微调的BERT模型和零样本大语言模型的性能。

Result: 创建了包含近2000个早期现代英语句子的InviTE语料库，并进行了相关模型的性能比较。

Conclusion: 预训练于历史数据并针对攻击性语言检测微调的模型表现优越。

Abstract: In this paper, we aim at the application of Natural Language Processing (NLP)
techniques to historical research endeavors, particularly addressing the study
of religious invectives in the context of the Protestant Reformation in Tudor
England. We outline a workflow spanning from raw data, through pre-processing
and data selection, to an iterative annotation process. As a result, we
introduce the InviTE corpus -- a corpus of almost 2000 Early Modern English
(EModE) sentences, which are enriched with expert annotations regarding
invective language throughout 16th-century England. Subsequently, we assess and
compare the performance of fine-tuned BERT-based models and zero-shot prompted
instruction-tuned large language models (LLMs), which highlights the
superiority of models pre-trained on historical data and fine-tuned to
invective detection.

</details>


### [244] [Conversational Implicatures: Modelling Relevance Theory Probabilistically](https://arxiv.org/abs/2509.22354)
*Christoph Unger,Hendrik Buschmeier*

Main category: cs.CL

TL;DR: 本论文探讨了如何使用贝叶斯概率理论来解释相关性理论中的隐含意义沟通，推动了语用学的理论发展。


<details>
  <summary>Details</summary>
Motivation: 受到贝叶斯概率理论在语用学和语义学领域的影响，旨在通过新的计算工具和方法提升对隐含意义的理解。

Method: 通过研究隐含意义传达中的对话含义，采用贝叶斯概率理论作为分析框架。

Result: 提出了一个框架，用于将贝叶斯方法应用于传统的相关性理论，以分析和理解言语交流中的隐含含义。

Conclusion: 这篇论文探讨了如何将贝叶斯方法应用于相关性理论的语用学，特别是隐含意义的沟通和会话含义。

Abstract: Recent advances in Bayesian probability theory and its application to
cognitive science in combination with the development of a new generation of
computational tools and methods for probabilistic computation have led to a
'probabilistic turn' in pragmatics and semantics. In particular, the framework
of Rational Speech Act theory has been developed to model broadly Gricean
accounts of pragmatic phenomena in Bayesian terms, starting with fairly simple
reference games and covering ever more complex communicative exchanges such as
verbal syllogistic reasoning. This paper explores in which way a similar
Bayesian approach might be applied to relevance-theoretic pragmatics (Sperber &
Wilson, 1995) by study a paradigmatic pragmatic phenomenon: the communication
of implicit meaning by ways of (conversational) implicatures.

</details>


### [245] [CHRONOBERG: Capturing Language Evolution and Temporal Awareness in Foundation Models](https://arxiv.org/abs/2509.22360)
*Niharika Hegde,Subarnaduti Paul,Lars Joel-Frey,Manuel Brack,Kristian Kersting,Martin Mundt,Patrick Schramowski*

Main category: cs.CL

TL;DR: 本研究介绍了CHRONOBERG，一个包含250年英语书籍的时间结构语料库，旨在改进语言模型对语义变化的识别能力，尤其是在不同时期的情感和歧视性语言的上下文中。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型在处理语言的时间性变化和情感变化时存在局限性，需要建立一个结构化的语料库来解决这一问题。

Method: 通过对Project Gutenberg中英语书籍的编辑和时间标注，进行基于VAD模型的语义变化分析，构建历史校准的情感词典。

Result: 构建了CHRONOBERG语料库，展示了现代语言模型如何在捕捉语义变化方面表现不佳，并强调了时间感知训练的重要性。

Conclusion: CHRONOBERG是一个具有时间结构的英语书籍语料库，能够支持语言变化和情感时效性的研究，但现有语言模型在捕捉语义变化方面仍需改进。

Abstract: Large language models (LLMs) excel at operating at scale by leveraging social
media and various data crawled from the web. Whereas existing corpora are
diverse, their frequent lack of long-term temporal structure may however limit
an LLM's ability to contextualize semantic and normative evolution of language
and to capture diachronic variation. To support analysis and training for the
latter, we introduce CHRONOBERG, a temporally structured corpus of English book
texts spanning 250 years, curated from Project Gutenberg and enriched with a
variety of temporal annotations. First, the edited nature of books enables us
to quantify lexical semantic change through time-sensitive
Valence-Arousal-Dominance (VAD) analysis and to construct historically
calibrated affective lexicons to support temporally grounded interpretation.
With the lexicons at hand, we demonstrate a need for modern LLM-based tools to
better situate their detection of discriminatory language and contextualization
of sentiment across various time-periods. In fact, we show how language models
trained sequentially on CHRONOBERG struggle to encode diachronic shifts in
meaning, emphasizing the need for temporally aware training and evaluation
pipelines, and positioning CHRONOBERG as a scalable resource for the study of
linguistic change and temporal generalization. Disclaimer: This paper includes
language and display of samples that could be offensive to readers. Open
Access: Chronoberg is available publicly on HuggingFace at (
https://huggingface.co/datasets/spaul25/Chronoberg). Code is available at
(https://github.com/paulsubarna/Chronoberg).

</details>


### [246] [Exploratory Semantic Reliability Analysis of Wind Turbine Maintenance Logs using Large Language Models](https://arxiv.org/abs/2509.22366)
*Max Malyi,Jonathan Shek,Andre Biscaya*

Main category: cs.CL

TL;DR: 本文探讨了如何利用大型语言模型（LLMs）进行风力涡轮维护日志的深层语义分析，以增强风能部门的运营智能。


<details>
  <summary>Details</summary>
Motivation: 识别出传统的定量可靠性分析无法处理的风力涡轮机维护日志中的丰富操作智能。

Method: 采用一种探索性框架，使用大型语言模型进行深层语义分析，执行多种分析工作流程。

Result: 研究结果表明，LLMs能够超越标签化，综合文本信息并生成可操作的专家级假设。

Conclusion: 本研究提出了一种新颖且可重复的方法，将大型语言模型（LLMs）作为推理工具，以解锁风能领域中的运营智能。

Abstract: A wealth of operational intelligence is locked within the unstructured
free-text of wind turbine maintenance logs, a resource largely inaccessible to
traditional quantitative reliability analysis. While machine learning has been
applied to this data, existing approaches typically stop at classification,
categorising text into predefined labels. This paper addresses the gap in
leveraging modern large language models (LLMs) for more complex reasoning
tasks. We introduce an exploratory framework that uses LLMs to move beyond
classification and perform deep semantic analysis. We apply this framework to a
large industrial dataset to execute four analytical workflows: failure mode
identification, causal chain inference, comparative site analysis, and data
quality auditing. The results demonstrate that LLMs can function as powerful
"reliability co-pilots," moving beyond labelling to synthesise textual
information and generate actionable, expert-level hypotheses. This work
contributes a novel and reproducible methodology for using LLMs as a reasoning
tool, offering a new pathway to enhance operational intelligence in the wind
energy sector by unlocking insights previously obscured in unstructured data.

</details>


### [247] [What Is The Political Content in LLMs' Pre- and Post-Training Data?](https://arxiv.org/abs/2509.22367)
*Tanise Ceron,Dmitry Nikolaev,Dominik Stammbach,Debora Nozza*

Main category: cs.CL

TL;DR: 本研究分析了OLMO2模型的训练数据，发现政治内容偏向左倾，与模型的政策立场偏见高度相关，强调未来需加强政治内容的审查和透明度。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型（LLMs）生成的政治偏见理解不足，尤其是其训练数据中的政治内容尚未得到充分探讨。

Method: 分析OLMO2的大型预训练和后训练语料库，从中随机抽样，自动注释文档的政治取向，并分析其源领域和内容。

Result: 研究发现，左倾文档在数据集中占主导地位，预训练语料库中的政治参与内容显著多于后训练数据。同时，左倾和右倾文档在相似主题上采用不同的价值观和合法性来源。

Conclusion: 本研究表明，训练数据中政治内容的主导倾向与模型在特定政策问题上的政治偏见密切相关，因此未来的数据策划需要更好地整合政治内容分析，以及深入记录过滤策略以提高透明度。

Abstract: Large language models (LLMs) are known to generate politically biased text,
yet how such biases arise remains unclear. A crucial step toward answering this
question is the analysis of training data, whose political content remains
largely underexplored in current LLM research. To address this gap, we present
in this paper an analysis of the pre- and post-training corpora of OLMO2, the
largest fully open-source model released together with its complete dataset.
From these corpora, we draw large random samples, automatically annotate
documents for political orientation, and analyze their source domains and
content. We then assess how political content in the training data correlates
with models' stance on specific policy issues. Our analysis shows that
left-leaning documents predominate across datasets, with pre-training corpora
containing significantly more politically engaged content than post-training
data. We also find that left- and right-leaning documents frame similar topics
through distinct values and sources of legitimacy. Finally, the predominant
stance in the training data strongly correlates with models' political biases
when evaluated on policy issues. These findings underscore the need to
integrate political content analysis into future data curation pipelines as
well as in-depth documentation of filtering strategies for transparency.

</details>


### [248] [Chimera: Diagnosing Shortcut Learning in Visual-Language Understanding](https://arxiv.org/abs/2509.22437)
*Ziheng Chi,Yifan Hou,Chenxi Pang,Shaobo Cui,Mubashara Akhtar,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 本研究通过引入Chimera测试套件评估视觉语言模型在图表理解上的能力，发现它们的高表现大多依赖于各种捷径，强调了需要更真实的理解能力评估。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在图表相关基准上表现良好，但它们对知识、推理和模态捷径的依赖引发了对它们是否真正理解图表的担忧，因此需要进行更深入的评估。

Method: 引入Chimera综合测试套件，包含7500个高质量图表及其符号内容，设计多层次问题来评估图表理解的四个基本方面：实体识别、关系理解、知识基础和视觉推理。

Result: 对15个开源视觉语言模型的评估发现，它们的强大表现主要源于捷径行为，包括视觉记忆、知识回忆和表面语言模式的利用，这暴露了当前模型的重大局限性。

Conclusion: 当前的视觉语言模型在处理复杂视觉输入（如图表）时表现出的能力往往依赖于各种捷径，而非真实的理解与推理，因此需要更强的评估协议以衡量实际的理解能力。

Abstract: Diagrams convey symbolic information in a visual format rather than a linear
stream of words, making them especially challenging for AI models to process.
While recent evaluations suggest that vision-language models (VLMs) perform
well on diagram-related benchmarks, their reliance on knowledge, reasoning, or
modality shortcuts raises concerns about whether they genuinely understand and
reason over diagrams. To address this gap, we introduce Chimera, a
comprehensive test suite comprising 7,500 high-quality diagrams sourced from
Wikipedia; each diagram is annotated with its symbolic content represented by
semantic triples along with multi-level questions designed to assess four
fundamental aspects of diagram comprehension: entity recognition, relation
understanding, knowledge grounding, and visual reasoning. We use Chimera to
measure the presence of three types of shortcuts in visual question answering:
(1) the visual-memorization shortcut, where VLMs rely on memorized visual
patterns; (2) the knowledge-recall shortcut, where models leverage memorized
factual knowledge instead of interpreting the diagram; and (3) the Clever-Hans
shortcut, where models exploit superficial language patterns or priors without
true comprehension. We evaluate 15 open-source VLMs from 7 model families on
Chimera and find that their seemingly strong performance largely stems from
shortcut behaviors: visual-memorization shortcuts have slight impact,
knowledge-recall shortcuts play a moderate role, and Clever-Hans shortcuts
contribute significantly. These findings expose critical limitations in current
VLMs and underscore the need for more robust evaluation protocols that
benchmark genuine comprehension of complex visual inputs (e.g., diagrams)
rather than question-answering shortcuts.

</details>


### [249] [Detecting (Un)answerability in Large Language Models with Linear Directions](https://arxiv.org/abs/2509.22449)
*Maor Juliet Lavi,Tova Milo,Mor Geva*

Main category: cs.CL

TL;DR: 本研究提出了一种新方法，通过识别模型激活空间中的特定方向，来检测大型语言模型在提取型问答中的不可回答性，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在缺乏必要信息时自信回答问题导致的虚假答案，特别关注可提取问答中的（不）回答检测问题。

Method: 通过在推理过程中应用激活添加，选择模型激活空间中捕捉不可回答性的方向，并使用该方向进行分类。

Result: 在两个开放权重的LLM和四个提取QA基准测试上，实验表明我们的方法能够有效检测不可回答的问题，且模型的放弃行为与所选方向显著相关。

Conclusion: 该方法有效检测不可回答的问题，并且比现有方法在不同数据集上具有更好的泛化能力。

Abstract: Large language models (LLMs) often respond confidently to questions even when
they lack the necessary information, leading to hallucinated answers. In this
work, we study the problem of (un)answerability detection, focusing on
extractive question answering (QA) where the model should determine if a
passage contains sufficient information to answer a given question. We propose
a simple approach for identifying a direction in the model's activation space
that captures unanswerability and uses it for classification. This direction is
selected by applying activation additions during inference and measuring their
impact on the model's abstention behavior. We show that projecting hidden
activations onto this direction yields a reliable score for (un)answerability
classification. Experiments on two open-weight LLMs and four extractive QA
benchmarks show that our method effectively detects unanswerable questions and
generalizes better across datasets than existing prompt-based and
classifier-based approaches. Moreover, the obtained directions extend beyond
extractive QA to unanswerability that stems from factors, such as lack of
scientific consensus and subjectivity. Last, causal interventions show that
adding or ablating the directions effectively controls the abstention behavior
of the model.

</details>


### [250] [Evaluating the Limits of Large Language Models in Multilingual Legal Reasoning](https://arxiv.org/abs/2509.22472)
*Antreas Ioannou,Andreas Shiamishis,Nora Hollenstein,Nezihe Merve Gürel*

Main category: cs.CL

TL;DR: 本研究评估了LLaMA和Gemini在多语种法律任务中的表现，结果显示它们在法律推理中的准确率低于50%，并揭示了在多语种法律应用中仍然存在的重大挑战。


<details>
  <summary>Details</summary>
Motivation: 在以大型语言模型为主导的时代，理解它们在法律等高风险领域中的能力和局限性至关重要。

Method: 评估LLaMA和Gemini在多语种法律和非法律基准上的表现，并通过字符和词级别的扰动评估其在法律任务中的对抗鲁棒性，采用了LLM作为评判者的方法进行人类对齐评估。

Result: LLaMA和Gemini在法律推理基准上的准确率通常低于50%，而在通用任务如XNLI上超过70%。英语通常结果更稳定，但不一定意味着更高的准确率，对提示的敏感性和对抗脆弱性普遍存在。LLaMA相比Gemini弱，后者在相同任务上平均优势约24个百分点。

Conclusion: 尽管新一代大型语言模型（LLMs）在技术上有了进步，但在关键的多语种法律应用中，使用它们仍面临挑战。

Abstract: In an era dominated by Large Language Models (LLMs), understanding their
capabilities and limitations, especially in high-stakes fields like law, is
crucial. While LLMs such as Meta's LLaMA, OpenAI's ChatGPT, Google's Gemini,
DeepSeek, and other emerging models are increasingly integrated into legal
workflows, their performance in multilingual, jurisdictionally diverse, and
adversarial contexts remains insufficiently explored. This work evaluates LLaMA
and Gemini on multilingual legal and non-legal benchmarks, and assesses their
adversarial robustness in legal tasks through character and word-level
perturbations. We use an LLM-as-a-Judge approach for human-aligned evaluation.
We moreover present an open-source, modular evaluation pipeline designed to
support multilingual, task-diverse benchmarking of any combination of LLMs and
datasets, with a particular focus on legal tasks, including classification,
summarization, open questions, and general reasoning. Our findings confirm that
legal tasks pose significant challenges for LLMs with accuracies often below
50% on legal reasoning benchmarks such as LEXam, compared to over 70% on
general-purpose tasks like XNLI. In addition, while English generally yields
more stable results, it does not always lead to higher accuracy. Prompt
sensitivity and adversarial vulnerability is also shown to persist across
languages. Finally, a correlation is found between the performance of a
language and its syntactic similarity to English. We also observe that LLaMA is
weaker than Gemini, with the latter showing an average advantage of about 24
percentage points across the same task. Despite improvements in newer LLMs,
challenges remain in deploying them reliably for critical, multilingual legal
applications.

</details>


### [251] [NeLLCom-Lex: A Neural-agent Framework to Study the Interplay between Lexical Systems and Language Use](https://arxiv.org/abs/2509.22479)
*Yuqing Zhang,Ecesu Ürker,Tessa Verhoef,Gemma Boleda,Arianna Bisazza*

Main category: cs.CL

TL;DR: 本研究提出了 NeLLCom-Lex 框架，通过模拟语义变化来揭示其机制，结果表明神经代理能有效再现人类的颜色命名行为。


<details>
  <summary>Details</summary>
Motivation: 传统的观察和实验方法不能充分揭示语义变化的因果机制，因此本研究旨在通过模拟来探讨这一问题。

Method: 使用神经代理框架 NeLLCom-Lex，基于真实词汇系统并操控代理的交流需求进行实验。

Result: 通过不同的监督和强化学习方法，神经代理能够在颜色命名任务中表现出人类相似的行为，支持了进一步使用 NeLLCom-Lex 探索语义变化机制的可能性。

Conclusion: NeLLCom-Lex 框架有效地模拟了语义变化的机制，神经代理能够再现人类在颜色命名中的行为模式。

Abstract: Lexical semantic change has primarily been investigated with observational
and experimental methods; however, observational methods (corpus analysis,
distributional semantic modeling) cannot get at causal mechanisms, and
experimental paradigms with humans are hard to apply to semantic change due to
the extended diachronic processes involved. This work introduces NeLLCom-Lex, a
neural-agent framework designed to simulate semantic change by first grounding
agents in a real lexical system (e.g. English) and then systematically
manipulating their communicative needs. Using a well-established color naming
task, we simulate the evolution of a lexical system within a single generation,
and study which factors lead agents to: (i) develop human-like naming behavior
and lexicons, and (ii) change their behavior and lexicons according to their
communicative needs. Our experiments with different supervised and
reinforcement learning pipelines show that neural agents trained to 'speak' an
existing language can reproduce human-like patterns in color naming to a
remarkable extent, supporting the further use of NeLLCom-Lex to elucidate the
mechanisms of semantic change.

</details>


### [252] [Exploring Solution Divergence and Its Effect on Large Language Model Problem Solving](https://arxiv.org/abs/2509.22480)
*Hang Li,Kaiqi Yang,Yucheng Chu,Hui Liu,Jiliang Tang*

Main category: cs.CL

TL;DR: 本文提出解决方案的多样性作为一种新指标，能够有效提高大型语言模型的解决问题能力，并在多个领域的应用中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探讨当前大型语言模型性能提升的方向，尤其是寻找一种新的指标来评估和改善问题解决能力。

Method: 通过研究LLM在单一问题上的解决方案差异，提出解决方案多样性作为新指标，并在三个代表性问题领域进行测试。

Result: 使用解决方案多样性作为指标，保持了一致的成功率提升。

Conclusion: 解决方案的多样性是提高大型语言模型（LLM）解决问题能力的有效工具，可以增强其训练和评估。

Abstract: Large language models (LLMs) have been widely used for problem-solving tasks.
Most recent work improves their performance through supervised fine-tuning
(SFT) with labeled data or reinforcement learning (RL) from task feedback. In
this paper, we study a new perspective: the divergence in solutions generated
by LLMs for a single problem. We show that higher solution divergence is
positively related to better problem-solving abilities across various models.
Based on this finding, we propose solution divergence as a novel metric that
can support both SFT and RL strategies. We test this idea on three
representative problem domains and find that using solution divergence
consistently improves success rates. These results suggest that solution
divergence is a simple but effective tool for advancing LLM training and
evaluation.

</details>


### [253] [JGU Mainz's Submission to the WMT25 Shared Task on LLMs with Limited Resources for Slavic Languages: MT and QA](https://arxiv.org/abs/2509.22490)
*Hossain Shaikh Saadi,Minh Duc Bui,Mario Sanz-Guerrero,Katharina von der Wense*

Main category: cs.CL

TL;DR: 本文展示了在有限资源条件下，针对乌克兰语和上、下索宾语的有效机器翻译和问答系统的开发，模型在各任务上超越了基准表现。


<details>
  <summary>Details</summary>
Motivation: 针对有限资源的斯拉夫语言（特别是乌克兰语和上、下索宾语）进行有效的机器翻译和问答系统开发。

Method: 对Qwen2.5-3B-Instruct模型进行联合微调，采用参数高效的微调方式，并整合翻译和多选QA数据。

Result: 在乌克兰QA中使用了检索增强生成，并对上、下索宾语的QA应用了集成技术。

Conclusion: 模型在所有任务上的表现优于基准，展示了在有限资源条件下的有效性。

Abstract: This paper presents the JGU Mainz submission to the WMT25 Shared Task on LLMs
with Limited Resources for Slavic Languages: Machine Translation and Question
Answering, focusing on Ukrainian, Upper Sorbian, and Lower Sorbian. For each
language, we jointly fine-tune a Qwen2.5-3B-Instruct model for both tasks with
parameter-efficient finetuning. Our pipeline integrates additional translation
and multiple-choice question answering (QA) data. For Ukrainian QA, we further
use retrieval-augmented generation. We also apply ensembling for QA in Upper
and Lower Sorbian. Experiments show that our models outperform the baseline on
both tasks.

</details>


### [254] [Representing LLMs in Prompt Semantic Task Space](https://arxiv.org/abs/2509.22506)
*Idan Kashani,Avi Mendelson,Yaniv Nemcovsky*

Main category: cs.CL

TL;DR: 本研究提出了一种无需训练的线性算子方法，能够高效表示大型语言模型，具备良好的可解释性和扩展性，为在不同任务中选择最优模型提供新思路。


<details>
  <summary>Details</summary>
Motivation: 随着预训练模型库的扩展，识别适合特定任务的大型语言模型变得越来越具挑战性，现有方法缺乏扩展性且需要高成本的重新训练。

Method: 采用封闭形式的几何性质计算，来表征大型语言模型，侧重于模型在具体任务中的应用。

Result: 在成功预测和模型选择任务中，展示了我们的方法，其性能在样本外场景中也表现出竞争力或领先水平。

Conclusion: 本研究提出了一种高效且无需训练的方法来表示大型语言模型，在语义任务空间内作为线性算子，具备良好的可解释性和扩展性，能实时适应动态扩展的模型库。

Abstract: Large language models (LLMs) achieve impressive results over various tasks,
and ever-expanding public repositories contain an abundance of pre-trained
models. Therefore, identifying the best-performing LLM for a given task is a
significant challenge. Previous works have suggested learning LLM
representations to address this. However, these approaches present limited
scalability and require costly retraining to encompass additional models and
datasets. Moreover, the produced representation utilizes distinct spaces that
cannot be easily interpreted. This work presents an efficient, training-free
approach to representing LLMs as linear operators within the prompts' semantic
task space, thus providing a highly interpretable representation of the models'
application. Our method utilizes closed-form computation of geometrical
properties and ensures exceptional scalability and real-time adaptability to
dynamically expanding repositories. We demonstrate our approach on success
prediction and model selection tasks, achieving competitive or state-of-the-art
results with notable performance in out-of-sample scenarios.

</details>


### [255] [We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before They Go Wrong](https://arxiv.org/abs/2509.22510)
*Gautam Siddharth Kashyap,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 本文提出了自适应多分支引导（AMBS），通过合理的框架提高大型语言模型在有用性、安全性和诚实性上的对齐能力。


<details>
  <summary>Details</summary>
Motivation: 多目标对齐（有用性、安全性和诚实性）在大型语言模型的安全可靠部署中至关重要。

Method: 提出了一种两阶段的1对N框架：第一阶段计算共享表示，第二阶段通过策略引用机制进行目标特定控制。

Result: AMBS在多个7B LLM模型中一致改善了HHH对齐，尤其在DeepSeek-7B模型上提高了对齐分数32.4%，减少了11.0%的不安全输出。

Conclusion: 自适应多分支引导（AMBS）实现了多目标一致性对齐，显著提高了大型语言模型的有用性、安全性和诚实性。

Abstract: Alignment of Large Language Models (LLMs) along multiple
objectives-helpfulness, harmlessness, and honesty (HHH)-is critical for safe
and reliable deployment. Prior work has used steering vector-small control
signals injected into hidden states-to guide LLM outputs, typically via
one-to-one (1-to-1) Transformer decoders. In this setting, optimizing a single
alignment objective can inadvertently overwrite representations learned for
other objectives, leading to catastrophic forgetting. More recent approaches
extend steering vectors via one-to-many (1-to-N) Transformer decoders. While
this alleviates catastrophic forgetting, naive multi-branch designs optimize
each objective independently, which can cause inference fragmentation-outputs
across HHH objectives may become inconsistent. We propose Adaptive Multi-Branch
Steering (AMBS), a two-stage 1-to-N framework for unified and efficient
multi-objective alignment. In Stage I, post-attention hidden states of the
Transformer layer are computed once to form a shared representation. In Stage
II, this representation is cloned into parallel branches and steered via a
policy-reference mechanism, enabling objective-specific control while
maintaining cross-objective consistency. Empirical evaluations on Alpaca,
BeaverTails, and TruthfulQA show that AMBS consistently improves HHH alignment
across multiple 7B LLM backbones. For example, on DeepSeek-7B, AMBS improves
average alignment scores by +32.4% and reduces unsafe outputs by 11.0% compared
to a naive 1-to-N baseline, while remaining competitive with state-of-the-art
methods.

</details>


### [256] [InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models](https://arxiv.org/abs/2509.22536)
*Wenjun Wang,Shuo Cai,Congkai Xie,Mingfa Feng,Yiming Zhang,Zhen Li,Kejing Yang,Ming Li,Jiannong Cao,Yuan Xie,Hongxia Yang*

Main category: cs.CL

TL;DR: 本研究提出了一种新的端到端FP8训练方案，显著提高了计算效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型训练的高计算成本问题，促进FP8的广泛应用。

Method: 引入端到端的FP8训练方案，结合持续预训练和有监督微调，并采用细粒度混合量化策略。

Result: 通过广泛实验，FP8训练方法在多个推理基准上表现优异，训练时间减少22%，峰值内存使用量降低14%，吞吐量提高19%。

Conclusion: FP8是一种实用且稳健的替代BF16的训练方法，并且将在未来发布相应代码以促进大规模模型训练的民主化。

Abstract: The immense computational cost of training Large Language Models (LLMs)
presents a major barrier to innovation. While FP8 training offers a promising
solution with significant theoretical efficiency gains, its widespread adoption
has been hindered by the lack of a comprehensive, open-source training recipe.
To bridge this gap, we introduce an end-to-end FP8 training recipe that
seamlessly integrates continual pre-training and supervised fine-tuning. Our
methodology employs a fine-grained, hybrid-granularity quantization strategy to
maintain numerical fidelity while maximizing computational efficiency. Through
extensive experiments, including the continue pre-training of models on a
160B-token corpus, we demonstrate that our recipe is not only remarkably stable
but also essentially lossless, achieving performance on par with the BF16
baseline across a suite of reasoning benchmarks. Crucially, this is achieved
with substantial efficiency improvements, including up to a 22% reduction in
training time, a 14% decrease in peak memory usage, and a 19% increase in
throughput. Our results establish FP8 as a practical and robust alternative to
BF16, and we will release the accompanying code to further democratize
large-scale model training.

</details>


### [257] [Think Socially via Cognitive Reasoning](https://arxiv.org/abs/2509.22546)
*Jinfeng Zhou,Zheyu Chen,Shuai Wang,Quanyu Dai,Zhenhua Dong,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: CogFlow是一种新框架，通过引入认知推理，增强LLMs在社交场合中的推理和决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在逻辑推理方面表现优秀，但在社交场景中对模糊线索的解析能力不足，因此提出一种新范式以弥补这一不足。

Method: 通过监督微调和强化学习，采用树状规划模拟人类思想的联想与进阶特性来构建认知流程，进而培养LLMs的社会认知能力。

Result: 广泛实验表明，CogFlow显著提升了大型语言模型的社交认知能力，改善了人类的决策效果。

Conclusion: CogFlow有效提高了大型语言模型的社会认知能力，从而改善社交决策过程。

Abstract: LLMs trained for logical reasoning excel at step-by-step deduction to reach
verifiable answers. However, this paradigm is ill-suited for navigating social
situations, which induce an interpretive process of analyzing ambiguous cues
that rarely yield a definitive outcome. To bridge this gap, we introduce
Cognitive Reasoning, a paradigm modeled on human social cognition. It
formulates the interpretive process into a structured cognitive flow of
interconnected cognitive units (e.g., observation or attribution), which
combine adaptively to enable effective social thinking and responses. We then
propose CogFlow, a complete framework that instills this capability in LLMs.
CogFlow first curates a dataset of cognitive flows by simulating the
associative and progressive nature of human thought via tree-structured
planning. After instilling the basic cognitive reasoning capability via
supervised fine-tuning, CogFlow adopts reinforcement learning to enable the
model to improve itself via trial and error, guided by a multi-objective reward
that optimizes both cognitive flow and response quality. Extensive experiments
show that CogFlow effectively enhances the social cognitive capabilities of
LLMs, and even humans, leading to more effective social decision-making.

</details>


### [258] [Retrieval-Augmented Guardrails for AI-Drafted Patient-Portal Messages: Error Taxonomy Construction and Large-Scale Evaluation](https://arxiv.org/abs/2509.22565)
*Wenyuan Chen,Fateme Nateghi Haredasht,Kameron C. Black,Francois Grolleau,Emily Alsentzer,Jonathan H. Chen,Stephen P. Ma*

Main category: cs.CL

TL;DR: 本文介绍了一种基于大语言模型的异步患者-临床消息的评估管道，通过引入错误本体和检索增强的评估，提高了响应质量的评估效果，尤其在临床完整性和工作流程的适用性方面。


<details>
  <summary>Details</summary>
Motivation: 随着异步患者-临床医生的消息交流增多，需要评估大语言模型（LLMs）生成响应的准确性和质量，以减少临床工作负担。

Method: 开发了一个检索增强评估管道（RAEC）和两阶段DSPy提示架构，并对超过1500条患者消息进行了比较评估。

Result: 在上下文增强的评估中，错误识别方面显著提高，尤其是在临床完整性和工作流程适当性等领域；并且人类验证显示上下文增强的标签在一致性和性能方面优于基线。

Conclusion: 使用上下文增强标签的RAEC管道在患者消息的质量评估中表现出优越性，支持其作为患者消息的AI保护装置。

Abstract: Asynchronous patient-clinician messaging via EHR portals is a growing source
of clinician workload, prompting interest in large language models (LLMs) to
assist with draft responses. However, LLM outputs may contain clinical
inaccuracies, omissions, or tone mismatches, making robust evaluation
essential. Our contributions are threefold: (1) we introduce a clinically
grounded error ontology comprising 5 domains and 59 granular error codes,
developed through inductive coding and expert adjudication; (2) we develop a
retrieval-augmented evaluation pipeline (RAEC) that leverages semantically
similar historical message-response pairs to improve judgment quality; and (3)
we provide a two-stage prompting architecture using DSPy to enable scalable,
interpretable, and hierarchical error detection. Our approach assesses the
quality of drafts both in isolation and with reference to similar past
message-response pairs retrieved from institutional archives. Using a two-stage
DSPy pipeline, we compared baseline and reference-enhanced evaluations on over
1,500 patient messages. Retrieval context improved error identification in
domains such as clinical completeness and workflow appropriateness. Human
validation on 100 messages demonstrated superior agreement (concordance = 50%
vs. 33%) and performance (F1 = 0.500 vs. 0.256) of context-enhanced labels vs.
baseline, supporting the use of our RAEC pipeline as AI guardrails for patient
messaging.

</details>


### [259] [Fine-Grained Detection of Context-Grounded Hallucinations Using LLMs](https://arxiv.org/abs/2509.22582)
*Yehonatan Pesiakhovsky,Zorik Gekhman,Yosi Mass,Liat Ein-Dor,Roi Reichart*

Main category: cs.CL

TL;DR: 本研究探讨了LLM在定位幻觉方面的表现，构建了新的评估基准，并提出了捕获各种错误的新方法，展示了任务的复杂性和LLM的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究模型输出中存在不符合源文本的幻觉现象，并探索LLM在本任务中的适用性。

Method: 构建一个包括1000多条人工注释的例子，并与基于LLM的评估协议结合以验证其质量。

Result: 对四个大规模LLM进行评估，最好的模型只达到了0.67的F1分数，表现出基准的挑战性，并分析了有效的提示策略与LLM面临的主要困难。

Conclusion: 本研究构建了一个用于评估大规模语言模型在定位幻觉方面的基准，并提出了一种新的表征方法，呈现出本任务的难度和特定的挑战因素。

Abstract: Context-grounded hallucinations are cases where model outputs contain
information not verifiable against the source text. We study the applicability
of LLMs for localizing such hallucinations, as a more practical alternative to
existing complex evaluation pipelines. In the absence of established benchmarks
for meta-evaluation of hallucinations localization, we construct one tailored
to LLMs, involving a challenging human annotation of over 1,000 examples. We
complement the benchmark with an LLM-based evaluation protocol, verifying its
quality in a human evaluation. Since existing representations of hallucinations
limit the types of errors that can be expressed, we propose a new
representation based on free-form textual descriptions, capturing the full
range of possible errors. We conduct a comprehensive study, evaluating four
large-scale LLMs, which highlights the benchmark's difficulty, as the best
model achieves an F1 score of only 0.67. Through careful analysis, we offer
insights into optimal prompting strategies for the task and identify the main
factors that make it challenging for LLMs: (1) a tendency to incorrectly flag
missing details as inconsistent, despite being instructed to check only facts
in the output; and (2) difficulty with outputs containing factually correct
information absent from the source - and thus not verifiable - due to alignment
with the model's parametric knowledge.

</details>


### [260] [ArabJobs: A Multinational Corpus of Arabic Job Ads](https://arxiv.org/abs/2509.22589)
*Mo El-Haj*

Main category: cs.CL

TL;DR: ArabJobs是一个涵盖阿拉伯四国工作广告的数据集，分析性别表现和方言差异，展示其在自然语言处理中的多种应用。


<details>
  <summary>Details</summary>
Motivation: 调查阿拉伯劳动力市场中的语言、地区和社会经济变化，特别是在性别表现和职业结构方面。

Method: 通过收集来自埃及、约旦、沙特阿拉伯和阿联酋的超过8500条工作广告构建数据集。

Result: 分析显示数据集在施行公平的阿拉伯语NLP和劳动力市场研究中的有效性，并提供了工资估算和职业类别规范化等应用示例。

Conclusion: ArabJobs 数据集对于阿拉伯语自然语言处理和劳动力市场研究具有重要价值，并可用于多个应用领域。

Abstract: ArabJobs is a publicly available corpus of Arabic job advertisements
collected from Egypt, Jordan, Saudi Arabia, and the United Arab Emirates.
Comprising over 8,500 postings and more than 550,000 words, the dataset
captures linguistic, regional, and socio-economic variation in the Arab labour
market. We present analyses of gender representation and occupational
structure, and highlight dialectal variation across ads, which offers
opportunities for future research. We also demonstrate applications such as
salary estimation and job category normalisation using large language models,
alongside benchmark tasks for gender bias detection and profession
classification. The findings show the utility of ArabJobs for fairness-aware
Arabic NLP and labour market research. The dataset is publicly available on
GitHub: https://github.com/drelhaj/ArabJobs.

</details>


### [261] [From Formal Language Theory to Statistical Learning: Finite Observability of Subregular Languages](https://arxiv.org/abs/2509.22598)
*Katsuhiko Hayashi,Hidetaka Kamigaito*

Main category: cs.CL

TL;DR: 本文证明标准子正则语言类的判定谓词是线性可分的，推动了自然语言处理模型的学习能力与可解释性。


<details>
  <summary>Details</summary>
Motivation: 为自然语言结构建模提供一个严格且可解释的基础。

Method: 通过证明所有标准子正则语言类的判定谓词是线性可分的，以及进行合成实验和真实数据实验。

Result: 在无噪音条件下的合成实验确认了完美的可分性，真实数据实验显示学习到的特征与已知语言约束一致。

Conclusion: 子正则语言类可以通过简单线性模型进行学习，具有完美的可分性和解释性。

Abstract: We prove that all standard subregular language classes are linearly separable
when represented by their deciding predicates. This establishes finite
observability and guarantees learnability with simple linear models. Synthetic
experiments confirm perfect separability under noise-free conditions, while
real-data experiments on English morphology show that learned features align
with well-known linguistic constraints. These results demonstrate that the
subregular hierarchy provides a rigorous and interpretable foundation for
modeling natural language structure. Our code used in real-data experiments is
available at https://github.com/UTokyo-HayashiLab/subregular.

</details>


### [262] [Capturing Opinion Shifts in Deliberative Discourse through Frequency-based Quantum deep learning methods](https://arxiv.org/abs/2509.22603)
*Rakesh Thakur,Harsh Chaturvedi,Ruqayya Shah,Janvi Chauhan,Ayush Sharma*

Main category: cs.CL

TL;DR: 本研究比较了多种自然语言处理技术，以验证模型对审议话语的解读及其产生的见解，显示了在政策制定及舆情分析中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言处理的最新进展，计算建模审议的可能性增加，能够分析意见变化并预测不同情景下的潜在结果。

Method: 使用多种NLP技术对模型如何解读审议话语进行比较分析，并模拟了通过产品呈现的审议过程。

Result: 比较分析了两种模型，发现频率基础的话语调制模型和量子审议框架优于现有的最先进模型。

Conclusion: 该研究的发现强调了在公共政策制定、辩论评估、决策支持框架和大规模社交媒体舆情挖掘中的实际应用。

Abstract: Deliberation plays a crucial role in shaping outcomes by weighing diverse
perspectives before reaching decisions. With recent advancements in Natural
Language Processing, it has become possible to computationally model
deliberation by analyzing opinion shifts and predicting potential outcomes
under varying scenarios. In this study, we present a comparative analysis of
multiple NLP techniques to evaluate how effectively models interpret
deliberative discourse and produce meaningful insights. Opinions from
individuals of varied backgrounds were collected to construct a self-sourced
dataset that reflects diverse viewpoints. Deliberation was simulated using
product presentations enriched with striking facts, which often prompted
measurable shifts in audience opinions. We have given comparative analysis
between two models namely Frequency-Based Discourse Modulation and
Quantum-Deliberation Framework which outperform the existing state of art
models. The findings highlight practical applications in public policy-making,
debate evaluation, decision-support frameworks, and large-scale social media
opinion mining.

</details>


### [263] [From tests to effect sizes: Quantifying uncertainty and statistical variability in multilingual and multitask NLP evaluation benchmarks](https://arxiv.org/abs/2509.22612)
*Jonne Sälevä,Duygu Ataman,Constantine Lignos*

Main category: cs.CL

TL;DR: 本文介绍了重抽样方法在多语言和多任务NLP基准中量化评估指标不确定性和统计精度的应用，强调了考虑模型和数据因素的重要性。


<details>
  <summary>Details</summary>
Motivation: 实验性能分数的变化源于模型和数据相关因素，因此必须考虑这两者，以避免严重低估假设重复的整体变异性。

Method: 使用重抽样方法，计算用于排行榜的各种量的抽样分布，比如平均值/中位数、模型间的成对差异和排名。

Result: 通过多语言问答、机器翻译和命名实体识别等示例任务，展示了重抽样方法在计算采样分布中的有效性。

Conclusion: 本文提出了一系列基于重抽样的方法，以量化多语言和多任务NLP基准中的评估指标的不确定性和统计精度。

Abstract: In this paper, we introduce a set of resampling-based methods for quantifying
uncertainty and statistical precision of evaluation metrics in multilingual
and/or multitask NLP benchmarks. We show how experimental variation in
performance scores arises from both model- and data-related sources, and that
accounting for both of them is necessary to avoid substantially underestimating
the overall variability over hypothetical replications. Using multilingual
question answering, machine translation, and named entity recognition as
example tasks, we also demonstrate how resampling methods are useful for
computing sampling distributions for various quantities used in leaderboards
such as the average/median, pairwise differences between models, and rankings.

</details>


### [264] [StateX: Enhancing RNN Recall via Post-training State Expansion](https://arxiv.org/abs/2509.22630)
*Xingyu Shen,Yingfa Chen,Zhen Leng Thai,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 本论文提出了StateX，一个后训练管道，通过有效扩展预训练RNN的状态，解决了RNN在长上下文回忆能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在处理长上下文时复杂性高，RNN模型由于其常量的每个标记复杂性而受到欢迎，但在长上下文的回忆能力方面存在不足。

Method: 通过对预训练RNN进行后训练，设计架构修改以有效扩展状态大小。

Result: 在参数量达到1.3B的模型上，StateX显著提升了RNN的回忆能力和上下文学习能力，而不增加高昂的后训练成本或损害其他能力。

Conclusion: StateX能够在不显著增加模型参数的情况下高效提升RNN的回忆和上下文学习能力。

Abstract: While Transformer-based models have demonstrated remarkable language modeling
performance, their high complexities result in high costs when processing long
contexts. In contrast, recurrent neural networks (RNNs) such as linear
attention and state space models have gained popularity due to their constant
per-token complexities. However, these recurrent models struggle with tasks
that require accurate recall of contextual information from long contexts,
because all contextual information is compressed into a constant-size recurrent
state. Previous works have shown that recall ability is positively correlated
with the recurrent state size, yet directly training RNNs with larger recurrent
states results in high training costs. In this paper, we introduce StateX, a
training pipeline for efficiently expanding the states of pre-trained RNNs
through post-training. For two popular classes of RNNs, linear attention and
state space models, we design post-training architectural modifications to
scale up the state size with no or negligible increase in model parameters.
Experiments on models up to 1.3B parameters demonstrate that StateX efficiently
enhances the recall and in-context learning ability of RNNs without incurring
high post-training costs or compromising other capabilities.

</details>


### [265] [Variational Reasoning for Language Models](https://arxiv.org/abs/2509.22637)
*Xiangxin Zhou,Zichen Liu,Haonan Wang,Chao Du,Min Lin,Chongxuan Li,Liang Wang,Tianyu Pang*

Main category: cs.CL

TL;DR: 本论文提出了一种新的变分推理框架，通过多痕迹目标与前向KL公式优化语言模型的思维痕迹，并实证验证其在多种推理任务上的有效性。


<details>
  <summary>Details</summary>
Motivation: 旨在提升语言模型的推理能力，并解决训练中存在的偏倚问题，提供更稳定的训练目标。

Method: 引入了一种变分推理框架，通过变分推断优化思维痕迹作为潜在变量，并提出了多痕迹目标和前向KL公式。

Result: 通过实证validation，验证了在Qwen 2.5和Qwen 3模型系列上的有效性，显示出方法能够改善推理任务的表现。

Conclusion: 本研究提供了一种原则性的概率视角，将变分推断与强化学习方法统一起来，提出了稳定的目标，以提升语言模型的推理能力。

Abstract: We introduce a variational reasoning framework for language models that
treats thinking traces as latent variables and optimizes them through
variational inference. Starting from the evidence lower bound (ELBO), we extend
it to a multi-trace objective for tighter bounds and propose a forward-KL
formulation that stabilizes the training of the variational posterior. We
further show that rejection sampling finetuning and binary-reward RL, including
GRPO, can be interpreted as local forward-KL objectives, where an implicit
weighting by model accuracy naturally arises from the derivation and reveals a
previously unnoticed bias toward easier questions. We empirically validate our
method on the Qwen 2.5 and Qwen 3 model families across a wide range of
reasoning tasks. Overall, our work provides a principled probabilistic
perspective that unifies variational inference with RL-style methods and yields
stable objectives for improving the reasoning ability of language models. Our
code is available at https://github.com/sail-sg/variational-reasoning.

</details>


### [266] [Language Models Can Learn from Verbal Feedback Without Scalar Rewards](https://arxiv.org/abs/2509.22638)
*Renjie Luo,Zichen Liu,Xiangyan Liu,Chao Du,Min Lin,Wenhu Chen,Wei Lu,Tianyu Pang*

Main category: cs.CL

TL;DR: 本文提出反馈条件策略（FCP），通过将口头反馈视为条件信号，改善大语言模型的学习过程，提升模型对反馈的利用。


<details>
  <summary>Details</summary>
Motivation: 当前通常将人类或AI反馈压缩为标量奖励，这种方式丢失了反馈的丰富性，因此需要一种新的方法以更好地利用反馈信息。

Method: 引入反馈条件策略（FCP），通过最大似然训练来学习响应-反馈对，并在在线自举阶段中生成积极条件下的输出以获得新反馈。

Result: 反馈条件策略（FCP）有效地将反馈驱动的学习重新构架为条件生成，提高了大语言模型在复杂反馈场景下的学习能力。

Conclusion: 通过将反馈视为条件信号，我们提出了一种更丰富和直观的方式来利用口头反馈进行学习，有助于提高大语言模型的表现。

Abstract: LLMs are often trained with RL from human or AI feedback, yet such methods
typically compress nuanced feedback into scalar rewards, discarding much of
their richness and inducing scale imbalance. We propose treating verbal
feedback as a conditioning signal. Inspired by language priors in text-to-image
generation, which enable novel outputs from unseen prompts, we introduce the
feedback-conditional policy (FCP). FCP learns directly from response-feedback
pairs, approximating the feedback-conditional posterior through maximum
likelihood training on offline data. We further develop an online bootstrapping
stage where the policy generates under positive conditions and receives fresh
feedback to refine itself. This reframes feedback-driven learning as
conditional generation rather than reward optimization, offering a more
expressive way for LLMs to directly learn from verbal feedback. Our code is
available at https://github.com/sail-sg/feedback-conditional-policy.

</details>


### [267] [Death of the Novel(ty): Beyond n-Gram Novelty as a Metric for Textual Creativity](https://arxiv.org/abs/2509.22641)
*Arkadiy Saakyan,Najoung Kim,Smaranda Muresan,Tuhin Chakrabarty*

Main category: cs.CL

TL;DR: 研究表明n-gram新颖性与文本创造力相关，但不能单独使用，特别是在评估开放源代码LLM时，其新颖性较高但实用性较低。


<details>
  <summary>Details</summary>
Motivation: 探讨创造力的双重性质：新颖性和适切性，及其与n-gram新颖性的关系。

Method: 通过7542个专家作家的标注，对人类和AI生成文本进行文本分析。

Result: 尽管n-gram新颖性与创造力存在正相关，但91%的高新颖性表达未被评为创造性，且开源LLMs的新颖性与实用性呈负相关。

Conclusion: N-gram新颖性与文本创造力相关，但单独依赖此指标不足，需考虑适切性。

Abstract: N-gram novelty is widely used to evaluate language models' ability to
generate text outside of their training data. More recently, it has also been
adopted as a metric for measuring textual creativity. However, theoretical work
on creativity suggests that this approach may be inadequate, as it does not
account for creativity's dual nature: novelty (how original the text is) and
appropriateness (how sensical and pragmatic it is). We investigate the
relationship between this notion of creativity and n-gram novelty through 7542
expert writer annotations (n=26) of novelty, pragmaticality, and sensicality
via close reading of human and AI-generated text. We find that while n-gram
novelty is positively associated with expert writer-judged creativity, ~91% of
top-quartile expressions by n-gram novelty are not judged as creative,
cautioning against relying on n-gram novelty alone. Furthermore, unlike
human-written text, higher n-gram novelty in open-source LLMs correlates with
lower pragmaticality. In an exploratory study with frontier close-source
models, we additionally confirm that they are less likely to produce creative
expressions than humans. Using our dataset, we test whether zero-shot,
few-shot, and finetuned models are able to identify creative expressions (a
positive aspect of writing) and non-pragmatic ones (a negative aspect).
Overall, frontier LLMs exhibit performance much higher than random but leave
room for improvement, especially struggling to identify non-pragmatic
expressions. We further find that LLM-as-a-Judge novelty scores from the
best-performing model were predictive of expert writer preferences.

</details>


### [268] [WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning](https://arxiv.org/abs/2509.22644)
*Zimu Lu,Houxing Ren,Yunqiao Yang,Ke Wang,Zhuofan Zong,Junting Pan,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: 提出了WebGen-Agent，利用多级视觉反馈和Step-GRPO方法提升网站代码生成的性能，显著提高生成的准确率和外观评分。


<details>
  <summary>Details</summary>
Motivation: 当前的代码生成代理在处理依赖于视觉效果和用户交互反馈的任务时，仅依靠简单的代码执行进行反馈和验证，无法有效评估生成代码的质量。

Method: 提出了WebGen-Agent，通过利用截图和GUI-Agent评分，使用Step-GRPO训练模式迭代生成和优化网站代码，整合多级视觉反馈。

Result: WebGen-Agent在WebGen-Bench数据集上显著提高了生成代码的准确率和外观评分，超越了以前的代理系统。

Conclusion: WebGen-Agent通过综合的视觉反馈显著提升了网站代码生成的准确性和质量，超越了现有的最先进系统。

Abstract: Agent systems powered by large language models (LLMs) have demonstrated
impressive performance on repository-level code-generation tasks. However, for
tasks such as website codebase generation, which depend heavily on visual
effects and user-interaction feedback, current code agents rely only on simple
code execution for feedback and verification. This approach fails to capture
the actual quality of the generated code. In this paper, we propose
WebGen-Agent, a novel website-generation agent that leverages comprehensive and
multi-level visual feedback to iteratively generate and refine the website
codebase. Detailed and expressive text descriptions and suggestions regarding
the screenshots and GUI-agent testing of the websites are generated by a visual
language model (VLM), together with scores that quantify their quality. The
screenshot and GUI-agent scores are further integrated with a backtracking and
select-best mechanism, enhancing the performance of the agent. Utilizing the
accurate visual scores inherent in the WebGen-Agent workflow, we further
introduce \textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve
the ability of LLMs to act as the reasoning engine of WebGen-Agent. By using
the screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we
provide a dense and reliable process supervision signal, which effectively
improves the model's website-generation ability. On the WebGen-Bench dataset,
WebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9%
and its appearance score from 3.0 to 3.9, outperforming the previous
state-of-the-art agent system. Additionally, our Step-GRPO training approach
increases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and
raises the appearance score from 3.4 to 3.7.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [269] [Golden Tonnetz](https://arxiv.org/abs/2509.21428)
*Yusuke Imai*

Main category: cs.SD

TL;DR: 研究黄金比例在音乐中的应用，提出“黄金Tonnetz”以表示音阶和和声结构。


<details>
  <summary>Details</summary>
Motivation: 探索音乐概念与几何之间的进一步联系，尤其是黄金比例在音阶和和声结构中的应用。

Method: 通过研究音调在黄金三角形中的排列，探讨音乐与黄金比例之间的关系。

Result: 发现一个7音的排列，可以通过黄金三角形表示特定的大调和小调音阶及其和弦结构。

Conclusion: 提出了一种新的“黄金Tonnetz”，通过黄金三角形表示所有大调/小调音阶和三和弦，并能够体现Neo-Riemannian理论中的音阶转换关系。

Abstract: Musical concepts have been represented by geometry with tones. For example,
in the chromatic circle, the twelve tones are represented by twelve points on a
circle, and in Tonnetz, the relationships among harmonies are represented by a
triangular lattice. Recently, we have shown that several arrangements of tones
on the regular icosahedron can be associated with chromatic scales, whole-tone
scales, major tones, and minor tones through the golden ratio. Here, we
investigate another type of connection between music and the golden ratio. We
show that there exists an arrangement of 7 tones on a golden triangle that can
represent a given major/minor scale and its tonic, dominant, and subdominant
chords by golden triangles. By applying this finding, we propose "golden
Tonnetz" which represents all the major/minor scales and triads by the golden
triangles or gnomons and also represents relative, parallel, and leading-tone
exchange transformations in Neo-Riemannian theory by transformations among the
golden triangles and gnomons.

</details>


### [270] [Shortcut Flow Matching for Speech Enhancement: Step-Invariant flows via single stage training](https://arxiv.org/abs/2509.21522)
*Naisong Zhou,Saisamarth Rajesh Phaye,Milos Cernak,Tijana Stojkovic,Andy Pearce,Andrea Cavallaro,Andy Harper*

Main category: cs.SD

TL;DR: SFMSE通过一种高效的方式进行语音增强，克服了传统扩散模型在实时应用中的限制。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在语音增强方面表现出色，但其迭代性质限制了实时应用。

Method: 提出了一种Shortcut Flow Matching方法，通过条件化速度场来进行多种步长的去噪。

Result: SFMSE在消费级GPU上的实时因子为0.013，且感知质量与强扩散基线相当，仅需60次神经函数评估。

Conclusion: SFMSE在语音增强中实现了高质量合成，并具有低延迟的优势。

Abstract: Diffusion-based generative models have achieved state-of-the-art performance
for perceptual quality in speech enhancement (SE). However, their iterative
nature requires numerous Neural Function Evaluations (NFEs), posing a challenge
for real-time applications. On the contrary, flow matching offers a more
efficient alternative by learning a direct vector field, enabling high-quality
synthesis in just a few steps using deterministic ordinary differential
equation~(ODE) solvers. We thus introduce Shortcut Flow Matching for Speech
Enhancement (SFMSE), a novel approach that trains a single, step-invariant
model. By conditioning the velocity field on the target time step during a
one-stage training process, SFMSE can perform single, few, or multi-step
denoising without any architectural changes or fine-tuning. Our results
demonstrate that a single-step SFMSE inference achieves a real-time factor
(RTF) of 0.013 on a consumer GPU while delivering perceptual quality comparable
to a strong diffusion baseline requiring 60 NFEs. This work also provides an
empirical analysis of the role of stochasticity in training and inference,
bridging the gap between high-quality generative SE and low-latency
constraints.

</details>


### [271] [Real-time implementation of vibrato transfer as an audio effect](https://arxiv.org/abs/2509.21544)
*Jeremy Hyrkas*

Main category: cs.SD

TL;DR: 该论文介绍了一种改进的实时振动转移算法，具有更强的音频效果应用能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决原始振动转移算法在实时实施过程中的计算限制。

Method: 通过引入基本频率估计算法和时间域多相IIR滤波器来改进原有算法，以实现实时处理。

Result: 提出了一种新的实时近似算法，能够有效转移振动模式和幅度调制，超越传统延迟效果的能力。

Conclusion: 提出的算法能够实现实时音频效果，具有广泛的应用潜力。

Abstract: An algorithm for deriving delay functions based on real examples of vibrato
was recently introduced and can be used to perform a vibrato transfer, in which
the vibrato pattern of a target signal is imparted onto an incoming sound using
a delay line. The algorithm contains methods that computationally restrict a
real-time implementation. Here, a real-time approximation is presented that
incorporates an efficient fundamental frequency estimation algorithm and
time-domain polyphase IIR filters that approximate an analytic signal. The
vibrato transfer algorithm is further supplemented with a proposed method to
transfer the amplitude modulation of the target sound, moving this method
beyond the capabilities of typical delay-based vibrato effects. Modifications
to the original algorithm for real-time use are detailed here and available as
source code for an implementation as a VST plugin. This algorithm has
applications as an audio effect in sound design, sound morphing, and real-time
vibrato control of synthesized sounds.

</details>


### [272] [Preserving Russek's "Summermood" Using Reality Check and a DeltaLab DL-4 Approximation](https://arxiv.org/abs/2509.21560)
*Jeremy Hyrkas,Pablo Dodero Carrillo,Teresa Díaz de Cossio Sánchez*

Main category: cs.SD

TL;DR: 本文介绍了一套Pure Data补丁，用以模拟Antonio Russek的《Summermood》，以便在不依赖停产设备DL-4的情况下进行现场表演。


<details>
  <summary>Details</summary>
Motivation: 为了解决如何在没有原始硬件的情况下维护和表演电声作品的挑战。

Method: 通过比较乐谱和两份官方录音的设置，近似模拟DL-4的声音和功能，改进实施并进行回归测试。

Result: 成功开发了一套Pure Data补丁，使得《Summermood》能够适应不同的计算机环境，并确保其可演奏性。

Conclusion: 使用Pure Data补丁，能够在不再使用已停产的DL-4设备的情况下，继续进行Antonio Russek的《Summermood》作品的现场演出。

Abstract: As a contribution towards ongoing efforts to maintain electroacoustic
compositions for live performance, we present a collection of Pure Data patches
to preserve and perform Antonio Russek's piece "Summermood" for bass flute and
live electronics. The piece, originally written for the DeltaLab DL-4 delay
rack unit, contains score markings specific to the DL-4. Here, we approximate
the sound and unique functionality of the DL-4 in Pure Data, then refine our
implementation to better match the unit on which the piece was performed by
comparing settings from the score to two official recordings of the piece. The
DL-4 emulation is integrated into a patch for live performance based on the
Null Piece, and regression tested using the Reality Check framework for Pure
Data. Using this library of patches, Summermood can be brought back into live
rotation without the use of the now discontinued DL-4. The patches will be
continuously tested to ensure that the piece is playable across computer
environments and as the Pure Data programming language is updated.

</details>


### [273] [Guiding Audio Editing with Audio Language Model](https://arxiv.org/abs/2509.21625)
*Zitong Lan,Yiduo Hao,Mingmin Zhao*

Main category: cs.SD

TL;DR: SmartDJ是一个新型立体音频编辑框架，通过结合音频语言模型与潜在扩散技术，实现更好的编辑效果，克服了现有模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的音频编辑模型无法处理声明式音频编辑，其指令格式局限，且仅支持单声道音频。

Method: 将音频语言模型与潜在扩散模型相结合，分解高级指令为一系列原子编辑操作，并通过扩散模型执行这些操作。

Result: 实验结果表明，SmartDJ在感知质量、空间真实性和语义一致性方面优于先前的方法。

Conclusion: SmartDJ 在音频编辑中展示了更好的感知质量、空间真实性和语义对齐能力。

Abstract: Audio editing plays a central role in VR/AR immersion, virtual conferencing,
sound design, and other interactive media. However, recent generative audio
editing models depend on template-like instruction formats and are restricted
to mono-channel audio. These models fail to deal with declarative audio
editing, where the user declares what the desired outcome should be, while
leaving the details of editing operations to the system. We introduce SmartDJ,
a novel framework for stereo audio editing that combines the reasoning
capability of audio language models with the generative power of latent
diffusion. Given a high-level instruction, SmartDJ decomposes it into a
sequence of atomic edit operations, such as adding, removing, or spatially
relocating events. These operations are then executed by a diffusion model
trained to manipulate stereo audio. To support this, we design a data synthesis
pipeline that produces paired examples of high-level instructions, atomic edit
operations, and audios before and after each edit operation. Experiments
demonstrate that SmartDJ achieves superior perceptual quality, spatial realism,
and semantic alignment compared to prior audio editing methods. Demos are
available at https://zitonglan.github.io/project/smartdj/smartdj.html.

</details>


### [274] [MusicWeaver: Coherent Long-Range and Editable Music Generation from a Beat-Aligned Structural Plan](https://arxiv.org/abs/2509.21714)
*Xuanchen Wang,Heng Wang,Weidong Cai*

Main category: cs.SD

TL;DR: MusicWeaver是一种新的音乐生成模型，通过基于节拍的结构计划来提高长程结构的建模，提升了生成音乐的保真度和可控性。


<details>
  <summary>Details</summary>
Motivation: 当前的音乐生成模型主要捕捉局部纹理，但在建模长距离结构方面存在不足，导致生成音乐在节拍、部分过渡和编辑能力方面表现较弱。

Method: 该模型包括一个规划器，将提示翻译为编码音乐形式和作曲提示的结构计划，以及一个基于扩散的生成器，在该计划的指导下合成音乐。

Result: 实验表明，MusicWeaver通过引入结构一致性评分（SCS）和编辑忠实度评分（EFS）来评估生成和编辑质量，取得了优异的成果。

Conclusion: MusicWeaver在音乐生成的保真度和可控性方面达到了最先进水平，生成的音乐与人类创作的作品更为接近。

Abstract: Current music generators capture local textures but often fail to model
long-range structure, leading to off-beat outputs, weak section transitions,
and limited editing capability. We present MusicWeaver, a music generation
model conditioned on a beat-aligned structural plan. This plan serves as an
editable intermediate between the input prompt and the generated music,
preserving global form and enabling professional, localized edits. MusicWeaver
consists of a planner, which translates prompts into a structural plan encoding
musical form and compositional cues, and a diffusion-based generator, which
synthesizes music under the plan's guidance. To assess generation and editing
quality, we introduce two metrics: the Structure Coherence Score (SCS) for
evaluating long-range form and timing, and the Edit Fidelity Score (EFS) for
measuring the accuracy of realizing plan edits. Experiments demonstrate that
MusicWeaver achieves state-of-the-art fidelity and controllability, producing
music closer to human-composed works. Music results can be found on our project
page: https://musicweaver.github.io/.

</details>


### [275] [Frustratingly Easy Zero-Day Audio DeepFake Detection via Retrieval Augmentation and Profile Matching](https://arxiv.org/abs/2509.21728)
*Xuechen Liu,Xin Wang,Junichi Yamagishi*

Main category: cs.SD

TL;DR: 本研究提出了一种无需训练的音频深伪检测框架，成功应对零日攻击，性能与传统微调模型相当。


<details>
  <summary>Details</summary>
Motivation: 应对现代音频深伪检测器在面对新型合成方法（零日攻击）时的脆弱性，减少对模型微调的依赖。

Method: 基于知识表示、检索增强和声音特征匹配的方法进行检测。

Result: 所提框架在DeepFake-Eval-2024上表现与微调模型相当，并通过消融实验验证了相关属性的有效性。

Conclusion: 提出了一种无需训练的框架，用于零日音频深伪检测，该框架在效果上与微调模型相当。

Abstract: Modern audio deepfake detectors using foundation models and large training
datasets have achieved promising detection performance. However, they struggle
with zero-day attacks, where the audio samples are generated by novel synthesis
methods that models have not seen from reigning training data. Conventional
approaches against such attacks require fine-tuning the detectors, which can be
problematic when prompt response is required. This study introduces a
training-free framework for zero-day audio deepfake detection based on
knowledge representations, retrieval augmentation, and voice profile matching.
Based on the framework, we propose simple yet effective knowledge retrieval and
ensemble methods that achieve performance comparable to fine-tuned models on
DeepFake-Eval-2024, without any additional model-wise training. We also conduct
ablation studies on retrieval pool size and voice profile attributes,
validating their relevance to the system efficacy.

</details>


### [276] [Noise-to-Notes: Diffusion-based Generation and Refinement for Automatic Drum Transcription](https://arxiv.org/abs/2509.21739)
*Michael Yeung,Keisuke Toyama,Toya Teramoto,Shusuke Takahashi,Tamaki Kojima*

Main category: cs.SD

TL;DR: 本文将自动鼓谱写重构为条件生成任务，引入N2N框架，并通过加入MFM特征提升鲁棒性，取得新的性能基准。


<details>
  <summary>Details</summary>
Motivation: 将自动鼓谱写重新定义为条件生成任务，以实现更灵活的速度-准确性权衡和强大的修复能力。

Method: 采用扩散建模的条件生成框架N2N，利用改进的Annealed Pseudo-Huber损失进行联合优化。

Result: 实验证明包括MFM特征后，N2N在多个ADT基准上表现出最先进的性能。

Conclusion: N2N建立了ADT的新基准，显著提高了鲁棒性和性能。

Abstract: Automatic drum transcription (ADT) is traditionally formulated as a
discriminative task to predict drum events from audio spectrograms. In this
work, we redefine ADT as a conditional generative task and introduce
Noise-to-Notes (N2N), a framework leveraging diffusion modeling to transform
audio-conditioned Gaussian noise into drum events with associated velocities.
This generative diffusion approach offers distinct advantages, including a
flexible speed-accuracy trade-off and strong inpainting capabilities. However,
the generation of binary onset and continuous velocity values presents a
challenge for diffusion models, and to overcome this, we introduce an Annealed
Pseudo-Huber loss to facilitate effective joint optimization. Finally, to
augment low-level spectrogram features, we propose incorporating features
extracted from music foundation models (MFMs), which capture high-level
semantic information and enhance robustness to out-of-domain drum audio.
Experimental results demonstrate that including MFM features significantly
improves robustness and N2N establishes a new state-of-the-art performance
across multiple ADT benchmarks.

</details>


### [277] [Lightweight Front-end Enhancement for Robust ASR via Frame Resampling and Sub-Band Pruning](https://arxiv.org/abs/2509.21833)
*Siyi Zhao,Wei Wang,Yanmin Qian*

Main category: cs.SD

TL;DR: 本文提出了一种优化语音增强计算成本的方法，通过层级帧重采样和逐步子带剪枝提升自动语音识别在嘈杂环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管自动语音识别技术已取得进展，但在嘈杂环境下的稳健性仍然是一个挑战，而现有的语音增强方法通常带来较大的计算开销。

Method: 集成层级帧重采样和逐步子带剪枝的方法。帧重采样在层内对输入进行下采样，利用残差连接减少信息损失，而子带剪枝则逐步排除不太重要的频率带。

Result: 在合成和真实嘈杂数据集上的大量实验证明，与标准BSRNN相比，我们的系统将语音增强的计算开销减少了66以上，同时保持了强大的自动语音识别性能。

Conclusion: 该方法显著减少了语音增强的计算开销，同时保持了可接受的自动语音识别性能。

Abstract: Recent advancements in automatic speech recognition (ASR) have achieved
notable progress, whereas robustness in noisy environments remains challenging.
While speech enhancement (SE) front-ends are widely used to mitigate noise as a
preprocessing step for ASR, they often introduce computational non-negligible
overhead. This paper proposes optimizations to reduce SE computational costs
without compromising ASR performance. Our approach integrates layer-wise frame
resampling and progressive sub-band pruning. Frame resampling downsamples
inputs within layers, utilizing residual connections to mitigate information
loss. Simultaneously, sub-band pruning progressively excludes less informative
frequency bands, further reducing computational demands. Extensive experiments
on synthetic and real-world noisy datasets demonstrate that our system reduces
SE computational overhead over 66 compared to the standard BSRNN, while
maintaining strong ASR performance.

</details>


### [278] [Text2Move: Text-to-moving sound generation via trajectory prediction and temporal alignment](https://arxiv.org/abs/2509.21919)
*Yunyi Liu,Shaofan Yang,Kai Li,Xu Li*

Main category: cs.SD

TL;DR: 本研究提出了一种新的生成动态声音的框架，利用合成数据集和文本到轨迹模型，实现了根据文本提示在三维空间中生成移动的声音。


<details>
  <summary>Details</summary>
Motivation: 人类的听觉感知受到三维空间中移动声源的影响，但之前的生成声音建模研究主要集中在单声道信号或静态空间音频上。

Method: 构建了一个合成数据集，该数据集记录了移动声音的双耳格式、空间轨迹和音效的文本说明，然后训练了一个文本到轨迹预测模型，以及一个经过微调的文本到音频生成模型。

Result: 实验验证表明，文本到轨迹模型在空间理解方面表现良好，该方法能够方便地集成到现有的文本到音频生成工作流，并能够扩展到其他空间音频格式的移动声音生成。

Conclusion: 该研究提出了一种生成动态声音的新框架，可以根据文本提示生成三维空间中的移动声音，通过结合文本和轨迹的理解来实现动态空间音频的创作。

Abstract: Human auditory perception is shaped by moving sound sources in 3D space, yet
prior work in generative sound modelling has largely been restricted to mono
signals or static spatial audio. In this work, we introduce a framework for
generating moving sounds given text prompts in a controllable fashion. To
enable training, we construct a synthetic dataset that records moving sounds in
binaural format, their spatial trajectories, and text captions about the sound
event and spatial motion. Using this dataset, we train a text-to-trajectory
prediction model that outputs the three-dimensional trajectory of a moving
sound source given text prompts. To generate spatial audio, we first fine-tune
a pre-trained text-to-audio generative model to output temporally aligned mono
sound with the trajectory. The spatial audio is then simulated using the
predicted temporally-aligned trajectory. Experimental evaluation demonstrates
reasonable spatial understanding of the text-to-trajectory model. This approach
could be easily integrated into existing text-to-audio generative workflow and
extended to moving sound generation in other spatial audio formats.

</details>


### [279] [Decoding Deception: Understanding Automatic Speech Recognition Vulnerabilities in Evasion and Poisoning Attacks](https://arxiv.org/abs/2509.22060)
*Aravindhan G,Yuvaraj Govindarajulu,Parin Shah*

Main category: cs.SD

TL;DR: 本文探讨了自动语音识别系统中的对抗性攻击，尤其是成本效益高的白盒攻击和非可转移黑盒攻击，揭示了其脆弱性及对抗样本的安全隐患。


<details>
  <summary>Details</summary>
Motivation: 鉴于自动语音识别系统面临的安全挑战，研究旨在探讨新的对抗攻击方式，以识别和分析这些系统的弱点。

Method: 本文探讨了成本效益高的白盒攻击和非可转移的黑盒对抗攻击，结合了快速梯度符号法和零阶优化等方法进行实验与分析。

Result: 通过实验，我们发现混合模型能够产生微小扰动下影响显著的对抗样本，展示了这些对抗样本如何在35dB的信噪比条件下快速生成。

Conclusion: 本研究揭示了现代自动语音识别系统的脆弱性，强调了对抗性攻击在实际安全方面的隐患，并提出了改进措施的必要性。

Abstract: Recent studies have demonstrated the vulnerability of Automatic Speech
Recognition systems to adversarial examples, which can deceive these systems
into misinterpreting input speech commands. While previous research has
primarily focused on white-box attacks with constrained optimizations, and
transferability based black-box attacks against commercial Automatic Speech
Recognition devices, this paper explores cost efficient white-box attack and
non transferability black-box adversarial attacks on Automatic Speech
Recognition systems, drawing insights from approaches such as Fast Gradient
Sign Method and Zeroth-Order Optimization. Further, the novelty of the paper
includes how poisoning attack can degrade the performances of state-of-the-art
models leading to misinterpretation of audio signals. Through experimentation
and analysis, we illustrate how hybrid models can generate subtle yet impactful
adversarial examples with very little perturbation having Signal Noise Ratio of
35dB that can be generated within a minute. These vulnerabilities of
state-of-the-art open source model have practical security implications, and
emphasize the need for adversarial security.

</details>


### [280] [Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling](https://arxiv.org/abs/2509.22062)
*Junjie Cao,Yichen Han,Ruonan Zhang,Xiaoyang Hao,Hongxiang Li,Shuaijiang Zhao,Yue Liu,Xiao-Ping Zhng*

Main category: cs.SD

TL;DR: 本研究提出了一种新框架CaT-TTS，通过引入S3Codec和双Transformer架构，解决现有LLM基础文本到语音合成模型的主要缺陷，提高了合成的语义结构化和稳定性。


<details>
  <summary>Details</summary>
Motivation: 目前的LLM基础TTS系统在信息损失和生成稳定性方面存在重要挑战，需要改进。

Method: 引入S3Codec和双Transformer架构，用于理解与生成的解耦；使用Masked Audio Parallel Inference提高生成稳定性。

Result: 实现了更强的鲁棒性和语义基础的零-shot合成，提升了合成质量并减少了模型的学习负担。

Conclusion: 提出了一种新的框架CaT-TTS，解决了现有模型的几个关键挑战，特别是在信息结构和生成稳定性方面。

Abstract: Existing Large Language Model (LLM) based autoregressive (AR) text-to-speech
(TTS) systems, while achieving state-of-the-art quality, still face critical
challenges. The foundation of this LLM-based paradigm is the discretization of
the continuous speech waveform into a sequence of discrete tokens by neural
audio codec. However, single codebook modeling is well suited to text LLMs, but
suffers from significant information loss; hierarchical acoustic tokens,
typically generated via Residual Vector Quantization (RVQ), often lack explicit
semantic structure, placing a heavy learning burden on the model. Furthermore,
the autoregressive process is inherently susceptible to error accumulation,
which can degrade generation stability. To address these limitations, we
propose CaT-TTS, a novel framework for robust and semantically-grounded
zero-shot synthesis. First, we introduce S3Codec, a split RVQ codec that
injects explicit linguistic features into its primary codebook via semantic
distillation from a state-of-the-art ASR model, providing a structured
representation that simplifies the learning task. Second, we propose an
``Understand-then-Generate'' dual-Transformer architecture that decouples
comprehension from rendering. An initial ``Understanding'' Transformer models
the cross-modal relationship between text and the audio's semantic tokens to
form a high-level utterance plan. A subsequent ``Generation'' Transformer then
executes this plan, autoregressively synthesizing hierarchical acoustic tokens.
Finally, to enhance generation stability, we introduce Masked Audio Parallel
Inference (MAPI), a nearly parameter-free inference strategy that dynamically
guides the decoding process to mitigate local errors.

</details>


### [281] [Cross-Dialect Bird Species Recognition with Dialect-Calibrated Augmentation](https://arxiv.org/abs/2509.22317)
*Jiani Ding,Qiyang Sun,Alican Akman,Björn W. Schuller*

Main category: cs.SD

TL;DR: 研究提出了一种基于TDNN的框架，针对鸟类声音中的方言变异问题，通过多种技术提升了识别准确性。


<details>
  <summary>Details</summary>
Motivation: 方言变异妨碍了通过被动声学监测收集的鸟鸣自动识别。

Method: 采用时间延迟神经网络（TDNN）构建框架，并结合频率敏感归一化和梯度反转对抗训练，学习区域不变的嵌入。

Result: 完整系统在交叉方言准确度上相较于基准TDNN提升了最高20个百分点，同时保持了区域内表现。

Conclusion: 该研究表明，通过轻量级且具透明性的框架，可以实现对鸟类声音识别的方言适应能力。

Abstract: Dialect variation hampers automatic recognition of bird calls collected by
passive acoustic monitoring. We address the problem on DB3V, a three-region,
ten-species corpus of 8-s clips, and propose a deployable framework built on
Time-Delay Neural Networks (TDNNs). Frequency-sensitive normalisation (Instance
Frequency Normalisation and a gated Relaxed-IFN) is paired with
gradient-reversal adversarial training to learn region-invariant embeddings. A
multi-level augmentation scheme combines waveform perturbations, Mixup for rare
classes, and CycleGAN transfer that synthesises Region 2 (Interior
Plains)-style audio, , with Dialect-Calibrated Augmentation (DCA) softly
down-weighting synthetic samples to limit artifacts. The complete system lifts
cross-dialect accuracy by up to twenty percentage points over baseline TDNNs
while preserving in-region performance. Grad-CAM and LIME analyses show that
robust models concentrate on stable harmonic bands, providing ecologically
meaningful explanations. The study demonstrates that lightweight, transparent,
and dialect-resilient bird-sound recognition is attainable.

</details>


### [282] [Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach](https://arxiv.org/abs/2509.22378)
*Zijian Zhao,Dian Jin,Zijing Zhou*

Main category: cs.SD

TL;DR: 本文提出了一种基于视觉语言模型的图像到音乐生成框架，具备高可解释性与低计算成本，且在音乐质量及一致性上优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 针对现有I2M方法缺乏可解释性和计算资源需求高的挑战，提出一种具有高可解释性和低计算成本的解决方案。

Method: 提出一种基于视觉语言模型的I2M框架，利用ABC记谱法连接文本和音乐模态，并应用多模态检索增强生成和自我精炼技术。

Result: 通过人类研究和机器评估，验证了该方法在音乐生成质量和音乐与图像一致性方面的优越性。

Conclusion: 该方法在音乐质量和音乐-图像一致性方面优于其它方法，显示出良好的前景。

Abstract: Recently, Image-to-Music (I2M) generation has garnered significant attention,
with potential applications in fields such as gaming, advertising, and
multi-modal art creation. However, due to the ambiguous and subjective nature
of I2M tasks, most end-to-end methods lack interpretability, leaving users
puzzled about the generation results. Even methods based on emotion mapping
face controversy, as emotion represents only a singular aspect of art.
Additionally, most learning-based methods require substantial computational
resources and large datasets for training, hindering accessibility for common
users. To address these challenges, we propose the first Vision Language Model
(VLM)-based I2M framework that offers high interpretability and low
computational cost. Specifically, we utilize ABC notation to bridge the text
and music modalities, enabling the VLM to generate music using natural
language. We then apply multi-modal Retrieval-Augmented Generation (RAG) and
self-refinement techniques to allow the VLM to produce high-quality music
without external training. Furthermore, we leverage the generated motivations
in text and the attention maps from the VLM to provide explanations for the
generated results in both text and image modalities. To validate our method, we
conduct both human studies and machine evaluations, where our method
outperforms others in terms of music quality and music-image consistency,
indicating promising results. Our code is available at
https://github.com/RS2002/Image2Music .

</details>


### [283] [From Coarse to Fine: Recursive Audio-Visual Semantic Enhancement for Speech Separation](https://arxiv.org/abs/2509.22425)
*Ke Xue,Rongfei Fan,Lixin,Dawei Zhao,Chao Zhu,Han Hu*

Main category: cs.SD

TL;DR: CSFNet通过递归语义增强框架，实现了音频-视觉语音分离的显著提升，表现出最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 由于现有方法未能充分利用视觉信息，本文旨在通过引入递归语义增强方法来提升音频-视觉语音分离的效果。

Method: 提出了一个分为粗略分离和细致分离两个阶段的CSFNet模型，并结合音频-视觉语音识别模型进行递归处理。

Result: 在三个基准数据集和两个噪声数据集上，CSFNet达到了最先进的表现，显示出显著的从粗到细的改进。

Conclusion: CSFNet在音频-视觉语音分离方面表现出色，并验证了递归语义增强框架的必要性和有效性。

Abstract: Audio-visual speech separation aims to isolate each speaker's clean voice
from mixtures by leveraging visual cues such as lip movements and facial
features. While visual information provides complementary semantic guidance,
existing methods often underexploit its potential by relying on static visual
representations. In this paper, we propose CSFNet, a Coarse-to-Separate-Fine
Network that introduces a recursive semantic enhancement paradigm for more
effective separation. CSFNet operates in two stages: (1) Coarse Separation,
where a first-pass estimation reconstructs a coarse audio waveform from the
mixture and visual input; and (2) Fine Separation, where the coarse audio is
fed back into an audio-visual speech recognition (AVSR) model together with the
visual stream. This recursive process produces more discriminative semantic
representations, which are then used to extract refined audio. To further
exploit these semantics, we design a speaker-aware perceptual fusion block to
encode speaker identity across modalities, and a multi-range spectro-temporal
separation network to capture both local and global time-frequency patterns.
Extensive experiments on three benchmark datasets and two noisy datasets show
that CSFNet achieves state-of-the-art (SOTA) performance, with substantial
coarse-to-fine improvements, validating the necessity and effectiveness of our
recursive semantic enhancement framework.

</details>


### [284] [MDAR: A Multi-scene Dynamic Audio Reasoning Benchmark](https://arxiv.org/abs/2509.22461)
*Hui Li,Changhao Jiang,Hongyu Wang,Ming Zhang,Jiajun Sun,Zhixiong Yang,Yifei Cao,Shihan Dou,Xiaoran Fan,Baoyu Fan,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.SD

TL;DR: MDAR基准为复杂音频推理提供了新的考量，展示了现有模型的不足，并为音频推理研究提供了重要价值。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要集中在静态或单一场景设置上，无法充分捕捉多说话者、动态事件和异质音源互动的场景。

Method: 通过建立MDAR基准及其相关的3000个问答对，针对复杂的多场景音频推理任务进行评估。

Result: 在26个最先进的音频语言模型中，模型在复杂推理任务上表现出局限性，最高准确率为76.67%。没有任何模型在三个问题类型上实现80%的性能。

Conclusion: MDAR基准展示了当前音频语言模型在复杂推理任务中的局限性，并强调了MDAR作为音频推理研究的价值。

Abstract: The ability to reason from audio, including speech, paralinguistic cues,
environmental sounds, and music, is essential for AI agents to interact
effectively in real-world scenarios. Existing benchmarks mainly focus on static
or single-scene settings and do not fully capture scenarios where multiple
speakers, unfolding events, and heterogeneous audio sources interact. To
address these challenges, we introduce MDAR, a benchmark for evaluating models
on complex, multi-scene, and dynamically evolving audio reasoning tasks. MDAR
comprises 3,000 carefully curated question-answer pairs linked to diverse audio
clips, covering five categories of complex reasoning and spanning three
question types. We benchmark 26 state-of-the-art audio language models on MDAR
and observe that they exhibit limitations in complex reasoning tasks. On
single-choice questions, Qwen2.5-Omni (open-source) achieves 76.67% accuracy,
whereas GPT-4o Audio (closed-source) reaches 68.47%; however, GPT-4o Audio
substantially outperforms Qwen2.5-Omni on the more challenging multiple-choice
and open-ended tasks. Across all three question types, no model achieves 80%
performance. These findings underscore the unique challenges posed by MDAR and
its value as a benchmark for advancing audio reasoning research.Code and
benchmark can be found at https://github.com/luckyerr/MDAR.

</details>

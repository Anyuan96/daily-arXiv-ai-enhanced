<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 12]
- [cs.CL](#cs.CL) [Total: 6]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning](https://arxiv.org/abs/2511.21688)
*Wenbo Hu,Jingli Lin,Yilin Long,Yunlong Ran,Lihan Jiang,Yifan Wang,Chenming Zhu,Runsen Xu,Tai Wang,Jiangmiao Pang*

Main category: cs.CV

TL;DR: G²VLM是一个几何基础视觉-语言模型，通过在视觉-语言模型中融入3D几何学习，提升空间理解和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型在空间智能方面表现不佳，主要原因是缺乏从2D图像重建3D空间的视觉几何学习过程。

Method: 提出了G²VLM模型，统一了空间3D重建和空间理解两个任务。模型利用学习到的3D视觉几何特征直接预测3D属性，并通过上下文学习和交错推理增强空间推理任务。

Result: G²VLM在两个任务上都表现出色：在3D重建任务上达到与最先进的反馈式模型相当的结果，在空间理解和推理任务上取得更好或具有竞争力的结果。

Conclusion: 通过将语义强大的VLM与底层3D视觉任务统一，G²VLM可以作为社区的未来基准，有望解锁更多应用如3D场景编辑。

Abstract: Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.

</details>


### [2] [MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training](https://arxiv.org/abs/2511.21592)
*Haotian Xue,Qi Chen,Zhonghao Wang,Xun Huang,Eli Shechtman,Jinrong Xie,Yongxin Chen*

Main category: cs.CV

TL;DR: MoGAN 是一个专门针对视频扩散模型的运动优化框架，通过光学流鉴别器和分布匹配正则化来提升生成视频的运动连贯性和真实性，无需人类偏好数据或奖励模型。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在帧级保真度方面表现良好，但在运动连贯性、动态性和真实性方面存在不足，容易出现抖动、重影或不合理的动态效果。标准去噪MSE目标缺乏对时间一致性的直接监督，导致模型在损失较低的情况下仍生成运动质量差的视频。

Method: MoGAN基于3步蒸馏视频扩散模型，训练基于DiT的光学流鉴别器来区分真实与生成的运动，并结合分布匹配正则化器来保持视觉保真度。该框架无需奖励模型或人类偏好数据，通过运动中心的鉴别器训练来直接优化运动质量。

Result: 在Wan2.1-T2V-1.3B上的实验表明，MoGAN显著提升了运动质量：在VBench上运动得分比50步教师模型提升7.3%，比3步DMD模型提升13.3%；在VideoJAM-Bench上分别提升7.4%和8.8%，同时保持甚至改善了美学和图像质量得分。人类研究也确认MoGAN在运动质量上的优势。

Conclusion: MoGAN在不牺牲视觉保真度或效率的前提下，显著提升了生成视频的运动真实性，为快速高质量视频生成提供了一条实用路径。

Abstract: Video diffusion models achieve strong frame-level fidelity but still struggle with motion coherence, dynamics and realism, often producing jitter, ghosting, or implausible dynamics. A key limitation is that the standard denoising MSE objective provides no direct supervision on temporal consistency, allowing models to achieve low loss while still generating poor motion. We propose MoGAN, a motion-centric post-training framework that improves motion realism without reward models or human preference data. Built atop a 3-step distilled video diffusion model, we train a DiT-based optical-flow discriminator to differentiate real from generated motion, combined with a distribution-matching regularizer to preserve visual fidelity. With experiments on Wan2.1-T2V-1.3B, MoGAN substantially improves motion quality across benchmarks. On VBench, MoGAN boosts motion score by +7.3% over the 50-step teacher and +13.3% over the 3-step DMD model. On VideoJAM-Bench, MoGAN improves motion score by +7.4% over the teacher and +8.8% over DMD, while maintaining comparable or even better aesthetic and image-quality scores. A human study further confirms that MoGAN is preferred for motion quality (52% vs. 38% for the teacher; 56% vs. 29% for DMD). Overall, MoGAN delivers significantly more realistic motion without sacrificing visual fidelity or efficiency, offering a practical path toward fast, high-quality video generation. Project webpage is: https://xavihart.github.io/mogan.

</details>


### [3] [ReSAM: Refine, Requery, and Reinforce: Self-Prompting Point-Supervised Segmentation for Remote Sensing Images](https://arxiv.org/abs/2511.21606)
*M. Naseer Subhani*

Main category: cs.CV

TL;DR: 提出了一种点监督的自提示框架，通过Refine-Requery-Reinforce循环机制，仅使用稀疏点注释就能让SAM模型适配遥感图像分割任务。


<details>
  <summary>Details</summary>
Motivation: 解决SAM模型在遥感图像上因领域偏移和密集标注稀缺导致的性能不佳问题，探索仅使用稀疏点标注的高效适配方法。

Method: 采用Refine-Requery-Reinforce循环：从初始点生成粗伪掩码（Refine），用自构建的框提示改进（Requery），通过迭代间嵌入对齐减少确认偏差（Reinforce）。

Result: 在WHU、HRSID和NWPU VHR-10三个遥感基准数据集上，该方法均超越了预训练SAM和现有点监督分割方法。

Conclusion: 自提示和语义对齐为遥感应用中基础分割模型的可扩展点级适配提供了高效路径。

Abstract: Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM's segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.

</details>


### [4] [Active Learning for GCN-based Action Recognition](https://arxiv.org/abs/2511.21625)
*Hichem Sahbi*

Main category: cs.CV

TL;DR: 提出了一种标签高效的GCN模型用于骨架动作识别，通过对抗性策略选择代表性样本，结合双向稳定GCN架构，在小样本场景下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图卷积网络在骨架动作识别中依赖大量标注数据，但实际应用中标注数据往往稀缺，需要开发标签高效的解决方案。

Method: 1) 提出对抗性采集函数选择代表性、多样性和不确定性平衡的样本标签；2) 设计双向稳定GCN架构，改进环境空间与潜在空间的映射关系。

Result: 在两个具有挑战性的骨架动作识别基准测试中，该方法相比现有工作取得了显著改进。

Conclusion: 所提出的标签高效GCN模型有效解决了标注数据稀缺问题，通过智能样本选择和网络架构优化实现了优越的识别性能。

Abstract: Despite the notable success of graph convolutional networks (GCNs) in skeleton-based action recognition, their performance often depends on large volumes of labeled data, which are frequently scarce in practical settings. To address this limitation, we propose a novel label-efficient GCN model. Our work makes two primary contributions. First, we develop a novel acquisition function that employs an adversarial strategy to identify a compact set of informative exemplars for labeling. This selection process balances representativeness, diversity, and uncertainty. Second, we introduce bidirectional and stable GCN architectures. These enhanced networks facilitate a more effective mapping between the ambient and latent data spaces, enabling a better understanding of the learned exemplar distribution. Extensive evaluations on two challenging skeleton-based action recognition benchmarks reveal significant improvements achieved by our label-efficient GCNs compared to prior work.

</details>


### [5] [Qwen3-VL Technical Report](https://arxiv.org/abs/2511.21631)
*Shuai Bai,Yuxuan Cai,Ruizhe Chen,Keqin Chen,Xionghui Chen,Zesen Cheng,Lianghao Deng,Wei Ding,Chang Gao,Chunjiang Ge,Wenbin Ge,Zhifang Guo,Qidong Huang,Jie Huang,Fei Huang,Binyuan Hui,Shutong Jiang,Zhaohai Li,Mingsheng Li,Mei Li,Kaixin Li,Zicheng Lin,Junyang Lin,Xuejing Liu,Jiawei Liu,Chenglong Liu,Yang Liu,Dayiheng Liu,Shixuan Liu,Dunjie Lu,Ruilin Luo,Chenxu Lv,Rui Men,Lingchen Meng,Xuancheng Ren,Xingzhang Ren,Sibo Song,Yuchong Sun,Jun Tang,Jianhong Tu,Jianqiang Wan,Peng Wang,Pengfei Wang,Qiuyue Wang,Yuxuan Wang,Tianbao Xie,Yiheng Xu,Haiyang Xu,Jin Xu,Zhibo Yang,Mingkun Yang,Jianxin Yang,An Yang,Bowen Yu,Fei Zhang,Hang Zhang,Xi Zhang,Bo Zheng,Humen Zhong,Jingren Zhou,Fan Zhou,Jing Zhou,Yuanzhi Zhu,Ke Zhu*

Main category: cs.CV

TL;DR: Qwen3-VL是目前Qwen系列中最强大的视觉语言模型，在多种多模态基准测试中表现出色，支持256K token的文本、图像和视频交错上下文，包含密集型和MoE两种架构变体。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够无缝集成文本、图像和视频，具备强大长上下文理解和多模态推理能力的视觉语言模型，以满足现实工作流程中对图像推理、智能决策和多模态代码智能的需求。

Method: 采用增强的交错MRoPE技术加强时空建模，集成DeepStack以利用多级ViT特征强化视觉语言对齐，以及基于文本的时间对齐技术从T-RoPE演进到显式时间戳对齐。

Result: 在可比token预算和延迟约束下，Qwen3-VL在密集和MoE架构中均实现了优越性能，在MMMU、MathVista等综合评估中表现领先。

Conclusion: Qwen3-VL可作为基于图像的推理、智能决策和多模态代码智能的基础引擎，适用于现实工作流程。

Abstract: We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.

</details>


### [6] [Continual Error Correction on Low-Resource Devices](https://arxiv.org/abs/2511.21652)
*Kirill Paramonov,Mete Ozay,Aristeidis Mystakidis,Nikolaos Tsalikidis,Dimitrios Sotos,Anastasios Drosou,Dimitrios Tzovaras,Hyunjun Kim,Kiseok Chang,Sangdok Mo,Namwoong Kim,Woojong Yoo,Jijoong Moon,Umberto Michieli*

Main category: cs.CV

TL;DR: 提出了一种通过少样本学习实现在资源受限设备上高效纠正AI误分类的新系统，结合服务器端基础模型训练和设备端原型分类，只需更新原型而非重新训练模型即可修正错误。


<details>
  <summary>Details</summary>
Motivation: AI模型在日常设备中的普及突显了预测错误影响用户体验的关键挑战，现有解决方案主要关注错误检测，但缺乏高效的纠错机制。

Method: 系统包含两个关键组件：1）服务器端利用知识蒸馏将基础模型的鲁棒特征表示迁移到设备兼容架构；2）设备端通过原型适应实现超高效错误纠正。

Result: 在Food-101和Flowers-102数据集上，一次性场景中实现超过50%的错误纠正，同时遗忘率低于0.02%，计算开销可忽略。

Conclusion: 通过Android演示应用验证了系统在实际场景中的实用性，为资源受限设备提供了高效的AI错误纠正解决方案。

Abstract: The proliferation of AI models in everyday devices has highlighted a critical challenge: prediction errors that degrade user experience. While existing solutions focus on error detection, they rarely provide efficient correction mechanisms, especially for resource-constrained devices. We present a novel system enabling users to correct AI misclassifications through few-shot learning, requiring minimal computational resources and storage. Our approach combines server-side foundation model training with on-device prototype-based classification, enabling efficient error correction through prototype updates rather than model retraining. The system consists of two key components: (1) a server-side pipeline that leverages knowledge distillation to transfer robust feature representations from foundation models to device-compatible architectures, and (2) a device-side mechanism that enables ultra-efficient error correction through prototype adaptation. We demonstrate our system's effectiveness on both image classification and object detection tasks, achieving over 50% error correction in one-shot scenarios on Food-101 and Flowers-102 datasets while maintaining minimal forgetting (less than 0.02%) and negligible computational overhead. Our implementation, validated through an Android demonstration app, proves the system's practicality in real-world scenarios.

</details>


### [7] [CaFlow: Enhancing Long-Term Action Quality Assessment with Causal Counterfactual Flow](https://arxiv.org/abs/2511.21653)
*Ruisheng Han,Kanglei Zhou,Shuang Chen,Amir Atapour-Abarghouei,Hubert P. H. Shum*

Main category: cs.CV

TL;DR: CaFlow是一个针对长时动作质量评估（AQA）的统一框架，结合反事实解耦和双向时间流建模，解决长时序动态建模和语境混淆问题。


<details>
  <summary>Details</summary>
Motivation: 长时AQA（如花样滑冰、艺术体操）需要建模长时程动态并抵抗语境混淆，现有方法依赖昂贵标注或单向时序建模，易受伪相关影响。

Method: CaFlow包含因果反事实正则化（CCR）模块自监督解耦因果和混淆特征，以及BiT-Flow模块通过双向时序流和循环一致性约束建模平滑时序表示。

Result: 在多个长时AQA基准测试中，CaFlow取得了最先进的性能。

Conclusion: CaFlow通过因果解耦和双向时序建模有效提升了长时动作质量评估的鲁棒性和准确性。

Abstract: Action Quality Assessment (AQA) predicts fine-grained execution scores from action videos and is widely applied in sports, rehabilitation, and skill evaluation. Long-term AQA, as in figure skating or rhythmic gymnastics, is especially challenging since it requires modeling extended temporal dynamics while remaining robust to contextual confounders. Existing approaches either depend on costly annotations or rely on unidirectional temporal modeling, making them vulnerable to spurious correlations and unstable long-term representations. To this end, we propose CaFlow, a unified framework that integrates counterfactual de-confounding with bidirectional time-conditioned flow. The Causal Counterfactual Regularization (CCR) module disentangles causal and confounding features in a self-supervised manner and enforces causal robustness through counterfactual interventions, while the BiT-Flow module models forward and backward dynamics with a cycle-consistency constraint to produce smoother and more coherent representations. Extensive experiments on multiple long-term AQA benchmarks demonstrate that CaFlow achieves state-of-the-art performance. Code is available at https://github.com/Harrison21/CaFlow

</details>


### [8] [Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following](https://arxiv.org/abs/2511.21662)
*Tianyi Xiong,Yi Ge,Ming Li,Zuolong Zhang,Pranav Kulkarni,Kaishen Wang,Qi He,Zeying Zhu,Chenxi Liu,Ruibo Chen,Tong Zheng,Yanshuo Chen,Xiyao Wang,Renrui Zhang,Wenhu Chen,Heng Huang*

Main category: cs.CV

TL;DR: Multi-Crit是一个基准测试，用于评估多模态模型在遵循多样化评估标准和生成可靠标准级别判断方面的能力。该基准涵盖开放式生成和可验证推理任务，通过严格的数据收集流程构建，并引入三个新指标来系统评估标准遵循性、标准切换灵活性和识别标准级别偏好冲突的能力。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型（LMMs）越来越多地用作多模态评估系统，但其遵循多样化、细粒度评估标准的能力尚未得到充分探索。作者旨在解决这一问题，开发一个基准来评估模型在遵循多元标准和生成可靠判断方面的表现。

Method: 通过严格的数据收集流程构建Multi-Crit基准，收集具有多标准人工标注的挑战性响应对。该基准涵盖开放式生成和可验证推理任务，并引入了三个新指标：多元遵循性、标准切换灵活性和识别标准级别偏好冲突的能力。

Result: 对25个LMMs的综合分析表明：1）专有模型在保持对多元标准的一致性遵循方面仍存在困难，特别是在开放式评估中；2）开源模型在灵活遵循多样化标准方面进一步落后；3）使用整体判断信号进行的批评微调虽然增强了视觉基础能力，但未能推广到多元标准级别的判断。

Conclusion: Multi-Crit为构建可靠和可引导的多模态AI评估奠定了基础。该研究揭示了当前多模态评估模型的局限性，并提出了改进方向，特别是在标准遵循性和灵活性方面。

Abstract: Large multimodal models (LMMs) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences. However, their ability to follow diverse, fine-grained evaluation criteria remains underexplored. We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments. Covering both open-ended generation and verifiable reasoning tasks, Multi-Crit is built through a rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations. It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehensive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteria--especially in open-ended evaluation; 2) open-source models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning, test-time scaling, and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As a pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation.

</details>


### [9] [Attention-Guided Patch-Wise Sparse Adversarial Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2511.21663)
*Naifu Zhang,Wei Tao,Xi Xiao,Qianpu Sun,Yuxin Zheng,Wentao Mo,Peiqiang Wang,Nan Zhang*

Main category: cs.CV

TL;DR: ADVLA框架通过在视觉编码器到文本特征空间的投影特征上直接应用对抗扰动，高效破坏VLA模型的下游动作预测，具有低幅度约束和稀疏关注特性，显著优于传统基于补丁的攻击方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型的对抗攻击方法需要昂贵的端到端训练，且常产生明显扰动补丁。需要解决这些限制，提高攻击效率和隐蔽性。

Method: 提出了ADVLA框架，直接在视觉编码器投影到文本特征空间的特征上应用对抗扰动。采用三种策略增强敏感性、强制稀疏性和集中扰动。结合Top-K掩码实现局部稀疏攻击。

Result: 在L∞=4/255约束下，ADVLA+Top-K掩码修改少于10%的补丁，攻击成功率近100%。扰动集中在关键区域，整体图像中几乎不可察觉，单步迭代仅需0.06秒，显著优于传统补丁攻击。

Conclusion: ADVLA在低幅度和局部稀疏条件下有效削弱VLA模型的下游动作预测，避免了传统攻击的高训练成本和明显扰动，对攻击VLA特征空间具有独特有效性和实用价值。

Abstract: In recent years, Vision-Language-Action (VLA) models in embodied intelligence have developed rapidly. However, existing adversarial attack methods require costly end-to-end training and often generate noticeable perturbation patches. To address these limitations, we propose ADVLA, a framework that directly applies adversarial perturbations on features projected from the visual encoder into the textual feature space. ADVLA efficiently disrupts downstream action predictions under low-amplitude constraints, and attention guidance allows the perturbations to be both focused and sparse. We introduce three strategies that enhance sensitivity, enforce sparsity, and concentrate perturbations. Experiments demonstrate that under an $L_{\infty}=4/255$ constraint, ADVLA combined with Top-K masking modifies less than 10% of the patches while achieving an attack success rate of nearly 100%. The perturbations are concentrated on critical regions, remain almost imperceptible in the overall image, and a single-step iteration takes only about 0.06 seconds, significantly outperforming conventional patch-based attacks. In summary, ADVLA effectively weakens downstream action predictions of VLA models under low-amplitude and locally sparse conditions, avoiding the high training costs and conspicuous perturbations of traditional patch attacks, and demonstrates unique effectiveness and practical value for attacking VLA feature spaces.

</details>


### [10] [Revolutionizing Glioma Segmentation & Grading Using 3D MRI - Guided Hybrid Deep Learning Models](https://arxiv.org/abs/2511.21673)
*Pandiyaraju V,Sreya Mynampati,Abishek Karthik,Poovarasan L,D. Saraswathi*

Main category: cs.CV

TL;DR: 该研究提出了一种融合U-Net分割和DenseNet-VGG分类的混合深度学习模型，结合多头注意力和空间通道注意力机制，用于胶质瘤的早期诊断和分级。


<details>
  <summary>Details</summary>
Motivation: 胶质瘤作为高死亡率脑肿瘤，早期准确诊断对治疗干预至关重要，现有方法在精确分割和分类方面存在挑战。

Method: 开发混合模型：U-Net进行3D MRI肿瘤分割，结合DenseNet和VGG的混合分类网络，集成多头注意力和空间通道注意力机制，通过标准化、重采样和数据增强预处理3D MRI数据。

Result: 实验显示该框架在肿瘤分割中达到98%的Dice系数，分类准确率达99%，优于传统CNN和无注意力方法。

Conclusion: 该框架通过注意力机制增强临床相关特征的关注，提高了可解释性和准确性，在胶质瘤及时可靠诊断和分级方面具有巨大潜力，有助于改善患者治疗规划。

Abstract: Gliomas are brain tumor types that have a high mortality rate which means early and accurate diagnosis is important for therapeutic intervention for the tumors. To address this difficulty, the proposed research will develop a hybrid deep learning model which integrates U-Net based segmentation and a hybrid DenseNet-VGG classification network with multihead attention and spatial-channel attention capabilities. The segmentation model will precisely demarcate the tumors in a 3D volume of MRI data guided by spatial and contextual information. The classification network which combines a branch of both DenseNet and VGG, will incorporate the demarcated tumor on which features with attention mechanisms would be focused on clinically relevant features. High-dimensional 3D MRI data could successfully be utilized in the model through preprocessing steps which are normalization, resampling, and data augmentation. Through a variety of measures the framework is evaluated: measures of performance in segmentation are Dice coefficient and Mean Intersection over Union (IoU) and measures of performance in classification are accuracy precision, recall, and F1-score. The hybrid framework that has been proposed has demonstrated through physical testing that it has the capability of obtaining a Dice coefficient of 98% in tumor segmentation, and 99% on classification accuracy, outperforming traditional CNN models and attention-free methods. Utilizing multi-head attention mechanisms enhances notions of priority in aspects of the tumor that are clinically significant, and enhances interpretability and accuracy. The results suggest a great potential of the framework in facilitating the timely and reliable diagnosis and grading of glioma by clinicians is promising, allowing for better planning of patient treatment.

</details>


### [11] [Seeing without Pixels: Perception from Camera Trajectories](https://arxiv.org/abs/2511.21681)
*Zihui Xue,Kristen Grauman,Dima Damen,Andrew Zisserman,Tengda Han*

Main category: cs.CV

TL;DR: 本文首次系统研究了仅通过相机轨迹感知视频内容的可能性，提出了CamFormer框架将相机轨迹与自然语言对齐，发现相机轨迹是一种轻量、鲁棒且多功能的视频内容感知模态。


<details>
  <summary>Details</summary>
Motivation: 探索是否能仅从相机轨迹（而非像素）来感知视频内容，研究相机路径所包含的信息丰富性及其在视频理解中的潜力。

Method: 提出对比学习框架训练CamFormer编码器，将相机姿态轨迹投影到联合嵌入空间，与自然语言对齐，适用于多种下游任务。

Result: 发现相机轨迹是异常信息丰富的信号，能够揭示行为内容（自我中心）或观察内容（他者中心），且对不同相机姿态估计方法具有鲁棒性。

Conclusion: 相机轨迹作为一种轻量级、鲁棒且多功能的模态，能够有效感知视频内容，为视频理解提供了新的视角和方法。

Abstract: Can one perceive a video's content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, "how you move" can indeed reveal "what you are doing" (egocentric) or "observing" (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.

</details>


### [12] [Canvas-to-Image: Compositional Image Generation with Multimodal Controls](https://arxiv.org/abs/2511.21691)
*Yusuf Dalva,Guocheng Gordon Qian,Maya Goldenberg,Tsai-Shien Chen,Kfir Aberman,Sergey Tulyakov,Pinar Yanardag,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [13] [Auxiliary Metrics Help Decoding Skill Neurons in the Wild](https://arxiv.org/abs/2511.21610)
*Yixiu Zhao,Xiaozhi Wang,Zijun Yao,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 该论文提出一种简单、轻量且广泛适用的方法来识别LLM中编码特定技能的神经元。该方法基于软提示训练识别'技能神经元'，并通过神经元激活与外部标签和模型置信度等辅助指标的相关性分析，揭示了可解释的任务特定行为。实验验证了该方法在开放文本生成和自然语言推理任务中的有效性，并发现了BigBench算术推理中先前未知的捷径。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型展现出强大的多任务能力，但其内部机制仍然不透明。本研究旨在开发一种简单有效的方法来识别编码特定技能的神经元，以增强模型的可解释性。

Method: 基于软提示训练识别技能神经元，并将分析扩展到多技能复杂场景。通过神经元激活与外部标签、模型置信度等辅助指标的相关性分析，无需手动标记聚合即可发现可解释的任务特定行为。

Result: 在开放文本生成和自然语言推理任务上的实证验证表明，该方法不仅能检测已知技能相关的神经元，还在BigBench算术推理任务中发现了先前未知的捷径。

Conclusion: 提出的方法简单有效，能够揭示LLM中技能编码的神经元机制，为模型可解释性研究提供了新的工具和见解。

Abstract: Large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, yet their internal mechanisms remain largely opaque. In this paper, we introduce a simple, lightweight, and broadly applicable method with a focus on isolating neurons that encode specific skills. Building upon prior work that identified "skill neurons" via soft prompt training on classification tasks, our approach extends the analysis to complex scenarios involving multiple skills. We correlate neuron activations with auxiliary metrics -- such as external labels and the model's own confidence score -- thereby uncovering interpretable and task-specific behaviors without the need for manual token aggregation. We empirically validate our method on tasks spanning open-ended text generation and natural language inference, demonstrating its ability to detect neurons that not only drive known skills but also reveal previously unidentified shortcuts in arithmetic reasoning on BigBench.

</details>


### [14] [Beyond URLs: Metadata Diversity and Position for Efficient LLM Pretraining](https://arxiv.org/abs/2511.21613)
*Dongyang Fan,Diba Hashemi,Sai Praneeth Karimireddy,Martin Jaggi*

Main category: cs.CL

TL;DR: 本文研究发现多种元数据类型可以加速大语言模型预训练，特别是细粒度的文档质量指标，并提出元数据追加、可学习元标记等方法进一步优化训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅关注URL这一种元数据信号，但其他形式的元数据可能带来更大收益，因此需要系统探索不同类型元数据对LLM预训练的影响。

Method: 研究多种元数据类型（如文档质量指标），采用元数据前置和追加策略，引入可学习元标记配合掩码损失，并通过探针分析潜在表征。

Result: 发现细粒度元数据能有效加速预训练，元数据追加作为辅助任务可提升训练效率，可学习元标记能通过质量感知的潜在结构恢复部分加速效果。

Conclusion: 研究结果为整合元数据提升LLM预训练效率和效果提供了实用指导，证明细粒度元数据是加速训练的关键特征。

Abstract: Incorporating metadata in Large Language Models (LLMs) pretraining has recently emerged as a promising approach to accelerate training. However prior work highlighted only one useful signal-URLs, leaving open the question of whether other forms of metadata could yield greater benefits. In this study, we investigate a wider range of metadata types and find other types of metadata, such as fine-grained indicators of document quality that can also accelerate pretraining when prepended. We identify a common feature among effective metadata: they encode information at a finer granularity. We further introduce metadata appending as a means of improving training efficiency, where predicting an appropriate metadata as auxiliary task can help speed up pretraining. In addition, learnable meta-tokens trained with masked loss can recover part of the speedup by inducing quality-aware latent structure. Using probing, we analyze latent representations to understand how metadata shapes learning. Together, these results yield practical guidelines for integrating metadata to improve both the efficiency and effectiveness of LLM pretraining.

</details>


### [15] [The author is dead, but what if they never lived? A reception experiment on Czech AI- and human-authored poetry](https://arxiv.org/abs/2511.21629)
*Anna Marklová,Ondřej Vinš,Martina Vokáčová,Jiří Milička*

Main category: cs.CL

TL;DR: AI-generated Czech poetry is indistinguishable from human-written poetry, with participants achieving only chance-level accuracy in authorship identification. Aesthetic evaluations show a bias against poems believed to be AI-generated, though AI poems were actually rated equally or more favorably than human ones.


<details>
  <summary>Details</summary>
Motivation: To examine the perception of AI- and human-written Czech poetry, specifically whether native speakers can identify authorship and how they aesthetically judge the poems, given that most studies focus on English.

Method: Participants (Czech native speakers) were asked to guess the authorship of poems (AI-generated vs. human-written) and provide aesthetic evaluations. A logistic regression model was used to analyze the relationship between liking a poem and accurate authorship assignment.

Result: Participants performed at chance level in authorship identification (45.8% correct on average). Aesthetic evaluations revealed a bias: poems believed to be AI-generated were rated less favorably, even though AI poems were rated equally or more favorably. The more participants liked a poem, the less likely they were to accurately assign authorship. Familiarity with poetry or literary background had no effect on recognition accuracy.

Conclusion: AI can convincingly produce poetry even in a morphologically complex, low-resource language like Czech. Readers' beliefs about authorship and aesthetic evaluation are interconnected, with biases affecting perceptions regardless of actual quality.

Abstract: Large language models are increasingly capable of producing creative texts, yet most studies on AI-generated poetry focus on English -- a language that dominates training data. In this paper, we examine the perception of AI- and human-written Czech poetry. We ask if Czech native speakers are able to identify it and how they aesthetically judge it. Participants performed at chance level when guessing authorship (45.8\% correct on average), indicating that Czech AI-generated poems were largely indistinguishable from human-written ones. Aesthetic evaluations revealed a strong authorship bias: when participants believed a poem was AI-generated, they rated it as less favorably, even though AI poems were in fact rated equally or more favorably than human ones on average. The logistic regression model uncovered that the more the people liked a poem, the less probable was that they accurately assign the authorship. Familiarity with poetry or literary background had no effect on recognition accuracy. Our findings show that AI can convincingly produce poetry even in a morphologically complex, low-resource (with respect of the training data of AI models) Slavic language such as Czech. The results suggest that readers' beliefs about authorship and the aesthetic evaluation of the poem are interconnected.

</details>


### [16] [Matrix: Peer-to-Peer Multi-Agent Synthetic Data Generation Framework](https://arxiv.org/abs/2511.21686)
*Dong Wang,Yang Li,Ansong Ni,Ching-Feng Yeh,Youssef Emad,Xinjie Lei,Liam Robbins,Karthik Padthe,Hu Xu,Xian Li,Asli Celikyilmaz,Ramya Raghavendra,Lifei Huang,Carole-Jean Wu,Shang-Wen Li*

Main category: cs.CL

TL;DR: Matrix是一个去中心化的多智能体合成数据框架，通过消除中心协调器瓶颈，实现高吞吐量的数据生成。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体合成框架存在中心化协调器导致的扩展性瓶颈和领域特定硬编码的限制。

Method: 采用基于Ray的分布式队列架构，将控制和数据流表示为序列化消息，由轻量级智能体独立处理任务，计算密集型操作由分布式服务处理。

Result: 在多种合成场景下，Matrix在相同硬件资源下实现2-15倍的数据生成吞吐量提升，且不牺牲输出质量。

Conclusion: Matrix框架通过去中心化设计有效解决了多智能体数据合成的扩展性和灵活性挑战。

Abstract: Synthetic data has become increasingly important for training large language models, especially when real data is scarce, expensive, or privacy-sensitive. Many such generation tasks require coordinated multi-agent workflows, where specialized agents collaborate to produce data that is higher quality, more diverse, and structurally richer. However, existing frameworks for multi-agent synthesis often depend on a centralized orchestrator, creating scalability bottlenecks, or are hardcoded for specific domains, limiting flexibility. We present \textbf{Matrix}, a decentralized framework that represents both control and data flow as serialized messages passed through distributed queues. This peer-to-peer design eliminates the central orchestrator. Each task progresses independently through lightweight agents, while compute-intensive operations, such as LLM inference or containerized environments, are handled by distributed services. Built on Ray, Matrix scales to tens of thousands of concurrent agentic workflows and provides a modular, configurable design that enables easy adaptation to a wide range of data generation workflows. We evaluate Matrix across diverse synthesis scenarios, such as multi-agent collaborative dialogue, web-based reasoning data extraction, and tool-use trajectory generation in customer service environments. In all cases, Matrix achieves $2$--$15\times$ higher data generation throughput under identical hardware resources, without compromising output quality.

</details>


### [17] [ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration](https://arxiv.org/abs/2511.21689)
*Hongjin Su,Shizhe Diao,Ximing Lu,Mingjie Liu,Jiacheng Xu,Xin Dong,Yonggan Fu,Peter Belcak,Hanrong Ye,Hongxu Yin,Yi Dong,Evelina Bakhturina,Tao Yu,Yejin Choi,Jan Kautz,Pavlo Molchanov*

Main category: cs.CL

TL;DR: ToolOrchestra方法训练小型编排模型协调智能工具，8B参数的Orchestrator模型在HLE测试中表现优于GPT-5且效率更高，成本仅为30%，实现性能与成本的最佳平衡


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽强大但解决复杂问题时概念挑战大且计算成本高，需要更高效的方法来协调工具解决智能体任务

Method: ToolOrchestra方法使用强化学习训练小型编排模型，包含结果、效率和用户偏好的多维度奖励机制

Result: Orchestrator在HLE测试中得分为37.1%，超越GPT-5的35.1%，效率提升2.5倍；在tau2-Bench和FRAMES基准上大幅领先GPT-5，成本仅为其30%

Conclusion: 使用轻量级编排模型组合多样化工具比现有方法更高效有效，为实用可扩展的工具增强推理系统铺平道路

Abstract: Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.

</details>


### [18] [Revisiting Generalization Across Difficulty Levels: It's Not So Easy](https://arxiv.org/abs/2511.21692)
*Yeganeh Kordi,Nihal V. Nayak,Max Zuo,Ilana Nguyen,Stephen H. Bach*

Main category: cs.CL

TL;DR: 本文通过使用数千个不同大型语言模型（LLM）和项目反应理论（IRT）对六个数据集中的示例进行难度排序，系统评估了LLM在不同任务难度上的泛化能力，发现难度交叉泛化通常有限，训练数据无论难易都无法在所有难度范围内实现一致的改进。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在不同任务难度上的泛化能力，以解决数据整理和评估中的关键问题，现有研究对于训练数据难易程度对结果的影响存在分歧。

Method: 使用数千个不同LLM的输出和项目反应理论（IRT）对六个数据集中的示例进行难度排序，排除人类对难度的主观判断，进行更客观、大规模和细粒度的分析。

Result: 难度交叉泛化通常有限，训练在简单或困难数据上都无法在所有难度范围内实现一致的改进。

Conclusion: 在LLM的训练和评估数据中保持难度范围的重要性，以及在难度方面走捷径存在风险。

Abstract: We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.

</details>

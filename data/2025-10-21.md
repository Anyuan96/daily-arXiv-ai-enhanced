<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 181]
- [cs.CL](#cs.CL) [Total: 103]
- [cs.SD](#cs.SD) [Total: 14]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [ESCA: Contextualizing Embodied Agents via Scene-Graph Generation](https://arxiv.org/abs/2510.15963)
*Jiani Huang,Amish Sethi,Matthew Kuo,Mayank Keoliya,Neelay Velingker,JungHo Jung,Ser-Nam Lim,Ziyang Li,Mayur Naik*

Main category: cs.CV

TL;DR: 提出ESCA框架和SGClip模型，通过结构化时空理解提升多模态大语言模型在具身智能体中的性能，无需人工标注即可生成场景图，显著减少感知错误并超越现有基准。


<details>
  <summary>Details</summary>
Motivation: 当前多模态训练 pipeline 缺乏像素级视觉内容与文本语义之间的细粒度对齐，限制了具身智能体的感知能力。

Method: 提出ESCA框架，核心是SGClip模型——一种基于CLIP、开放域、可提示的场景图生成模型；采用神经符号学习 pipeline，在超过87K个视频上利用自监督和结构化推理进行训练，无需人工标注场景图。

Result: SGClip在场景图生成和动作定位任务上表现优异；ESCA显著提升开源和商用多模态大模型性能，在两个具身环境中达到SOTA，减少智能体感知错误，使开源模型超越专有基线。

Conclusion: 通过结构化的空间-时间理解，ESCA和SGClip有效增强了多模态大模型的感知能力，为具身智能体提供了更可靠、可扩展的训练框架。

Abstract: Multi-modal large language models (MLLMs) are making rapid progress toward
general-purpose embodied agents. However, current training pipelines primarily
rely on high-level vision-sound-text pairs and lack fine-grained, structured
alignment between pixel-level visual content and textual semantics. To overcome
this challenge, we propose ESCA, a new framework for contextualizing embodied
agents through structured spatial-temporal understanding. At its core is
SGClip, a novel CLIP-based, open-domain, and promptable model for generating
scene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic
learning pipeline, which harnesses model-driven self-supervision from
video-caption pairs and structured reasoning, thereby eliminating the need for
human-labeled scene graph annotations. We demonstrate that SGClip supports both
prompt-based inference and task-specific fine-tuning, excelling in scene graph
generation and action localization benchmarks. ESCA with SGClip consistently
improves both open-source and commercial MLLMs, achieving state-of-the-art
performance across two embodied environments. Notably, it significantly reduces
agent perception errors and enables open-source models to surpass proprietary
baselines.

</details>


### [2] [CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection](https://arxiv.org/abs/2510.15991)
*Huiming Yang*

Main category: cs.CV

TL;DR: 本文提出了一种基于几何结构和类别分布优化的稀疏多模态三维检测器CrossRay3D，通过Ray-Aware Supervision和Class-Balanced Supervision提升token表示质量，在nuScenes上实现了最先进的性能，同时具有更高的效率和强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏跨模态检测器忽略了token表示的质量，导致前景质量不佳、性能受限，尤其在小物体检测和模态分布差异方面存在挑战。

Method: 提出Sparse Selector（SS），包含Ray-Aware Supervision（RAS）以保留几何信息，Class-Balanced Supervision以增强小物体的token保留，并设计Ray Positional Encoding（Ray PE）缓解LiDAR与图像间的分布差异，最终构建端到端的稀疏多模态检测器CrossRay3D。

Result: 在nuScenes数据集上，CrossRay3D达到72.4 mAP和74.7 NDS，性能领先且推理速度比其他先进方法快1.84倍，并在部分或缺失模态输入下仍保持强鲁棒性。

Conclusion: 通过优化几何结构保持和类别平衡监督，显著提升了稀疏检测器的token表示质量，CrossRay3D在性能、效率和鲁棒性之间实现了优异平衡。

Abstract: The sparse cross-modality detector offers more advantages than its
counterpart, the Bird's-Eye-View (BEV) detector, particularly in terms of
adaptability for downstream tasks and computational cost savings. However,
existing sparse detectors overlook the quality of token representation, leaving
it with a sub-optimal foreground quality and limited performance. In this
paper, we identify that the geometric structure preserved and the class
distribution are the key to improving the performance of the sparse detector,
and propose a Sparse Selector (SS). The core module of SS is Ray-Aware
Supervision (RAS), which preserves rich geometric information during the
training stage, and Class-Balanced Supervision, which adaptively reweights the
salience of class semantics, ensuring that tokens associated with small objects
are retained during token sampling. Thereby, outperforming other sparse
multi-modal detectors in the representation of tokens. Additionally, we design
Ray Positional Encoding (Ray PE) to address the distribution differences
between the LiDAR modality and the image. Finally, we integrate the
aforementioned module into an end-to-end sparse multi-modality detector, dubbed
CrossRay3D. Experiments show that, on the challenging nuScenes benchmark,
CrossRay3D achieves state-of-the-art performance with 72.4 mAP and 74.7 NDS,
while running 1.84 faster than other leading methods. Moreover, CrossRay3D
demonstrates strong robustness even in scenarios where LiDAR or camera data are
partially or entirely missing.

</details>


### [3] [InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting and Managing Urban Defects](https://arxiv.org/abs/2510.16017)
*Ibrahim Sheikh Mohamed,Abdullah Yahya Abdullah Omaisan*

Main category: cs.CV

TL;DR: 本文提出了一种利用城市CCTV视频流进行道路缺陷（如裂缝、坑洼、泄漏）检测与分割，并通过视觉语言模型生成结构化维修行动方案的综合管道。


<details>
  <summary>Details</summary>
Motivation: 城市基础设施缺陷检测对公共安全至关重要，但人工巡检成本高且危险，现有自动系统多局限于单一缺陷类型或输出无结构信息，难以直接指导维修。

Method: 采用YOLO系列目标检测器进行多缺陷检测与分割，将检测结果输入视觉语言模型（VLM），结合场景理解生成JSON格式的结构化行动方案，包含事件描述、建议工具、尺寸、维修计划和紧急警报。

Result: 在公开数据集和实际CCTV片段上的实验表明，该系统能准确识别多种缺陷，并生成连贯的结构化总结。

Conclusion: 该方法有望提升智能城市基础设施维护的自动化与效率，但需进一步解决模型泛化性、实时性和大规模部署的挑战。

Abstract: Infrastructure in smart cities is increasingly monitored by networks of
closed circuit television (CCTV) cameras. Roads, bridges and tunnels develop
cracks, potholes, and fluid leaks that threaten public safety and require
timely repair. Manual inspection is costly and hazardous, and existing
automatic systems typically address individual defect types or provide
unstructured outputs that cannot directly guide maintenance crews. This paper
proposes a comprehensive pipeline that leverages street CCTV streams for multi
defect detection and segmentation using the YOLO family of object detectors and
passes the detections to a vision language model (VLM) for scene aware
summarization. The VLM generates a structured action plan in JSON format that
includes incident descriptions, recommended tools, dimensions, repair plans,
and urgent alerts. We review literature on pothole, crack and leak detection,
highlight recent advances in large vision language models such as QwenVL and
LLaVA, and describe the design of our early prototype. Experimental evaluation
on public datasets and captured CCTV clips demonstrates that the system
accurately identifies diverse defects and produces coherent summaries. We
conclude by discussing challenges and directions for scaling the system to city
wide deployments.

</details>


### [4] [IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection](https://arxiv.org/abs/2510.16036)
*Zewen Li,Zitong Yu,Qilang Ye,Weicheng Xie,Wei Zhuo,Linlin Shen*

Main category: cs.CV

TL;DR: 本文提出IAD-GPT，一种基于多模态大语言模型的工业异常检测新范式，结合图像级与像素级信息和丰富文本语义，通过异常提示生成器、文本引导增强模块和多掩码融合模块，实现对异常的精准检测、分割与多轮交互描述。在MVTec-AD和VisA数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统工业异常检测方法缺乏多轮人机对话和细粒度描述能力，而基于大模型的方法未能充分激发其在异常检测中的潜力。因此，需要结合多模态大语言模型的语义理解与视觉定位能力，提升异常检测的可解释性与准确性。

Method: 提出IAD-GPT框架：1）Abnormal Prompt Generator（APG）生成针对具体异常对象的详细文本提示；2）Text-Guided Enhancer通过正常与异常文本提示交互增强图像特征，动态选择增强路径；3）Multi-Mask Fusion模块融合掩码作为专家知识，提升LLM对像素级异常的感知能力，并利用CLIP等视觉语言模型实现检测与分割。

Result: 在MVTec-AD和VisA数据集上实现了自监督和少样本设置下异常检测与分割的SOTA性能，验证了方法在检测精度、分割效果和文本描述能力上的优势。

Conclusion: IAD-GPT有效融合了多模态大模型的语言生成能力与视觉理解能力，显著提升了工业异常检测的性能，支持细粒度描述与多轮对话，为智能质检提供了新的解决方案。

Abstract: The robust causal capability of Multimodal Large Language Models (MLLMs) hold
the potential of detecting defective objects in Industrial Anomaly Detection
(IAD). However, most traditional IAD methods lack the ability to provide
multi-turn human-machine dialogues and detailed descriptions, such as the color
of objects, the shape of an anomaly, or specific types of anomalies. At the
same time, methods based on large pre-trained models have not fully stimulated
the ability of large models in anomaly detection tasks. In this paper, we
explore the combination of rich text semantics with both image-level and
pixel-level information from images and propose IAD-GPT, a novel paradigm based
on MLLMs for IAD. We employ Abnormal Prompt Generator (APG) to generate
detailed anomaly prompts for specific objects. These specific prompts from the
large language model (LLM) are used to activate the detection and segmentation
functions of the pre-trained visual-language model (i.e., CLIP). To enhance the
visual grounding ability of MLLMs, we propose Text-Guided Enhancer, wherein
image features interact with normal and abnormal text prompts to dynamically
select enhancement pathways, which enables language models to focus on specific
aspects of visual data, enhancing their ability to accurately interpret and
respond to anomalies within images. Moreover, we design a Multi-Mask Fusion
module to incorporate mask as expert knowledge, which enhances the LLM's
perception of pixel-level anomalies. Extensive experiments on MVTec-AD and VisA
datasets demonstrate our state-of-the-art performance on self-supervised and
few-shot anomaly detection and segmentation tasks, such as MVTec-AD and VisA
datasets. The codes are available at
\href{https://github.com/LiZeWen1225/IAD-GPT}{https://github.com/LiZeWen1225/IAD-GPT}.

</details>


### [5] [Effect of Reporting Mode and Clinical Experience on Radiologists' Gaze and Image Analysis Behavior in Chest Radiography](https://arxiv.org/abs/2510.16070)
*Mahta Khoobi,Marc Sebastian von der Stueck,Felix Barajas Ordonez,Anca-Maria Iancu,Eric Corban,Julia Nowak,Aleksandar Kargaliev,Valeria Perelygina,Anna-Sophie Schott,Daniel Pinto dos Santos,Christiane Kuhl,Daniel Truhn,Sven Nebelung,Robert Siepmann*

Main category: cs.CV

TL;DR: 该研究比较了自由文本（FT）、结构化报告（SR）和AI辅助结构化报告（AI-SR）三种模式对放射科医生图像分析行为的影响，发现AI-SR显著提高诊断准确性、报告效率和用户体验。


<details>
  <summary>Details</summary>
Motivation: 探索结构化报告和人工智能如何优化放射科医生的阅片行为、诊断准确性及工作效率，特别是在床旁胸片评估中。

Method: 在前瞻性研究中，8名阅片者（4名新手和4名非新手）使用定制查看器和眼动仪，每人分析35张床旁胸片，比较三种报告模式下的诊断准确性（Cohen's κ）、报告时间、眼动指标和用户体验；采用广义线性混合模型进行统计分析。

Result: AI-SR的诊断准确性显著更高（κ=0.71 vs FT 0.58，P<0.001），报告时间明显缩短（AI-SR为25±9秒，FT为88±38秒）；SR和AI-SR减少眼跳次数和报告区域注视时间；新手在SR模式下更多关注图像，而非新手保持原有注视模式；AI-SR最受青睐。

Conclusion: 结构化报告通过引导视觉注意力提升效率，而AI预填的结构化报告进一步提高诊断准确性与用户满意度，是未来放射学报告的优选模式。

Abstract: Structured reporting (SR) and artificial intelligence (AI) may transform how
radiologists interact with imaging studies. This prospective study (July to
December 2024) evaluated the impact of three reporting modes: free-text (FT),
structured reporting (SR), and AI-assisted structured reporting (AI-SR), on
image analysis behavior, diagnostic accuracy, efficiency, and user experience.
Four novice and four non-novice readers (radiologists and medical students)
each analyzed 35 bedside chest radiographs per session using a customized
viewer and an eye-tracking system. Outcomes included diagnostic accuracy
(compared with expert consensus using Cohen's $\kappa$), reporting time per
radiograph, eye-tracking metrics, and questionnaire-based user experience.
Statistical analysis used generalized linear mixed models with Bonferroni
post-hoc tests with a significance level of ($P \le .01$). Diagnostic accuracy
was similar in FT ($\kappa = 0.58$) and SR ($\kappa = 0.60$) but higher in
AI-SR ($\kappa = 0.71$, $P < .001$). Reporting times decreased from $88 \pm 38$
s (FT) to $37 \pm 18$ s (SR) and $25 \pm 9$ s (AI-SR) ($P < .001$). Saccade
counts for the radiograph field ($205 \pm 135$ (FT), $123 \pm 88$ (SR), $97 \pm
58$ (AI-SR)) and total fixation duration for the report field ($11 \pm 5$ s
(FT), $5 \pm 3$ s (SR), $4 \pm 1$ s (AI-SR)) were lower with SR and AI-SR ($P <
.001$ each). Novice readers shifted gaze towards the radiograph in SR, while
non-novice readers maintained their focus on the radiograph. AI-SR was the
preferred mode. In conclusion, SR improves efficiency by guiding visual
attention toward the image, and AI-prefilled SR further enhances diagnostic
accuracy and user satisfaction.

</details>


### [6] [Data-Driven Analysis of Intersectional Bias in Image Classification: A Framework with Bias-Weighted Augmentation](https://arxiv.org/abs/2510.16072)
*Farjana Yesmin*

Main category: cs.CV

TL;DR: 提出了一种数据驱动框架来分析和减轻图像分类中的交叉偏差，包括公平性评估框架IFEF和一种新的数据增强策略BWA，实验显示显著提升了少数子组的准确性和公平性。




<details>
  <summary>Details</summary>
Motivation: 机器学习模型在不平衡数据集上训练时常常表现出由多个属性（如对象类别和环境条件）交互引起的系统性误差（即交叉偏差）。这种偏差可能导致模型对某些敏感子群体表现不佳，因此需要有效的分析与缓解方法。



Method: 本文提出了两个关键组件：1）交叉公平性评估框架（IFEF），结合定量公平性指标和可解释性工具，以系统识别模型预测中的偏差模式；2）偏差加权增强（BWA），一种新颖的数据增强策略，根据子群体分布统计信息自适应调整变换强度。



Result: 在Open Images V7数据集五个对象类上的实验表明，BWA将代表性不足的类别-环境交叉子组的准确性提高了最高达24个百分点，同时减少了35%的公平性指标差异。多次独立运行的统计分析证实了改进的显著性（p < 0.05）。



Conclusion: 所提方法为分析和解决图像分类系统中的交叉偏差提供了一种可复制的途径，有效提升了模型在复杂真实场景下的公平性与鲁棒性。

Abstract: Machine learning models trained on imbalanced datasets often exhibit
intersectional biases-systematic errors arising from the interaction of
multiple attributes such as object class and environmental conditions. This
paper presents a data-driven framework for analyzing and mitigating such biases
in image classification. We introduce the Intersectional Fairness Evaluation
Framework (IFEF), which combines quantitative fairness metrics with
interpretability tools to systematically identify bias patterns in model
predictions. Building on this analysis, we propose Bias-Weighted Augmentation
(BWA), a novel data augmentation strategy that adapts transformation
intensities based on subgroup distribution statistics. Experiments on the Open
Images V7 dataset with five object classes demonstrate that BWA improves
accuracy for underrepresented class-environment intersections by up to 24
percentage points while reducing fairness metric disparities by 35%.
Statistical analysis across multiple independent runs confirms the significance
of improvements (p < 0.05). Our methodology provides a replicable approach for
analyzing and addressing intersectional biases in image classification systems.

</details>


### [7] [Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch](https://arxiv.org/abs/2510.16088)
*Zia Badar*

Main category: cs.CV

TL;DR: 本文提出了一种可微分的、具有收敛性证明的神经网络量化方法，支持权重和激活的移位/对数量化，可在15个训练周期内达到与SOTA方法相当的精度，仅需少量额外CPU指令，无需高精度乘法。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法大多不可微，依赖手动设置梯度，影响学习能力；同时在激活和权重同时量化时精度较低，缺乏对n位对数量化的有效学习机制。

Method: 提出一种可微分的量化方法，支持形式为$2^n$的对数量化，提供n比特量化能力，并证明其收敛至最优网络的理论保证。

Result: 在ImageNet上使用ResNet18进行实验，仅权重量化时精度损失小于1%，15个epoch即收敛；在权重与激活均量化时达到SOTA精度水平，推理成本略高于1比特量化但无需高精度乘法。

Conclusion: 该方法解决了量化中不可微和低精度问题，实现了快速训练与高效推理的平衡，适用于低资源场景下的高效神经网络部署。

Abstract: Quantization of neural networks provides benefits of inference in less
compute and memory requirements. Previous work in quantization lack two
important aspects which this work provides. First almost all previous work in
quantization used a non-differentiable approach and for learning; the
derivative is usually set manually in backpropogation which make the learning
ability of algorithm questionable, our approach is not just differentiable, we
also provide proof of convergence of our approach to the optimal neural
network. Second previous work in shift/logrithmic quantization either have
avoided activation quantization along with weight quantization or achieved less
accuracy. Learning logrithmic quantize values of form $2^n$ requires the
quantization function can scale to more than 1 bit quantization which is
another benifit of our quantization that it provides $n$ bits quantization as
well. Our approach when tested with image classification task using imagenet
dataset, resnet18 and weight quantization only achieves less than 1 percent
accuracy compared to full precision accuracy while taking only 15 epochs to
train using shift bit quantization and achieves comparable to SOTA approaches
accuracy in both weight and activation quantization using shift bit
quantization in 15 training epochs with slightly higher(only higher cpu
instructions) inference cost compared to 1 bit quantization(without logrithmic
quantization) and not requiring any higher precision multiplication.

</details>


### [8] [StripRFNet: A Strip Receptive Field and Shape-Aware Network for Road Damage Detection](https://arxiv.org/abs/2510.16115)
*Jianhan Lin,Yuchu Qin,Shuai Gao,Yikang Rui,Jie Liu,Yanjie Lv*

Main category: cs.CV

TL;DR: 提出了一种名为StripRFNet的新型深度神经网络，用于准确检测道路表面损伤，尤其在细长裂缝和小尺度损伤检测方面表现优异，实现了最先进的精度和实时效率。


<details>
  <summary>Details</summary>
Motivation: 道路网络的良好维护对于实现可持续发展目标至关重要。然而，由于损伤形态多样、细长裂缝难以捕捉以及小尺度损伤识别错误率高等问题，准确检测道路损伤仍然具有挑战性。

Method: 提出了StripRFNet，包含三个模块：（1）形状感知模块（SPM），通过多尺度特征聚合中的大可分离核注意力（LSKA）增强形状辨别能力；（2）条带感受野模块（SRFM），利用大条带卷积和池化捕捉细长裂缝特征；（3）小尺度增强模块（SSEM），通过高分辨率P2特征图、专用检测头和动态上采样提高小目标检测性能。

Result: 在RDD2022基准测试中，StripRFNet超越了现有方法。在中国子集上，相比基线，F1分数、mAP50和mAP50:95分别提高了4.4、2.9和3.4个百分点；在完整数据集上，F1分数达到80.33%，为目前最高，同时保持了有竞争力的推理速度。

Conclusion: StripRFNet在检测精度和实时性方面均表现出色，为智能道路维护和可持续基础设施管理提供了有力工具。

Abstract: Well-maintained road networks are crucial for achieving Sustainable
Development Goal (SDG) 11. Road surface damage not only threatens traffic
safety but also hinders sustainable urban development. Accurate detection,
however, remains challenging due to the diverse shapes of damages, the
difficulty of capturing slender cracks with high aspect ratios, and the high
error rates in small-scale damage recognition. To address these issues, we
propose StripRFNet, a novel deep neural network comprising three modules: (1) a
Shape Perception Module (SPM) that enhances shape discrimination via large
separable kernel attention (LSKA) in multi-scale feature aggregation; (2) a
Strip Receptive Field Module (SRFM) that employs large strip convolutions and
pooling to capture features of slender cracks; and (3) a Small-Scale
Enhancement Module (SSEM) that leverages a high-resolution P2 feature map, a
dedicated detection head, and dynamic upsampling to improve small-object
detection. Experiments on the RDD2022 benchmark show that StripRFNet surpasses
existing methods. On the Chinese subset, it improves F1-score, mAP50, and
mAP50:95 by 4.4, 2.9, and 3.4 percentage points over the baseline,
respectively. On the full dataset, it achieves the highest F1-score of 80.33%
compared with CRDDC'2022 participants and ORDDC'2024 Phase 2 results, while
maintaining competitive inference speed. These results demonstrate that
StripRFNet achieves state-of-the-art accuracy and real-time efficiency,
offering a promising tool for intelligent road maintenance and sustainable
infrastructure management.

</details>


### [9] [ObjectTransforms for Uncertainty Quantification and Reduction in Vision-Based Perception for Autonomous Vehicles](https://arxiv.org/abs/2510.16118)
*Nishad Sahu,Shounak Sural,Aditya Satish Patil,Ragunathan,Rajkumar*

Main category: cs.CV

TL;DR: 提出了一种名为ObjectTransforms的新方法，通过在训练和推理时对物体进行特定变换来量化和减少基于视觉的目标检测中的不确定性，提升了检测的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 基于视觉的目标检测在自动驾驶中面临由数据偏差和分布偏移引起的不确定性问题，影响感知可靠性，因此需要有效方法来量化并减少此类不确定性。

Method: 在训练阶段，对单个物体进行颜色空间扰动，并利用扩散模型生成多样且逼真的行人样本；在推理阶段，对检测到的物体施加扰动并通过检测分数的方差来实时量化预测不确定性，利用该不确定性信号过滤误检并恢复漏检。

Result: 在NuImages 10K数据集上使用YOLOv8的实验表明，该方法在训练阶段显著提升了各类物体的检测精度并降低了不确定性，在推理阶段能为误检分配更高的不确定性值，从而改善了整体的精确率-召回率曲线。

Conclusion: ObjectTransforms是一种轻量且有效的方法，能够在训练和推理阶段分别减少和量化基于视觉感知中的不确定性，具有在自动驾驶等安全关键系统中应用的潜力。

Abstract: Reliable perception is fundamental for safety critical decision making in
autonomous driving. Yet, vision based object detector neural networks remain
vulnerable to uncertainty arising from issues such as data bias and
distributional shifts. In this paper, we introduce ObjectTransforms, a
technique for quantifying and reducing uncertainty in vision based object
detection through object specific transformations at both training and
inference times. At training time, ObjectTransforms perform color space
perturbations on individual objects, improving robustness to lighting and color
variations. ObjectTransforms also uses diffusion models to generate realistic,
diverse pedestrian instances. At inference time, object perturbations are
applied to detected objects and the variance of detection scores are used to
quantify predictive uncertainty in real time. This uncertainty signal is then
used to filter out false positives and also recover false negatives, improving
the overall precision recall curve. Experiments with YOLOv8 on the NuImages 10K
dataset demonstrate that our method yields notable accuracy improvements and
uncertainty reduction across all object classes during training, while
predicting desirably higher uncertainty values for false positives as compared
to true positives during inference. Our results highlight the potential of
ObjectTransforms as a lightweight yet effective mechanism for reducing and
quantifying uncertainty in vision-based perception during training and
inference respectively.

</details>


### [10] [Aria Gen 2 Pilot Dataset](https://arxiv.org/abs/2510.16134)
*Chen Kong,James Fort,Aria Kang,Jonathan Wittmer,Simon Green,Tianwei Shen,Yipu Zhao,Cheng Peng,Gustavo Solaira,Andrew Berkovich,Nikhil Raina,Vijay Baiyya,Evgeniy Oleinik,Eric Huang,Fan Zhang,Julian Straub,Mark Schwesinger,Luis Pesqueira,Xiaqing Pan,Jakob Julian Engel,Carl Ren,Mingfei Yan,Richard Newcombe*

Main category: cs.CV

TL;DR: Aria Gen 2 Pilot Dataset (A2PD) 是一个使用 Aria Gen 2 眼镜采集的前瞻性第一人称多模态公开数据集，分阶段发布，包含日常活动场景的丰富传感器数据和感知算法输出。


<details>
  <summary>Details</summary>
Motivation: 为了推动增强现实和智能眼镜相关研究的发展，提供高质量、真实场景下的多模态数据，支持对用户自身、环境及交互的感知研究。

Method: 通过配备 Aria Gen 2 眼镜的主试和朋友在五种主要场景（清洁、烹饪、进餐、游戏、户外行走）中记录数据，并集成多种机器感知算法的输出结果。

Result: 发布了包含原始传感器数据和感知算法输出的 A2PD 数据集，展示了设备在不同用户和条件下的鲁棒性能，并提供开源工具和示例支持。

Conclusion: A2PD 为 egocentric vision 和可穿戴计算研究提供了宝贵资源，有望促进相关算法和技术的发展。

Abstract: The Aria Gen 2 Pilot Dataset (A2PD) is an egocentric multimodal open dataset
captured using the state-of-the-art Aria Gen 2 glasses. To facilitate timely
access, A2PD is released incrementally with ongoing dataset enhancements. The
initial release features Dia'ane, our primary subject, who records her daily
activities alongside friends, each equipped with Aria Gen 2 glasses. It
encompasses five primary scenarios: cleaning, cooking, eating, playing, and
outdoor walking. In each of the scenarios, we provide comprehensive raw sensor
data and output data from various machine perception algorithms. These data
illustrate the device's ability to perceive the wearer, the surrounding
environment, and interactions between the wearer and the environment, while
maintaining robust performance across diverse users and conditions. The A2PD is
publicly available at projectaria.com, with open-source tools and usage
examples provided in Project Aria Tools.

</details>


### [11] [GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer](https://arxiv.org/abs/2510.16136)
*Sayan Deb Sarkar,Sinisa Stekovic,Vincent Lepetit,Iro Armeni*

Main category: cs.CV

TL;DR: 提出一种基于预训练rectified flow模型的无训练方法，通过周期性引入外观和自相似性的部分感知引导，实现跨显著几何差异的3D资产外观迁移。


<details>
  <summary>Details</summary>
Motivation: 现有方法在输入与外观对象几何差异大时表现不佳，直接应用3D生成模型效果不理想，需更有效的外观迁移方法。

Method: 基于universal guidance思想，在预训练的图像或文本条件rectified flow模型采样过程中，周期性加入可微的不同类型引导损失（如部分感知损失、自相似性损失）。

Result: 该方法在纹理和几何细节迁移上优于基线方法，定性和定量结果更优；传统评估指标不适用，因此采用基于GPT的排序系统进行客观评估，并通过用户研究验证。

Conclusion: 所提方法能有效实现跨几何差异的高质量外观迁移，具通用性，可扩展至其他扩散模型和引导函数。

Abstract: Transferring appearance to 3D assets using different representations of the
appearance object - such as images or text - has garnered interest due to its
wide range of applications in industries like gaming, augmented reality, and
digital content creation. However, state-of-the-art methods still fail when the
geometry between the input and appearance objects is significantly different. A
straightforward approach is to directly apply a 3D generative model, but we
show that this ultimately fails to produce appealing results. Instead, we
propose a principled approach inspired by universal guidance. Given a
pretrained rectified flow model conditioned on image or text, our training-free
method interacts with the sampling process by periodically adding guidance.
This guidance can be modeled as a differentiable loss function, and we
experiment with two different types of guidance including part-aware losses for
appearance and self-similarity. Our experiments show that our approach
successfully transfers texture and geometric details to the input 3D asset,
outperforming baselines both qualitatively and quantitatively. We also show
that traditional metrics are not suitable for evaluating the task due to their
inability of focusing on local details and comparing dissimilar inputs, in
absence of ground truth data. We thus evaluate appearance transfer quality with
a GPT-based system objectively ranking outputs, ensuring robust and human-like
assessment, as further confirmed by our user study. Beyond showcased scenarios,
our method is general and could be extended to different types of diffusion
models and guidance functions.

</details>


### [12] [C-arm Guidance: A Self-supervised Approach To Automated Positioning During Stroke Thrombectomy](https://arxiv.org/abs/2510.16145)
*Ahmad Arrabi,Jay hwasung Jung,J Le,A Nguyen,J Reed,E Stahl,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: 提出一种自监督深度学习框架，通过回归预文本任务自动分类骨骼标志点，提升缺血性中风取栓术中C臂控制的效率与安全性。


<details>
  <summary>Details</summary>
Motivation: 缺血性中风取栓术资源和人力成本高，需通过自动化关键环节提升手术效率与安全性。

Method: 采用基于回归的预文本任务进行自监督学习，以分类多种骨骼标志点，并利用该表示提升下游分类任务性能。

Result: 所提模型在回归与分类任务上均优于现有方法，位置预文本任务显著提升分类性能。

Conclusion: 该框架有望扩展至完全自主的C臂控制，优化从骨盆到头部的运动轨迹，推动自动化取栓手术发展。

Abstract: Thrombectomy is one of the most effective treatments for ischemic stroke, but
it is resource and personnel-intensive. We propose employing deep learning to
automate critical aspects of thrombectomy, thereby enhancing efficiency and
safety. In this work, we introduce a self-supervised framework that classifies
various skeletal landmarks using a regression-based pretext task. Our
experiments demonstrate that our model outperforms existing methods in both
regression and classification tasks. Notably, our results indicate that the
positional pretext task significantly enhances downstream classification
performance. Future work will focus on extending this framework toward fully
autonomous C-arm control, aiming to optimize trajectories from the pelvis to
the head during stroke thrombectomy procedures. All code used is available at
https://github.com/AhmadArrabi/C_arm_guidance

</details>


### [13] [DuetMatch: Harmonizing Semi-Supervised Brain MRI Segmentation via Decoupled Branch Optimization](https://arxiv.org/abs/2510.16146)
*Thanh-Huy Nguyen,Hoang-Thien Nguyen,Vi Vu,Ba-Thinh Lam,Phat Huynh,Tianyang Wang,Xingjian Li,Ulas Bagci,Min Xu*

Main category: cs.CV

TL;DR: 提出DuetMatch，一种具有异步优化的双分支半监督框架，用于医学图像分割，通过解耦dropout扰动、配对CutMix交叉引导和一致性匹配提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像中标注数据有限，半监督学习虽具吸引力，但现有教师-学生框架在联合优化时存在收敛和稳定性问题，尤其在挑战性场景下表现更差。

Method: 提出DuetMatch框架：1）双分支异步优化，分别训练编码器和解码器；2）解耦Dropout扰动增强一致性；3）配对CutMix交叉引导提升多样性；4）一致性匹配利用教师模型稳定预测减轻伪标签噪声偏差。

Result: 在ISLES2022和BraTS等脑MRI分割基准上，DuetMatch显著优于当前最先进方法，展现出更强的鲁棒性和泛化能力。

Conclusion: DuetMatch通过异步优化和多种一致性正则化策略，有效提升了半监督医学图像分割的性能，适用于标注数据稀缺的复杂场景。

Abstract: The limited availability of annotated data in medical imaging makes
semi-supervised learning increasingly appealing for its ability to learn from
imperfect supervision. Recently, teacher-student frameworks have gained
popularity for their training benefits and robust performance. However, jointly
optimizing the entire network can hinder convergence and stability, especially
in challenging scenarios. To address this for medical image segmentation, we
propose DuetMatch, a novel dual-branch semi-supervised framework with
asynchronous optimization, where each branch optimizes either the encoder or
decoder while keeping the other frozen. To improve consistency under noisy
conditions, we introduce Decoupled Dropout Perturbation, enforcing
regularization across branches. We also design Pair-wise CutMix Cross-Guidance
to enhance model diversity by exchanging pseudo-labels through augmented input
pairs. To mitigate confirmation bias from noisy pseudo-labels, we propose
Consistency Matching, refining labels using stable predictions from frozen
teacher models. Extensive experiments on benchmark brain MRI segmentation
datasets, including ISLES2022 and BraTS, show that DuetMatch consistently
outperforms state-of-the-art methods, demonstrating its effectiveness and
robustness across diverse semi-supervised segmentation scenarios.

</details>


### [14] [Automated C-Arm Positioning via Conformal Landmark Localization](https://arxiv.org/abs/2510.16160)
*Ahmad Arrabi,Jay Hwasung Jung,Jax Luo,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: 提出一种利用X射线图像自动导航C臂至解剖标志的管道，通过不确定性量化和校准提高定位可靠性，结合概率损失与骨骼姿态正则化，在合成数据上验证了高精度和良好校准的预测边界。


<details>
  <summary>Details</summary>
Motivation: 手动C臂对位增加辐射暴露和手术延迟，亟需自动化解决方案以提升介入手术的安全性和效率。

Method: 基于输入X射线图像预测朝向多个解剖目标的3D位移向量，采用概率深度学习模型并引入骨骼姿态正则化；结合共形预测进行不确定性校准，生成3D置信区域。

Result: 在DeepDRR生成的合成X射线数据上测试，多种网络架构均表现出优异的定位精度和良好校准的预测范围，不确定性估计可靠。

Conclusion: 该方法可作为安全可靠的自主C臂系统组件，具备临床应用潜力，有助于减少辐射暴露和操作延迟。

Abstract: Accurate and reliable C-arm positioning is essential for fluoroscopy-guided
interventions. However, clinical workflows rely on manual alignment that
increases radiation exposure and procedural delays. In this work, we present a
pipeline that autonomously navigates the C-arm to predefined anatomical
landmarks utilizing X-ray images. Given an input X-ray image from an arbitrary
starting location on the operating table, the model predicts a 3D displacement
vector toward each target landmark along the body. To ensure reliable
deployment, we capture both aleatoric and epistemic uncertainties in the
model's predictions and further calibrate them using conformal prediction. The
derived prediction regions are interpreted as 3D confidence regions around the
predicted landmark locations. The training framework combines a probabilistic
loss with skeletal pose regularization to encourage anatomically plausible
outputs. We validate our approach on a synthetic X-ray dataset generated from
DeepDRR. Results show not only strong localization accuracy across multiple
architectures but also well-calibrated prediction bounds. These findings
highlight the pipeline's potential as a component in safe and reliable
autonomous C-arm systems. Code is available at
https://github.com/AhmadArrabi/C_arm_guidance_APAH

</details>


### [15] [Cost Savings from Automatic Quality Assessment of Generated Images](https://arxiv.org/abs/2510.16179)
*Xavier Giro-i-Nieto,Nefeli Andreou,Anqi Liang,Manel Baradad,Francesc Moreno-Noguer,Aleix Martinez*

Main category: cs.CV

TL;DR: 提出了一种估算自动图像质量评估（IQA）引擎成本节约的公式，并在背景修复用例中展示了51.61%的成本节约。


<details>
  <summary>Details</summary>
Motivation: 当前生成图像的质量尚无法完全满足传统摄影标准，导致生产流程中需手动进行图像质量评估，耗时且昂贵。

Method: 引入自动预过滤阶段，使用基于AutoML的简单IQA引擎，并推导出估算成本节约的公式，基于精度和通过率评估效果。

Result: 在背景修复任务中，使用该IQA预过滤系统可实现51.61%的成本节约。

Conclusion: 自动IQA预过滤可显著降低图像生成流程中的人工评估成本，提升整体生产效率。

Abstract: Deep generative models have shown impressive progress in recent years, making
it possible to produce high quality images with a simple text prompt or a
reference image. However, state of the art technology does not yet meet the
quality standards offered by traditional photographic methods. For this reason,
production pipelines that use generated images often include a manual stage of
image quality assessment (IQA). This process is slow and expensive, especially
because of the low yield of automatically generated images that pass the
quality bar. The IQA workload can be reduced by introducing an automatic
pre-filtering stage, that will increase the overall quality of the images sent
to review and, therefore, reduce the average cost required to obtain a high
quality image. We present a formula that estimates the cost savings depending
on the precision and pass yield of a generic IQA engine. This formula is
applied in a use case of background inpainting, showcasing a significant cost
saving of 51.61% obtained with a simple AutoML solution.

</details>


### [16] [Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI](https://arxiv.org/abs/2510.16196)
*Zheng Huang,Enpei Zhang,Yinghao Cai,Weikang Qiu,Carl Yang,Elynn Chen,Xiang Zhang,Rex Ying,Dawei Zhou,Yujun Yan*

Main category: cs.CV

TL;DR: 提出PRISM模型，将fMRI信号投影到结构化文本空间以重建视觉刺激，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索如何有效利用fMRI信号重建视觉刺激，并确定最适合的中间表示空间。

Method: 将fMRI信号映射到结构化文本空间，结合对象中心扩散模块和属性关系搜索模块，适配生成模型以捕捉视觉刺激的组成性。

Result: 在真实数据集上实验表明，该方法优于现有技术，感知损失最多降低8%。

Conclusion: 结构化文本空间能更有效地桥接fMRI信号与图像重建，提升视觉解码性能。

Abstract: Understanding how the brain encodes visual information is a central challenge
in neuroscience and machine learning. A promising approach is to reconstruct
visual stimuli, essentially images, from functional Magnetic Resonance Imaging
(fMRI) signals. This involves two stages: transforming fMRI signals into a
latent space and then using a pretrained generative model to reconstruct
images. The reconstruction quality depends on how similar the latent space is
to the structure of neural activity and how well the generative model produces
images from that space. Yet, it remains unclear which type of latent space best
supports this transformation and how it should be organized to represent visual
stimuli effectively. We present two key findings. First, fMRI signals are more
similar to the text space of a language model than to either a vision based
space or a joint text image space. Second, text representations and the
generative model should be adapted to capture the compositional nature of
visual stimuli, including objects, their detailed attributes, and
relationships. Building on these insights, we propose PRISM, a model that
Projects fMRI sIgnals into a Structured text space as an interMediate
representation for visual stimuli reconstruction. It includes an object centric
diffusion module that generates images by composing individual objects to
reduce object detection errors, and an attribute relationship search module
that automatically identifies key attributes and relationships that best align
with the neural activity. Extensive experiments on real world datasets
demonstrate that our framework outperforms existing methods, achieving up to an
8% reduction in perceptual loss. These results highlight the importance of
using structured text as the intermediate space to bridge fMRI signals and
image reconstruction.

</details>


### [17] [Data-Centric AI for Tropical Agricultural Mapping: Challenges, Strategies and Scalable Solutions](https://arxiv.org/abs/2510.16207)
*Mateus Pinto da Silva,Sabrina P. L. P. Correa,Hugo N. Oliveira,Ian M. Nunes,Jefersson A. dos Santos*

Main category: cs.CV

TL;DR: 本文提倡一种以数据为中心的人工智能（DCAI）方法，通过提升数据质量和管理来增强热带地区农业遥感制图中AI模型的鲁棒性和可扩展性，并提出一个包含9种成熟方法的实用管道。


<details>
  <summary>Details</summary>
Motivation: 热带地区的农业遥感制图面临标注数据质量差、标注成本高、数据变异性大和区域泛化难等挑战，传统以模型为中心的方法受限，因此需要强调数据质量的新型解决方案。

Method: 采用数据中心化AI框架，综述并优先选择可信学习、核心集选择、数据增强和主动学习等技术，评估25种策略在大规模农业制图中的适用性，并构建一个融合9种最成熟且简单方法的实用管道。

Result: 识别出25种适用于大规模农业映射的数据中心策略，并提出一个可直接应用于热带农业遥感项目的、由9种最成熟方法组成的实用管道。

Conclusion: 以数据为中心的方法能有效应对热带地区农业遥感中的挑战，提升模型性能，具有良好的可扩展性和实际应用价值。

Abstract: Mapping agriculture in tropical areas through remote sensing presents unique
challenges, including the lack of high-quality annotated data, the elevated
costs of labeling, data variability, and regional generalisation. This paper
advocates a Data-Centric Artificial Intelligence (DCAI) perspective and
pipeline, emphasizing data quality and curation as key drivers for model
robustness and scalability. It reviews and prioritizes techniques such as
confident learning, core-set selection, data augmentation, and active learning.
The paper highlights the readiness and suitability of 25 distinct strategies in
large-scale agricultural mapping pipelines. The tropical context is of high
interest, since high cloudiness, diverse crop calendars, and limited datasets
limit traditional model-centric approaches. This tutorial outlines practical
solutions as a data-centric approach for curating and training AI models better
suited to the dynamic realities of tropical agriculture. Finally, we propose a
practical pipeline using the 9 most mature and straightforward methods that can
be applied to a large-scale tropical agricultural mapping project.

</details>


### [18] [StretchySnake: Flexible SSM Training Unlocks Action Recognition Across Spatio-Temporal Scales](https://arxiv.org/abs/2510.16209)
*Nyle Siddiqui,Rohit Gupta,Sirnam Swetha,Mubarak Shah*

Main category: cs.CV

TL;DR: 提出了一种灵活的训练方法StretchySnake，通过在不同时空分辨率下采样和动态插值模型权重，增强了SSM在视频动作识别中的时空适应性，显著提升了在长短视频上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解训练方法主要针对Transformer设计，未能充分利用SSM的特性，导致模型在未见分辨率视频上性能下降，即存在时空不灵活性问题。

Method: 在训练中对视频进行多尺度时空采样，并动态插值模型权重以适应不同尺度；提出了五种灵活训练变体，并优化出最佳策略。

Result: StretchySnake在短动作（UCF-101, HMDB-51）和长动作（COIN, Breakfast）基准上优于Transformer和SSM基线达28%，并在细粒度动作识别（SSV2, Diving-48）中表现优异。

Conclusion: 该方法提供了一种简单、即插即用的训练方案，使视频SSM更具鲁棒性、分辨率无关性和高效性，适用于多样化的动作识别场景。

Abstract: State space models (SSMs) have emerged as a competitive alternative to
transformers in various tasks. Their linear complexity and hidden-state
recurrence make them particularly attractive for modeling long sequences,
whereas attention becomes quadratically expensive. However, current training
methods for video understanding are tailored towards transformers and fail to
fully leverage the unique attributes of SSMs. For example, video models are
often trained at a fixed resolution and video length to balance the quadratic
scaling of attention cost against performance. Consequently, these models
suffer from degraded performance when evaluated on videos with spatial and
temporal resolutions unseen during training; a property we call spatio-temporal
inflexibility. In the context of action recognition, this severely limits a
model's ability to retain performance across both short- and long-form videos.
Therefore, we propose a flexible training method that leverages and improves
the inherent adaptability of SSMs. Our method samples videos at varying
temporal and spatial resolutions during training and dynamically interpolates
model weights to accommodate any spatio-temporal scale. This instills our SSM,
which we call StretchySnake, with spatio-temporal flexibility and enables it to
seamlessly handle videos ranging from short, fine-grained clips to long,
complex activities. We introduce and compare five different variants of
flexible training, and identify the most effective strategy for video SSMs. On
short-action (UCF-101, HMDB-51) and long-action (COIN, Breakfast) benchmarks,
StretchySnake outperforms transformer and SSM baselines alike by up to 28%,
with strong adaptability to fine-grained actions (SSV2, Diving-48). Therefore,
our method provides a simple drop-in training recipe that makes video SSMs more
robust, resolution-agnostic, and efficient across diverse action recognition
scenarios.

</details>


### [19] [VM-BeautyNet: A Synergistic Ensemble of Vision Transformer and Mamba for Facial Beauty Prediction](https://arxiv.org/abs/2510.16220)
*Djamel Eddine Boukhari*

Main category: cs.CV

TL;DR: 提出了一种融合Vision Transformer和Mamba模型的异构集成架构VM-BeautyNet，用于面部美感预测，在SCUT-FBP5500数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在捕捉面部整体结构方面存在局限，而Vision Transformer虽能建模长距离依赖但计算复杂度高，希望结合ViT与Mamba的优势以提升美感预测效果。

Method: 构建VM-BeautyNet，融合Vision Transformer（捕获全局结构）和Mamba视觉模型（线性复杂度建模序列特征），通过异构集成学习提升预测性能，并利用Grad-CAM进行可解释性分析。

Result: 在SCUT-FBP5500数据集上取得PC=0.9212、MAE=0.2085、RMSE=0.2698的SOTA结果，Grad-CAM验证了两种骨干网络特征提取的互补性。

Conclusion: VM-BeautyNet有效结合ViT与Mamba的优势，为计算美学提供了高性能且可解释的新架构范式。

Abstract: Facial Beauty Prediction (FBP) is a complex and challenging computer vision
task, aiming to model the subjective and intricate nature of human aesthetic
perception. While deep learning models, particularly Convolutional Neural
Networks (CNNs), have made significant strides, they often struggle to capture
the global, holistic facial features that are critical to human judgment.
Vision Transformers (ViT) address this by effectively modeling long-range
spatial relationships, but their quadratic complexity can be a bottleneck. This
paper introduces a novel, heterogeneous ensemble architecture,
\textbf{VM-BeautyNet}, that synergistically fuses the complementary strengths
of a Vision Transformer and a Mamba-based Vision model, a recent advancement in
State-Space Models (SSMs). The ViT backbone excels at capturing global facial
structure and symmetry, while the Mamba backbone efficiently models long-range
dependencies with linear complexity, focusing on sequential features and
textures. We evaluate our approach on the benchmark SCUT-FBP5500 dataset. Our
proposed VM-BeautyNet achieves state-of-the-art performance, with a
\textbf{Pearson Correlation (PC) of 0.9212}, a \textbf{Mean Absolute Error
(MAE) of 0.2085}, and a \textbf{Root Mean Square Error (RMSE) of 0.2698}.
Furthermore, through Grad-CAM visualizations, we provide interpretability
analysis that confirms the complementary feature extraction of the two
backbones, offering new insights into the model's decision-making process and
presenting a powerful new architectural paradigm for computational aesthetics.

</details>


### [20] [Designing a Convolutional Neural Network for High-Accuracy Oral Cavity Squamous Cell Carcinoma (OCSCC) Detection](https://arxiv.org/abs/2510.16235)
*Vishal Manikanden,Aniketh Bandlamudi,Daniel Haehn*

Main category: cs.CV

TL;DR: 本研究开发了一种用于早期检测口腔鳞状细胞癌（OCSCC）的卷积神经网络（CNN），并设计了配套的图像采集硬件系统，验证了图像分辨率对检测精度的影响。


<details>
  <summary>Details</summary>
Motivation: 由于OCSCC早期症状隐蔽、发展位置深且生长缓慢，常被忽视，导致可预防的死亡。因此需要一种高效、准确的早期检测方法。

Method: 使用4293张包含良恶性肿瘤及阴性样本的图像训练CNN，并在不同分辨率下测试其精度、召回率和平均精度（mAP）；同时设计图像增强硬件采集图像，并开发应用程序支持系统运行。

Result: 高分辨率图像显著提升CNN预测准确率，但提升呈对数增长趋势，显示像素增加存在边际效益递减；硬件系统有效提升图像质量，应用实现开放访问。

Conclusion: 结合CNN与专用图像采集硬件可有效提高OCSCC早期检测准确性，系统对图像分辨率有依赖，未来应用需权衡分辨率与成本效益。

Abstract: Oral Cavity Squamous Cell Carcinoma (OCSCC) is the most common type of head
and neck cancer. Due to the subtle nature of its early stages, deep and hidden
areas of development, and slow growth, OCSCC often goes undetected, leading to
preventable deaths. However, properly trained Convolutional Neural Networks
(CNNs), with their precise image segmentation techniques and ability to apply
kernel matrices to modify the RGB values of images for accurate image pattern
recognition, would be an effective means for early detection of OCSCC. Pairing
this neural network with image capturing and processing hardware would allow
increased efficacy in OCSCC detection. The aim of our project is to develop a
Convolutional Neural Network trained to recognize OCSCC, as well as to design a
physical hardware system to capture and process detailed images, in order to
determine the image quality required for accurate predictions. A CNN was
trained on 4293 training images consisting of benign and malignant tumors, as
well as negative samples, and was evaluated for its precision, recall, and Mean
Average Precision (mAP) in its predictions of OCSCC. A testing dataset of
randomly assorted images of cancerous, non-cancerous, and negative images was
chosen, and each image was altered to represent 5 common resolutions. This test
data set was thoroughly analyzed by the CNN and predictions were scored on the
basis of accuracy. The designed enhancement hardware was used to capture
detailed images, and its impact was scored. An application was developed to
facilitate the testing process and bring open access to the CNN. Images of
increasing resolution resulted in higher-accuracy predictions on a logarithmic
scale, demonstrating the diminishing returns of higher pixel counts.

</details>


### [21] [Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset](https://arxiv.org/abs/2510.16258)
*Claire McLean,Makenzie Meendering,Tristan Swartz,Orri Gabbay,Alexandra Olsen,Rachel Jacobs,Nicholas Rosen,Philippe de Bree,Tony Garcia,Gadsden Merrill,Jake Sandakly,Julia Buffalini,Neham Jain,Steven Krenn,Moneish Kumar,Dejan Markovic,Evonne Ng,Fabian Prada,Andrew Saba,Siwei Zhang,Vasu Agrawal,Tim Godisart,Alexander Richard,Michael Zollhoefer*

Main category: cs.CV

TL;DR: Embody 3D是一个包含439名参与者、总计超过500小时和5400万帧的多模态3D运动数据集，涵盖单人动作和多人互动场景，提供人体运动追踪、手势、身体形态、文本标注及独立音频轨道。


<details>
  <summary>Details</summary>
Motivation: 为了推动虚拟化身和人机交互的研究，需要大规模、多样化的3D人类行为数据集，特别是包含多人互动和丰富模态信息的数据。

Method: 通过多摄像头系统采集439名参与者的3D运动数据，记录单人动作（如手势、行走）和多人互动（如对话、协作活动），并同步获取运动追踪、文本标注和独立音频。

Result: 构建了包含超过5400万帧的高质量3D运动数据集Embody 3D，覆盖广泛的行为类型，并提供手部追踪、身体形态、文本和音频等多模态标注。

Conclusion: Embody 3D为虚拟化身、行为建模和多模态交互研究提供了丰富且实用的数据资源，支持更真实的人类行为模拟与分析。

Abstract: The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of
500 individual hours of 3D motion data from 439 participants collected in a
multi-camera collection stage, amounting to over 54 million frames of tracked
3D motion. The dataset features a wide range of single-person motion data,
including prompted motions, hand gestures, and locomotion; as well as
multi-person behavioral and conversational data like discussions, conversations
in different emotional states, collaborative activities, and co-living
scenarios in an apartment-like space. We provide tracked human motion including
hand tracking and body shape, text annotations, and a separate audio track for
each participant.

</details>


### [22] [Proactive Scene Decomposition and Reconstruction](https://arxiv.org/abs/2510.16272)
*Baicheng Li,Zike Yan,Dong Wu,Hongbin Zha*

Main category: cs.CV

TL;DR: 本文提出了一种基于人-物交互的主动式场景分解与重建新方法，通过利用第一人称视频流中的交互线索，实现动态环境的在线、渐进式建模。


<details>
  <summary>Details</summary>
Motivation: 传统物体级重建方法在动态场景中存在歧义，难以准确分解和重建；而人类行为蕴含丰富的动态线索，可被用于指导更精确的场景理解。

Method: 提出一种在线方法，通过观察人-物交互行为，迭代地分解与重建环境，并结合高斯点阵技术进行动态建模。

Result: 在多个真实场景中验证了方法的有效性，实现了准确的相机与物体位姿估计、实例分解和在线地图更新，具有照片级真实感和高效渲染能力。

Conclusion: 该方法利用人类行为线索，为动态场景的重建提供了一种灵活、渐进且优于传统方法的解决方案。

Abstract: Human behaviors are the major causes of scene dynamics and inherently contain
rich cues regarding the dynamics. This paper formalizes a new task of proactive
scene decomposition and reconstruction, an online approach that leverages
human-object interactions to iteratively disassemble and reconstruct the
environment. By observing these intentional interactions, we can dynamically
refine the decomposition and reconstruction process, addressing inherent
ambiguities in static object-level reconstruction. The proposed system
effectively integrates multiple tasks in dynamic environments such as accurate
camera and object pose estimation, instance decomposition, and online map
updating, capitalizing on cues from human-object interactions in egocentric
live streams for a flexible, progressive alternative to conventional
object-level reconstruction methods. Aided by the Gaussian splatting technique,
accurate and consistent dynamic scene modeling is achieved with photorealistic
and efficient rendering. The efficacy is validated in multiple real-world
scenarios with promising advantages.

</details>


### [23] [Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models](https://arxiv.org/abs/2510.16290)
*Yue Zheng,Xiufang Shi,Jiming Chen,Yuanchao Shu*

Main category: cs.CV

TL;DR: Cerberus 是一种高效的两阶段级联系统，通过运动掩码提示和基于规则的偏离检测，在保持接近最先进准确率的同时，显著提升视频异常检测的推理速度，实现真正的实时应用。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视觉语言模型（VLM）的视频异常检测（VAD）方法虽然具备优越的零样本检测能力，但计算成本高、视觉定位不稳定，难以满足实时部署的需求。

Method: Cerberus 采用两阶段级联系统：离线学习正常行为规则，在线推理时结合轻量级过滤与细粒度 VLM 推理。核心创新包括运动掩码提示（引导 VLM 关注运动相关区域）和基于规则的偏离检测（将异常定义为对学习到的正常模式的偏离，而非枚举异常类型）。

Result: 在四个数据集上的广泛评估表明，Cerberus 在 NVIDIA L40S GPU 上平均达到 57.68 fps，比现有 VLM 方法快 151.79 倍，同时保持了 97.2% 的相似准确率。

Conclusion: Cerberus 在效率和准确性之间取得了良好平衡，是一种适用于实时视频分析的实用型视频异常检测解决方案。

Abstract: Video anomaly detection (VAD) has rapidly advanced by recent development of
Vision-Language Models (VLMs). While these models offer superior zero-shot
detection capabilities, their immense computational cost and unstable visual
grounding performance hinder real-time deployment. To overcome these
challenges, we introduce Cerberus, a two-stage cascaded system designed for
efficient yet accurate real-time VAD. Cerberus learns normal behavioral rules
offline, and combines lightweight filtering with fine-grained VLM reasoning
during online inference. The performance gains of Cerberus come from two key
innovations: motion mask prompting and rule-based deviation detection. The
former directs the VLM's attention to regions relevant to motion, while the
latter identifies anomalies as deviations from learned norms rather than
enumerating possible anomalies. Extensive evaluations on four datasets show
that Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a
151.79$\times$ speedup, and 97.2\% accuracy comparable to the state-of-the-art
VLM-based VAD methods, establishing it as a practical solution for real-time
video analytics.

</details>


### [24] [OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models](https://arxiv.org/abs/2510.16295)
*Ryoto Miyamoto,Xin Fan,Fuyuko Kido,Tsuneo Matsumoto,Hayato Yamana*

Main category: cs.CV

TL;DR: OpenLVLM-MIA是一个新的基准，揭示了在评估针对大型视觉语言模型的成员推断攻击时存在的根本挑战，发现先前高攻击成功率多源于数据集构建中的分布偏差而非真实成员状态识别。通过6000张图像的受控基准，结果显示当前最先进方法在无偏条件下表现趋近于随机猜测。


<details>
  <summary>Details</summary>
Motivation: 先前的成员推断攻击评估可能因数据分布偏差而产生误导性高成功率，缺乏公平、透明的评估标准，因此需要一个更可控、无偏的基准来真实反映攻击效果。

Method: 构建了一个包含6000张图像的受控基准OpenLVLM-MIA，其中成员与非成员样本分布均衡，并提供三个训练阶段的真实成员标签，用于公平评估现有MIA方法。

Result: 实验表明，在分布无偏的条件下，当前最先进的成员推断攻击方法性能退化至接近随机猜测水平，说明先前高成功率主要来自分布偏差检测。

Conclusion: OpenLVLM-MIA揭示了当前LVLM上成员推断攻击研究的局限性，提供了一个透明、无偏的评估基准，为未来隐私保护技术的发展奠定了基础。

Abstract: OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in
evaluating membership inference attacks (MIA) against large vision-language
models (LVLMs). While prior work has reported high attack success rates, our
analysis suggests that these results often arise from detecting distributional
bias introduced during dataset construction rather than from identifying true
membership status. To address this issue, we introduce a controlled benchmark
of 6{,}000 images where the distributions of member and non-member samples are
carefully balanced, and ground-truth membership labels are provided across
three distinct training stages. Experiments using OpenLVLM-MIA demonstrated
that the performance of state-of-the-art MIA methods converged to random chance
under unbiased conditions. By offering a transparent and unbiased benchmark,
OpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and
provides a solid foundation for developing stronger privacy-preserving
techniques.

</details>


### [25] [Stroke2Sketch: Harnessing Stroke Attributes for Training-Free Sketch Generation](https://arxiv.org/abs/2510.16319)
*Rui Yang,Huining Li,Yiyi Long,Xiaojun Wu,Shengfeng He*

Main category: cs.CV

TL;DR: 提出了一种无需训练的框架Stroke2Sketch，通过跨图像笔画注意力机制实现参考风格引导下的精确笔画属性迁移，同时保持语义结构和内容保真度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成风格化草图时难以兼顾笔画属性的精确迁移和语义结构的保持，因此需要一种能同时实现高质量风格迁移和内容保真的方法。

Method: 提出了Stroke2Sketch，引入跨图像笔画注意力机制，嵌入自注意力层中建立细粒度语义对应，实现笔画属性的准确迁移；并结合自适应对比度增强和语义聚焦注意力机制以增强内容保持和前景突出。

Result: Stroke2Sketch生成的草图在风格保真性和手绘效果方面优于现有方法，在表达性笔画控制和语义一致性上表现更优。

Conclusion: Stroke2Sketch是一种有效的无需训练的草图生成框架，能够在保持内容结构的同时实现高质量的风格化笔画迁移。

Abstract: Generating sketches guided by reference styles requires precise transfer of
stroke attributes, such as line thickness, deformation, and texture sparsity,
while preserving semantic structure and content fidelity. To this end, we
propose Stroke2Sketch, a novel training-free framework that introduces
cross-image stroke attention, a mechanism embedded within self-attention layers
to establish fine-grained semantic correspondences and enable accurate stroke
attribute transfer. This allows our method to adaptively integrate reference
stroke characteristics into content images while maintaining structural
integrity. Additionally, we develop adaptive contrast enhancement and
semantic-focused attention to reinforce content preservation and foreground
emphasis. Stroke2Sketch effectively synthesizes stylistically faithful sketches
that closely resemble handcrafted results, outperforming existing methods in
expressive stroke control and semantic coherence. Codes are available at
https://github.com/rane7/Stroke2Sketch.

</details>


### [26] [Scaling Laws for Deepfake Detection](https://arxiv.org/abs/2510.16320)
*Wenhao Wang,Longqi Cai,Taihong Xiao,Yuxiao Wang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 本文提出了首个针对深度伪造检测任务的系统性缩放定律研究，构建了迄今为止最大的数据集ScaleDF（包含51个真实图像域和102种深度伪造方法），发现检测误差随真实域或伪造方法数量增加呈幂律衰减，揭示了数据规模对检测性能的可预测影响。


<details>
  <summary>Details</summary>
Motivation: 随着深度伪造技术快速发展，现有检测方法面临泛化性差、评估不系统的问题。为揭示模型性能与数据规模之间的规律，需系统研究模型在真实图像域、伪造方法和训练样本数量上的缩放行为，而现有数据集无法满足此需求。

Method: 构建大规模数据集ScaleDF，包含5.8百万张来自51个域的真实图像和8.8百万张由102种方法生成的伪造图像；在此基础上训练和评估检测模型，分析性能随三个关键因素（真实域数、伪造方法数、训练样本量）变化的规律，并探索预训练和数据增强的影响。

Result: 发现深度伪造检测性能遵循幂律缩放规律：平均检测误差随真实图像域数或伪造方法数的增加而可预测地幂律衰减；该规律可用于预测达到目标性能所需的数据规模；同时发现预训练和数据增强在缩放条件下仍有效，但也揭示了单纯数据扩展的局限性。

Conclusion: 深度伪造检测存在类似大语言模型的缩放定律，表明数据多样性（域和方法数）对提升检测泛化性具有可预测的积极作用；这为未来应对新型伪造技术提供了数据驱动的策略指导，即通过扩大数据多样性来系统增强检测器鲁棒性。

Abstract: This paper presents a systematic study of scaling laws for the deepfake
detection task. Specifically, we analyze the model performance against the
number of real image domains, deepfake generation methods, and training images.
Since no existing dataset meets the scale requirements for this research, we
construct ScaleDF, the largest dataset to date in this field, which contains
over 5.8 million real images from 51 different datasets (domains) and more than
8.8 million fake images generated by 102 deepfake methods. Using ScaleDF, we
observe power-law scaling similar to that shown in large language models
(LLMs). Specifically, the average detection error follows a predictable
power-law decay as either the number of real domains or the number of deepfake
methods increases. This key observation not only allows us to forecast the
number of additional real domains or deepfake methods required to reach a
target performance, but also inspires us to counter the evolving deepfake
technology in a data-centric manner. Beyond this, we examine the role of
pre-training and data augmentations in deepfake detection under scaling, as
well as the limitations of scaling itself.

</details>


### [27] [Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical Local Attention](https://arxiv.org/abs/2510.16325)
*Yuyao Zhang,Yu-Wing Tai*

Main category: cs.CV

TL;DR: Scale-DiT是一种新的扩散模型框架，通过分层局部注意力和低分辨率全局引导，实现高效、可扩展且语义连贯的4K超高清图像生成，无需额外高分辨率训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型受限于注意力机制的二次复杂度和缺乏原生4K训练数据，难以实现超高清图像生成，本文旨在突破分辨率和效率瓶颈。

Method: 提出Scale-DiT框架：将高分辨率潜在表示划分为局部窗口以降低注意力复杂度至近线性；引入低分辨率潜在分支和缩放位置锚点注入全局语义；通过轻量LoRA模块融合全局与局部路径；采用希尔伯特曲线重排token序列，并设计融合内核跳过掩码操作以提升推理效率。

Result: Scale-DiT相比密集注意力基线实现2倍以上更快推理速度和更低内存占用，可稳定扩展至4K×4K分辨率。在FID、IS、CLIP Score等定量指标和视觉质量上优于或媲美依赖原生4K训练的最先进方法。

Conclusion: 分层局部注意力结合低分辨率全局引导是一种高效且可扩展的超高清图像生成方案，能够在不依赖高分辨率训练数据的情况下实现优异的全局结构和局部细节表现。

Abstract: Ultra-high-resolution text-to-image generation demands both fine-grained
texture synthesis and globally coherent structure, yet current diffusion models
remain constrained to sub-$1K \times 1K$ resolutions due to the prohibitive
quadratic complexity of attention and the scarcity of native $4K$ training
data. We present \textbf{Scale-DiT}, a new diffusion framework that introduces
hierarchical local attention with low-resolution global guidance, enabling
efficient, scalable, and semantically coherent image synthesis at ultra-high
resolutions. Specifically, high-resolution latents are divided into fixed-size
local windows to reduce attention complexity from quadratic to near-linear,
while a low-resolution latent equipped with scaled positional anchors injects
global semantics. A lightweight LoRA adaptation bridges global and local
pathways during denoising, ensuring consistency across structure and detail. To
maximize inference efficiency, we repermute token sequence in Hilbert curve
order and implement a fused-kernel for skipping masked operations, resulting in
a GPU-friendly design. Extensive experiments demonstrate that Scale-DiT
achieves more than $2\times$ faster inference and lower memory usage compared
to dense attention baselines, while reliably scaling to $4K \times 4K$
resolution without requiring additional high-resolution training data. On both
quantitative benchmarks (FID, IS, CLIP Score) and qualitative comparisons,
Scale-DiT delivers superior global coherence and sharper local detail, matching
or outperforming state-of-the-art methods that rely on native 4K training.
Taken together, these results highlight hierarchical local attention with
guided low-resolution anchors as a promising and effective approach for
advancing ultra-high-resolution image generation.

</details>


### [28] [DiffusionX: Efficient Edge-Cloud Collaborative Image Generation with Multi-Round Prompt Evolution](https://arxiv.org/abs/2510.16326)
*Yi Wei,Shunpu Tang,Liang Zhao,Qiangian Yang*

Main category: cs.CV

TL;DR: 提出DiffusionX，一种云-边协同框架，通过轻量级设备模型快速生成预览图像，云端大模型进行最终优化，结合噪声级别预测器动态平衡计算负载，显著降低生成时间和资源开销。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽在图像生成上取得进展，但生成过程计算密集，用户需多次迭代调整提示，增加延迟和云端负担，亟需高效支持多轮交互的方案。

Method: 设计云-边协同框架DiffusionX：设备端轻量模型快速生成预览，用户确定提示后，云端大模型进行精细生成；引入噪声级别预测器动态分配计算负载，优化延迟与资源使用。

Result: 相比Stable Diffusion v1.5平均生成时间减少15.8%，图像质量相当；仅比Tiny-SD慢0.9%，但质量显著更好，验证了效率与可扩展性。

Conclusion: DiffusionX在保证图像质量的同时显著提升生成效率，减轻云资源压力，适用于多轮提示交互的实用图像生成场景。

Abstract: Recent advances in diffusion models have driven remarkable progress in image
generation. However, the generation process remains computationally intensive,
and users often need to iteratively refine prompts to achieve the desired
results, further increasing latency and placing a heavy burden on cloud
resources. To address this challenge, we propose DiffusionX, a cloud-edge
collaborative framework for efficient multi-round, prompt-based generation. In
this system, a lightweight on-device diffusion model interacts with users by
rapidly producing preview images, while a high-capacity cloud model performs
final refinements after the prompt is finalized. We further introduce a noise
level predictor that dynamically balances the computation load, optimizing the
trade-off between latency and cloud workload. Experiments show that DiffusionX
reduces average generation time by 15.8% compared with Stable Diffusion v1.5,
while maintaining comparable image quality. Moreover, it is only 0.9% slower
than Tiny-SD with significantly improved image quality, thereby demonstrating
efficiency and scalability with minimal overhead.

</details>


### [29] [TokenAR: Multiple Subject Generation via Autoregressive Token-level enhancement](https://arxiv.org/abs/2510.16332)
*Haiyue Sun,Qingdong He,Jinlong Peng,Peng Tang,Jiangning Zhang,Junwei Zhu,Xiaobin Hu,Shuicheng Yan*

Main category: cs.CV

TL;DR: 本文提出了一种名为TokenAR的新型框架，通过引入token级别的增强机制，有效解决多参考图像生成中身份混淆的问题，并发布了首个大规模开源多参考图像生成数据集InstructAR。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归模型在多参考图像生成中难以解耦不同参考对象的身份信息，导致生成结果中出现身份混淆。本文旨在通过精细化的token级控制机制来解决这一问题。

Method: TokenAR框架包含三部分：1）Token Index Embedding，通过聚类相同参考图像的token索引来增强表示；2）Instruct Token Injection，引入指令token作为额外视觉特征容器，注入细节先验；3）身份- token解耦策略（ITD），显式引导token独立表征各自身份特征。

Result: 实验表明，TokenAR在多参考图像生成任务中优于当前最先进的方法，实现了更好的身份一致性与高质量的背景重建。同时发布了包含28K训练对的InstructAR数据集。

Conclusion: TokenAR通过token级增强显著提升了自回归模型在条件图像生成中的性能，为多主体图像生成提供了有效的解决方案和数据支持。

Abstract: Autoregressive Model (AR) has shown remarkable success in conditional image
generation. However, these approaches for multiple reference generation
struggle with decoupling different reference identities. In this work, we
propose the TokenAR framework, specifically focused on a simple but effective
token-level enhancement mechanism to address reference identity confusion
problem. Such token-level enhancement consists of three parts, 1). Token Index
Embedding clusters the tokens index for better representing the same reference
images; 2). Instruct Token Injection plays as a role of extra visual feature
container to inject detailed and complementary priors for reference tokens; 3).
The identity-token disentanglement strategy (ITD) explicitly guides the token
representations toward independently representing the features of each
identity.This token-enhancement framework significantly augments the
capabilities of existing AR based methods in conditional image generation,
enabling good identity consistency while preserving high quality background
reconstruction. Driven by the goal of high-quality and high-diversity in
multi-subject generation, we introduce the InstructAR Dataset, the first
open-source, large-scale, multi-reference input, open domain image generation
dataset that includes 28K training pairs, each example has two reference
subjects, a relative prompt and a background with mask annotation, curated for
multiple reference image generation training and evaluating. Comprehensive
experiments validate that our approach surpasses current state-of-the-art
models in multiple reference image generation task. The implementation code and
datasets will be made publicly. Codes are available, see
https://github.com/lyrig/TokenAR

</details>


### [30] [RL makes MLLMs see better than SFT](https://arxiv.org/abs/2510.16333)
*Junha Song,Sangdoo Yun,Dongyoon Han,Jaegul Choo,Byeongho Heo*

Main category: cs.CV

TL;DR: 本文研究了多模态语言模型（MLLM）中视觉编码器在不同训练策略下的变化，发现强化学习（RL）相比监督微调（SFT）能产生更强且定位更精准的视觉表征，并提出一种高效训练视觉编码器的新方法PIVOT，计算成本不足传统预训练的1%。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM研究过于依赖大语言模型的性能，忽视了视觉编码器的作用，尤其是在训练范式从SFT转向RL的背景下，缺乏对视觉编码器如何被重塑的深入分析。

Method: 通过多种实验（如ImageNet分类、分割、梯度可视化等）系统分析SFT与RL对视觉编码器的影响，并基于发现提出新方法PIVOT，即偏好指导的视觉优化。

Result: RL训练显著优于SFT，在视觉相关VQA任务上表现更佳；PIVOT训练的视觉编码器在性能上超越更大、训练更重的模型，且计算成本极低。

Conclusion: MLLM的训练策略会根本性重塑视觉表征，RL能生成更优的视觉编码器；PIVOT为构建高效强大的MLLM视觉主干提供了新路径。

Abstract: A dominant assumption in Multimodal Language Model (MLLM) research is that
its performance is largely inherited from the LLM backbone, given its immense
parameter scale and remarkable capabilities. This has created a void in the
understanding of the vision encoder, which determines how MLLMs perceive
images. The recent shift in MLLM training paradigms, from Supervised Finetuning
(SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the
significant lack of analysis on how such training reshapes the vision encoder
as well as the MLLM. To address this, we first investigate the impact of
training strategies on MLLMs, where RL shows a clear advantage over SFT in
strongly vision-related VQA benchmarks. Motivated by this, we conduct a
critical yet under-explored analysis of the vision encoder of MLLMs through
diverse and in-depth experiments, ranging from ImageNet classification and
segmentation to gradient visualization. Our results demonstrate that MLLM's
post-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on
MLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual
representations. Specifically, the key finding of our study is that RL produces
stronger and precisely localized visual representations compared to SFT,
boosting the ability of the vision encoder for MLLM. We then reframe our
findings into a simple recipe for building strong vision encoders for MLLMs,
Preference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs,
a PIVOT-trained vision encoder outperforms even larger and more heavily-trained
counterparts, despite requiring less than 1% of the computational cost of
standard vision pretraining. This result opens an effective and efficient path
for advancing the vision backbones of MLLMs. Project page available at
https://june-page.github.io/pivot/

</details>


### [31] [On the Provable Importance of Gradients for Language-Assisted Image Clustering](https://arxiv.org/abs/2510.16335)
*Bo Peng,Jie Lu,Guangquan Zhang,Zhen Fang*

Main category: cs.CV

TL;DR: 本文提出了一种基于梯度的新型框架GradNorm，用于语言辅助图像聚类（LaIC），通过理论保证和实证表现优异，实现了最先进的聚类性能。


<details>
  <summary>Details</summary>
Motivation: 在缺乏真实类别名称的情况下，如何从无标签的大规模文本语料中筛选出与目标图像语义接近的正向名词是语言辅助图像聚类的核心挑战之一。现有方法依赖CLIP的特征空间但缺乏严谨的理论基础。

Method: 提出GradNorm框架，利用从交叉熵损失反向传播的梯度幅值来衡量每个名词的“正向性”，该方法具有理论保证，并可统一现有过滤策略作为其特例。

Result: 理论分析给出了GradNorm区分正向名词的误差界，实验证明其在多个基准数据集上达到最先进的聚类性能。

Conclusion: GradNorm为语言辅助图像聚类提供了一个理论严谨且高效的方法，显著提升了聚类效果，并统一了现有方法。

Abstract: This paper investigates the recently emerged problem of Language-assisted
Image Clustering (LaIC), where textual semantics are leveraged to improve the
discriminability of visual representations to facilitate image clustering. Due
to the unavailability of true class names, one of core challenges of LaIC lies
in how to filter positive nouns, i.e., those semantically close to the images
of interest, from unlabeled wild corpus data. Existing filtering strategies are
predominantly based on the off-the-shelf feature space learned by CLIP;
however, despite being intuitive, these strategies lack a rigorous theoretical
foundation. To fill this gap, we propose a novel gradient-based framework,
termed as GradNorm, which is theoretically guaranteed and shows strong
empirical performance. In particular, we measure the positiveness of each noun
based on the magnitude of gradients back-propagated from the cross-entropy
between the predicted target distribution and the softmax output.
Theoretically, we provide a rigorous error bound to quantify the separability
of positive nouns by GradNorm and prove that GradNorm naturally subsumes
existing filtering strategies as extremely special cases of itself.
Empirically, extensive experiments show that GradNorm achieves the
state-of-the-art clustering performance on various benchmarks.

</details>


### [32] [MIRAD - A comprehensive real-world robust anomaly detection dataset for Mass Individualization](https://arxiv.org/abs/2510.16370)
*Pulin Li,Guocheng Wu,Li Yin,Yuxin Zheng,Wei Zhang,Yanjie Zhou*

Main category: cs.CV

TL;DR: 本文提出了首个面向社交制造中异常检测的基准数据集MIRAD，旨在解决大规模个性化生产中的缺陷检测难题，数据集涵盖多样化产品、分布式采集和成像异质性，并评估了多种先进异常检测方法，表现出现有方法在真实个性化生产中的性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 社交制造中的大规模个性化生产带来了产品质量控制的挑战，尤其是缺陷检测方面，由于产品定制化高、生产批次小且分布分散、成像环境差异大，导致现有数据集和算法难以适用，因此需要一个贴近实际场景的基准数据集来推动研究。

Method: 构建了一个名为MIRAD的新数据集，包含来自六个地理分布制造节点的数据，涵盖个性化产品的多样性和显著的成像异质性（如光照、背景、运动条件变化），并系统评估了当前最先进的单类、多类和零样本异常检测方法在该数据集上的表现。

Result: 在MIRAD数据集上，所有先进异常检测方法相比传统基准性能显著下降，表明现有技术在应对真实社交制造场景中的复杂性方面仍存在重大挑战。

Conclusion: MIRAD为社交制造中的鲁棒异常检测提供了现实且具有挑战性的基准，有助于弥合工业实际需求与学术研究之间的差距，推动面向工业5.0的质量控制技术发展。

Abstract: Social manufacturing leverages community collaboration and scattered
resources to realize mass individualization in modern industry. However, this
paradigm shift also introduces substantial challenges in quality control,
particularly in defect detection. The main difficulties stem from three
aspects. First, products often have highly customized configurations. Second,
production typically involves fragmented, small-batch orders. Third, imaging
environments vary considerably across distributed sites. To overcome the
scarcity of real-world datasets and tailored algorithms, we introduce the Mass
Individualization Robust Anomaly Detection (MIRAD) dataset. As the first
benchmark explicitly designed for anomaly detection in social manufacturing,
MIRAD captures three critical dimensions of this domain: (1) diverse
individualized products with large intra-class variation, (2) data collected
from six geographically dispersed manufacturing nodes, and (3) substantial
imaging heterogeneity, including variations in lighting, background, and motion
conditions. We then conduct extensive evaluations of state-of-the-art (SOTA)
anomaly detection methods on MIRAD, covering one-class, multi-class, and
zero-shot approaches. Results show a significant performance drop across all
models compared with conventional benchmarks, highlighting the unresolved
complexities of defect detection in real-world individualized production. By
bridging industrial requirements and academic research, MIRAD provides a
realistic foundation for developing robust quality control solutions essential
for Industry 5.0. The dataset is publicly available at
https://github.com/wu33learn/MIRAD.

</details>


### [33] [Cataract-LMM: Large-Scale, Multi-Source, Multi-Task Benchmark for Deep Learning in Surgical Video Analysis](https://arxiv.org/abs/2510.16371)
*Mohammad Javad Ahmadi,Iman Gandomi,Parisa Abdi,Seyed-Farzad Mohammadi,Amirhossein Taslimi,Mehdi Khodaparast,Hassan Hashemi,Mahdi Tavakoli,Hamid D. Taghirad*

Main category: cs.CV

TL;DR: 提出包含3000个白内障手术视频的多中心数据集，具备多层次标注，支持手术流程识别、场景分割和技能评估等AI任务的基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有白内障手术数据集缺乏多样性和深度标注，限制了深度学习模型的泛化能力，需构建更全面的数据资源。

Method: 收集来自两个医疗中心、不同经验水平医生执行的3000例超声乳化手术视频，提供四个层次的标注：手术阶段时序、器械与解剖结构实例分割、器械-组织交互跟踪及基于ICO-OSCAR等标准的定量技能评分，并进行多项基准实验与域自适应分析。

Result: 数据集支持手术流程识别、场景分割和自动技能评估等多项AI任务的基准测试，且域自适应实验建立了跨中心相位识别的基线性能。

Conclusion: 该数据集为白内障手术的计算机辅助系统提供了高质量、多维度标注的资源，有助于提升手术AI模型的泛化能力和临床适用性。

Abstract: The development of computer-assisted surgery systems depends on large-scale,
annotated datasets. Current resources for cataract surgery often lack the
diversity and annotation depth needed to train generalizable deep-learning
models. To address this gap, we present a dataset of 3,000 phacoemulsification
cataract surgery videos from two surgical centers, performed by surgeons with a
range of experience levels. This resource is enriched with four annotation
layers: temporal surgical phases, instance segmentation of instruments and
anatomical structures, instrument-tissue interaction tracking, and quantitative
skill scores based on the established competency rubrics like the ICO-OSCAR.
The technical quality of the dataset is supported by a series of benchmarking
experiments for key surgical AI tasks, including workflow recognition, scene
segmentation, and automated skill assessment. Furthermore, we establish a
domain adaptation baseline for the phase recognition task by training a model
on a subset of surgical centers and evaluating its performance on a held-out
center. The dataset and annotations are available in Google Form
(https://docs.google.com/forms/d/e/1FAIpQLSfmyMAPSTGrIy2sTnz0-TMw08ZagTimRulbAQcWdaPwDy187A/viewform?usp=dialog).

</details>


### [34] [iWatchRoadv2: Pothole Detection, Geospatial Mapping, and Intelligent Road Governance](https://arxiv.org/abs/2510.16375)
*Rishi Raj Sahoo,Surbhi Saswati Mohanty,Subhankar Mishra*

Main category: cs.CV

TL;DR: iWatchRoadv2是一个全自动、端到端的实时路面坑洼检测平台，结合YOLO模型与GPS定位，在印度复杂道路条件下实现精准识别和地理标记，并通过智能治理功能支持道路维护的自动化问责与维修验证。


<details>
  <summary>Details</summary>
Motivation: 印度道路网络复杂且维护不足，坑洼频发带来安全隐患与管理难题，亟需一种高效、可扩展的自动化监测系统以提升道路维护效率与治理透明度。

Method: 构建了一个包含7000多帧行车记录仪图像的自注释数据集，基于Ultralytics YOLO模型进行微调以实现坑洼检测；结合OCR提取的时间戳与外部GPS日志进行同步定位；开发基于OpenStreetMap的动态道路健康可视化系统，并集成包含合同信息管理的后端数据库。

Result: 系统能准确检测并精准地理标记坑洼，实现检测、报警、责任分配到维修验证的全流程自动化；支持向承包商和官员自动发送预警，提供直观的Web界面用于数据分析和公共参与。

Conclusion: iWatchRoadv2为道路基础设施的智能管理提供了可扩展、低成本的解决方案，推动了数据驱动的城市治理、透明化管理和可持续的道路维护实践。

Abstract: Road potholes pose significant safety hazards and maintenance challenges,
particularly on India's diverse and under-maintained road networks. This paper
presents iWatchRoadv2, a fully automated end-to-end platform for real-time
pothole detection, GPS-based geotagging, and dynamic road health visualization
using OpenStreetMap (OSM). We curated a self-annotated dataset of over 7,000
dashcam frames capturing diverse Indian road conditions, weather patterns, and
lighting scenarios, which we used to fine-tune the Ultralytics YOLO model for
accurate pothole detection. The system synchronizes OCR-extracted video
timestamps with external GPS logs to precisely geolocate each detected pothole,
enriching detections with comprehensive metadata, including road segment
attribution and contractor information managed through an optimized backend
database. iWatchRoadv2 introduces intelligent governance features that enable
authorities to link road segments with contract metadata through a secure login
interface. The system automatically sends alerts to contractors and officials
when road health deteriorates, supporting automated accountability and warranty
enforcement. The intuitive web interface delivers actionable analytics to
stakeholders and the public, facilitating evidence-driven repair planning,
budget allocation, and quality assessment. Our cost-effective and scalable
solution streamlines frame processing and storage while supporting seamless
public engagement for urban and rural deployments. By automating the complete
pothole monitoring lifecycle, from detection to repair verification,
iWatchRoadv2 enables data-driven smart city management, transparent governance,
and sustainable improvements in road infrastructure maintenance. The platform
and live demonstration are accessible at
https://smlab.niser.ac.in/project/iwatchroad.

</details>


### [35] [Demeter: A Parametric Model of Crop Plant Morphology from the Real World](https://arxiv.org/abs/2510.16377)
*Tianhang Cheng,Albert J. Zhai,Evan Z. Chen,Rui Zhou,Yawen Deng,Zitong Li,Kejie Zhao,Janice Shiu,Qianyu Zhao,Yide Xu,Xinlei Wang,Yuan Shen,Sheng Wang,Lisa Ainsworth,Kaiyu Guan,Shenlong Wang*

Main category: cs.CV

TL;DR: 本文提出Demeter，一种数据驱动的参数化植物形态模型，能够编码植物的拓扑、形状、关节和变形，并支持跨物种的形态变化建模。


<details>
  <summary>Details</summary>
Motivation: 现有的3D参数化形状模型在人类和动物领域已有广泛应用，但对植物尤其是作物的建模仍缺乏表达能力强的模型。植物具有复杂的拓扑变化和多种形态变异，需要更灵活的建模方法。

Method: 提出Demeter模型，采用紧凑的学习表示来编码植物的关键形态因素，包括拓扑结构、形状、关节和非刚性变形；支持三种形态变异：关节运动、子部件形状变化和非刚性变形；并在一个大规模、真实标注的大豆农场数据集上进行训练和验证。

Result: 实验表明，Demeter在植物形状生成、结构重建以及生物物理过程模拟方面均表现有效，优于现有参数化模型。

Conclusion: Demeter为植物特别是作物的3D建模提供了强大且灵活的数据驱动解决方案，填补了当前在植物参数化建模方面的空白。

Abstract: Learning 3D parametric shape models of objects has gained popularity in
vision and graphics and has showed broad utility in 3D reconstruction,
generation, understanding, and simulation. While powerful models exist for
humans and animals, equally expressive approaches for modeling plants are
lacking. In this work, we present Demeter, a data-driven parametric model that
encodes key factors of a plant morphology, including topology, shape,
articulation, and deformation into a compact learned representation. Unlike
previous parametric models, Demeter handles varying shape topology across
various species and models three sources of shape variation: articulation,
subcomponent shape variation, and non-rigid deformation. To advance crop plant
modeling, we collected a large-scale, ground-truthed dataset from a soybean
farm as a testbed. Experiments show that Demeter effectively synthesizes
shapes, reconstructs structures, and simulates biophysical processes. Code and
data is available at https://tianhang-cheng.github.io/Demeter/.

</details>


### [36] [SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation](https://arxiv.org/abs/2510.16396)
*Yeh Keng Hao,Hsu Tzu Wei,Sun Min*

Main category: cs.CV

TL;DR: 提出一种轻量级编码器-解码器框架，结合稀疏卷积与SPLite解码器，在边缘设备上显著提升手部姿态估计的效率与速度，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: AR/VR设备对实时性、低功耗和低延迟提出高要求，现有深度学习模型在边缘设备上难以兼顾效率与性能。

Method: 采用稀疏卷积的ResNet-18作为编码器，设计新型SPLite解码器，并结合量化感知训练以优化模型效率与内存占用。

Result: 在Raspberry Pi 5上实现端到端效率提升42%，解码帧率提升3.1倍，整体CPU加速2.98倍，PA-MPJPE仅从9.0 mm微增至9.1 mm。

Conclusion: 所提框架在保持与SOTA方法相当精度的同时，显著提升计算效率，适合部署于资源受限的AR/VR边缘设备。

Abstract: With the increasing ubiquity of AR/VR devices, the deployment of deep
learning models on edge devices has become a critical challenge. These devices
require real-time inference, low power consumption, and minimal latency. Many
framework designers face the conundrum of balancing efficiency and performance.
We design a light framework that adopts an encoder-decoder architecture and
introduces several key contributions aimed at improving both efficiency and
accuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the
inherent sparsity in hand pose images, achieving a 42% end-to-end efficiency
improvement. Moreover, we propose our SPLite decoder. This new architecture
significantly boosts the decoding process's frame rate by 3.1x on the Raspberry
Pi 5, while maintaining accuracy on par. To further optimize performance, we
apply quantization-aware training, reducing memory usage while preserving
accuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on
FreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5
CPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on
compound benchmark datasets, demonstrating comparable accuracy to
state-of-the-art approaches while significantly enhancing computational
efficiency.

</details>


### [37] [REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting](https://arxiv.org/abs/2510.16410)
*Changyue Shi,Minghao Chen,Yiping Mao,Chuxiao Yang,Xinyuan Hu,Jiajun Ding,Zhou Yu*

Main category: cs.CV

TL;DR: 提出REALM框架，通过多视图渲染和全局到局部的空间定位策略，实现无需大量3D后训练的开放世界推理型3D对象分割。


<details>
  <summary>Details</summary>
Motivation: 现有3D分割方法难以理解模糊或需推理的指令，而擅长推理的2D视觉语言模型缺乏3D空间理解能力，因此需要一种结合两者优势的新方法。

Method: 利用3D Gaussian Splatting生成逼真新视角图像，输入MLLM代理；采用全局到局部的空间定位策略：先并行输入多个全局视图进行粗略定位，再合成局部特写视图实现精细分割。

Result: 在LERF、3D-OVS和新建的REALM3D基准上表现出色，能准确解析显式和隐式指令，生成精确一致的3D掩码。

Conclusion: REALM实现了高性能的推理型3D对象分割，并支持物体移除、替换和风格迁移等交互任务，具有实用性和通用性。

Abstract: Bridging the gap between complex human instructions and precise 3D object
grounding remains a significant challenge in vision and robotics. Existing 3D
segmentation methods often struggle to interpret ambiguous, reasoning-based
instructions, while 2D vision-language models that excel at such reasoning lack
intrinsic 3D spatial understanding. In this paper, we introduce REALM, an
innovative MLLM-agent framework that enables open-world reasoning-based
segmentation without requiring extensive 3D-specific post-training. We perform
segmentation directly on 3D Gaussian Splatting representations, capitalizing on
their ability to render photorealistic novel views that are highly suitable for
MLLM comprehension. As directly feeding one or more rendered views to the MLLM
can lead to high sensitivity to viewpoint selection, we propose a novel
Global-to-Local Spatial Grounding strategy. Specifically, multiple global views
are first fed into the MLLM agent in parallel for coarse-level localization,
aggregating responses to robustly identify the target object. Then, several
close-up novel views of the object are synthesized to perform fine-grained
local segmentation, yielding accurate and consistent 3D masks. Extensive
experiments show that REALM achieves remarkable performance in interpreting
both explicit and implicit instructions across LERF, 3D-OVS, and our newly
introduced REALM3D benchmarks. Furthermore, our agent framework seamlessly
supports a range of 3D interaction tasks, including object removal,
replacement, and style transfer, demonstrating its practical utility and
versatility. Project page: https://ChangyueShi.github.io/REALM.

</details>


### [38] [SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning](https://arxiv.org/abs/2510.16416)
*Xiaojun Guo,Runyu Zhou,Yifei Wang,Qi Zhang,Chenheng Zhang,Stefanie Jegelka,Xiaohan Wang,Jiajun Chai,Guojun Yin,Wei Lin,Yisen Wang*

Main category: cs.CV

TL;DR: 提出SSL4RL框架，利用自监督学习任务作为强化学习的可验证奖励信号，显著提升视觉-语言模型在视觉主导和视觉-语言推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型常依赖语言先验或文本捷径，未能充分使用视觉证据；强化学习缺乏可扩展、可靠的奖励机制。

Method: 将自监督学习任务（如图像旋转预测、掩码块重建）转化为密集、自动的奖励信号，用于强化学习微调，无需人类偏好数据或不可靠的AI评估器。

Result: SSL4RL在视觉和视觉-语言推理基准上显著提升性能，并在图学习中也展现有效性，验证了框架的通用性。

Conclusion: SSL4RL建立了一种通用、有效的多模态模型对齐范式，使用可验证的自监督目标，为未来工作提供了新设计原则。

Abstract: Vision-language models (VLMs) have shown remarkable abilities by integrating
large language models with visual inputs. However, they often fail to utilize
visual evidence adequately, either depending on linguistic priors in
vision-centric tasks or resorting to textual shortcuts during reasoning.
Although reinforcement learning (RL) can align models with desired behaviors,
its application to VLMs has been hindered by the lack of scalable and reliable
reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel
framework that leverages self-supervised learning (SSL) tasks as a source of
verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL
objectives-such as predicting image rotation or reconstructing masked
patches-into dense, automatic reward signals, eliminating the need for human
preference data or unreliable AI evaluators. Experiments show that SSL4RL
substantially improves performance on both vision-centric and vision-language
reasoning benchmarks. Furthermore, through systematic ablations, we identify
key factors-such as task difficulty, model scale, and semantic alignment with
the target domain-that influence the effectiveness of SSL4RL tasks, offering
new design principles for future work. We also demonstrate the framework's
generality by applying it to graph learning, where it yields significant gains.
SSL4RL establishes a versatile and effective paradigm for aligning multimodal
models using verifiable, self-supervised objectives.

</details>


### [39] [LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching](https://arxiv.org/abs/2510.16438)
*Aidyn Ubingazhibov,Rémi Pautrat,Iago Suárez,Shaohui Liu,Marc Pollefeys,Viktor Larsson*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的点和线段联合匹配方法LightGlueStick，通过引入注意力线消息传递（ALMP）机制，在保持高性能的同时显著降低了计算复杂度，实现了新的性能纪录。


<details>
  <summary>Details</summary>
Motivation: 现有的点和线匹配方法通常独立处理，而联合匹配虽已提出（如GlueStick），但其复杂结构限制了实时应用和边缘设备部署，因此需要一种更高效的方法。

Method: 受点匹配最新进展启发，提出LightGlueStick，引入注意力线消息传递（ALMP）模块，显式建模线段的连接性，实现节点间高效通信，并采用轻量级架构以提升效率。

Result: 实验表明，LightGlueStick在多个基准上达到了新的最先进水平，同时减少了计算开销，适合实时和边缘设备应用。

Conclusion: LightGlueStick通过轻量设计和ALMP机制，有效结合点和线段匹配，为SLAM、运动恢复结构等任务提供了高效、先进的解决方案。

Abstract: Lines and points are complementary local features, whose combination has
proven effective for applications such as SLAM and Structure-from-Motion. The
backbone of these pipelines are the local feature matchers, establishing
correspondences across images. Traditionally, point and line matching have been
treated as independent tasks. Recently, GlueStick proposed a GNN-based network
that simultaneously operates on points and lines to establish matches. While
running a single joint matching reduced the overall computational complexity,
the heavy architecture prevented real-time applications or deployment to edge
devices.
  Inspired by recent progress in point matching, we propose LightGlueStick, a
lightweight matcher for points and line segments. The key novel component in
our architecture is the Attentional Line Message Passing (ALMP), which
explicitly exposes the connectivity of the lines to the network, allowing for
efficient communication between nodes. In thorough experiments we show that
LightGlueStick establishes a new state-of-the-art across different benchmarks.
The code is available at https://github.com/aubingazhib/LightGlueStick.

</details>


### [40] [EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning](https://arxiv.org/abs/2510.16442)
*Haoran Sun,Chen Cai,Huiping Zhuang,Kong Aik Lee,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: 本文提出了一种可解释的深度伪造视频检测框架EDVD-LLaMA，结合时空特征提取与多模态推理，实现高精度检测与可信解释。


<details>
  <summary>Details</summary>
Motivation: 传统深度伪造检测方法缺乏透明性和泛化能力，难以应对不断演进的伪造技术，亟需具备可解释性和强泛化能力的检测模型。

Method: 提出EDVD任务和EDVD-LLaMA框架：1）设计时空细微信息分词（ST-SIT）提取并融合跨帧深伪特征；2）构建细粒度多模态思维链（Fg-MCoT），引入面部特征作为硬约束实现像素级定位；3）构建可解释推理基准数据集ER-FF++set。

Result: 实验表明，EDVD-LLaMA在检测准确率、可解释性及跨伪造方法、跨数据集场景下均表现出优异性能和强鲁棒性。

Conclusion: 该方法实现了可追溯的推理过程与可信解释，为深度伪造检测提供了更可解释、更优的解决方案。

Abstract: The rapid development of deepfake video technology has not only facilitated
artistic creation but also made it easier to spread misinformation. Traditional
deepfake video detection (DVD) methods face issues such as a lack of
transparency in their principles and insufficient generalization capabilities
to cope with evolving forgery techniques. This highlights an urgent need for
detectors that can identify forged content and provide verifiable reasoning
explanations. This paper proposes the explainable deepfake video detection
(EDVD) task and designs the EDVD-LLaMA multimodal, a large language model
(MLLM) reasoning framework, which provides traceable reasoning processes
alongside accurate detection results and trustworthy explanations. Our approach
first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT)
to extract and fuse global and local cross-frame deepfake features, providing
rich spatio-temporal semantic information input for MLLM reasoning. Second, we
construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which
introduces facial feature data as hard constraints during the reasoning process
to achieve pixel-level spatio-temporal video localization, suppress
hallucinated outputs, and enhance the reliability of the chain of thought. In
addition, we build an Explainable Reasoning FF++ benchmark dataset
(ER-FF++set), leveraging structured data to annotate videos and ensure quality
control, thereby supporting dual supervision for reasoning and detection.
Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding
performance and robustness in terms of detection accuracy, explainability, and
its ability to handle cross-forgery methods and cross-dataset scenarios.
Compared to previous DVD methods, it provides a more explainable and superior
solution. The source code and dataset will be publicly available.

</details>


### [41] [RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba](https://arxiv.org/abs/2510.16444)
*Kunyu Peng,Di Wen,Jia Fu,Jiamin Wu,Kailun Yang,Junwei Zheng,Ruiping Liu,Yufan Chen,Yuqian Fu,Danda Pani Paudel,Luc Van Gool,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 本文提出了RefAtomNet++，一种用于自然语言指导下的细粒度视频动作识别的新框架，并发布了更大规模的RefAVA++数据集，通过多层级语义对齐和Mamba建模显著提升了跨模态理解性能。


<details>
  <summary>Details</summary>
Motivation: 现有的动作识别方法在多模态对齐和定位特定人物的细粒度动作方面能力有限，尤其是在复杂多人群场景中需结合语言指引进行精准动作理解，因此需要更强大的跨模态建模方法。

Method: 提出RefAtomNet++，引入多层级语义对齐的交叉注意力机制，并结合多轨迹Mamba建模，在部分关键词、场景属性和完整句子层级实现跨模态令牌聚合；通过动态选择最近的视觉空间令牌构建扫描轨迹以增强时空特征对齐。

Result: 在RefAVA++数据集上，RefAtomNet++显著优于包括RefAtomNet在内的多种基线模型，实现了新的最先进性能。

Conclusion: RefAtomNet++通过多层次语义对齐与高效跨模态聚合，有效提升了语言引导的细粒度视频动作识别效果，为交互式复杂场景下的人类动作分析提供了更强的解决方案。

Abstract: Referring Atomic Video Action Recognition (RAVAR) aims to recognize
fine-grained, atomic-level actions of a specific person of interest conditioned
on natural language descriptions. Distinct from conventional action recognition
and detection tasks, RAVAR emphasizes precise language-guided action
understanding, which is particularly critical for interactive human action
analysis in complex multi-person scenarios. In this work, we extend our
previously introduced RefAVA dataset to RefAVA++, which comprises >2.9 million
frames and >75.1k annotated persons in total. We benchmark this dataset using
baselines from multiple related domains, including atomic action localization,
video question answering, and text-video retrieval, as well as our earlier
model, RefAtomNet. Although RefAtomNet surpasses other baselines by
incorporating agent attention to highlight salient features, its ability to
align and retrieve cross-modal information remains limited, leading to
suboptimal performance in localizing the target person and predicting
fine-grained actions. To overcome the aforementioned limitations, we introduce
RefAtomNet++, a novel framework that advances cross-modal token aggregation
through a multi-hierarchical semantic-aligned cross-attention mechanism
combined with multi-trajectory Mamba modeling at the partial-keyword,
scene-attribute, and holistic-sentence levels. In particular, scanning
trajectories are constructed by dynamically selecting the nearest visual
spatial tokens at each timestep for both partial-keyword and scene-attribute
levels. Moreover, we design a multi-hierarchical semantic-aligned
cross-attention strategy, enabling more effective aggregation of spatial and
temporal tokens across different semantic hierarchies. Experiments show that
RefAtomNet++ establishes new state-of-the-art results. The dataset and code are
released at https://github.com/KPeng9510/refAVA2.

</details>


### [42] [Enhancing Rotated Object Detection via Anisotropic Gaussian Bounding Box and Bhattacharyya Distance](https://arxiv.org/abs/2510.16445)
*Chien Thai,Mai Xuan Trang,Huong Ninh,Hoang Hiep Ly,Anh Son Le*

Main category: cs.CV

TL;DR: 本文提出了一种基于高斯边界框表示和巴塔恰里亚距离的改进损失函数，用于提升旋转目标检测的精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测方法在处理轴对齐目标时表现良好，但在面对旋转目标时由于无法有效捕捉方向变化而性能下降，因此需要更适应旋转场景的检测方法。

Method: 采用各向异性高斯边界框表示，并结合巴塔恰里亚距离设计旋转不变的损失函数，将其集成到先进的深度学习旋转目标检测器中。

Result: 在多个数据集上实验表明，所提方法显著提升了平均精度均值（mAP）指标，优于现有方法。

Conclusion: 该方法在旋转目标检测中具有优越性能，有望成为新基准，并推动遥感、自动驾驶等领域的应用发展。

Abstract: Detecting rotated objects accurately and efficiently is a significant
challenge in computer vision, particularly in applications such as aerial
imagery, remote sensing, and autonomous driving. Although traditional object
detection frameworks are effective for axis-aligned objects, they often
underperform in scenarios involving rotated objects due to their limitations in
capturing orientation variations. This paper introduces an improved loss
function aimed at enhancing detection accuracy and robustness by leveraging the
Gaussian bounding box representation and Bhattacharyya distance. In addition,
we advocate for the use of an anisotropic Gaussian representation to address
the issues associated with isotropic variance in square-like objects. Our
proposed method addresses these challenges by incorporating a
rotation-invariant loss function that effectively captures the geometric
properties of rotated objects. We integrate this proposed loss function into
state-of-the-art deep learning-based rotated object detection detectors, and
extensive experiments demonstrated significant improvements in mean Average
Precision metrics compared to existing methods. The results highlight the
potential of our approach to establish new benchmark in rotated object
detection, with implications for a wide range of applications requiring precise
and reliable object localization irrespective of orientation.

</details>


### [43] [VIPAMIN: Visual Prompt Initialization via Embedding Selection and Subspace Expansion](https://arxiv.org/abs/2510.16446)
*Jaekyun Park,Hye Won Chung*

Main category: cs.CV

TL;DR: VIPAMIN是一种新的视觉提示初始化方法，通过在嵌入空间中对齐语义信息区域并引入新的表示方向，显著提升自监督模型在少样本和资源受限场景下的适应性能，且仅需一次前向传播和轻量操作。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉提示调优方法在应用于自监督骨干网络时，往往未能有效特化提示或扩充表示空间，尤其在任务复杂或数据稀缺时表现不佳。因此需要一种更有效的提示初始化策略来增强模型适应能力。

Method: 提出VIPAMIN，其核心方法包括两部分：(1) 将提示与嵌入空间中语义丰富区域对齐；(2) 注入超出预训练子空间的新表示方向。该方法仅需一次前向传播和轻量级计算即可完成初始化。

Result: VIPAMIN在多种任务和数据集规模下均显著优于现有提示调优方法，尤其在数据稀缺场景中表现突出，实现了新的性能标杆。

Conclusion: VIPAMIN通过更智能的提示初始化策略，有效提升了自监督模型在视觉提示调优中的适应能力，为轻量级模型微调提供了高效且实用的解决方案。

Abstract: In the era of large-scale foundation models, fully fine-tuning pretrained
networks for each downstream task is often prohibitively resource-intensive.
Prompt tuning offers a lightweight alternative by introducing tunable prompts
while keeping the backbone frozen. However, existing visual prompt tuning
methods often fail to specialize the prompts or enrich the representation
space--especially when applied to self-supervised backbones. We show that these
limitations become especially pronounced in challenging tasks and data-scarce
settings, where effective adaptation is most critical. In this work, we
introduce VIPAMIN, a visual prompt initialization strategy that enhances
adaptation of self-supervised models by (1) aligning prompts with semantically
informative regions in the embedding space, and (2) injecting novel
representational directions beyond the pretrained subspace. Despite its
simplicity--requiring only a single forward pass and lightweight
operations--VIPAMIN consistently improves performance across diverse tasks and
dataset sizes, setting a new state of the art in visual prompt tuning. Our code
is available at https://github.com/iamjaekyun/vipamin.

</details>


### [44] [Instance-Aware Pseudo-Labeling and Class-Focused Contrastive Learning for Weakly Supervised Domain Adaptive Segmentation of Electron Microscopy](https://arxiv.org/abs/2510.16450)
*Shan Xiong,Jiabao Chen,Ye Wang,Jialin Peng*

Main category: cs.CV

TL;DR: 提出了一种基于弱监督域适应的电子显微图像中线粒体实例分割方法，通过多任务学习框架和新颖的交叉教学机制，在少量点标注下实现高效准确的分割。


<details>
  <summary>Details</summary>
Motivation: 现有无监督域适应方法在实际应用中性能较低，而完全监督标注成本高；因此需要一种仅依赖少量稀疏标注即可有效提升跨域分割性能的方法。

Method: 提出一个多任务学习框架，联合进行分割与中心点检测，引入交叉教学机制和类别聚焦的跨域对比学习，并设计实例感知伪标签（IPL）选择策略用于自训练，充分利用不完整标注和未标注区域。

Result: 在多个具有挑战性的数据集上验证表明，该方法显著优于现有的UDA和WDA方法，大幅缩小与全监督上限之间的性能差距；在UDA设定下也优于其他UDA技术。

Conclusion: 所提出的弱监督域适应框架能以极低标注成本高效利用目标域数据，为生物和神经科学研究中的电子显微图像分割提供了实用且高性能的解决方案。

Abstract: Annotation-efficient segmentation of the numerous mitochondria instances from
various electron microscopy (EM) images is highly valuable for biological and
neuroscience research. Although unsupervised domain adaptation (UDA) methods
can help mitigate domain shifts and reduce the high costs of annotating each
domain, they typically have relatively low performance in practical
applications. Thus, we investigate weakly supervised domain adaptation (WDA)
that utilizes additional sparse point labels on the target domain, which
require minimal annotation effort and minimal expert knowledge. To take full
use of the incomplete and imprecise point annotations, we introduce a multitask
learning framework that jointly conducts segmentation and center detection with
a novel cross-teaching mechanism and class-focused cross-domain contrastive
learning. While leveraging unlabeled image regions is essential, we introduce
segmentation self-training with a novel instance-aware pseudo-label (IPL)
selection strategy. Unlike existing methods that typically rely on pixel-wise
pseudo-label filtering, the IPL semantically selects reliable and diverse
pseudo-labels with the help of the detection task. Comprehensive validations
and comparisons on challenging datasets demonstrate that our method outperforms
existing UDA and WDA methods, significantly narrowing the performance gap with
the supervised upper bound. Furthermore, under the UDA setting, our method also
achieves substantial improvements over other UDA techniques.

</details>


### [45] [NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation](https://arxiv.org/abs/2510.16457)
*Peiran Xu,Xicheng Gong,Yadong MU*

Main category: cs.CV

TL;DR: 提出一种基于Q-learning的前瞻性视觉-语言导航方法，利用未标记轨迹数据学习室内场景布局和物体关系，结合历史与未来信息实现更有效的路径规划。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言导航方法多依赖历史信息做决策，忽视动作的长期影响，本文旨在提升代理的前瞻性能力。

Method: 采用Q-learning利用大规模未标注轨迹数据训练Q模型，生成每个候选动作的Q特征；通过跨模态未来编码器将Q特征与导航指令融合，生成反映未来前景的动作评分，并结合历史评分使用A*风格搜索策略进行导航。

Result: 在常用的目标导向型VLN数据集上进行了广泛实验，结果表明所提方法在路径规划有效性方面优于现有方法。

Conclusion: 通过引入基于Q-learning的前瞻性建模，能够有效提升视觉-语言导航代理的性能，验证了利用未来信息的重要性。

Abstract: In this work we concentrate on the task of goal-oriented Vision-and-Language
Navigation (VLN). Existing methods often make decisions based on historical
information, overlooking the future implications and long-term outcomes of the
actions. In contrast, we aim to develop a foresighted agent. Specifically, we
draw upon Q-learning to train a Q-model using large-scale unlabeled trajectory
data, in order to learn the general knowledge regarding the layout and object
relations within indoor scenes. This model can generate a Q-feature, analogous
to the Q-value in traditional Q-network, for each candidate action, which
describes the potential future information that may be observed after taking
the specific action. Subsequently, a cross-modal future encoder integrates the
task-agnostic Q-feature with navigation instructions to produce a set of action
scores reflecting future prospects. These scores, when combined with the
original scores based on history, facilitate an A*-style searching strategy to
effectively explore the regions that are more likely to lead to the
destination. Extensive experiments conducted on widely used goal-oriented VLN
datasets validate the effectiveness of the proposed method.

</details>


### [46] [HGC-Avatar: Hierarchical Gaussian Compression for Streamable Dynamic 3D Avatars](https://arxiv.org/abs/2510.16463)
*Haocheng Tang,Ruoke Yan,Xinhui Yin,Qi Zhang,Xinfeng Zhang,Siwei Ma,Wen Gao,Chuanmin Jia*

Main category: cs.CV

TL;DR: 提出了一种用于高效传输和高质量渲染动态3D头像的分层高斯压缩框架HGC-Avatar，显著提升了压缩效率和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于通用3D高斯点阵的压缩方法缺乏人体先验知识，在用于数字人传输时比特率效率低、重建质量差，限制了其在可流式3D头像系统中的应用。

Method: 提出HGC-Avatar框架，将高斯表示分解为结构层（通过StyleUNet生成姿态相关的高斯）和运动层（利用SMPL-X模型紧凑表示姿态时序变化），并引入面部注意力机制以在低码率下保留面部细节。

Result: 实验结果表明，HGC-Avatar在视觉质量和压缩效率上均显著优于先前方法，支持逐层压缩、渐进解码和多姿态输入控制渲染。

Conclusion: HGC-Avatar为动态3D头像的快速渲染和流式传输提供了高效解决方案，特别适合对人脸真实感要求高的应用场景。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled fast,
photorealistic rendering of dynamic 3D scenes, showing strong potential in
immersive communication. However, in digital human encoding and transmission,
the compression methods based on general 3DGS representations are limited by
the lack of human priors, resulting in suboptimal bitrate efficiency and
reconstruction quality at the decoder side, which hinders their application in
streamable 3D avatar systems. We propose HGC-Avatar, a novel Hierarchical
Gaussian Compression framework designed for efficient transmission and
high-quality rendering of dynamic avatars. Our method disentangles the Gaussian
representation into a structural layer, which maps poses to Gaussians via a
StyleUNet-based generator, and a motion layer, which leverages the SMPL-X model
to represent temporal pose variations compactly and semantically. This
hierarchical design supports layer-wise compression, progressive decoding, and
controllable rendering from diverse pose inputs such as video sequences or
text. Since people are most concerned with facial realism, we incorporate a
facial attention mechanism during StyleUNet training to preserve identity and
expression details under low-bitrate constraints. Experimental results
demonstrate that HGC-Avatar provides a streamable solution for rapid 3D avatar
rendering, while significantly outperforming prior methods in both visual
quality and compression efficiency.

</details>


### [47] [PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies](https://arxiv.org/abs/2510.16505)
*Lukas Selch,Yufang Hou,M. Jehanzeb Mirza,Sivan Doveh,James Glass,Rogerio Feris,Wei Lin*

Main category: cs.CV

TL;DR: PRISMM-Bench是首个基于真实审稿人标记的科学论文多模态不一致性基准，包含262个来自242篇论文的真实不一致性，用于评估大模型在识别、修复和匹配跨文本、图表、公式等模态不一致性的能力，揭示当前大模型在此类科学推理任务上表现较差（26.1-54.2%），并提出结构化JSON答题格式以减少语言偏差。


<details>
  <summary>Details</summary>
Motivation: 现有基准未能有效评估大模型在科学论文中跨文本、图表、表格和公式等多模态内容中识别和解决不一致性的能力，且多依赖合成错误或单一模态，缺乏真实性和复杂性。因此需要一个基于真实审稿反馈的多模态不一致性评估基准。

Method: 通过多阶段流程构建PRISMM-Bench：从同行评审中挖掘不一致性，使用大语言模型辅助筛选，并经人工验证，最终整理出262个真实不一致性样本。设计三个任务——不一致性识别、修复和配对匹配，评估模型的多模态推理能力；同时引入结构化的JSON答案格式，避免选择题中模型利用语言捷径（如选项模式）而非真正理解问题。

Result: 在21个主流大模型上的评测显示，当前最先进模型在该基准上的表现普遍较低，准确率仅为26.1%至54.2%，表明现有模型在处理科学文献中的多模态不一致性方面存在显著不足。结构化输出格式有效减少了语言偏倚。

Conclusion: PRISMM-Bench揭示了现有大模型在真实科学多模态推理任务上的严重局限性，凸显了开发更可靠科学助手的必要性，为未来提升模型在科学理解、跨模态一致性检测与修复方面的能力提供了重要基准和方向。

Abstract: Large Multimodal Models (LMMs) are increasingly applied to scientific
research, yet it remains unclear whether they can reliably understand and
reason over the multimodal complexity of papers. A central challenge lies in
detecting and resolving inconsistencies across text, figures, tables, and
equations, issues that are often subtle, domain-specific, and ultimately
undermine clarity, reproducibility, and trust. Existing benchmarks overlook
this issue, either isolating single modalities or relying on synthetic errors
that fail to capture real-world complexity. We introduce PRISMM-Bench
(Peer-Review-sourced Inconsistency Set for Multimodal Models), the first
benchmark grounded in real reviewer-flagged inconsistencies in scientific
papers. Through a multi-stage pipeline of review mining, LLM-assisted filtering
and human verification, we curate 262 inconsistencies from 242 papers. Based on
this set, we design three tasks, namely inconsistency identification, remedy
and pair matching, which assess a model's capacity to detect, correct, and
reason over inconsistencies across different modalities. Furthermore, to
address the notorious problem of choice-only shortcuts in multiple-choice
evaluation, where models exploit answer patterns without truly understanding
the question, we further introduce structured JSON-based answer representations
that minimize linguistic biases by reducing reliance on superficial stylistic
cues. We benchmark 21 leading LMMs, including large open-weight models
(GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5
with high reasoning). Results reveal strikingly low performance (26.1-54.2%),
underscoring the challenge of multimodal scientific reasoning and motivating
progress towards trustworthy scientific assistants.

</details>


### [48] [OOS-DSD: Improving Out-of-stock Detection in Retail Images using Auxiliary Tasks](https://arxiv.org/abs/2510.16508)
*Franko Šikić,Sven Lončarić*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的新型缺货检测方法OOS-DSD，通过引入辅助学习（产品分割和深度估计）显著提升了检测性能，mAP超越现有最先进方法1.8%。


<details>
  <summary>Details</summary>
Motivation: 缺货检测对零售业至关重要，但现有方法性能有限，难以准确识别货架上商品的缺货状态，因此需要更鲁棒和精确的检测方法。

Method: 在YOLOv8基础上扩展多个卷积分支，实现缺货检测、产品分割和场景深度估计的多任务学习；深度估计分支使用Depth Anything V2生成的伪标签进行训练，并提出一种深度归一化方法以稳定训练过程。

Result: 所提方法在mAP上比现有最先进方法提升1.8%；消融实验显示辅助学习使mAP提高3.7%，深度归一化进一步提升4.2%。

Conclusion: 引入辅助任务（尤其是深度估计结合归一化）能有效提升缺货检测性能，验证了多任务与伪标签学习在零售视觉任务中的潜力。

Abstract: Out-of-stock (OOS) detection is a very important retail verification process
that aims to infer the unavailability of products in their designated areas on
the shelf. In this paper, we introduce OOS-DSD, a novel deep learning-based
method that advances OOS detection through auxiliary learning. In particular,
we extend a well-established YOLOv8 object detection architecture with
additional convolutional branches to simultaneously detect OOS, segment
products, and estimate scene depth. While OOS detection and product
segmentation branches are trained using ground truth data, the depth estimation
branch is trained using pseudo-labeled annotations produced by the
state-of-the-art (SOTA) depth estimation model Depth Anything V2. Furthermore,
since the aforementioned pseudo-labeled depth estimates display relative depth,
we propose an appropriate depth normalization procedure that stabilizes the
training process. The experimental results show that the proposed method
surpassed the performance of the SOTA OOS detection methods by 1.8% of the mean
average precision (mAP). In addition, ablation studies confirm the
effectiveness of auxiliary learning and the proposed depth normalization
procedure, with the former increasing mAP by 3.7% and the latter by 4.2%.

</details>


### [49] [Image Categorization and Search via a GAT Autoencoder and Representative Models](https://arxiv.org/abs/2510.16514)
*Duygu Sap,Martin Lotz,Connor Mattinson*

Main category: cs.CV

TL;DR: 提出了一种基于图注意力网络（GAT）自编码器的图像分类与检索方法，通过构建图像和类别的代表模型，利用图结构和注意力机制生成上下文感知的潜在表示，实现代表性中心的分类与检索。


<details>
  <summary>Details</summary>
Motivation: 传统图像分类与检索方法在捕捉图像间复杂关系和上下文信息方面存在不足，因此需要一种能有效建模图像相似性并提取关键特征的方法。

Method: 构建以图像为代表节点、以相似性为边的图，使用GAT自编码器学习上下文感知的潜在表示；从嵌入中获得类别代表，通过比较查询图像与类别代表进行分类，并在其所属类别中检索最相似图像。

Result: 实验表明，该代表性中心方法在图像分类和检索任务上优于标准特征基线方法，验证了GAT自编码器的有效性。

Conclusion: 基于GAT自编码器的代表性中心框架能有效捕捉图像间关系，提升分类与检索性能，具备良好的应用潜力。

Abstract: We propose a method for image categorization and retrieval that leverages
graphs and a graph attention network (GAT)-based autoencoder. Our approach is
representative-centric, that is, we execute the categorization and retrieval
process via the representative models we construct for the images and image
categories. We utilize a graph where nodes represent images (or their
representatives) and edges capture similarity relationships. GAT highlights
important features and relationships between images, enabling the autoencoder
to construct context-aware latent representations that capture the key features
of each image relative to its neighbors. We obtain category representatives
from these embeddings and categorize a query image by comparing its
representative to the category representatives. We then retrieve the most
similar image to the query image within its identified category. We demonstrate
the effectiveness of our representative-centric approach through experiments
with both the GAT autoencoders and standard feature-based techniques.

</details>


### [50] [Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions](https://arxiv.org/abs/2510.16540)
*Jihoon Kwon,Kyle Min,Jy-yong Sohn*

Main category: cs.CV

TL;DR: 本文提出了一种名为READ的微调方法，通过引入重构和对齐两个辅助目标来增强视觉-语言模型的组合推理能力，在多个基准上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的对比学习方法在组合推理上表现不足，主要因为文本编码器倾向于关注单个词汇而非词汇之间的关系，这限制了模型对视觉与语言元素之间结构化关系的理解。

Method: 提出READ方法，包含两种辅助目标：（1）词元级重构目标，使用冻结的预训练解码器基于原始描述的嵌入重构替代字幕；（2）句子级对齐目标，显式对齐嵌入空间中的同义句。

Result: 将READ应用于CLIP及其变体后，在五个主要组合推理基准上均达到最先进性能，相较于最强基线提升高达4.1%。重构和对齐目标分别增强了词间关系捕捉和句式变化下的表示一致性。

Conclusion: READ有效提升了视觉-语言模型的组合推理能力，具有良好的通用性和互补性，为改进现有模型提供了新思路。

Abstract: Despite recent advances, vision-language models trained with standard
contrastive objectives still struggle with compositional reasoning -- the
ability to understand structured relationships between visual and linguistic
elements. This shortcoming is largely due to the tendency of the text encoder
to focus on individual words rather than their relations, a limitation
reinforced by contrastive training that primarily aligns words with visual
objects. In this paper, we introduce REconstruction and Alignment of text
Descriptions (READ), a fine-tuning method designed to enhance compositional
reasoning by adding two auxiliary objectives to the contrastive learning: (1) a
token-level reconstruction objective, where a frozen pre-trained decoder
reconstructs alternative captions based on the embedding of the original
caption; and (2) a sentence-level alignment objective, which explicitly aligns
paraphrased sentences in the embedding space. We show that READ-CLIP, a model
derived by applying the READ method to the pre-trained CLIP model, achieves the
state-of-the-art performance across five major compositional reasoning
benchmarks, outperforming the strongest conventional fine-tuning baseline by up
to 4.1%. Furthermore, applying the READ to existing CLIP variants (including
NegCLIP and FSC-CLIP) also improves performance on these benchmarks.
Quantitative and qualitative analyses reveal that our proposed objectives --
reconstruction and alignment -- offer complementary benefits: the former
encourages the encoder to capture relationships between words within a caption,
while the latter ensures consistent representations for paraphrases expressed
with different wording.

</details>


### [51] [Watch Where You Move: Region-aware Dynamic Aggregation and Excitation for Gait Recognition](https://arxiv.org/abs/2510.16541)
*Binyuan Huang,Yongdong Luo,Xianda Guo,Xiawu Zheng,Zheng Zhu,Jiahui Pan,Chengju Zhou*

Main category: cs.CV

TL;DR: 提出了一种区域感知的动态聚合与激励框架（GaitRDAE），用于自适应地搜索运动区域、分配时序尺度并应用注意力机制，显著提升了步态识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常使用预定义区域和固定的时序建模方式，难以适应不同运动区域的动态变化和特定行为模式，尤其在协变量影响外观时表现受限。

Method: 设计了两个核心模块：区域感知动态聚合（RDA）模块，用于为每个区域动态搜索最优时序感受野；区域感知动态激励（RDE）模块，用于增强稳定行为区域的学习，抑制对易受协变量影响的静态区域的关注。

Result: GaitRDAE 在多个基准数据集上取得了最先进的性能。

Conclusion: 通过动态建模不同运动区域的时序特性和注意力分配，GaitRDAE 能更鲁棒地捕捉步态特征，提升识别精度。

Abstract: Deep learning-based gait recognition has achieved great success in various
applications. The key to accurate gait recognition lies in considering the
unique and diverse behavior patterns in different motion regions, especially
when covariates affect visual appearance. However, existing methods typically
use predefined regions for temporal modeling, with fixed or equivalent temporal
scales assigned to different types of regions, which makes it difficult to
model motion regions that change dynamically over time and adapt to their
specific patterns. To tackle this problem, we introduce a Region-aware Dynamic
Aggregation and Excitation framework (GaitRDAE) that automatically searches for
motion regions, assigns adaptive temporal scales and applies corresponding
attention. Specifically, the framework includes two core modules: the
Region-aware Dynamic Aggregation (RDA) module, which dynamically searches the
optimal temporal receptive field for each region, and the Region-aware Dynamic
Excitation (RDE) module, which emphasizes the learning of motion regions
containing more stable behavior patterns while suppressing attention to static
regions that are more susceptible to covariates. Experimental results show that
GaitRDAE achieves state-of-the-art performance on several benchmark datasets.

</details>


### [52] [Fit for Purpose? Deepfake Detection in the Real World](https://arxiv.org/abs/2510.16556)
*Guangyu Lin,Li Lin,Christina P. Walker,Daniel S. Schiff,Shu Hu*

Main category: cs.CV

TL;DR: 本文介绍了首个基于真实世界政治深度伪造内容的数据集的系统性基准测试，发现现有检测模型在实际应用中表现不佳，尤其对社交媒体上传播的政治深度伪造视频难以有效识别，呼吁构建更具政治情境适应性的检测框架。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的发展，AI生成内容泛滥，尤其是政治领域的深度伪造加剧了虚假信息传播，威胁公众信任。然而现有检测模型多基于实验室数据，缺乏在真实场景中的泛化能力，亟需针对真实政治深度伪造进行系统评估。

Method: 基于自建的‘政治深度伪造事件数据库’（自2018年以来在社交媒体传播的真实案例），对学术界、政府和工业界最先进的深度伪造检测工具进行系统性评测，涵盖免费与付费模型，并测试其在视频和图像上的鲁棒性。

Result: 学术界和政府开发的检测器表现较差；尽管付费工具性能相对较好，但所有模型在面对真实政治深度伪造时均表现不佳，尤其在视频领域易受简单篡改干扰，泛化能力有限。

Conclusion: 现有深度伪造检测技术在真实政治语境下面临严峻挑战，需发展融合政治背景信息的、更具鲁棒性和适应性的检测框架，以提升实际防护能力。

Abstract: The rapid proliferation of AI-generated content, driven by advances in
generative adversarial networks, diffusion models, and multimodal large
language models, has made the creation and dissemination of synthetic media
effortless, heightening the risks of misinformation, particularly political
deepfakes that distort truth and undermine trust in political institutions. In
turn, governments, research institutions, and industry have strongly promoted
deepfake detection initiatives as solutions. Yet, most existing models are
trained and validated on synthetic, laboratory-controlled datasets, limiting
their generalizability to the kinds of real-world political deepfakes
circulating on social platforms that affect the public. In this work, we
introduce the first systematic benchmark based on the Political Deepfakes
Incident Database, a curated collection of real-world political deepfakes
shared on social media since 2018. Our study includes a systematic evaluation
of state-of-the-art deepfake detectors across academia, government, and
industry. We find that the detectors from academia and government perform
relatively poorly. While paid detection tools achieve relatively higher
performance than free-access models, all evaluated detectors struggle to
generalize effectively to authentic political deepfakes, and are vulnerable to
simple manipulations, especially in the video domain. Results urge the need for
politically contextualized deepfake detection frameworks to better safeguard
the public in real-world settings.

</details>


### [53] [SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense](https://arxiv.org/abs/2510.16596)
*Yiyang Huang,Liang Shi,Yitian Zhang,Yi Xu,Yun Fu*

Main category: cs.CV

TL;DR: SHIELD 是一个无需训练的框架，通过重新加权视觉标记、引入噪声标记和对抗攻击结合对比解码，有效缓解大视觉语言模型中的对象幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型（LVLMs）在跨模态任务中表现出色，但存在对象幻觉问题，即生成看似合理但不准确的对象描述。本文首次将幻觉归因于视觉编码器，并识别出三个关键问题：统计偏差、固有偏差和脆弱性。

Method: 提出 SHIELD 框架，采用三种策略：重新加权视觉标记以减少统计偏差，引入噪声衍生标记以抵消固有偏差，应用对抗攻击结合对比解码以应对脆弱性。

Result: 实验表明，SHIELD 在多种基准和 LVLM 家族中均能有效缓解对象幻觉，并在通用 LVLM 基准上表现优异，展现出广泛的适用性。

Conclusion: SHIELD 为解决 LVLM 中由视觉编码器引发的对象幻觉问题提供了有效且通用的训练-free 方法。

Abstract: Large Vision-Language Models (LVLMs) excel in diverse cross-modal tasks.
However, object hallucination, where models produce plausible but inaccurate
object descriptions, remains a significant challenge. In contrast to previous
work focusing on LLM components, this paper is the first to trace LVLM
hallucinations to visual encoders and identifies three key issues: statistical
bias, inherent bias, and vulnerability. To address these challenges, we propose
SHIELD, a training-free framework that mitigates hallucinations through three
strategies: re-weighting visual tokens to reduce statistical bias, introducing
noise-derived tokens to counter inherent bias, and applying adversarial attacks
with contrastive decoding to address vulnerability. Experiments demonstrate
that SHIELD effectively mitigates object hallucinations across diverse
benchmarks and LVLM families. Moreover, SHIELD achieves strong performance on
the general LVLM benchmark, highlighting its broad applicability. Code will be
released.

</details>


### [54] [VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.16598)
*Jiaying Zhu,Yurui Zhu,Xin Lu,Wenrui Yan,Dong Li,Kunlin Liu,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 提出VisionSelector，一种轻量级、可学习的视觉token选择框架，有效解决多模态大模型中高分辨率图像带来的计算与内存瓶颈，实现高效自适应token压缩。


<details>
  <summary>Details</summary>
Motivation: 现有token压缩方法依赖启发式规则，易丢失关键信息，存在注意力陷阱等偏差，导致高压缩比下性能显著下降。

Method: 将token压缩建模为端到端可学习的决策过程，设计解耦的scorer模块VisionSelector，结合可微Top-K机制和课程退火策略，实现任意压缩率下的自适应选择。

Result: 仅12.85M可训练参数，通用性强，在30%保留率下MME准确率达100%，10%保留率下性能超过先前方法12.14%，预填充速度提升一倍。

Conclusion: VisionSelector是一种高效、灵活且即插即用的token压缩方案，显著提升多模态大模型在不同压缩比下的性能与效率。

Abstract: Multimodal Large Language Models (MLLMs) encounter significant computational
and memory bottlenecks from the massive number of visual tokens generated by
high-resolution images or multi-image inputs. Previous token compression
techniques are often constrained by heuristic rules that risk discarding
critical information. They may suffer from biases, such as attention sinks,
that lead to sharp performance drops under aggressive compression ratios. To
address these limitations, we reformulate token compression as a lightweight
plug-and-play framework that reformulates token compression into an end-to-end
learnable decision process. To be specific, we propose VisionSelector, a scorer
module decoupled from the MLLM backbone that incorporates a differentiable
Top-K mechanism and a curriculum annealing strategy to bridge the
training-inference gap, enabling efficient and adaptive token selection various
arbitrary compression rates. Remarkably lightweight with only 12.85M trainable
parameters, VisionSelector demonstrates generalization across various
compression rates and adaptively identifying critical tokens. This leads to
superior performance across all compression budgets, evidenced by preserving
100% accuracy on MME with 30% retention budget, outperforming prior methods by
12.14% at 10% retention budget, and doubling prefill speed. Our code is
available at https://github.com/JulietChoo/VisionSelector .

</details>


### [55] [A Deep Learning Framework for Real-Time Image Processing in Medical Diagnostics: Enhancing Accuracy and Speed in Clinical Applications](https://arxiv.org/abs/2510.16611)
*Melika Filvantorkaman,Maral Filvan Torkaman*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的实时医学图像分析框架，结合多种神经网络架构与优化策略，实现高精度、高效率的多模态医学图像诊断，并支持在多种设备上部署，显著提升临床诊断速度与可解释性。


<details>
  <summary>Details</summary>
Motivation: 医学影像诊断中，传统图像处理方法存在精度低、鲁棒性差和速度慢的问题，且高分辨率影像的解读耗时且易受医生间差异影响，亟需一种高效、准确且可实时应用的智能分析系统。

Method: 提出一种融合U-Net、EfficientNet和Transformer等先进神经网络的深度学习框架，结合模型剪枝、量化和GPU加速等实时优化技术，并集成Grad-CAM和分割叠加等可视化解释工具，支持在边缘设备、本地服务器和云平台部署，兼容PACS和EHR等临床系统。

Result: 在公开数据集上的实验表明，该框架分类准确率超过92%，分割Dice分数高于91%，推理时间低于80毫秒，性能达到先进水平，同时具备良好的可解释性和临床可用性。

Conclusion: 所提框架能有效提升医学影像分析的速度与准确性，减轻临床工作负担，推动可信赖AI在时间敏感型医疗场景中的实际应用。

Abstract: Medical imaging plays a vital role in modern diagnostics; however,
interpreting high-resolution radiological data remains time-consuming and
susceptible to variability among clinicians. Traditional image processing
techniques often lack the precision, robustness, and speed required for
real-time clinical use. To overcome these limitations, this paper introduces a
deep learning framework for real-time medical image analysis designed to
enhance diagnostic accuracy and computational efficiency across multiple
imaging modalities, including X-ray, CT, and MRI. The proposed system
integrates advanced neural network architectures such as U-Net, EfficientNet,
and Transformer-based models with real-time optimization strategies including
model pruning, quantization, and GPU acceleration. The framework enables
flexible deployment on edge devices, local servers, and cloud infrastructures,
ensuring seamless interoperability with clinical systems such as PACS and EHR.
Experimental evaluations on public benchmark datasets demonstrate
state-of-the-art performance, achieving classification accuracies above 92%,
segmentation Dice scores exceeding 91%, and inference times below 80
milliseconds. Furthermore, visual explanation tools such as Grad-CAM and
segmentation overlays enhance transparency and clinical interpretability. These
results indicate that the proposed framework can substantially accelerate
diagnostic workflows, reduce clinician workload, and support trustworthy AI
integration in time-critical healthcare environments.

</details>


### [56] [Self-Supervised Learning to Fly using Efficient Semantic Segmentation and Metric Depth Estimation for Low-Cost Autonomous UAVs](https://arxiv.org/abs/2510.16624)
*Sebastian Mocanu,Emil Slusanschi,Marius Leordeanu*

Main category: cs.CV

TL;DR: 本文提出了一种仅基于视觉的无人机自主飞行系统，通过语义分割与单目深度估计结合，实现在无GPS或LiDAR的室内环境中避障、探索与安全降落。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的小型无人机上实现无需GPS和昂贵传感器的自主飞行，解决室内导航中度量深度估计和计算效率的挑战。

Method: 采用知识蒸馏框架训练轻量级U-Net网络进行实时语义分割，并提出自适应尺度因子算法，结合语义地面平面检测与相机内参将单目深度预测转化为度量距离；进一步通过端到端学习训练紧凑网络执行飞行策略。

Result: 在5x4米实验室环境中进行30次真实飞行和100次数字孪生测试，系统实现100%任务成功率，平均距离误差14.4厘米，提升了航程并缩短了任务时间；端到端版本达到87.5%自主任务成功率。

Conclusion: 该方法推动了结构化环境中基于视觉的无人机导航实用性，解决了度量深度估计与计算效率问题，可在资源受限平台部署。

Abstract: This paper presents a vision-only autonomous flight system for small UAVs
operating in controlled indoor environments. The system combines semantic
segmentation with monocular depth estimation to enable obstacle avoidance,
scene exploration, and autonomous safe landing operations without requiring GPS
or expensive sensors such as LiDAR. A key innovation is an adaptive scale
factor algorithm that converts non-metric monocular depth predictions into
accurate metric distance measurements by leveraging semantic ground plane
detection and camera intrinsic parameters, achieving a mean distance error of
14.4 cm. The approach uses a knowledge distillation framework where a
color-based Support Vector Machine (SVM) teacher generates training data for a
lightweight U-Net student network (1.6M parameters) capable of real-time
semantic segmentation. For more complex environments, the SVM teacher can be
replaced with a state-of-the-art segmentation model. Testing was conducted in a
controlled 5x4 meter laboratory environment with eight cardboard obstacles
simulating urban structures. Extensive validation across 30 flight tests in a
real-world environment and 100 flight tests in a digital-twin environment
demonstrates that the combined segmentation and depth approach increases the
distance traveled during surveillance and reduces mission time while
maintaining 100% success rates. The system is further optimized through
end-to-end learning, where a compact student neural network learns complete
flight policies from demonstration data generated by our best-performing
method, achieving an 87.5% autonomous mission success rate. This work advances
practical vision-based drone navigation in structured environments,
demonstrating solutions for metric depth estimation and computational
efficiency challenges that enable deployment on resource-constrained platforms.

</details>


### [57] [MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models](https://arxiv.org/abs/2510.16641)
*Young-Jun Lee,Byung-Kwan Lee,Jianshu Zhang,Yechan Hwang,Byungsoo Ko,Han-Gyu Kim,Dongyu Yao,Xuankun Rong,Eojin Joo,Seung-Ho Han,Bowon Ko,Ho-Jin Choi*

Main category: cs.CV

TL;DR: MultiVerse是一个新的多轮对话基准，包含647个平均四轮的对话，源自12个流行的VLM评估基准，涵盖484个任务和目标，评估18个VLMs显示即使最强模型在复杂对话中成功率也仅50%。


<details>
  <summary>Details</summary>
Motivation: 现有的多轮对话数据集未能充分捕捉用户在真实场景中遇到的对话广度和深度，需要一个更全面的基准来评估视觉-语言模型（VLMs）在复杂多轮交互中的能力。

Method: 构建MultiVerse基准，整合12个现有VLM评估基准的数据，形成多样化的多轮对话；提出基于清单的自动化评估方法，使用GPT-4o评估模型在37个关键方面的表现。

Result: 评估了18个VLMs，发现最强模型（如GPT-4o）在复杂多轮对话中仅达到约50%的成功率；提供完整对话上下文显著提升较小或较弱模型的表现。

Conclusion: MultiVerse是一个具有挑战性的多轮对话评估基准，揭示了当前VLMs在复杂交互中的局限性，并强调了上下文学习的重要性，为未来VLM的多轮交互能力评估提供了新方向。

Abstract: Vision-and-Language Models (VLMs) have shown impressive capabilities on
single-turn benchmarks, yet real-world applications often demand more intricate
multi-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only
partially capture the breadth and depth of conversational scenarios encountered
by users. In this work, we introduce MultiVerse, a novel multi-turn
conversation benchmark featuring 647 dialogues - each averaging four turns -
derived from a diverse set of 12 popular VLM evaluation benchmarks. With 484
tasks and 484 interaction goals, MultiVerse covers a wide range of topics, from
factual knowledge and perception to advanced reasoning tasks such as
mathematics and coding. To facilitate robust assessment, we propose a
checklist-based evaluation method that leverages GPT-4o as the automated
evaluator, measuring performance across 37 key aspects, including perceptual
accuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on
MultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve
only a 50% success rate in complex multi-turn conversations, highlighting the
dataset's challenging nature. Notably, we find that providing full dialogue
context significantly enhances performance for smaller or weaker models,
emphasizing the importance of in-context learning. We believe MultiVerse is a
landscape of evaluating multi-turn interaction abilities for VLMs.

</details>


### [58] [Structured Interfaces for Automated Reasoning with 3D Scene Graphs](https://arxiv.org/abs/2510.16643)
*Aaron Ray,Jacob Arkin,Harel Biggie,Chuchu Fan,Luca Carlone,Nicholas Roy*

Main category: cs.CV

TL;DR: 提出一种基于检索增强生成的方法，通过图数据库和Cypher查询语言接口，使大语言模型能高效利用3D场景图进行自然语言 grounding，显著提升可扩展性和任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的将3D场景图序列化输入大语言模型的方法难以扩展到大规模或丰富的场景图，限制了语言 grounding 的效果和效率。

Method: 将3D场景图存储于图数据库中，利用检索增强生成（RAG）机制，通过Cypher查询语言让大语言模型动态检索与任务相关的子图内容，实现高效语言 grounding。

Result: 在指令跟随和场景问答任务中，该方法相比基线模型显著提升了性能，同时大幅减少了使用的 token 数量，在本地和云端模型上均表现出更好的可扩展性。

Conclusion: 使用Cypher作为3D场景图的接口，结合检索增强生成，是一种高效、可扩展的自然语言 grounding 方法，适用于复杂机器人环境中的语言理解。

Abstract: In order to provide a robot with the ability to understand and react to a
user's natural language inputs, the natural language must be connected to the
robot's underlying representations of the world. Recently, large language
models (LLMs) and 3D scene graphs (3DSGs) have become a popular choice for
grounding natural language and representing the world. In this work, we address
the challenge of using LLMs with 3DSGs to ground natural language. Existing
methods encode the scene graph as serialized text within the LLM's context
window, but this encoding does not scale to large or rich 3DSGs. Instead, we
propose to use a form of Retrieval Augmented Generation to select a subset of
the 3DSG relevant to the task. We encode a 3DSG in a graph database and provide
a query language interface (Cypher) as a tool to the LLM with which it can
retrieve relevant data for language grounding. We evaluate our approach on
instruction following and scene question-answering tasks and compare against
baseline context window and code generation methods. Our results show that
using Cypher as an interface to 3D scene graphs scales significantly better to
large, rich graphs on both local and cloud-based models. This leads to large
performance improvements in grounded language tasks while also substantially
reducing the token count of the scene graph content. A video supplement is
available at https://www.youtube.com/watch?v=zY_YI9giZSA.

</details>


### [59] [Universal and Transferable Attacks on Pathology Foundation Models](https://arxiv.org/abs/2510.16660)
*Yuntian Wang,Xilin Yang,Che-Yung Shen,Nir Pillar,Aydogan Ozcan*

Main category: cs.CV

TL;DR: 提出了一种通用且可迁移的对抗性扰动（UTAP），能够以微小的噪声模式破坏多种病理学基础模型的性能，揭示其在实际应用中的关键脆弱性。


<details>
  <summary>Details</summary>
Motivation: 病理学基础模型在医疗AI中具有广泛应用前景，但其对抗攻击的鲁棒性尚不明确。本文旨在揭示这些模型在面对通用且可迁移对抗扰动时的系统性脆弱性。

Method: 通过深度学习优化生成一种固定的弱噪声模式（UTAP），该模式可添加到病理图像中，破坏多个基础模型的特征表示能力，并在不依赖特定数据集或模型的情况下实现跨模型、跨视野的攻击。

Result: UTAP在多个最先进的病理学基础模型和数据集上显著降低了模型性能，导致下游任务中的误分类，且扰动对人眼几乎不可察觉；同时展现出强通用性和跨黑盒模型的可迁移性。

Conclusion: UTAP构成对多种病理学基础模型的广泛威胁，为模型鲁棒性评估建立了高标准基准，强调了发展防御机制和对抗训练的重要性，以确保AI在病理学中的安全可靠部署。

Abstract: We introduce Universal and Transferable Adversarial Perturbations (UTAP) for
pathology foundation models that reveal critical vulnerabilities in their
capabilities. Optimized using deep learning, UTAP comprises a fixed and weak
noise pattern that, when added to a pathology image, systematically disrupts
the feature representation capabilities of multiple pathology foundation
models. Therefore, UTAP induces performance drops in downstream tasks that
utilize foundation models, including misclassification across a wide range of
unseen data distributions. In addition to compromising the model performance,
we demonstrate two key features of UTAP: (1) universality: its perturbation can
be applied across diverse field-of-views independent of the dataset that UTAP
was developed on, and (2) transferability: its perturbation can successfully
degrade the performance of various external, black-box pathology foundation
models - never seen before. These two features indicate that UTAP is not a
dedicated attack associated with a specific foundation model or image dataset,
but rather constitutes a broad threat to various emerging pathology foundation
models and their applications. We systematically evaluated UTAP across various
state-of-the-art pathology foundation models on multiple datasets, causing a
significant drop in their performance with visually imperceptible modifications
to the input images using a fixed noise pattern. The development of these
potent attacks establishes a critical, high-standard benchmark for model
robustness evaluation, highlighting a need for advancing defense mechanisms and
potentially providing the necessary assets for adversarial training to ensure
the safe and reliable deployment of AI in pathology.

</details>


### [60] [HYDRA: HYbrid knowledge Distillation and spectral Reconstruction Algorithm for high channel hyperspectral camera applications](https://arxiv.org/abs/2510.16664)
*Christopher Thirgood,Oscar Mendez,Erin Ling,Jon Storey,Simon Hadfield*

Main category: cs.CV

TL;DR: 提出了一种基于混合知识蒸馏的光谱重建新方法HYDRA，在跨场景广义光谱重建任务中实现了最先进的性能，显著提升了重建精度和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有光谱重建方法在面对高光谱通道数较多（如数百通道）的真实场景时，泛化能力有限，难以实现高质量重建。

Method: 提出HYDRA框架，利用教师-学生模型结构，教师模型编码高光谱图像的潜在表示，学生模型学习从自然图像到该表示的映射，并结合新颖的训练策略进行混合知识蒸馏。

Result: 在多个评价指标上达到SOTA，准确率提升18%，推理速度优于当前SOTA模型，且在不同通道深度下均表现优异。

Conclusion: HYDRA有效解决了现有光谱重建模型在泛化性和性能上的局限，为高光谱图像重建提供了高效且鲁棒的新方法。

Abstract: Hyperspectral images (HSI) promise to support a range of new applications in
computer vision. Recent research has explored the feasibility of generalizable
Spectral Reconstruction (SR), the problem of recovering a HSI from a natural
three-channel color image in unseen scenarios.
  However, previous Multi-Scale Attention (MSA) works have only demonstrated
sufficient generalizable results for very sparse spectra, while modern HSI
sensors contain hundreds of channels.
  This paper introduces a novel approach to spectral reconstruction via our
HYbrid knowledge Distillation and spectral Reconstruction Architecture (HYDRA).
  Using a Teacher model that encapsulates latent hyperspectral image data and a
Student model that learns mappings from natural images to the Teacher's encoded
domain, alongside a novel training method, we achieve high-quality spectral
reconstruction.
  This addresses key limitations of prior SR models, providing SOTA performance
across all metrics, including an 18\% boost in accuracy, and faster inference
times than current SOTA models at various channel depths.

</details>


### [61] [Pursuing Minimal Sufficiency in Spatial Reasoning](https://arxiv.org/abs/2510.16688)
*Yejie Guo,Yunzhong Hou,Wufei Ma,Meng Tang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 提出了一种双代理框架MSSR，通过构建最小充分集（MSS）来提升视觉语言模型在3D空间推理任务中的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型由于依赖2D预训练且在处理冗余3D信息时推理失败，导致空间推理能力不足。

Method: 构建一个包含感知代理和推理代理的双代理框架：感知代理使用专家模型和SOG模块从3D场景中提取语言对齐的方向信息；推理代理迭代优化信息，去除冗余并补充缺失，形成最小充分集（MSS）。

Result: 在两个具有挑战性的基准上实现了最先进的性能，显著提高了空间推理任务的准确性，并生成可解释的推理路径。

Conclusion: 通过显式追求信息的充分性和最小性，MSSR有效提升了VLM的3D空间推理能力，同时为未来模型提供了高质量的训练数据来源。

Abstract: Spatial reasoning, the ability to ground language in 3D understanding,
remains a persistent challenge for Vision-Language Models (VLMs). We identify
two fundamental bottlenecks: inadequate 3D understanding capabilities stemming
from 2D-centric pre-training, and reasoning failures induced by redundant 3D
information. To address these, we first construct a Minimal Sufficient Set
(MSS) of information before answering a given question: a compact selection of
3D perception results from \textit{expert models}. We introduce MSSR (Minimal
Sufficient Spatial Reasoner), a dual-agent framework that implements this
principle. A Perception Agent programmatically queries 3D scenes using a
versatile perception toolbox to extract sufficient information, including a
novel SOG (Situated Orientation Grounding) module that robustly extracts
language-grounded directions. A Reasoning Agent then iteratively refines this
information to pursue minimality, pruning redundant details and requesting
missing ones in a closed loop until the MSS is curated. Extensive experiments
demonstrate that our method, by explicitly pursuing both sufficiency and
minimality, significantly improves accuracy and achieves state-of-the-art
performance across two challenging benchmarks. Furthermore, our framework
produces interpretable reasoning paths, offering a promising source of
high-quality training data for future models. Source code is available at
https://github.com/gyj155/mssr.

</details>


### [62] [SDPA++: A General Framework for Self-Supervised Denoising with Patch Aggregation](https://arxiv.org/abs/2510.16702)
*Huy Minh Nhat Nguyen,Triet Hoang Minh Dao,Chau Vinh Hoang Truong,Cuong Tuan Nguyen*

Main category: cs.CV

TL;DR: 提出了一种自监督去噪框架SDPA++，仅利用噪声OCT图像生成伪真实标签，通过patch聚合策略训练去噪模型，在无干净参考图像的情况下有效提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 获取成对的干净与真实噪声OCT图像用于监督去噪训练面临挑战，且临床中存在固有斑点噪声，限制了传统监督方法的应用。

Method: 提出SDPA++框架，首先通过自融合和自监督去噪生成伪真实图像，然后采用基于patch的策略训练去噪模型，实现对噪声OCT图像的高质量恢复。

Result: 在IEEE SPS VIP Cup真实噪声OCT数据集上验证，使用CNR、MSR、TP和EP等指标显示出性能提升，表明该方法能有效增强图像清晰度并保留纹理与边缘。

Conclusion: SDPA++无需干净图像即可训练，适用于真实临床场景，为OCT图像去噪提供了高效、实用的自监督解决方案。

Abstract: Optical Coherence Tomography (OCT) is a widely used non-invasive imaging
technique that provides detailed three-dimensional views of the retina, which
are essential for the early and accurate diagnosis of ocular diseases.
Consequently, OCT image analysis and processing have emerged as key research
areas in biomedical imaging. However, acquiring paired datasets of clean and
real-world noisy OCT images for supervised denoising models remains a
formidable challenge due to intrinsic speckle noise and practical constraints
in clinical imaging environments. To address these issues, we propose SDPA++: A
General Framework for Self-Supervised Denoising with Patch Aggregation. Our
novel approach leverages only noisy OCT images by first generating
pseudo-ground-truth images through self-fusion and self-supervised denoising.
These refined images then serve as targets to train an ensemble of denoising
models using a patch-based strategy that effectively enhances image clarity.
Performance improvements are validated via metrics such as Contrast-to-Noise
Ratio (CNR), Mean Square Ratio (MSR), Texture Preservation (TP), and Edge
Preservation (EP) on the real-world dataset from the IEEE SPS Video and Image
Processing Cup. Notably, the VIP Cup dataset contains only real-world noisy OCT
images without clean references, highlighting our method's potential for
improving image quality and diagnostic outcomes in clinical practice.

</details>


### [63] [Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization](https://arxiv.org/abs/2510.16704)
*Tianxin Wei,Yifan Chen,Xinrui He,Wenxuan Bao,Jingrui He*

Main category: cs.CV

TL;DR: 本文提出了一种新的对比学习范式DCCL，通过增强跨域的类内连通性来提升域泛化解性能，结合数据增强、跨域正样本和模型锚定策略，在无域监督下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于训练与测试样本之间存在分布偏移，域泛化面临挑战，而直接应用对比学习效果不佳，原因是缺乏类内连通性，因此需要改进对比学习以适应域泛化需求。

Method: 提出域连接对比学习（DCCL），在数据层面引入更强的数据增强和跨域正样本，在模型层面采用模型锚定和生成变换损失来利用预训练表示中的类内连通性。

Result: 在五个标准域泛化基准上的实验表明，DCCL在无域监督的情况下优于当前最优方法。

Conclusion: 通过增强类内和跨域的连通性，DCCL能有效提升域泛化性能，验证了连接性在对比学习中的关键作用。

Abstract: Distribution shifts between training and testing samples frequently occur in
practice and impede model generalization performance. This crucial challenge
thereby motivates studies on domain generalization (DG), which aim to predict
the label on unseen target domain data by solely using data from source
domains. It is intuitive to conceive the class-separated representations
learned in contrastive learning (CL) are able to improve DG, while the reality
is quite the opposite: users observe directly applying CL deteriorates the
performance. We analyze the phenomenon with the insights from CL theory and
discover lack of intra-class connectivity in the DG setting causes the
deficiency. We thus propose a new paradigm, domain-connecting contrastive
learning (DCCL), to enhance the conceptual connectivity across domains and
obtain generalizable representations for DG. On the data side, more aggressive
data augmentation and cross-domain positive samples are introduced to improve
intra-class connectivity. On the model side, to better embed the unseen test
domains, we propose model anchoring to exploit the intra-class connectivity in
pre-trained representations and complement the anchoring with generative
transformation loss. Extensive experiments on five standard DG benchmarks are
performed. The results verify that DCCL outperforms state-of-the-art baselines
even without domain supervision. The detailed model implementation and the code
are provided through https://github.com/weitianxin/DCCL

</details>


### [64] [HumanCM: One Step Human Motion Prediction](https://arxiv.org/abs/2510.16709)
*Liu Haojie,Gao Suixiang*

Main category: cs.CV

TL;DR: HumanCM是一种基于一致性模型的单步人体运动预测框架，通过学习噪声与干净运动状态之间的自洽映射，实现高效生成，显著减少推理步数的同时保持优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的人体运动预测方法依赖多步去噪，推理效率低。为提升生成速度并保持高精度，需要一种更高效的生成框架。

Method: 提出HumanCM，采用一致性模型实现单步生成；使用基于Transformer的时空架构并结合时间嵌入，以建模长距离依赖关系并保持运动连贯性。

Result: 在Human3.6M和HumanEva-I数据集上的实验表明，HumanCM在推理步数减少多达两个数量级的情况下，达到了与当前最优扩散模型相当甚至更好的准确性。

Conclusion: HumanCM通过单步生成实现了高效且准确的人体运动预测，为实际应用提供了更快且可靠的解决方案。

Abstract: We present HumanCM, a one-step human motion prediction framework built upon
consistency models. Instead of relying on multi-step denoising as in
diffusion-based methods, HumanCM performs efficient single-step generation by
learning a self-consistent mapping between noisy and clean motion states. The
framework adopts a Transformer-based spatiotemporal architecture with temporal
embeddings to model long-range dependencies and preserve motion coherence.
Experiments on Human3.6M and HumanEva-I demonstrate that HumanCM achieves
comparable or superior accuracy to state-of-the-art diffusion models while
reducing inference steps by up to two orders of magnitude.

</details>


### [65] [Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes](https://arxiv.org/abs/2510.16714)
*Xiongkun Linghu,Jiangyong Huang,Ziyu Zhu,Baoxiong Jia,Siyuan Huang*

Main category: cs.CV

TL;DR: 本文提出了一种用于3D场景理解的新型 grounded Chain-of-Thought（SCENECOT）推理方法，并构建了首个大规模3D CoT推理数据集SCENECOT-185K，实现了在复杂3D场景问答中具有高接地一致性的强性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有3D大语言模型在实现基于场景对象的 grounded 问答方面仍存在困难，主要原因是缺乏对人类类似场景-对象推理机制的深入探索。本文旨在填补这一空白。

Method: 提出SCENECOT框架，将复杂的推理任务分解为更简单可管理的子问题，并通过多模态专家模块生成相应的视觉线索，实现3D场景中的逐步推理。同时构建包含18.5万高质量样本的数据集SCENECOT-185K以支持该方法。

Result: 在多个复杂3D场景推理基准上的实验表明，该方法显著提升了 grounded 问答的性能，并展现出高度的推理与视觉接地一致性，是首个成功将CoT应用于3D场景理解的框架。

Conclusion: SCENECOT首次实现了在3D场景中类人逐步推理，为3D视觉语言模型的可解释性和泛化能力提供了新路径，并具备向更广泛3D场景理解任务扩展的潜力。

Abstract: Existing research on 3D Large Language Models (LLMs) still struggles to
achieve grounded question-answering, primarily due to the under-exploration of
the mech- anism of human-like scene-object grounded reasoning. This paper
bridges the gap by presenting a novel framework. We first introduce a grounded
Chain-of- Thought reasoning method in 3D scenes (SCENECOT), decoupling a
complex reasoning task into simpler and manageable problems, and building
corresponding visual clues based on multimodal expert modules. To enable such a
method, we develop SCENECOT-185K, the first large-scale grounded CoT reasoning
dataset, consisting of 185K high-quality instances. Extensive experiments
across various complex 3D scene reasoning benchmarks demonstrate that our new
framework achieves strong performance with high grounding-QA coherence. To the
best of our knowledge, this is the first successful application of CoT
reasoning to 3D scene understanding, enabling step-by-step human-like reasoning
and showing potential for extension to broader 3D scene understanding
scenarios.

</details>


### [66] [Vision-Centric 4D Occupancy Forecasting and Planning via Implicit Residual World Models](https://arxiv.org/abs/2510.16729)
*Jianbiao Mei,Yu Yang,Xuemeng Yang,Licheng Wen,Jiajun Lv,Botian Shi,Yong Liu*

Main category: cs.CV

TL;DR: 提出了一种隐式残差世界模型IR-WM，通过聚焦环境变化而非完整重建未来场景，有效提升自动驾驶中4D占用预测和轨迹规划的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉为中心的世界模型在重建未来场景时浪费计算资源于静态背景，导致效率低下。为提升建模效率与预测精度，需聚焦动态变化部分。

Method: IR-WM首先从视觉观测建立当前状态的鸟瞰图（BEV）表示，利用上一时刻的BEV特征作为时间先验，仅预测在自车动作和场景上下文条件下的‘残差’变化，并引入对齐模块缓解语义与动态错位问题。

Result: 在nuScenes数据集上，IR-WM在4D占用预测和轨迹规划任务中均达到最优性能。

Conclusion: 通过隐式建模环境变化并结合对齐机制，IR-WM能更高效准确地支持端到端自动驾驶中的预测与规划。

Abstract: End-to-end autonomous driving systems increasingly rely on vision-centric
world models to understand and predict their environment. However, a common
ineffectiveness in these models is the full reconstruction of future scenes,
which expends significant capacity on redundantly modeling static backgrounds.
To address this, we propose IR-WM, an Implicit Residual World Model that
focuses on modeling the current state and evolution of the world. IR-WM first
establishes a robust bird's-eye-view representation of the current state from
the visual observation. It then leverages the BEV features from the previous
timestep as a strong temporal prior and predicts only the "residual", i.e., the
changes conditioned on the ego-vehicle's actions and scene context. To
alleviate error accumulation over time, we further apply an alignment module to
calibrate semantic and dynamic misalignments. Moreover, we investigate
different forecasting-planning coupling schemes and demonstrate that the
implicit future state generated by world models substantially improves planning
accuracy. On the nuScenes benchmark, IR-WM achieves top performance in both 4D
occupancy forecasting and trajectory planning.

</details>


### [67] [UKANFormer: Noise-Robust Semantic Segmentation for Coral Reef Mapping via a Kolmogorov-Arnold Network-Transformer Hybrid](https://arxiv.org/abs/2510.16730)
*Tianyang Dou,Ming Li,Jiangying Qin,Xuan Liao,Jiageng Zhong,Armin Gruen,Mengyi Deng*

Main category: cs.CV

TL;DR: 提出了一种名为UKANFormer的新语义分割模型，通过结合全局-局部Transformer模块，在噪声标签下实现了对珊瑚礁的高精度映射，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有全球珊瑚礁分布图（如Allen Coral Atlas）在空间精度和语义一致性方面存在不足，尤其是在精细边界划分上，需要更精确的大规模制图方法。

Method: 基于UKAN架构，引入全局-局部Transformer（GL-Trans）模块到解码器中，以同时捕获全局语义结构和局部边界细节，实现对噪声标签下的高精度语义分割。

Result: UKANFormer在珊瑚类别上的IoU达到67.00%，像素准确率为83.98%，优于在相同噪声标签下的传统基线模型，且预测结果在视觉和结构上比训练所用标签更准确。

Conclusion: 模型架构设计可以有效缓解标签噪声的影响，表明数据质量并非模型性能的绝对限制，为标签稀缺条件下的生态监测提供了可行方案。

Abstract: Coral reefs are vital yet fragile ecosystems that require accurate
large-scale mapping for effective conservation. Although global products such
as the Allen Coral Atlas provide unprecedented coverage of global coral reef
distri-bution, their predictions are frequently limited in spatial precision
and semantic consistency, especially in regions requiring fine-grained boundary
delineation. To address these challenges, we propose UKANFormer, a novel
se-mantic segmentation model designed to achieve high-precision mapping under
noisy supervision derived from Allen Coral Atlas. Building upon the UKAN
architecture, UKANFormer incorporates a Global-Local Transformer (GL-Trans)
block in the decoder, enabling the extraction of both global semantic
structures and local boundary details. In experiments, UKANFormer achieved a
coral-class IoU of 67.00% and pixel accuracy of 83.98%, outperforming
conventional baselines under the same noisy labels setting. Remarkably, the
model produces predictions that are visually and structurally more accurate
than the noisy labels used for training. These results challenge the notion
that data quality directly limits model performance, showing that architectural
design can mitigate label noise and sup-port scalable mapping under imperfect
supervision. UKANFormer provides a foundation for ecological monitoring where
reliable labels are scarce.

</details>


### [68] [A Comprehensive Survey on World Models for Embodied AI](https://arxiv.org/abs/2510.16732)
*Xinqing Li,Xin He,Le Zhang,Yun Liu*

Main category: cs.CV

TL;DR: 本文提出了一种用于具身智能（Embodied AI）的统一世界模型框架，通过三轴分类法系统化地梳理了现有方法，并对数据集、评估指标和前沿模型进行了综述与定量比较，指出了长期一致性、物理合理性与计算效率之间的权衡等关键挑战。


<details>
  <summary>Details</summary>
Motivation: 具身智能体需要理解环境动态并预测动作对未来状态的影响，而现有世界模型研究分散，缺乏统一框架，亟需系统性的分类、评估和挑战分析以推动该领域发展。

Method: 提出了一个三轴分类体系：功能（决策耦合型 vs. 通用型）、时序建模（序列模拟与推断 vs. 全局差异预测）和空间表示（全局隐变量、特征序列、空间隐网格、分解渲染表示），并建立了涵盖多领域的数据资源与评估指标体系。

Result: 系统梳理了机器人、自动驾驶和通用视频领域的世界模型研究，提供了前沿模型的定量比较，维护了一个精选文献库，并识别出如数据集不统一、评估偏向像素精度而非物理一致性、计算效率与性能权衡等核心问题。

Conclusion: 建立统一的世界模型框架对具身智能至关重要，未来研究需关注物理合理性、长期时序一致性与轻量化建模，同时发展更贴近实际任务需求的评估标准。

Abstract: Embodied AI requires agents that perceive, act, and anticipate how actions
reshape future world states. World models serve as internal simulators that
capture environment dynamics, enabling forward and counterfactual rollouts to
support perception, prediction, and decision making. This survey presents a
unified framework for world models in embodied AI. Specifically, we formalize
the problem setting and learning objectives, and propose a three-axis taxonomy
encompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2)
Temporal Modeling, Sequential Simulation and Inference vs. Global Difference
Prediction; (3) Spatial Representation, Global Latent Vector, Token Feature
Sequence, Spatial Latent Grid, and Decomposed Rendering Representation. We
systematize data resources and metrics across robotics, autonomous driving, and
general video settings, covering pixel prediction quality, state-level
understanding, and task performance. Furthermore, we offer a quantitative
comparison of state-of-the-art models and distill key open challenges,
including the scarcity of unified datasets and the need for evaluation metrics
that assess physical consistency over pixel fidelity, the trade-off between
model performance and the computational efficiency required for real-time
control, and the core modeling difficulty of achieving long-horizon temporal
consistency while mitigating error accumulation. Finally, we maintain a curated
bibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.

</details>


### [69] [Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling](https://arxiv.org/abs/2510.16751)
*Erik Riise,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: 本文提出，视觉自回归模型的离散序列特性使其适用于有效的搜索策略，使用束搜索显著提升了文本到图像生成的质量，使2B参数模型超越12B参数扩散模型。


<details>
  <summary>Details</summary>
Motivation: 尽管推理时搜索在大语言模型中取得了显著进展，但在图像生成领域却难以复制类似成功，尤其是在连续扩散模型上搜索效果有限，因此探索更适合搜索的生成架构成为关键问题。

Method: 研究采用离散的视觉自回归模型，利用其离散序列生成特性，引入束搜索（beam search）进行推理时优化，并通过系统性消融实验和验证器分析评估其效果。

Result: 实验表明，束搜索显著提升了文本到图像生成性能，一个2B参数的自回归模型在多个基准上超越了12B参数的扩散模型，且离散token空间支持早期剪枝和计算复用，提升效率。

Conclusion: 模型架构（尤其是离散性与序列性）对推理时优化至关重要，而不仅仅是模型规模，这为图像生成模型的设计提供了新方向。

Abstract: While inference-time scaling through search has revolutionized Large Language
Models, translating these gains to image generation has proven difficult.
Recent attempts to apply search strategies to continuous diffusion models show
limited benefits, with simple random sampling often performing best. We
demonstrate that the discrete, sequential nature of visual autoregressive
models enables effective search for image generation. We show that beam search
substantially improves text-to-image generation, enabling a 2B parameter
autoregressive model to outperform a 12B parameter diffusion model across
benchmarks. Systematic ablations show that this advantage comes from the
discrete token space, which allows early pruning and computational reuse, and
our verifier analysis highlights trade-offs between speed and reasoning
capability. These findings suggest that model architecture, not just scale, is
critical for inference-time optimization in visual generation.

</details>


### [70] [Prominence-Aware Artifact Detection and Dataset for Image Super-Resolution](https://arxiv.org/abs/2510.16752)
*Ivan Molodetskikh,Kirill Malyshev,Mark Mirgaleev,Nikita Zagainov,Evgeney Bogatyrev,Dmitriy Vatolin*

Main category: cs.CV

TL;DR: 提出了一种基于人类感知显著性的生成式图像超分辨率（SR）伪影评估方法，构建了一个包含1302个伪影样本的数据集，并训练了一个轻量级回归模型生成伪影显著性热图。


<details>
  <summary>Details</summary>
Motivation: 现有SR方法生成的伪影对人眼感知的影响不同，但通常被统一视为缺陷，缺乏对伪影显著性的细粒度建模。

Method: 收集了来自11种现代SR方法的1302个伪影样本，通过众包获取每个伪影的人类感知显著性评分，并基于此训练一个轻量级回归器生成空间显著性热图。

Result: 所训练的回归器在检测显著伪影方面优于现有方法，能有效生成伪影的显著性热图。

Conclusion: 伪影应根据其对人类观察者的显著性来表征，该数据集和模型有助于实现对SR伪影的感知感知评估与缓解。

Abstract: Generative image super-resolution (SR) is rapidly advancing in visual quality
and detail restoration. As the capacity of SR models expands, however, so does
their tendency to produce artifacts: incorrect, visually disturbing details
that reduce perceived quality. Crucially, their perceptual impact varies: some
artifacts are barely noticeable while others strongly degrade the image. We
argue that artifacts should be characterized by their prominence to human
observers rather than treated as uniform binary defects. Motivated by this, we
present a novel dataset of 1302 artifact examples from 11 contemporary image-SR
methods, where each artifact is paired with a crowdsourced prominence score.
Building on this dataset, we train a lightweight regressor that produces
spatial prominence heatmaps and outperforms existing methods at detecting
prominent artifacts. We release the dataset and code to facilitate
prominence-aware evaluation and mitigation of SR artifacts.

</details>


### [71] [WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and Mamba-based Channel Modeling with Texture Enhancement](https://arxiv.org/abs/2510.16765)
*Shengyu Zhu,Fan,Fuxuan Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的图像恢复框架WaMaIR，通过全局多尺度小波变换卷积（GMWTConvs）扩大感受野，结合基于Mamba的通道感知模块（MCAM）增强通道间的长程依赖建模，并设计多尺度纹理增强损失（MTELoss），有效提升了纹理细节的恢复效果和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有CNN方法在图像恢复中受限于较小感受野和缺乏通道特征建模，难以充分恢复精细纹理细节。因此，需要一种能扩大感受野并增强通道间依赖建模的新框架。

Method: 提出WaMaIR框架：1）GMWTConvs利用多尺度小波变换扩展感受野，保留和丰富纹理特征；2）MCAM模块利用Mamba结构建模特征通道的长程依赖；3）MTELoss损失函数用于增强多尺度纹理细节的重建。

Result: 在多个图像恢复任务上实验表明，WaMaIR优于当前最先进方法，在恢复图像质量（如PSNR、SSIM）和计算效率方面均取得更优表现。

Conclusion: WaMaIR通过扩展感受野、增强通道建模和专用损失函数，显著提升了图像恢复中的纹理细节重建能力，兼具高性能与高效性，具有良好的应用前景。

Abstract: Image restoration is a fundamental and challenging task in computer vision,
where CNN-based frameworks demonstrate significant computational efficiency.
However, previous CNN-based methods often face challenges in adequately
restoring fine texture details, which are limited by the small receptive field
of CNN structures and the lack of channel feature modeling. In this paper, we
propose WaMaIR, which is a novel framework with a large receptive field for
image perception and improves the reconstruction of texture details in restored
images. Specifically, we introduce the Global Multiscale Wavelet Transform
Convolutions (GMWTConvs) for expandding the receptive field to extract image
features, preserving and enriching texture features in model inputs. Meanwhile,
we propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to
capture long-range dependencies within feature channels, which enhancing the
model sensitivity to color, edges, and texture information. Additionally, we
propose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to
guide the model in preserving detailed texture structures effectively.
Extensive experiments confirm that WaMaIR outperforms state-of-the-art methods,
achieving better image restoration and efficient computational performance of
the model.

</details>


### [72] [Region in Context: Text-condition Image editing with Human-like semantic reasoning](https://arxiv.org/abs/2510.16772)
*Thuy Phuong Vu,Dinh-Cuong Hoang,Minhhuy Le,Phan Xuan Tan*

Main category: cs.CV

TL;DR: 提出一种名为Region in Context的新框架，通过多级语义对齐实现更协调的文本条件图像编辑，提升局部修改与全局结构的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法常忽视图像局部区域与整体语义的关系，导致编辑结果不连贯；本文旨在通过全局上下文理解实现更自然的图像编辑。

Method: 引入双层引导机制：区域级描述与包含全图上下文的区域表示对齐，同时整图与由大型视觉-语言模型生成的场景级描述匹配，实现局部与全局的语义一致。

Result: 实验表明，该方法在生成结果的连贯性和指令遵循方面优于现有方法。

Conclusion: 通过显式的多级语言引导，模型能更好地在全局语境下进行局部编辑，提升图像编辑的整体质量。

Abstract: Recent research has made significant progress in localizing and editing image
regions based on text. However, most approaches treat these regions in
isolation, relying solely on local cues without accounting for how each part
contributes to the overall visual and semantic composition. This often results
in inconsistent edits, unnatural transitions, or loss of coherence across the
image. In this work, we propose Region in Context, a novel framework for
text-conditioned image editing that performs multilevel semantic alignment
between vision and language, inspired by the human ability to reason about
edits in relation to the whole scene. Our method encourages each region to
understand its role within the global image context, enabling precise and
harmonized changes. At its core, the framework introduces a dual-level guidance
mechanism: regions are represented with full-image context and aligned with
detailed region-level descriptions, while the entire image is simultaneously
matched to a comprehensive scene-level description generated by a large
vision-language model. These descriptions serve as explicit verbal references
of the intended content, guiding both local modifications and global structure.
Experiments show that it produces more coherent and instruction-aligned
results. Code is available at:
https://github.com/thuyvuphuong/Region-in-Context.git

</details>


### [73] [EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation](https://arxiv.org/abs/2510.16776)
*Mingzheng Zhang,Jinfeng Gao,Dan Xu,Jiangrui Yu,Yuhan Qiao,Lan Chen,Jin Tang,Xiao Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于预训练Mamba网络和参数高效微调方法的新型X射线医学报告生成框架EMRRG，通过改进视觉骨干和语言模型的协同，实现了优越的生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学报告生成模型多依赖大语言模型，忽视了先进视觉基础模型和微调技术（如增强跨注意力机制）的潜力，且非Transformer架构（如Mamba）在该领域尚未充分探索。

Method: 将X射线图像分块并由基于状态空间模型（SSM）的视觉骨干提取特征，采用Partial LoRA进行参数高效微调；结合带有混合解码器的大语言模型生成报告，实现端到端训练。

Result: 在三个主流基准数据集上的大量实验验证了EMRRG的有效性，表现出优异的报告生成性能。

Conclusion: EMRRG展示了Mamba架构在医学报告生成中的潜力，验证了参数高效微调与非Transformer视觉模型结合的优势，为该领域提供了新的研究方向。

Abstract: X-ray image-based medical report generation (MRG) is a pivotal area in
artificial intelligence that can significantly reduce diagnostic burdens for
clinicians and patient wait times. Existing MRG models predominantly rely on
Large Language Models (LLMs) to improve report generation, with limited
exploration of pre-trained vision foundation models or advanced fine-tuning
techniques. Mainstream frameworks either avoid fine-tuning or utilize
simplistic methods like LoRA, often neglecting the potential of enhancing
cross-attention mechanisms. Additionally, while Transformer-based models
dominate vision-language tasks, non-Transformer architectures, such as the
Mamba network, remain underexplored for medical report generation, presenting a
promising avenue for future research. In this paper, we propose EMRRG, a novel
X-ray report generation framework that fine-tunes pre-trained Mamba networks
using parameter-efficient methods. Specifically, X-ray images are divided into
patches, tokenized, and processed by an SSM-based vision backbone for feature
extraction, with Partial LoRA yielding optimal performance. An LLM with a
hybrid decoder generates the medical report, enabling end-to-end training and
achieving strong results on benchmark datasets. Extensive experiments on three
widely used benchmark datasets fully validated the effectiveness of our
proposed strategies for the X-ray MRG. The source code of this paper will be
released on https://github.com/Event-AHU/Medical_Image_Analysis.

</details>


### [74] [GS2POSE: Marry Gaussian Splatting to 6D Object Pose Estimation](https://arxiv.org/abs/2510.16777)
*Junbo Li,Weimin Yuan,Yinuo Wang,Yue Zeng,Shihao Shu,Cai Meng,Xiangzhi Bai*

Main category: cs.CV

TL;DR: 提出了一种名为GS2POSE的新方法，用于6D物体姿态估计，通过扩展3DGS能力并优化姿态和颜色参数，在多个数据集上实现了更高的精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在无纹理物体和光照变化条件下表现不佳，需改进姿态估计的鲁棒性和准确性。

Method: 基于Bundle Adjustment思想，利用李代数构建位姿可微的渲染流程，迭代优化姿态，并在3DGS模型中更新颜色参数以适应光照变化。

Result: 在T-LESS、LineMod-Occlusion和LineMod数据集上分别比先前方法提高了1.4%、2.8%和2.5%的精度。

Conclusion: GS2POSE在挑战性场景下展现出更强的适应性和更高的姿态估计精度，验证了可微渲染与光照适应策略的有效性。

Abstract: Accurate 6D pose estimation of 3D objects is a fundamental task in computer
vision, and current research typically predicts the 6D pose by establishing
correspondences between 2D image features and 3D model features. However, these
methods often face difficulties with textureless objects and varying
illumination conditions. To overcome these limitations, we propose GS2POSE, a
novel approach for 6D object pose estimation. GS2POSE formulates a pose
regression algorithm inspired by the principles of Bundle Adjustment (BA). By
leveraging Lie algebra, we extend the capabilities of 3DGS to develop a
pose-differentiable rendering pipeline, which iteratively optimizes the pose by
comparing the input image to the rendered image. Additionally, GS2POSE updates
color parameters within the 3DGS model, enhancing its adaptability to changes
in illumination. Compared to previous models, GS2POSE demonstrates accuracy
improvements of 1.4\%, 2.8\% and 2.5\% on the T-LESS, LineMod-Occlusion and
LineMod datasets, respectively.

</details>


### [75] [Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features](https://arxiv.org/abs/2510.16781)
*Shihao Ji,Zihui Song*

Main category: cs.CV

TL;DR: 提出一种无需训练的视频理解框架，通过结合预训练视觉语言模型和经典机器学习算法，实现零样本的视频时空聚类与多模态内容摘要生成。


<details>
  <summary>Details</summary>
Motivation: 现有的视频理解模型依赖大量标注数据和特定任务训练，成本高且难以扩展；而大型视觉语言模型在静态图像上的零样本推理能力尚未有效迁移到视频领域。

Method: 将视频理解重构为高维语义特征空间中的自监督时空聚类问题：首先使用冻结的预训练VLM视觉编码器提取视频的语义特征轨迹，然后采用核时间分割（KTS）进行事件分段，再通过无监督密度聚类识别重复出现的宏观场景，最后选取代表帧并利用VLM生成文本描述。

Result: 框架实现了无需训练的视频结构化分析，能自动生成连贯的多模态视频摘要，并在语义一致性与结构划分质量上表现良好。

Conclusion: 该方法提供了一条高效、可解释且模型无关的路径，实现了零样本下的自动化视频内容结构分析，具有良好的泛化性与应用潜力。

Abstract: The remarkable zero-shot reasoning capabilities of large-scale Visual
Language Models (VLMs) on static images have yet to be fully translated to the
video domain. Conventional video understanding models often rely on extensive,
task-specific training on annotated datasets, a process that is both costly and
limited in scalability. This paper introduces a novel, training-free framework
for video understanding that circumvents end-to-end training by synergistically
combining the rich semantic priors of pre-trained VLMs with classic machine
learning algorithms for pattern discovery. Our core idea is to reframe video
understanding as a self-supervised spatio-temporal clustering problem within a
high-dimensional semantic feature space. The proposed pipeline first transforms
a video stream into a semantic feature trajectory using the frozen visual
encoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal
Segmentation (KTS), a robust machine learning technique, to partition the
continuous feature stream into discrete, semantically coherent event segments.
These segments are then subjected to unsupervised density-based clustering to
identify recurring macroscopic scenes and themes throughout the video. By
selecting representative keyframes from each discovered cluster and leveraging
the VLM's generative capabilities for textual description, our framework
automatically produces a structured, multi-modal summary of the video content.
This approach provides an effective, interpretable, and model-agnostic pathway
for zero-shot, automated structural analysis of video content.

</details>


### [76] [Segmentation as A Plug-and-Play Capability for Frozen Multimodal LLMs](https://arxiv.org/abs/2510.16785)
*Jiazhen Liu,Long Chen*

Main category: cs.CV

TL;DR: LENS是一种即插即用的新方法，通过附加轻量级模块为多模态大模型（MLLM）添加像素级分割能力，无需微调模型参数，保持其原有泛化能力的同时实现优异分割性能。


<details>
  <summary>Details</summary>
Motivation: 将分割能力集成到统一的多模态大语言模型中存在挑战，现有方法通常需微调模型以适配掩码解码器，这会改变输出空间并损害模型的泛化性，违背统一模型的初衷。

Method: 提出LENS方法，在完全冻结的MLLM上附加一个轻量级可训练头，通过优化注意力图中的空间线索提取关键点，并将其转化为掩码解码器可识别的点特征，实现无需微调的分割能力扩展。

Result: 实验表明，LENS在分割性能上达到或超过需重新训练的方法，同时完全保留了MLLM原有的泛化能力，而传统微调方法会显著削弱该能力。

Conclusion: LENS的即插即用设计为扩展多模态大模型提供了高效且强大的新范式，推动实现真正多才多艺的统一模型。

Abstract: Integrating diverse visual capabilities into a unified model is a significant
trend in Multimodal Large Language Models (MLLMs). Among these, the inclusion
of segmentation poses a distinct set of challenges. To equip MLLMs with
pixel-level segmentation abilities, prevailing methods require finetuning the
model to produce specific outputs compatible with a mask decoder. This process
typically alters the model's output space and compromises its intrinsic
generalization, which undermines the goal of building a unified model. We
introduce LENS (Leveraging kEypoiNts for MLLMs' Segmentation), a novel
plug-and-play solution. LENS attaches a lightweight, trainable head to a
completely frozen MLLM. By refining the spatial cues embedded in attention
maps, LENS extracts keypoints and describes them into point-wise features
directly compatible with the mask decoder. Extensive experiments validate our
approach: LENS achieves segmentation performance competitive with or superior
to that of retraining-based methods. Crucially, it does so while fully
preserving the MLLM's generalization capabilities, which are significantly
degraded by finetuning approaches. As such, the attachable design of LENS
establishes an efficient and powerful paradigm for extending MLLMs, paving the
way for truly multi-talented, unified models.

</details>


### [77] [Unsupervised Monocular Road Segmentation for Autonomous Driving via Scene Geometry](https://arxiv.org/abs/2510.16790)
*Sara Hatami Rostami,Behrooz Nasihatkon*

Main category: cs.CV

TL;DR: 提出了一种完全无监督的二值道路分割方法，利用几何先验和时序一致性生成弱标签并优化分割结果，在Cityscapes数据集上达到82%的IoU。


<details>
  <summary>Details</summary>
Motivation: 减少对昂贵人工标注数据的依赖，实现可扩展的自动驾驶道路分割。

Method: 首先基于几何先验（地平线以上为非道路，车辆前方预定义四边形为道路）生成弱标签；然后通过跨帧跟踪局部特征点并利用互信息最大化施加时序一致性约束来优化标签。

Result: 在Cityscapes数据集上取得了82%的IoU，显示出高精度和良好的时间稳定性。

Conclusion: 结合几何约束和时序一致性是一种有效且可扩展的无监督道路分割方案。

Abstract: This paper presents a fully unsupervised approach for binary road
segmentation (road vs. non-road), eliminating the reliance on costly manually
labeled datasets. The method leverages scene geometry and temporal cues to
distinguish road from non-road regions. Weak labels are first generated from
geometric priors, marking pixels above the horizon as non-road and a predefined
quadrilateral in front of the vehicle as road. In a refinement stage, temporal
consistency is enforced by tracking local feature points across frames and
penalizing inconsistent label assignments using mutual information
maximization. This enhances both precision and temporal stability. On the
Cityscapes dataset, the model achieves an Intersection-over-Union (IoU) of
0.82, demonstrating high accuracy with a simple design. These findings
demonstrate the potential of combining geometric constraints and temporal
consistency for scalable unsupervised road segmentation in autonomous driving.

</details>


### [78] [Personalized Image Filter: Mastering Your Photographic Style](https://arxiv.org/abs/2510.16791)
*Chengxuan Zhu,Shuchen Weng,Jiacong Fang,Peixuan Zhang,Si Li,Chao Xu,Boxin Shi*

Main category: cs.CV

TL;DR: 本文提出了一种基于文本到图像扩散模型的个性化图像滤镜（PIF），能够有效提取并迁移摄影作品风格，克服了以往方法在概念学习和内容保持上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的图像风格迁移方法难以从参考图像中学习有意义的摄影概念，或无法保持内容图像的内容结构，因此需要一种能同时理解摄影风格编辑过程并保持内容的方法。

Method: 基于预训练的文本到图像扩散模型，利用生成先验学习摄影概念的平均外观，并结合文本反转技术，通过优化摄影概念的文本提示来学习参考图像的摄影风格。

Result: PIF在提取和迁移多种摄影风格方面表现出色，能准确捕捉风格特征并保持内容图像的语义完整性。

Conclusion: PIF通过结合扩散模型与文本反转，实现了高质量的个性化摄影风格迁移，为图像编辑提供了新的有效工具。

Abstract: Photographic style, as a composition of certain photographic concepts, is the
charm behind renowned photographers. But learning and transferring photographic
style need a profound understanding of how the photo is edited from the unknown
original appearance. Previous works either fail to learn meaningful
photographic concepts from reference images, or cannot preserve the content of
the content image. To tackle these issues, we proposed a Personalized Image
Filter (PIF). Based on a pretrained text-to-image diffusion model, the
generative prior enables PIF to learn the average appearance of photographic
concepts, as well as how to adjust them according to text prompts. PIF then
learns the photographic style of reference images with the textual inversion
technique, by optimizing the prompts for the photographic concepts. PIF shows
outstanding performance in extracting and transferring various kinds of
photographic style. Project page: https://pif.pages.dev/

</details>


### [79] [An RGB-D Image Dataset for Lychee Detection and Maturity Classification for Robotic Harvesting](https://arxiv.org/abs/2510.16800)
*Zhenpeng Zhang,Yi Wang,Shanglei Chai,Yingying Liu,Zekai Xie,Wenhao Huang,Pengyu Li,Zipei Luo,Dajiang Lu,Yibin Tian*

Main category: cs.CV

TL;DR: 本文构建了一个高质量、公开的荔枝数据集，包含多品种、多生长环境下的RGB图像和深度图像，用于支持基于视觉的采摘机器人在荔枝检测与成熟度分类任务中的研究。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏在自然生长环境下一致且全面标注的开源荔枝数据集，限制了基于视觉的荔枝采摘机器人开发。为此，作者旨在构建一个高质量、多样化且公开可用的数据集，以推动相关研究。

Method: 采集了多种天气条件、不同时段和多个荔枝品种（如糯米糍、妃子笑、黑叶、怀枝）的RGB图像，并通过数据增强和深度相机获取更多数据。数据集包含原始图像、增强图像和深度图像。三名标注者独立标注数据，由第四人审核整合，确保标注一致性。同时进行了详细的数据统计分析，并采用三种典型的深度学习模型对数据集进行检测与分类性能评估。

Result: 数据集共包含11,414张图像，其中包括878张原始RGB图像、8,780张增强RGB图像和1,756张深度图像，共标注9,658个荔枝样本，涵盖三个成熟阶段。实验结果表明，该数据集能有效支持深度学习模型在荔枝检测与成熟度分类任务中的训练与评估。

Conclusion: 该数据集填补了自然环境下公开荔枝图像数据的空白，具有良好的多样性和标注质量，适用于农业机器人中的果实识别与成熟度判断研究，且已公开供学术使用。

Abstract: Lychee is a high-value subtropical fruit. The adoption of vision-based
harvesting robots can significantly improve productivity while reduce reliance
on labor. High-quality data are essential for developing such harvesting
robots. However, there are currently no consistently and comprehensively
annotated open-source lychee datasets featuring fruits in natural growing
environments. To address this, we constructed a dataset to facilitate lychee
detection and maturity classification. Color (RGB) images were acquired under
diverse weather conditions, and at different times of the day, across multiple
lychee varieties, such as Nuomici, Feizixiao, Heiye, and Huaizhi. The dataset
encompasses three different ripeness stages and contains 11,414 images,
consisting of 878 raw RGB images, 8,780 augmented RGB images, and 1,756 depth
images. The images are annotated with 9,658 pairs of lables for lychee
detection and maturity classification. To improve annotation consistency, three
individuals independently labeled the data, and their results were then
aggregated and verified by a fourth reviewer. Detailed statistical analyses
were done to examine the dataset. Finally, we performed experiments using three
representative deep learning models to evaluate the dataset. It is publicly
available for academic

</details>


### [80] [ReefNet: A Large scale, Taxonomically Enriched Dataset and Benchmark for Hard Coral Classification](https://arxiv.org/abs/2510.16822)
*Yahia Battach,Abdulwahab Felemban,Faizan Farooq Khan,Yousef A. Radwan,Xiang Li,Fabio Marchese,Sara Beery,Burton H. Jones,Francesca Benzoni,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: ReefNet是一个大规模公开的珊瑚礁图像数据集，包含约92.5万个经专家验证的属级硬珊瑚点标签，并与世界海洋物种数据库（WoRMS）对接，支持精细化、全球范围的珊瑚分类与域泛化基准测试。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁正因气候变化等人为压力迅速退化，亟需可扩展、自动化的监测手段；现有数据集在规模、地理覆盖、标签细粒度或机器学习可用性方面存在局限。

Method: 整合来自76个CoralNet来源及红海Al Wajh站点的图像，构建带点标注的 ReefNet 数据集；提出两种评估设置：源内（within-source）和跨源（cross-source）基准，用于评估监督与零样本分类模型的性能。

Result: 监督模型在源内表现良好，但在跨源场景下性能显著下降；零样本模型整体表现较差，尤其对稀有和视觉相似的珊瑚属。

Conclusion: ReefNet 提供了一个具有挑战性的基准，旨在推动域泛化和细粒度珊瑚分类技术的发展，促进全球珊瑚礁监测与保护。

Abstract: Coral reefs are rapidly declining due to anthropogenic pressures such as
climate change, underscoring the urgent need for scalable, automated
monitoring. We introduce ReefNet, a large public coral reef image dataset with
point-label annotations mapped to the World Register of Marine Species (WoRMS).
ReefNet aggregates imagery from 76 curated CoralNet sources and an additional
site from Al Wajh in the Red Sea, totaling approximately 925000 genus-level
hard coral annotations with expert-verified labels. Unlike prior datasets,
which are often limited by size, geography, or coarse labels and are not
ML-ready, ReefNet offers fine-grained, taxonomically mapped labels at a global
scale to WoRMS. We propose two evaluation settings: (i) a within-source
benchmark that partitions each source's images for localized evaluation, and
(ii) a cross-source benchmark that withholds entire sources to test domain
generalization. We analyze both supervised and zero-shot classification
performance on ReefNet and find that while supervised within-source performance
is promising, supervised performance drops sharply across domains, and
performance is low across the board for zero-shot models, especially for rare
and visually similar genera. This provides a challenging benchmark intended to
catalyze advances in domain generalization and fine-grained coral
classification. We will release our dataset, benchmarking code, and pretrained
models to advance robust, domain-adaptive, global coral reef monitoring and
conservation.

</details>


### [81] [Robust Cross-Domain Adaptation in Texture Features Transferring for Wood Chip Moisture Content Prediction](https://arxiv.org/abs/2510.16832)
*Abdur Rahman,Mohammad Marufuzzaman,Jason Street,Haifeng Wang,Veera G. Gude,Randy Buchanan*

Main category: cs.CV

TL;DR: 本研究提出了一种名为AdaptMoist的域适应方法，利用五种纹理特征的组合，实现了跨来源木材碎片含水量的准确预测，准确率达到95%，跨域性能提升23%。


<details>
  <summary>Details</summary>
Motivation: 传统烘干法耗时且破坏样本，现有间接方法在木材来源多样时准确性下降，亟需一种能克服来源变异性的鲁棒预测方法。

Method: 从木材碎片图像中提取五种纹理特征，结合机器学习与提出的AdaptMoist域适应方法，利用调整互信息准则进行模型保存，实现跨域知识迁移。

Result: 组合纹理特征在单一域下达到95%的预测准确率；AdaptMoist方法使跨域预测准确率从57%提升至80%，平均提升23%。

Conclusion: AdaptMoist方法能有效缓解数据分布差异，实现稳定准确的跨域含水率预测，适用于依赖木材碎片的工业场景。

Abstract: Accurate and quick prediction of wood chip moisture content is critical for
optimizing biofuel production and ensuring energy efficiency. The current
widely used direct method (oven drying) is limited by its longer processing
time and sample destructiveness. On the other hand, existing indirect methods,
including near-infrared spectroscopy-based, electrical capacitance-based, and
image-based approaches, are quick but not accurate when wood chips come from
various sources. Variability in the source material can alter data
distributions, undermining the performance of data-driven models. Therefore,
there is a need for a robust approach that effectively mitigates the impact of
source variability. Previous studies show that manually extracted texture
features have the potential to predict wood chip moisture class. Building on
this, in this study, we conduct a comprehensive analysis of five distinct
texture feature types extracted from wood chip images to predict moisture
content. Our findings reveal that a combined feature set incorporating all five
texture features achieves an accuracy of 95% and consistently outperforms
individual texture features in predicting moisture content. To ensure robust
moisture prediction, we propose a domain adaptation method named AdaptMoist
that utilizes the texture features to transfer knowledge from one source of
wood chip data to another, addressing variability across different domains. We
also proposed a criterion for model saving based on adjusted mutual
information. The AdaptMoist method improves prediction accuracy across domains
by 23%, achieving an average accuracy of 80%, compared to 57% for non-adapted
models. These results highlight the effectiveness of AdaptMoist as a robust
solution for wood chip moisture content estimation across domains, making it a
potential solution for wood chip-reliant industries.

</details>


### [82] [From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display](https://arxiv.org/abs/2510.16833)
*Xiangyu Mu,Dongliang Zhou,Jie Hou,Haijun Zhang,Weili Guan*

Main category: cs.CV

TL;DR: 本文提出了一种名为M2HVideo的视频生成框架，用于将服装模特（mannequin）视频转换为具有身份可控性和高真实感的人类穿着视频，解决了头身运动不一致和身份漂移问题。


<details>
  <summary>Details</summary>
Motivation: 现有的服装展示多采用静态模特，缺乏真实人体动态展示的生动性。为了以低成本实现高质量、个性化的在线服装展示，需要将模特视频转换为逼真的人体穿着视频。

Method: 提出M2HVideo框架：1）动态姿态感知头部编码器，融合面部语义与身体姿态以保持身份一致性；2）基于DDIM的像素空间镜像损失，保留面部细节；3）分布感知适配器，对齐身份与服装特征的统计分布，增强时序一致性。

Result: 在UBC、ASOS和新构建的MannequinVideos数据集上实验表明，M2HVideo在服装一致性、身份保持和视频质量方面优于现有最先进方法。

Conclusion: M2HVideo有效实现了从模特到真实人类穿着视频的转化，为在线时尚展示提供了实用且高质量的解决方案。

Abstract: Mannequin-based clothing displays offer a cost-effective alternative to
real-model showcases for online fashion presentation, but lack realism and
expressive detail. To overcome this limitation, we introduce a new task called
mannequin-to-human (M2H) video generation, which aims to synthesize
identity-controllable, photorealistic human videos from footage of mannequins.
We propose M2HVideo, a pose-aware and identity-preserving video generation
framework that addresses two key challenges: the misalignment between head and
body motion, and identity drift caused by temporal modeling. In particular,
M2HVideo incorporates a dynamic pose-aware head encoder that fuses facial
semantics with body pose to produce consistent identity embeddings across
frames. To address the loss of fine facial details due to latent space
compression, we introduce a mirror loss applied in pixel space through a
denoising diffusion implicit model (DDIM)-based one-step denoising.
Additionally, we design a distribution-aware adapter that aligns statistical
distributions of identity and clothing features to enhance temporal coherence.
Extensive experiments on the UBC fashion dataset, our self-constructed ASOS
dataset, and the newly collected MannequinVideos dataset captured on-site
demonstrate that M2HVideo achieves superior performance in terms of clothing
consistency, identity preservation, and video fidelity in comparison to
state-of-the-art methods.

</details>


### [83] [2DGS-R: Revisiting the Normal Consistency Regularization in 2D Gaussian Splatting](https://arxiv.org/abs/2510.16837)
*Haofan Ren,Qingsong Yan,Ming Lu,Rongfeng Lu,Zunjie Zhu*

Main category: cs.CV

TL;DR: 提出2DGS-R方法，通过分层训练在几乎不增加开销的情况下提升2D高斯点阵的渲染质量并保持几何精度。


<details>
  <summary>Details</summary>
Motivation: 3D高斯点阵难以精确表示表面，而2D高斯虽提升几何保真度但牺牲了渲染质量，需平衡二者。

Method: 采用分层训练策略：先用法向一致性正则化训练2D高斯，再对渲染不佳的部分进行原位克隆增强，最后冻结透明度微调模型。

Result: 相比原2DGS仅增加1%存储和极少训练时间，显著提升渲染质量并保留精细几何结构。

Conclusion: 2DGS-R有效平衡了效率与性能，在视觉保真度和几何重建精度上均取得改进。

Abstract: Recent advancements in 3D Gaussian Splatting (3DGS) have greatly influenced
neural fields, as it enables high-fidelity rendering with impressive visual
quality. However, 3DGS has difficulty accurately representing surfaces. In
contrast, 2DGS transforms the 3D volume into a collection of 2D planar Gaussian
disks. Despite advancements in geometric fidelity, rendering quality remains
compromised, highlighting the challenge of achieving both high-quality
rendering and precise geometric structures. This indicates that optimizing both
geometric and rendering quality in a single training stage is currently
unfeasible. To overcome this limitation, we present 2DGS-R, a new method that
uses a hierarchical training approach to improve rendering quality while
maintaining geometric accuracy. 2DGS-R first trains the original 2D Gaussians
with the normal consistency regularization. Then 2DGS-R selects the 2D
Gaussians with inadequate rendering quality and applies a novel in-place
cloning operation to enhance the 2D Gaussians. Finally, we fine-tune the 2DGS-R
model with opacity frozen. Experimental results show that compared to the
original 2DGS, our method requires only 1\% more storage and minimal additional
training time. Despite this negligible overhead, it achieves high-quality
rendering results while preserving fine geometric structures. These findings
indicate that our approach effectively balances efficiency with performance,
leading to improvements in both visual fidelity and geometric reconstruction
accuracy.

</details>


### [84] [ArmFormer: Lightweight Transformer Architecture for Real-Time Multi-Class Weapon Segmentation and Classification](https://arxiv.org/abs/2510.16854)
*Akhila Kambhatla,Taminul Islam,Khaled R Ahmed*

Main category: cs.CV

TL;DR: 提出了一种轻量级Transformer架构ArmFormer，用于实现像素级武器分割，在保持高精度的同时实现实时推理，适用于边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 传统武器检测方法仅提供粗略的边界框定位，缺乏细粒度分割能力；现有语义分割模型难以兼顾精度与计算效率，难以部署于边缘设备。

Method: 结合Convolutional Block Attention Module (CBAM) 与 MixVision Transformer 架构，设计CBAM增强的编码器骨干网络与注意力集成的hamburger解码器，实现五类武器（手枪、步枪、刀、左轮手枪、人）的多类别语义分割。

Result: ArmFormer达到80.64% mIoU和89.13% mFscore的SOTA性能，推理速度达82.26 FPS，仅需4.886G FLOPs和3.66M参数，计算量仅为重型模型的1/48。

Conclusion: ArmFormer在精度、效率和实用性之间取得了优越平衡，是部署于便携式摄像头、监控无人机和嵌入式AI加速器等边缘安全设备的理想解决方案。

Abstract: The escalating threat of weapon-related violence necessitates automated
detection systems capable of pixel-level precision for accurate threat
assessment in real-time security applications. Traditional weapon detection
approaches rely on object detection frameworks that provide only coarse
bounding box localizations, lacking the fine-grained segmentation required for
comprehensive threat analysis. Furthermore, existing semantic segmentation
models either sacrifice accuracy for computational efficiency or require
excessive computational resources incompatible with edge deployment scenarios.
This paper presents ArmFormer, a lightweight transformer-based semantic
segmentation framework that strategically integrates Convolutional Block
Attention Module (CBAM) with MixVisionTransformer architecture to achieve
superior accuracy while maintaining computational efficiency suitable for
resource-constrained edge devices. Our approach combines CBAM-enhanced encoder
backbone with attention-integrated hamburger decoder to enable multi-class
weapon segmentation across five categories: handgun, rifle, knife, revolver,
and human. Comprehensive experiments demonstrate that ArmFormer achieves
state-of-the-art performance with 80.64% mIoU and 89.13% mFscore while
maintaining real-time inference at 82.26 FPS. With only 4.886G FLOPs and 3.66M
parameters, ArmFormer outperforms heavyweight models requiring up to 48x more
computation, establishing it as the optimal solution for deployment on portable
security cameras, surveillance drones, and embedded AI accelerators in
distributed security infrastructure.

</details>


### [85] [BARL: Bilateral Alignment in Representation and Label Spaces for Semi-Supervised Volumetric Medical Image Segmentation](https://arxiv.org/abs/2510.16863)
*Shujian Gao,Yuan Wang,Zekuan Yu*

Main category: cs.CV

TL;DR: 提出了一种名为BARL的新框架，通过在表示空间和标签空间中进行双向对齐，显著提升了半监督医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有半监督方法主要关注标签空间的一致性，忽略了表示空间的对齐，导致模型学习到的特征缺乏判别性和空间一致性。

Method: 设计了双路径协同框架BARL，结合双路径正则化（DPR）和渐进认知偏置校正（PCBC）实现标签空间对齐；通过区域级和病灶实例匹配实现表示空间对齐。

Result: 在四个公开基准和一个 proprietary CBCT 数据集上实验表明，BARL 均优于当前最先进的方法，消融实验验证了各组件的有效性。

Conclusion: 同时优化表示空间和标签空间的对齐能有效提升半监督医学图像分割性能，为未来研究提供了新方向。

Abstract: Semi-supervised medical image segmentation (SSMIS) seeks to match fully
supervised performance while sharply reducing annotation cost. Mainstream SSMIS
methods rely on \emph{label-space consistency}, yet they overlook the equally
critical \emph{representation-space alignment}. Without harmonizing latent
features, models struggle to learn representations that are both discriminative
and spatially coherent. To this end, we introduce \textbf{Bilateral Alignment
in Representation and Label spaces (BARL)}, a unified framework that couples
two collaborative branches and enforces alignment in both spaces. For
label-space alignment, inspired by co-training and multi-scale decoding, we
devise \textbf{Dual-Path Regularization (DPR)} and \textbf{Progressively
Cognitive Bias Correction (PCBC)} to impose fine-grained cross-branch
consistency while mitigating error accumulation from coarse to fine scales. For
representation-space alignment, we conduct region-level and lesion-instance
matching between branches, explicitly capturing the fragmented, complex
pathological patterns common in medical imagery. Extensive experiments on four
public benchmarks and a proprietary CBCT dataset demonstrate that BARL
consistently surpasses state-of-the-art SSMIS methods. Ablative studies further
validate the contribution of each component. Code will be released soon.

</details>


### [86] [Registration is a Powerful Rotation-Invariance Learner for 3D Anomaly Detection](https://arxiv.org/abs/2510.16865)
*Yuyang Yu,Zhengwei Chen,Xuemiao Xu,Lei Zhang,Haoxin Yang,Yongwei Nie,Shengfeng He*

Main category: cs.CV

TL;DR: 提出一种注册引导的旋转不变特征提取框架，通过将点云配准与基于记忆的异常检测结合，提升工业质量控制中3D异常检测的鲁棒性和判别能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于记忆库的方法在特征变换一致性、局部几何细节捕捉和旋转不变性方面存在不足，尤其在配准失败时检测结果不可靠，因此需要一种能同时优化配准与特征提取的框架。

Method: 提出注册诱导的旋转不变特征提取框架，将特征提取嵌入到配准学习过程中，联合优化点云对齐和表示学习，以建模局部几何结构并利用样本间特征相似性。

Result: 在Anomaly-ShapeNet和Real3D-AD数据集上的实验表明，该方法在检测效果和泛化能力上均优于现有方法。

Conclusion: 通过整合点云配准与异常检测，所提框架能有效提升特征的旋转不变性和局部判别力，显著增强3D点云异常检测的可靠性与性能。

Abstract: 3D anomaly detection in point-cloud data is critical for industrial quality
control, aiming to identify structural defects with high reliability. However,
current memory bank-based methods often suffer from inconsistent feature
transformations and limited discriminative capacity, particularly in capturing
local geometric details and achieving rotation invariance. These limitations
become more pronounced when registration fails, leading to unreliable detection
results. We argue that point-cloud registration plays an essential role not
only in aligning geometric structures but also in guiding feature extraction
toward rotation-invariant and locally discriminative representations. To this
end, we propose a registration-induced, rotation-invariant feature extraction
framework that integrates the objectives of point-cloud registration and
memory-based anomaly detection. Our key insight is that both tasks rely on
modeling local geometric structures and leveraging feature similarity across
samples. By embedding feature extraction into the registration learning
process, our framework jointly optimizes alignment and representation learning.
This integration enables the network to acquire features that are both robust
to rotations and highly effective for anomaly detection. Extensive experiments
on the Anomaly-ShapeNet and Real3D-AD datasets demonstrate that our method
consistently outperforms existing approaches in effectiveness and
generalizability.

</details>


### [87] [Uncovering Brain-Like Hierarchical Patterns in Vision-Language Models through fMRI-Based Neural Encoding](https://arxiv.org/abs/2510.16870)
*Yudan Ren,Xinlong Wang,Kexin Wang,Tian Xia,Zihan Ma,Zhaowei Li,Xiangrong Bi,Xiao Li,Xiaowei He*

Main category: cs.CV

TL;DR: 提出了一种新的神经元级分析框架，通过人类脑活动视角研究视觉-语言模型中的多模态信息处理机制，揭示了人工神经网络与大脑神经活动之间的深层相似性。


<details>
  <summary>Details</summary>
Motivation: 当前对人工神经网络（ANN）与人脑处理机制之间的相似性理解有限，尤其在多模态处理和神经元层级的分析上存在不足，因此需要从神经元层面探究ANN是否具备类似大脑的多模态处理机制。

Method: 结合细粒度的人工神经元（AN）分析与基于fMRI的体素编码技术，研究两种结构不同的视觉-语言模型（CLIP和METER），并将其人工神经元活动与生物神经元（BN）活动进行对比。

Result: 发现：（1）人工神经元能预测多个功能网络中生物神经元的活动；（2）两者均表现出功能冗余和重叠表征；（3）人工神经元呈现与生物神经元相似的极性模式；（4）不同架构导致不同的脑神经激活模式，CLIP具有模态特异性，METER则表现跨模态统一激活。

Conclusion: 视觉-语言模型在神经元层面展现出类脑的分层处理机制，模型架构显著影响其类脑特性，为设计更符合大脑处理机制的AI系统提供了依据。

Abstract: While brain-inspired artificial intelligence(AI) has demonstrated promising
results, current understanding of the parallels between artificial neural
networks (ANNs) and human brain processing remains limited: (1) unimodal ANN
studies fail to capture the brain's inherent multimodal processing
capabilities, and (2) multimodal ANN research primarily focuses on high-level
model outputs, neglecting the crucial role of individual neurons. To address
these limitations, we propose a novel neuron-level analysis framework that
investigates the multimodal information processing mechanisms in
vision-language models (VLMs) through the lens of human brain activity. Our
approach uniquely combines fine-grained artificial neuron (AN) analysis with
fMRI-based voxel encoding to examine two architecturally distinct VLMs: CLIP
and METER. Our analysis reveals four key findings: (1) ANs successfully predict
biological neurons (BNs) activities across multiple functional networks
(including language, vision, attention, and default mode), demonstrating shared
representational mechanisms; (2) Both ANs and BNs demonstrate functional
redundancy through overlapping neural representations, mirroring the brain's
fault-tolerant and collaborative information processing mechanisms; (3) ANs
exhibit polarity patterns that parallel the BNs, with oppositely activated BNs
showing mirrored activation trends across VLM layers, reflecting the complexity
and bidirectional nature of neural information processing; (4) The
architectures of CLIP and METER drive distinct BNs: CLIP's independent branches
show modality-specific specialization, whereas METER's cross-modal design
yields unified cross-modal activation, highlighting the architecture's
influence on ANN brain-like properties. These results provide compelling
evidence for brain-like hierarchical processing in VLMs at the neuronal level.

</details>


### [88] [Class-N-Diff: Classification-Induced Diffusion Model Can Make Fair Skin Cancer Diagnosis](https://arxiv.org/abs/2510.16887)
*Nusrat Munia,Abdullah Imran*

Main category: cs.CV

TL;DR: 提出了一种名为Class-N-Diff的分类引导扩散模型，可同时生成和分类皮肤镜图像，提升了生成图像的质量与分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统类别条件生成模型在生成特定医学类别图像时表现不佳，限制了其在皮肤癌诊断等医学应用中的实用性。

Method: 在扩散模型中集成分类器，利用类别条件引导图像生成过程，实现对生成图像类别的精确控制。

Result: 生成的皮肤镜图像更真实、更多样，且内置分类器在下游诊断任务中表现出更高的分类准确率。

Conclusion: Class-N-Diff通过分类与生成的联合建模，显著提升了医学图像合成的质量和临床应用潜力。

Abstract: Generative models, especially Diffusion Models, have demonstrated remarkable
capability in generating high-quality synthetic data, including medical images.
However, traditional class-conditioned generative models often struggle to
generate images that accurately represent specific medical categories, limiting
their usefulness for applications such as skin cancer diagnosis. To address
this problem, we propose a classification-induced diffusion model, namely,
Class-N-Diff, to simultaneously generate and classify dermoscopic images. Our
Class-N-Diff model integrates a classifier within a diffusion model to guide
image generation based on its class conditions. Thus, the model has better
control over class-conditioned image synthesis, resulting in more realistic and
diverse images. Additionally, the classifier demonstrates improved performance,
highlighting its effectiveness for downstream diagnostic tasks. This unique
integration in our Class-N-Diff makes it a robust tool for enhancing the
quality and utility of diffusion model-based synthetic dermoscopic image
generation. Our code is available at https://github.com/Munia03/Class-N-Diff.

</details>


### [89] [Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback](https://arxiv.org/abs/2510.16888)
*Zongjian Li,Zheyuan Liu,Qihui Zhang,Bin Lin,Shenghai Yuan,Zhiyuan Yan,Yang Ye,Wangbo Yu,Yuwei Niu,Li Yuan*

Main category: cs.CV

TL;DR: 提出Edit-R1框架，基于策略优化实现指令驱动的图像编辑，利用DiffusionNFT和MLLM作为无训练奖励模型，实现SOTA性能且模型无关。


<details>
  <summary>Details</summary>
Motivation: 现有指令图像编辑模型因监督微调易过拟合，泛化能力差；缺乏通用奖励模型限制了强化学习的应用。

Method: 提出Edit-R1框架：1）采用DiffusionNFT进行无似然的策略优化，兼容高阶采样器；2）利用多模态大语言模型（MLLM）作为无需训练的统一奖励模型；3）设计低方差分组过滤机制减少MLLM评分噪声。

Result: UniWorld-V2在ImgEdit和GEdit-Bench上分别达到4.49和7.83的SOTA得分，并在Qwen-Image-Edit和FLUX-Kontext等不同基模型上均显著提升性能。

Conclusion: Edit-R1是一种通用、模型无关的后训练优化框架，有效提升指令图像编辑的泛化能力与性能，具有广泛适用性。

Abstract: Instruction-based image editing has achieved remarkable progress; however,
models solely trained via supervised fine-tuning often overfit to annotated
patterns, hindering their ability to explore and generalize beyond training
distributions. To this end, we introduce Edit-R1, a novel post-training
framework for instruction-based image editing based on policy optimization.
Specifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a
likelihood-free policy optimization method consistent with the flow matching
forward process, thereby enabling the use of higher-order samplers and more
efficient training. Another key challenge here is the absence of a universal
reward model, resulting from the diverse nature of editing instructions and
tasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM)
as a unified, training-free reward model, leveraging its output logits to
provide fine-grained feedback. Furthermore, we carefully design a low-variance
group filtering mechanism to reduce MLLM scoring noise and stabilize
optimization. UniWorld-V2, trained with this framework, achieves
\textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks,
scoring 4.49 and 7.83, respectively. Crucially, our framework is
model-agnostic, delivering substantial performance gains when applied to
diverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its
wide applicability. Code and models are publicly available at
https://github.com/PKU-YuanGroup/UniWorld-V2.

</details>


### [90] [Contrail-to-Flight Attribution Using Ground Visible Cameras and Flight Surveillance Data](https://arxiv.org/abs/2510.16891)
*Ramon Dalmau,Gabriel Jarry,Philippe Very*

Main category: cs.CV

TL;DR: 本文提出了一种基于地面摄像头的模块化框架，用于将观测到的凝结尾迹与其源头航班进行关联，为航空业非二氧化碳气候影响的研究提供了强有力的基线和工具。


<details>
  <summary>Details</summary>
Motivation: 由于卫星数据在时空分辨率上的限制，难以准确追踪凝结尾迹的来源航班，而地面摄像头能在凝结尾迹形成初期以高分辨率捕捉其形态，因此需要一种更精确的凝结尾迹归因方法。

Method: 利用地面可见光摄像头凝结尾迹序列（GVCCS）数据集，结合飞机监视数据和气象数据生成理论凝结尾迹，通过多种几何表示、距离度量、时间平滑和概率分配策略，构建模块化归因框架。

Result: 成功开发了一个灵活且可扩展的框架，能够有效匹配地面观测到的凝结尾迹与潜在的航班来源，为模型验证和校准提供了新途径。

Conclusion: 该框架为基于地面观测的凝结尾迹归因提供了坚实基础，支持未来在航空气候影响评估中的进一步研究与应用。

Abstract: Aviation's non-CO2 effects, particularly contrails, are a significant
contributor to its climate impact. Persistent contrails can evolve into
cirrus-like clouds that trap outgoing infrared radiation, with radiative
forcing potentially comparable to or exceeding that of aviation's CO2
emissions. While physical models simulate contrail formation, evolution and
dissipation, validating and calibrating these models requires linking observed
contrails to the flights that generated them, a process known as
contrail-to-flight attribution. Satellite-based attribution is challenging due
to limited spatial and temporal resolution, as contrails often drift and deform
before detection. In this paper, we evaluate an alternative approach using
ground-based cameras, which capture contrails shortly after formation at high
spatial and temporal resolution, when they remain thin, linear, and visually
distinct. Leveraging the ground visible camera contrail sequences (GVCCS)
dataset, we introduce a modular framework for attributing contrails observed
using ground-based cameras to theoretical contrails derived from aircraft
surveillance and meteorological data. The framework accommodates multiple
geometric representations and distance metrics, incorporates temporal
smoothing, and enables flexible probability-based assignment strategies. This
work establishes a strong baseline and provides a modular framework for future
research in linking contrails to their source flight.

</details>


### [91] [Beyond RGB: Leveraging Vision Transformers for Thermal Weapon Segmentation](https://arxiv.org/abs/2510.16913)
*Akhila Kambhatla,Ahmed R Khaled*

Main category: cs.CV

TL;DR: 本研究评估了四种基于Transformer的架构在热成像武器分割中的性能，发现SegFormer在精度和速度之间实现了最佳平衡，显著优于传统CNN方法。


<details>
  <summary>Details</summary>
Motivation: 热成像武器分割在低光照和视觉遮挡条件下对监控和安全应用至关重要，但现有CNN模型在长距离依赖和细节捕捉方面存在局限，而Vision Transformers的潜力尚未在该领域充分探索。

Method: 采用并评估了四种Transformer架构（SegFormer、DeepLabV3+、SegNeXt、Swin Transformer）在包含9711张热成像图片的自建数据集上的二分类武器分割任务，使用MMSegmentation框架进行标准数据增强以实现公平比较。

Result: SegFormer-b5达到最高mIoU（94.15%）和像素准确率（97.04%），SegFormer-b0实现最快推理速度（98.32 FPS）且mIoU达90.84%；SegNeXt-mscans和DeepLabV3+ R101-D8也展现出良好的性能与速度权衡。

Conclusion: 基于Transformer的架构在热成像武器分割中表现优异，具备强泛化能力和灵活的精度-速度权衡，适用于多样化的实时安全应用。

Abstract: Thermal weapon segmentation is crucial for surveillance and security
applications, enabling robust detection under lowlight and visually obscured
conditions where RGB-based systems fail. While convolutional neural networks
(CNNs) dominate thermal segmentation literature, their ability to capture
long-range dependencies and fine structural details is limited. Vision
Transformers (ViTs), with their global context modeling capabilities, have
achieved state-of-the-art results in RGB segmentation tasks, yet their
potential in thermal weapon segmentation remains underexplored. This work
adapts and evaluates four transformer-based architectures SegFormer,
DeepLabV3\+, SegNeXt, and Swin Transformer for binary weapon segmentation on a
custom thermal dataset comprising 9,711 images collected from real world
surveillance videos and automatically annotated using SAM2. We employ standard
augmentation strategies within the MMSegmentation framework to ensure robust
model training and fair architectural comparison. Experimental results
demonstrate significant improvements in segmentation performance: SegFormer-b5
achieves the highest mIoU (94.15\%) and Pixel Accuracy (97.04\%), while
SegFormer-b0 provides the fastest inference speed (98.32 FPS) with competitive
mIoU (90.84\%). SegNeXt-mscans offers balanced performance with 85.12 FPS and
92.24\% mIoU, and DeepLabV3\+ R101-D8 reaches 92.76\% mIoU at 29.86 FPS. The
transformer architectures demonstrate robust generalization capabilities for
weapon detection in low-light and occluded thermal environments, with flexible
accuracy-speed trade-offs suitable for diverse real-time security applications.

</details>


### [92] [Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input](https://arxiv.org/abs/2510.16926)
*Chenxu Li,Zhicai Wang,Yuan Sheng,Xingyu Zhu,Yanbin Hao,Xiang Wang*

Main category: cs.CV

TL;DR: 本文提出了Res-Bench，一个用于评估多模态大语言模型在不同图像分辨率下性能稳定性的新基准，引入了多种鲁棒性度量指标，并对主流MLLM进行了大规模评估。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型评估主要关注语义性能，忽视了模型在不同输入分辨率下的鲁棒性问题，本文旨在填补这一空白。

Method: 构建了包含14,400个样本、12种分辨率和六个核心能力维度的Res-Bench基准，提出新的评估框架，引入Spearman相关系数、绝对/相对连续误差（ACE/RCE）等鲁棒性指标。

Result: 对主流MLLM的大规模评估揭示了模型在不同分辨率下的性能波动情况，分析了模型中心和任务中心的鲁棒性，以及填充和超分辨率等预处理策略的影响，并探索了通过微调提升稳定性的方法。

Conclusion: Res-Bench为评估MLLM的分辨率鲁棒性提供了有效工具，研究结果强调了提升模型在动态分辨率下稳定性的必要性，并为未来模型设计提供了指导。

Abstract: Multimodal Large Language Models (MLLMs) increasingly support dynamic image
resolutions. However, current evaluation paradigms primarily assess semantic
performance, overlooking the critical question of resolution robustness -
whether performance remains stable across varying input resolutions. To address
this gap, we introduce \textbf{Res-Bench}, a comprehensive benchmark comprising
14,400 samples across 12 resolution levels and six core capability dimensions.
We designed a novel evaluation framework that goes beyond traditional accuracy
metrics to capture performance stability. This framework introduces multiple
robustness metrics: Spearman's correlation for assessing resolution-performance
trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring
performance volatility. Using these metrics, we conducted a large-scale
evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and
task-centric robustness examination, (2) investigation of preprocessing
strategies including padding and super-resolution, and (3) exploration of
fine-tuning for stability enhancement.

</details>


### [93] [Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis](https://arxiv.org/abs/2510.16973)
*Praveenbalaji Rajendran,Mojtaba Safari,Wenfeng He,Mingzhe Hu,Shansong Wang,Jun Zhou,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 本文综述了基础模型（FMs）在医学图像分析中的最新进展，系统分类了视觉和视觉-语言FMs，并通过元分析揭示了数据集使用和应用领域的趋势，讨论了领域适应、高效微调等挑战及新兴解决方案，提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管FMs在医学影像中迅速发展，但领域碎片化，缺乏对架构、训练范式和临床应用的系统性总结，因此需要一篇综述来统一梳理该领域的发展脉络。

Method: 本文基于架构基础、训练策略和下游临床任务，将研究系统地分为仅视觉和视觉-语言FMs两类，并进行了定量的元分析以揭示时间趋势。

Result: 梳理了FMs在医学影像中的演进路径，揭示了数据集使用和应用领域的变化趋势，并总结了现有挑战与应对策略。

Conclusion: 未来的研究应聚焦于提升FMs的鲁棒性、可解释性及临床整合能力，以加速其在真实医疗场景中的转化应用。

Abstract: Recent advancements in artificial intelligence (AI), particularly foundation
models (FMs), have revolutionized medical image analysis, demonstrating strong
zero- and few-shot performance across diverse medical imaging tasks, from
segmentation to report generation. Unlike traditional task-specific AI models,
FMs leverage large corpora of labeled and unlabeled multimodal datasets to
learn generalized representations that can be adapted to various downstream
clinical applications with minimal fine-tuning. However, despite the rapid
proliferation of FM research in medical imaging, the field remains fragmented,
lacking a unified synthesis that systematically maps the evolution of
architectures, training paradigms, and clinical applications across modalities.
To address this gap, this review article provides a comprehensive and
structured analysis of FMs in medical image analysis. We systematically
categorize studies into vision-only and vision-language FMs based on their
architectural foundations, training strategies, and downstream clinical tasks.
Additionally, a quantitative meta-analysis of the studies was conducted to
characterize temporal trends in dataset utilization and application domains. We
also critically discuss persistent challenges, including domain adaptation,
efficient fine-tuning, computational constraints, and interpretability along
with emerging solutions such as federated learning, knowledge distillation, and
advanced prompting. Finally, we identify key future research directions aimed
at enhancing the robustness, explainability, and clinical integration of FMs,
thereby accelerating their translation into real-world medical practice.

</details>


### [94] [One-step Diffusion Models with Bregman Density Ratio Matching](https://arxiv.org/abs/2510.16983)
*Yuanzhi Zhu,Eleftherios Tsonis,Lucas Degeorge,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: 提出Di-Bregman框架，将扩散蒸馏统一建模为基于Bregman散度的密度比匹配，理论清晰且在单步生成中提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散蒸馏方法缺乏统一理论基础，限制了性能优化和理解。

Method: 提出Di-Bregman框架，基于Bregman散度进行密度比匹配，统一解释多种现有蒸馏目标。

Result: 在CIFAR-10和文本到图像生成任务上，优于reverse-KL蒸馏的单步FID，并保持高视觉保真度。

Conclusion: Bregman密度比匹配为高效单步扩散生成提供了理论可靠且实用的路径。

Abstract: Diffusion and flow models achieve high generative quality but remain
computationally expensive due to slow multi-step sampling. Distillation methods
accelerate them by training fast student generators, yet most existing
objectives lack a unified theoretical foundation. In this work, we propose
Di-Bregman, a compact framework that formulates diffusion distillation as
Bregman divergence-based density-ratio matching. This convex-analytic view
connects several existing objectives through a common lens. Experiments on
CIFAR-10 and text-to-image generation demonstrate that Di-Bregman achieves
improved one-step FID over reverse-KL distillation and maintains high visual
fidelity compared to the teacher model. Our results highlight Bregman
density-ratio matching as a practical and theoretically-grounded route toward
efficient one-step diffusion generation.

</details>


### [95] [CARE: Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams](https://arxiv.org/abs/2510.16988)
*Junhao Zhao,Zishuai Liu,Ruili Fang,Jin Lu,Linghan Zhang,Fei Dou*

Main category: cs.CV

TL;DR: 提出CARE框架，通过序列-图像对比对齐实现噪声鲁棒且空间感知的日常活动识别，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于序列或图像的日常活动识别方法在时序建模、空间感知或抗噪能力上存在局限，且简单融合未能有效对齐多模态表征。

Method: 提出CARE框架，结合时序感知的序列编码和空间感知的图像表示，通过序列-图像对比对齐（SICA）和分类损失联合优化，实现端到端学习。

Result: 在三个CASAS数据集上达到最优性能（Milan 89.8%，Cairo 88.9%，Kyoto7 73.3%），并表现出对传感器故障和布局变化的鲁棒性。

Conclusion: CARE通过跨模态对齐和联合学习，有效融合序列与图像表征优势，提升了事件触发传感器下ADL识别的准确性与可靠性。

Abstract: The recognition of Activities of Daily Living (ADLs) from event-triggered
ambient sensors is an essential task in Ambient Assisted Living, yet existing
methods remain constrained by representation-level limitations. Sequence-based
approaches preserve temporal order of sensor activations but are sensitive to
noise and lack spatial awareness, while image-based approaches capture global
patterns and implicit spatial correlations but compress fine-grained temporal
dynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation)
fail to enforce alignment between sequence- and image-based representation
views, underutilizing their complementary strengths. We propose Contrastive
Alignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), an
end-to-end framework that jointly optimizes representation learning via
Sequence-Image Contrastive Alignment (SICA) and classification via
cross-entropy, ensuring both cross-representation alignment and task-specific
discriminability. CARE integrates (i) time-aware, noise-resilient sequence
encoding with (ii) spatially-informed and frequency-sensitive image
representations, and employs (iii) a joint contrastive-classification objective
for end-to-end learning of aligned and discriminative embeddings. Evaluated on
three CASAS datasets, CARE achieves state-of-the-art performance (89.8% on
Milan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness to
sensor malfunctions and layout variability, highlighting its potential for
reliable ADL recognition in smart homes.

</details>


### [96] [Training-free Online Video Step Grounding](https://arxiv.org/abs/2510.16989)
*Luca Zanella,Massimiliano Mancini,Yiming Wang,Alessio Tonioni,Elisa Ricci*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练且在线的视频步骤定位方法BaGLM，利用大型多模态模型（LMMs）的零样本能力，并结合贝叶斯滤波机制融合历史帧信息，在多个数据集上超越了现有的基于训练的离线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视频步骤定位方法依赖标注数据进行训练且需离线处理整个视频，成本高且难以应用于需要实时决策的场景。因此，本文旨在探索无需训练、可在线处理的视频步骤定位方法。

Method: 利用大型多模态模型（LMMs）在零样本设置下对局部帧进行步骤预测，并提出BaGLM框架，引入贝叶斯滤波机制，通过大语言模型提取的依赖矩阵和步骤进度估计来建模步骤间的转移关系，实现对历史信息的有效利用。

Result: 实验表明，所提的在线无训练策略已优于依赖训练的离线方法，BaGLM在三个数据集上均取得了优于当前最先进离线方法的性能。

Conclusion: 无需训练且基于LMM的在线视频步骤定位是可行且高效的，结合贝叶斯滤波能进一步提升性能，为实际应用提供了新方向。

Abstract: Given a task and a set of steps composing it, Video Step Grounding (VSG) aims
to detect which steps are performed in a video. Standard approaches for this
task require a labeled training set (e.g., with step-level annotations or
narrations), which may be costly to collect. Moreover, they process the full
video offline, limiting their applications for scenarios requiring online
decisions. Thus, in this work, we explore how to perform VSG online and without
training. We achieve this by exploiting the zero-shot capabilities of recent
Large Multimodal Models (LMMs). In particular, we use LMMs to predict the step
associated with a restricted set of frames, without access to the whole video.
We show that this online strategy without task-specific tuning outperforms
offline and training-based models. Motivated by this finding, we develop
Bayesian Grounding with Large Multimodal Models (BaGLM), further injecting
knowledge of past frames into the LMM-based predictions. BaGLM exploits
Bayesian filtering principles, modeling step transitions via (i) a dependency
matrix extracted through large language models and (ii) an estimation of step
progress. Experiments on three datasets show superior performance of BaGLM over
state-of-the-art training-based offline methods.

</details>


### [97] [An empirical study of the effect of video encoders on Temporal Video Grounding](https://arxiv.org/abs/2510.17007)
*Ignacio M. De la Jara,Cristian Rodriguez-Opazo,Edison Marrese-Taylor,Felipe Bravo-Marquez*

Main category: cs.CV

TL;DR: 本文提出了一项实证研究，探讨不同视频特征对经典时间视频定位模型性能的影响，发现不同编码器显著影响模型表现，并揭示了特征互补的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中在少数视频表示方法上，可能导致架构过拟合，因此需要系统评估不同视频特征的影响。

Method: 在Charades-STA、ActivityNet-Captions和YouCookII三个基准上，使用基于CNN、时序推理和Transformer的视频编码器提取特征，并在经典架构上进行评估。

Result: 实验结果表明，仅更换视频编码器就可显著改变模型性能，并发现了特定特征带来的规律性误差和互补性。

Conclusion: 不同视频特征对时间视频定位任务有显著影响，未来工作应探索多特征融合以提升性能。

Abstract: Temporal video grounding is a fundamental task in computer vision, aiming to
localize a natural language query in a long, untrimmed video. It has a key role
in the scientific community, in part due to the large amount of video generated
every day. Although we find extensive work in this task, we note that research
remains focused on a small selection of video representations, which may lead
to architectural overfitting in the long run. To address this issue, we propose
an empirical study to investigate the impact of different video features on a
classical architecture. We extract features for three well-known benchmarks,
Charades-STA, ActivityNet-Captions and YouCookII, using video encoders based on
CNNs, temporal reasoning and transformers. Our results show significant
differences in the performance of our model by simply changing the video
encoder, while also revealing clear patterns and errors derived from the use of
certain features, ultimately indicating potential feature complementarity.

</details>


### [98] [Do Satellite Tasks Need Special Pretraining?](https://arxiv.org/abs/2510.17014)
*Ani Vanyan,Alvard Barseghyan,Hakob Tamazyan,Tigran Galstyan,Vahan Huroyan,Naira Hovakimyan,Hrant Khachatrian*

Main category: cs.CV

TL;DR: 本文研究了针对遥感图像的专用基础模型是否优于通用视觉基础模型，发现在ViT-B规模下，专用模型并未持续优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 遥感图像具有独特特性，专用基础模型被提出以提升性能，但其相对于通用模型的优势尚不明确。

Method: 设计了一个衡量遥感模型在低分辨率图像下泛化能力的基准，并在MillionAID数据集上训练了针对遥感改进的iBOT模型。

Result: 实验表明，在ViT-B规模下，这些专用预训练模型并未在下游任务中持续优于通用视觉模型。

Conclusion: 当前在小规模设置下，专用遥感基础模型未必优于通用视觉基础模型，对专用模型的必要性提出了质疑。

Abstract: Foundation models have advanced machine learning across various modalities,
including images. Recently multiple teams trained foundation models specialized
for remote sensing applications. This line of research is motivated by the
distinct characteristics of remote sensing imagery, specific applications and
types of robustness useful for satellite image analysis. In this work we
systematically challenge the idea that specific foundation models are more
useful than general-purpose vision foundation models, at least in the small
scale. First, we design a simple benchmark that measures generalization of
remote sensing models towards images with lower resolution for two downstream
tasks. Second, we train iBOT, a self-supervised vision encoder, on MillionAID,
an ImageNet-scale satellite imagery dataset, with several modifications
specific to remote sensing. We show that none of those pretrained models bring
consistent improvements upon general-purpose baselines at the ViT-B scale.

</details>


### [99] [Enrich and Detect: Video Temporal Grounding with Multimodal LLMs](https://arxiv.org/abs/2510.17023)
*Shraman Pramanick,Effrosyni Mavroudi,Yale Song,Rama Chellappa,Lorenzo Torresani,Triantafyllos Afouras*

Main category: cs.CV

TL;DR: ED-VTG是一种利用多模态大语言模型进行细粒度视频时序定位的方法，通过两阶段过程实现自然语言查询在视频中的精准定位。


<details>
  <summary>Details</summary>
Motivation: 现有的视频时序定位方法在处理细粒度查询时存在细节缺失和幻觉问题，难以有效利用多模态大语言模型的潜力。

Method: 提出ED-VTG方法，第一阶段将语言查询转换为包含更多细节和线索的增强句子；第二阶段使用轻量级解码器，基于增强查询的上下文化表示预测精确的时间边界，并采用多实例学习目标动态选择最优查询版本以减少噪声和幻觉影响。

Result: 在多种时序定位和段落定位基准上实现了最先进的性能，显著优于所有现有的基于LLM的方法，并在零样本评估场景中表现出明显优势。

Conclusion: ED-VTG有效结合了多模态大语言模型的能力和专门设计的两阶段架构，在视频时序定位任务中表现出卓越性能，尤其在零样本场景下具有强泛化能力。

Abstract: We introduce ED-VTG, a method for fine-grained video temporal grounding
utilizing multi-modal large language models. Our approach harnesses the
capabilities of multimodal LLMs to jointly process text and video, in order to
effectively localize natural language queries in videos through a two-stage
process. Rather than being directly grounded, language queries are initially
transformed into enriched sentences that incorporate missing details and cues
to aid in grounding. In the second stage, these enriched queries are grounded,
using a lightweight decoder, which specializes at predicting accurate
boundaries conditioned on contextualized representations of the enriched
queries. To mitigate noise and reduce the impact of hallucinations, our model
is trained with a multiple-instance-learning objective that dynamically selects
the optimal version of the query for each training sample. We demonstrate
state-of-the-art results across various benchmarks in temporal video grounding
and paragraph grounding settings. Experiments reveal that our method
significantly outperforms all previously proposed LLM-based temporal grounding
approaches and is either superior or comparable to specialized models, while
maintaining a clear advantage against them in zero-shot evaluation scenarios.

</details>


### [100] [Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding](https://arxiv.org/abs/2510.17034)
*Yutong Zhong*

Main category: cs.CV

TL;DR: 提出了一种名为What-Where Representation Re-Forming（W2R2）的新训练框架，通过解耦表示学习和抑制2D语义捷径，提升视觉语言模型中的3D多模态接地性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在多模态3D接地任务中过度依赖2D图像特征，导致对3D几何信息利用不足，出现“2D语义偏见”，影响定位精度和模态融合效果。

Method: 提出W2R2框架，将2D特征用作“What”语义识别的指引，3D特征用作“Where”空间定位的锚点；采用解耦表示学习，并设计双目标损失函数：包含用于多模态对齐的对齐损失和通过边缘机制惩罚2D主导伪输出的伪标签损失，以抑制捷径学习。

Result: 在ScanRefer和ScanQA数据集上的实验表明，W2R2显著提升了3D接地的定位精度和鲁棒性，尤其在杂乱的室外场景中表现突出。

Conclusion: W2R2通过重构模型内部表示空间，在不改变推理结构的前提下有效缓解了2D语义偏见，实现了更优的多模态融合与3D空间定位。

Abstract: Multimodal 3D grounding has garnered considerable interest in Vision-Language
Models (VLMs) \cite{yin2025spatial} for advancing spatial reasoning in complex
environments. However, these models suffer from a severe "2D semantic bias"
that arises from over-reliance on 2D image features for coarse localization,
largely disregarding 3D geometric inputs and resulting in suboptimal fusion
performance. In this paper, we propose a novel training framework called
What-Where Representation Re-Forming (W2R2) to tackle this issue via
disentangled representation learning and targeted shortcut suppression. Our
approach fundamentally reshapes the model's internal space by designating 2D
features as semantic beacons for "What" identification and 3D features as
spatial anchors for "Where" localization, enabling precise 3D grounding without
modifying inference architecture. Key components include a dual-objective loss
function with an Alignment Loss that supervises fused predictions using adapted
cross-entropy for multimodal synergy, and a Pseudo-Label Loss that penalizes
overly effective 2D-dominant pseudo-outputs via a margin-based mechanism.
Experiments conducted on ScanRefer and ScanQA demonstrate the effectiveness of
W2R2, with significant gains in localization accuracy and robustness,
particularly in cluttered outdoor scenes.

</details>


### [101] [Conditional Synthetic Live and Spoof Fingerprint Generation](https://arxiv.org/abs/2510.17035)
*Syed Konain Abbas,Sandip Purnapatra,M. G. Sarwar Murshed,Conor Miller-Lynch,Lambert Igene,Soumyabrata Dey,Stephanie Schuckers,Faraz Hussain*

Main category: cs.CV

TL;DR: 提出一种基于条件StyleGAN和CycleGAN的合成指纹图象生成方法，生成高质量、隐私保护的活体与伪造指纹数据集，用于解决真实指纹数据获取成本高和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 真实指纹数据收集耗时、昂贵且涉及隐私风险，限制了生物特征识别系统的研究与发展；需要安全、低成本且可扩展的数据来源。

Method: 采用条件StyleGAN2-ADA和StyleGAN3生成高分辨率、按手指身份（从拇指到小指）条件控制的合成活体指纹图像，并利用CycleGAN将活体指纹转换为模拟多种材质（如EcoFlex、Play-Doh）的逼真伪造指纹。

Result: 构建了两个包含1500枚指纹图像的数据集（DB2和DB3），每枚指纹含多个按压样本及八种材质的对应伪造图像；StyleGAN3的FID低至5，TAR达99.47%（FAR=0.01%），StyleGAN2-ADA达98.67%；NFIQ2和MINDTCT评估显示图像质量高，匹配实验无显著身份泄露。

Conclusion: 所提方法能高效生成高质量、多样化的活体与伪造指纹图像，兼具良好隐私保护能力，适用于训练和评估指纹识别与防伪系统。

Abstract: Large fingerprint datasets, while important for training and evaluation, are
time-consuming and expensive to collect and require strict privacy measures.
Researchers are exploring the use of synthetic fingerprint data to address
these issues. This paper presents a novel approach for generating synthetic
fingerprint images (both spoof and live), addressing concerns related to
privacy, cost, and accessibility in biometric data collection. Our approach
utilizes conditional StyleGAN2-ADA and StyleGAN3 architectures to produce
high-resolution synthetic live fingerprints, conditioned on specific finger
identities (thumb through little finger). Additionally, we employ CycleGANs to
translate these into realistic spoof fingerprints, simulating a variety of
presentation attack materials (e.g., EcoFlex, Play-Doh). These synthetic spoof
fingerprints are crucial for developing robust spoof detection systems. Through
these generative models, we created two synthetic datasets (DB2 and DB3), each
containing 1,500 fingerprint images of all ten fingers with multiple
impressions per finger, and including corresponding spoofs in eight material
types. The results indicate robust performance: our StyleGAN3 model achieves a
Fr\'echet Inception Distance (FID) as low as 5, and the generated fingerprints
achieve a True Accept Rate of 99.47% at a 0.01% False Accept Rate. The
StyleGAN2-ADA model achieved a TAR of 98.67% at the same 0.01% FAR. We assess
fingerprint quality using standard metrics (NFIQ2, MINDTCT), and notably,
matching experiments confirm strong privacy preservation, with no significant
evidence of identity leakage, confirming the strong privacy-preserving
properties of our synthetic datasets.

</details>


### [102] [Click, Predict, Trust: Clinician-in-the-Loop AI Segmentation for Lung Cancer CT-Based Prognosis within the Knowledge-to-Action Framework](https://arxiv.org/abs/2510.17039)
*Mohammad R. Salmanpour,Sonya Falahati,Amir Hossein Pouria,Amin Mousavi,Somayeh Sadat Mehrnia,Morteza Alizadeh,Arman Gorji,Zeinab Farsangi,Alireza Safarian,Mehdi Maghsudi,Carlos Uribe,Arman Rahmim,Ren Yuan*

Main category: cs.CV

TL;DR: 本研究开发了一种以临床医生参与为核心的深度学习（DL）流程，结合VNet模型与半监督学习（SSL），在多中心CT数据上实现了高精度、可重复且临床可信的肺部肿瘤分割与预后预测。


<details>
  <summary>Details</summary>
Motivation: 肺癌是癌症死亡的主因，CT影像分析至关重要，但手动分割费时且变异大，现有深度学习方法临床采纳率低。因此需要一种兼具准确性、可重复性并能获得临床信任的自动化方案。

Method: 基于“知识到行动”框架，构建了医生参与的DL流程；使用来自12个公开数据集的999例患者CT数据，比较五种3D模型（3D Attention U-Net、ResUNet、VNet、ReconNet、SAM-Med3D）在全图与点击裁剪图像上的表现；通过497个放射组学特征评估分割可重复性（Spearman、ICC、Wilcoxon、MANOVA）；比较38种降维方法和24种分类器在监督与半监督学习下的预后性能；六名医师从七个维度对分割结果进行定性评估。

Result: VNet表现最优（Dice=0.83，IoU=0.71），放射组学稳定性高（平均相关性=0.76，ICC=0.65），半监督学习下预后准确率最佳（准确率=0.88，F1=0.83）；SSL整体优于SL；放射科医生更偏好VNet在瘤周结构和边界平滑性上的表现，并倾向用AI初筛结果辅助而非替代人工修正。

Conclusion: 结合VNet与半监督学习的医生参与式DL流程能实现准确、可重复且临床可信的肺部肿瘤分析，为面向临床实践的人工智能转化提供了可行路径。

Abstract: Lung cancer remains the leading cause of cancer mortality, with CT imaging
central to screening, prognosis, and treatment. Manual segmentation is variable
and time-intensive, while deep learning (DL) offers automation but faces
barriers to clinical adoption. Guided by the Knowledge-to-Action framework,
this study develops a clinician-in-the-loop DL pipeline to enhance
reproducibility, prognostic accuracy, and clinical trust. Multi-center CT data
from 999 patients across 12 public datasets were analyzed using five DL models
(3D Attention U-Net, ResUNet, VNet, ReconNet, SAM-Med3D), benchmarked against
expert contours on whole and click-point cropped images. Segmentation
reproducibility was assessed using 497 PySERA-extracted radiomic features via
Spearman correlation, ICC, Wilcoxon tests, and MANOVA, while prognostic
modeling compared supervised (SL) and semi-supervised learning (SSL) across 38
dimensionality reduction strategies and 24 classifiers. Six physicians
qualitatively evaluated masks across seven domains, including clinical
meaningfulness, boundary quality, prognostic value, trust, and workflow
integration. VNet achieved the best performance (Dice = 0.83, IoU = 0.71),
radiomic stability (mean correlation = 0.76, ICC = 0.65), and predictive
accuracy under SSL (accuracy = 0.88, F1 = 0.83). SSL consistently outperformed
SL across models. Radiologists favored VNet for peritumoral representation and
smoother boundaries, preferring AI-generated initial masks for refinement
rather than replacement. These results demonstrate that integrating VNet with
SSL yields accurate, reproducible, and clinically trusted CT-based lung cancer
prognosis, highlighting a feasible path toward physician-centered AI
translation.

</details>


### [103] [Person Re-Identification via Generalized Class Prototypes](https://arxiv.org/abs/2510.17043)
*Md Ahmed Al Muzaddid,William J. Beksi*

Main category: cs.CV

TL;DR: 提出一种广义的类代表选择方法，超越传统使用类中心的方法，提升行人重识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究在目标函数和特征提取上已有较多进展，但对如何选择更优的类代表（如非类中心）研究较少，尤其是在检索阶段。

Method: 提出一种广义的类代表选择方法，不限于类中心，可灵活调整每类的代表数量，并结合多种重识别嵌入方法进行优化。

Result: 该方法在多个基准上显著优于当前先进方法，在准确率和平均精度之间取得更好平衡。

Conclusion: 选择合适的类代表对重识别性能至关重要，所提方法通过灵活选择代表，有效提升了检索效果。

Abstract: Advanced feature extraction methods have significantly contributed to
enhancing the task of person re-identification. In addition, modifications to
objective functions have been developed to further improve performance.
Nonetheless, selecting better class representatives is an underexplored area of
research that can also lead to advancements in re-identification performance.
Although past works have experimented with using the centroid of a gallery
image class during training, only a few have investigated alternative
representations during the retrieval stage. In this paper, we demonstrate that
these prior techniques yield suboptimal results in terms of re-identification
metrics. To address the re-identification problem, we propose a generalized
selection method that involves choosing representations that are not limited to
class centroids. Our approach strikes a balance between accuracy and mean
average precision, leading to improvements beyond the state of the art. For
example, the actual number of representations per class can be adjusted to meet
specific application requirements. We apply our methodology on top of multiple
re-identification embeddings, and in all cases it substantially improves upon
contemporary results

</details>


### [104] [Video Reasoning without Training](https://arxiv.org/abs/2510.17045)
*Deepak Sridhar,Kartikeya Bhardwaj,Jeya Pradha Jeyaraj,Nuno Vasconcelos,Ankita Nayak,Harris Teague*

Main category: cs.CV

TL;DR: 本文提出V-Reason，一种无需强化学习或监督微调的视频推理优化方法，通过控制输出熵在推理时调整大模型行为，实现高效且准确的视频理解。


<details>
  <summary>Details</summary>
Motivation: 现有的视频推理方法依赖成本高昂的强化学习和冗长的思维链，计算开销大，且对推理过程的控制机制有限，亟需更高效、可控的推理优化方案。

Method: 利用模型输出熵作为信号，观察到高质量模型在推理中存在微探索与微利用阶段，并据此设计V-Reason方法：在推理时通过一个小的可训练控制器，基于熵目标对LMM的值缓存进行少量优化，调节其探索与利用行为。

Result: V-Reason在多个视频推理数据集上显著优于基础指令调优模型，平均准确率接近强化学习模型仅差0.6%，同时推理输出令牌减少58.6%，大幅提升效率。

Conclusion: 通过熵可控的推理机制，V-Reason实现了无需训练的高效视频推理，在保持高准确性的同时大幅降低计算开销，为大模型推理提供了新思路。

Abstract: Video reasoning using Large Multimodal Models (LMMs) relies on costly
reinforcement learning (RL) and verbose chain-of-thought, resulting in
substantial computational overhead during both training and inference.
Moreover, the mechanisms that control the thinking process in these reasoning
models are very limited. In this paper, using entropy of the model's output as
a signal, we discover that the high-quality models go through a series of
micro-explorations and micro-exploitations which keep the reasoning process
grounded (i.e., avoid excessive randomness while the model is exploring or
thinking through an answer). We further observe that once this "thinking"
process is over, more accurate models demonstrate a better convergence by
reducing the entropy significantly via a final exploitation phase (i.e., a more
certain convergence towards a solution trajectory). We then use these novel,
theoretically-grounded insights to tune the model's behavior directly at
inference, without using any RL or supervised fine-tuning. Specifically, during
inference, our proposed approach called V-Reason (Video-Reason) adapts the
value cache of the LMM via a few optimization steps on a small, trainable
controller using an entropy-based objective, i.e., no supervision from any
dataset or RL is necessary. This tuning improves the model's micro-exploration
and exploitation behavior during inference. Our experiments show that our
proposed method achieves significant improvements over the base
instruction-tuned models across several video reasoning datasets, narrowing the
gap with RL-trained models to within 0.6% average accuracy without any
training, while offering massive efficiency benefits: output tokens are reduced
by 58.6% compared to the RL model.

</details>


### [105] [How Universal Are SAM2 Features?](https://arxiv.org/abs/2510.17051)
*Masoud Khairi Atani,Alon Harell,Hyomin Choi,Runyu Yang,Fabien Racape,Ivan V. Bajic*

Main category: cs.CV

TL;DR: 该研究比较了通用视觉模型Hiera与专用模型SAM2的特征通用性，发现SAM2在特定任务上表现优异但牺牲了跨任务的通用性，揭示了专业化带来的表征瓶颈。


<details>
  <summary>Details</summary>
Motivation: 探讨通用视觉基础模型与专用模型之间的权衡关系，理解特征编码设计中的通用性与专业化代价。

Method: 使用轻量级可训练的neck模块探测冻结特征的适应性，并通过信息论方法量化专业化的代价，同时提出跨neck分析方法。

Result: SAM2在深度估计等空间相关任务上表现好，但在姿态估计和图像描述等概念相距较远的任务上弱于Hiera，显示出更窄的语义覆盖；每一级适配都会加剧表征瓶颈。

Conclusion: 模型专业化虽提升特定任务性能，却牺牲了特征的通用性，研究为下游应用的高效特征编码与适配策略提供了量化依据。

Abstract: The trade-off between general-purpose foundation vision models and their
specialized counterparts is critical for efficient feature coding design and is
not yet fully understood. We investigate this trade-off by comparing the
feature versatility of the general-purpose Hiera encoder against the
segmentation-specialized Segment Anything Model 2 (SAM2). Using a lightweight,
trainable neck to probe the adaptability of their frozen features, we quantify
the information-theoretic cost of specialization. Our results reveal that while
SAM2's specialization is highly effective for spatially-related tasks like
depth estimation, it comes at a cost. The specialized SAM2 encoder
underperforms its generalist predecessor, Hiera, on conceptually distant tasks
such as pose estimation and image captioning, demonstrating a measurable loss
of broader semantic information. A novel cross-neck analysis on SAM2 reveals
that each level of adaptation creates a further representational bottleneck.
Our analysis illuminates these trade-offs in feature universality, providing a
quantitative foundation for designing efficient feature coding and adaptation
strategies for diverse downstream applications.

</details>


### [106] [ProDAT: Progressive Density-Aware Tail-Drop for Point Cloud Coding](https://arxiv.org/abs/2510.17068)
*Zhe Luo,Wenjing Jia,Stuart Perry*

Main category: cs.CV

TL;DR: 提出ProDAT，一种密度感知的尾部截断机制，实现单个模型下的点云渐进解码，显著提升编码效率。


<details>
  <summary>Details</summary>
Motivation: 现有的学习型点云编码方法缺乏对渐进解码的支持，难以满足资源受限环境下的实时低延迟需求。

Method: 提出ProDAT，利用密度信息作为引导信号，自适应地解码潜在特征和坐标，实现基于单一模型的多码率渐进解码。

Result: 在SemanticKITTI上PSNR-D2的BD-rate提升超过28.6%，ShapeNet上超过18.15%，优于现有最先进方法。

Conclusion: ProDAT有效实现了高效的渐进式点云编码，在保持高质量重建的同时支持多级解码，适用于带宽受限场景。

Abstract: Three-dimensional (3D) point clouds are becoming increasingly vital in
applications such as autonomous driving, augmented reality, and immersive
communication, demanding real-time processing and low latency. However, their
large data volumes and bandwidth constraints hinder the deployment of
high-quality services in resource-limited environments. Progres- sive coding,
which allows for decoding at varying levels of detail, provides an alternative
by allowing initial partial decoding with subsequent refinement. Although
recent learning-based point cloud geometry coding methods have achieved notable
success, their fixed latent representation does not support progressive
decoding. To bridge this gap, we propose ProDAT, a novel density-aware
tail-drop mechanism for progressive point cloud coding. By leveraging density
information as a guidance signal, latent features and coordinates are decoded
adaptively based on their significance, therefore achieving progressive
decoding at multiple bitrates using one single model. Experimental results on
benchmark datasets show that the proposed ProDAT not only enables progressive
coding but also achieves superior coding efficiency compared to
state-of-the-art learning-based coding techniques, with over 28.6% BD-rate
improvement for PSNR- D2 on SemanticKITTI and over 18.15% for ShapeNet

</details>


### [107] [Towards a Generalizable Fusion Architecture for Multimodal Object Detection](https://arxiv.org/abs/2510.17078)
*Jad Berjawi,Yoann Dupas,Christophe C'erin*

Main category: cs.CV

TL;DR: 提出了一种名为FMCAF的多模态融合方法，通过频率域滤波和交叉注意力机制提升RGB和红外图像的目标检测性能，具有良好的跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为了提高多模态目标检测在复杂条件下的鲁棒性，并解决现有融合方法泛化能力差、依赖数据集特定调参的问题。

Method: 提出Filtered Multi-Modal Cross Attention Fusion (FMCAF)，包含频率域滤波模块(Freq-Filter)去除冗余频谱特征，以及基于交叉注意力的融合模块(MCAF)增强模态间特征共享，无需针对特定数据集调整。

Result: 在LLVIP和VEDAI数据集上超越传统融合方法，VEDAI上mAP@50提升13.9%，LLVIP上提升1.1%。

Conclusion: FMCAF具有良好的通用性和性能提升，有望成为未来多模态检测系统中鲁棒融合的基础模块。

Abstract: Multimodal object detection improves robustness in chal- lenging conditions
by leveraging complementary cues from multiple sensor modalities. We introduce
Filtered Multi- Modal Cross Attention Fusion (FMCAF), a preprocess- ing
architecture designed to enhance the fusion of RGB and infrared (IR) inputs.
FMCAF combines a frequency- domain filtering block (Freq-Filter) to suppress
redun- dant spectral features with a cross-attention-based fusion module (MCAF)
to improve intermodal feature sharing. Unlike approaches tailored to specific
datasets, FMCAF aims for generalizability, improving performance across
different multimodal challenges without requiring dataset- specific tuning. On
LLVIP (low-light pedestrian detec- tion) and VEDAI (aerial vehicle detection),
FMCAF outper- forms traditional fusion (concatenation), achieving +13.9% mAP@50
on VEDAI and +1.1% on LLVIP. These results support the potential of FMCAF as a
flexible foundation for robust multimodal fusion in future detection pipelines.

</details>


### [108] [GSPlane: Concise and Accurate Planar Reconstruction via Structured Representation](https://arxiv.org/abs/2510.17095)
*Ruitong Gan,Junran Peng,Yang Liu,Chuanchen Luo,Qing Li,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: 本文提出GSPlane，通过引入平面先验信息提升高斯点阵在平面区域的几何重建精度，同时保持渲染质量，并生成结构化网格用于场景编辑和物理仿真。


<details>
  <summary>Details</summary>
Motivation: 现有高斯点阵方法在重建平面区域时常存在平滑性和精度不足的问题，限制了其在需要精确几何结构的应用（如场景编辑和物理仿真）中的表现。

Method: 利用现成的分割和法向预测模型提取平面先验，构建结构化的平面高斯表示；引入动态高斯重分类器以提升训练鲁棒性，并利用优化后的平面先验精修网格结构。

Result: 在不牺牲渲染质量的前提下，显著提升了多种基线方法中提取网格的几何准确性，优化了拓扑结构，减少了顶点和面数。

Conclusion: GSPlane有效增强了平面区域的重建质量，生成结构清晰的网格，支持平面物体的解耦与灵活操控，拓展了高斯点阵在结构化场景重建中的应用。

Abstract: Planes are fundamental primitives of 3D sences, especially in man-made
environments such as indoor spaces and urban streets. Representing these planes
in a structured and parameterized format facilitates scene editing and physical
simulations in downstream applications. Recently, Gaussian Splatting (GS) has
demonstrated remarkable effectiveness in the Novel View Synthesis task, with
extensions showing great potential in accurate surface reconstruction. However,
even state-of-the-art GS representations often struggle to reconstruct planar
regions with sufficient smoothness and precision. To address this issue, we
propose GSPlane, which recovers accurate geometry and produces clean and
well-structured mesh connectivity for plane regions in the reconstructed scene.
By leveraging off-the-shelf segmentation and normal prediction models, GSPlane
extracts robust planar priors to establish structured representations for
planar Gaussian coordinates, which help guide the training process by enforcing
geometric consistency. To further enhance training robustness, a Dynamic
Gaussian Re-classifier is introduced to adaptively reclassify planar Gaussians
with persistently high gradients as non-planar, ensuring more reliable
optimization. Furthermore, we utilize the optimized planar priors to refine the
mesh layouts, significantly improving topological structure while reducing the
number of vertices and faces. We also explore applications of the structured
planar representation, which enable decoupling and flexible manipulation of
objects on supportive planes. Extensive experiments demonstrate that, with no
sacrifice in rendering quality, the introduction of planar priors significantly
improves the geometric accuracy of the extracted meshes across various
baselines.

</details>


### [109] [Boosting Fidelity for Pre-Trained-Diffusion-Based Low-Light Image Enhancement via Condition Refinement](https://arxiv.org/abs/2510.17105)
*Xiaogang Xu,Jian Wang,Yunfan Lu,Ruihang Chu,Ruixing Wang,Jiafei Wu,Bei Yu,Liang Lin*

Main category: cs.CV

TL;DR: 提出一种新的优化策略，通过潜变量 refinement 和动态交互机制，提升基于预训练扩散模型在低光照图像增强中的内容保真度。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练扩散模型（PTDB）的方法在追求感知真实感时牺牲了内容保真度，尤其在低光条件下表现更差，主要受限于条件潜变量建模不足及条件与噪声潜变量间缺乏双向交互。

Method: 提出一种即插即用的优化策略：引入包含生成先验的潜变量 refinement 管道，恢复VAE编码中丢失的空间细节，并实现条件潜变量与噪声潜变量在扩散过程中的动态交互。

Result: 在多个低级视觉任务上实验表明，该方法显著提升了图像重建的保真度，同时保持了感知真实性和视觉美感，且兼容现有扩散模型架构。

Conclusion: 通过改进条件控制机制，有效缓解了PTDB方法在低光环境下的保真度损失问题，为扩散模型在低级视觉任务中的应用提供了更优的解决方案。

Abstract: Diffusion-based methods, leveraging pre-trained large models like Stable
Diffusion via ControlNet, have achieved remarkable performance in several
low-level vision tasks. However, Pre-Trained Diffusion-Based (PTDB) methods
often sacrifice content fidelity to attain higher perceptual realism. This
issue is exacerbated in low-light scenarios, where severely degraded
information caused by the darkness limits effective control. We identify two
primary causes of fidelity loss: the absence of suitable conditional latent
modeling and the lack of bidirectional interaction between the conditional
latent and noisy latent in the diffusion process. To address this, we propose a
novel optimization strategy for conditioning in pre-trained diffusion models,
enhancing fidelity while preserving realism and aesthetics. Our method
introduces a mechanism to recover spatial details lost during VAE encoding,
i.e., a latent refinement pipeline incorporating generative priors.
Additionally, the refined latent condition interacts dynamically with the noisy
latent, leading to improved restoration performance. Our approach is
plug-and-play, seamlessly integrating into existing diffusion networks to
provide more effective control. Extensive experiments demonstrate significant
fidelity improvements in PTDB methods.

</details>


### [110] [Towards Imperceptible Watermarking Via Environment Illumination for Consumer Cameras](https://arxiv.org/abs/2510.17114)
*Hodaka Kawachi,Tomoya Nakamura,Hiroaki Santo,SaiKiran Kumar Tedla,Trevor Dalton Canham,Yasushi Yagi,Michael S. Brown*

Main category: cs.CV

TL;DR: 提出了一种利用LED环境照明在消费级相机中嵌入视觉不可见水印的方法，通过优化LED光谱特性实现人眼不可察觉但相机可检测的水印。


<details>
  <summary>Details</summary>
Motivation: 为了在保护隐私和验证内容的同时，避免传统可见光通信对人眼的干扰，需要一种视觉不可感知但设备可检测的水印技术。

Method: 通过联合考虑人眼视觉系统、消费级相机传感器的光谱敏感性以及窄带LED产生宽带光谱的能力，优化LED光源的光谱分布，并采用光谱调制而非强度调制来保证隐蔽性。

Result: 能够在10秒视频中嵌入128比特信息，并在标准帧率（30-60 fps）下成功提取水印，实现较低数据率但实用的元数据传输。

Conclusion: 该方法实现了在不引起人眼注意的情况下，利用环境LED照明为消费级相机提供有效的视觉水印，适用于隐私保护和内容验证场景。

Abstract: This paper introduces a method for using LED-based environmental lighting to
produce visually imperceptible watermarks for consumer cameras. Our approach
optimizes an LED light source's spectral profile to be minimally visible to the
human eye while remaining highly detectable by typical consumer cameras. The
method jointly considers the human visual system's sensitivity to visible
spectra, modern consumer camera sensors' spectral sensitivity, and narrowband
LEDs' ability to generate broadband spectra perceived as "white light"
(specifically, D65 illumination). To ensure imperceptibility, we employ
spectral modulation rather than intensity modulation. Unlike conventional
visible light communication, our approach enables watermark extraction at
standard low frame rates (30-60 fps). While the information transfer rate is
modest-embedding 128 bits within a 10-second video clip-this capacity is
sufficient for essential metadata supporting privacy protection and content
verification.

</details>


### [111] [GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection](https://arxiv.org/abs/2510.17131)
*Xin Gao,Jiyao Liu,Guanghao Li,Yueming Lyu,Jianxiong Gao,Weichen Yu,Ningsheng Xu,Liang Wang,Caifeng Shan,Ziwei Liu,Chenyang Si*

Main category: cs.CV

TL;DR: 提出了一种名为GOOD的新框架，利用双层次引导在扩散模型中生成更可控、更多样化的异常分布（OOD）样本，显著提升OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本到图像扩散模型的OOD样本生成方法依赖于扰动文本条件嵌入，导致语义不稳定和分布偏移多样性不足，限制了在真实场景中的泛化能力。

Method: GOOD框架结合双层次引导：(1) 图像级引导利用对数配分函数梯度降低输入似然，驱动生成样本向像素空间低密度区域移动；(2) 特征级引导基于分类器潜在空间中的k近邻距离，促进在特征稀疏区域采样。并使用ID分类器直接引导扩散轨迹。

Result: 实验表明，使用GOOD生成的样本进行训练可显著提升OOD检测性能，提出的统一OOD评分机制增强了检测的鲁棒性，且生成样本具有更高的多样性与可控性。

Conclusion: GOOD提供了一种灵活有效的方法，通过直接引导扩散过程生成高质量OOD样本，克服了语义不稳定和多样性不足的问题，推动了OOD检测的实际应用。

Abstract: Recent advancements have explored text-to-image diffusion models for
synthesizing out-of-distribution (OOD) samples, substantially enhancing the
performance of OOD detection. However, existing approaches typically rely on
perturbing text-conditioned embeddings, resulting in semantic instability and
insufficient shift diversity, which limit generalization to realistic OOD. To
address these challenges, we propose GOOD, a novel and flexible framework that
directly guides diffusion sampling trajectories towards OOD regions using
off-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level
guidance: (1) Image-level guidance based on the gradient of log partition to
reduce input likelihood, drives samples toward low-density regions in pixel
space. (2) Feature-level guidance, derived from k-NN distance in the
classifier's latent space, promotes sampling in feature-sparse regions. Hence,
this dual-guidance design enables more controllable and diverse OOD sample
generation. Additionally, we introduce a unified OOD score that adaptively
combines image and feature discrepancies, enhancing detection robustness. We
perform thorough quantitative and qualitative analyses to evaluate the
effectiveness of GOOD, demonstrating that training with samples generated by
GOOD can notably enhance OOD detection performance.

</details>


### [112] [KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation](https://arxiv.org/abs/2510.17137)
*WenBo Xu,Liu Liu,Li Zhang,Ran Zhang,Hao Wu,Dan Guo,Meng Wang*

Main category: cs.CV

TL;DR: 提出KineDiff3D，一个统一框架，用于从单视图输入中实现类别级铰接物体的三维重建与姿态估计，结合变分自编码器与扩散模型，并通过迭代优化提升精度。


<details>
  <summary>Details</summary>
Motivation: 铰接物体因多部件几何结构和可变关节配置导致三维重建与姿态估计困难，现有方法难以处理结构多样性和部分观测下的完整恢复问题。

Method: 设计Kinematic-Aware VAE（KA-VAE）将SDF、关节角和部件分割编码至结构化隐空间；采用两个条件扩散模型分别回归全局姿态（SE(3)）与关节参数、生成隐码；引入基于Chamfer距离最小化并保持运动学约束的双向迭代优化模块。

Result: 在合成、半合成和真实数据集上验证了方法的有效性，能够准确重建铰接物体形状并估计其运动学参数，优于现有方法。

Conclusion: KineDiff3D实现了从单视图对类别级铰接物体的高质量重建与运动参数估计，结合扩散模型与迭代优化，显著提升了鲁棒性与精度。

Abstract: Articulated objects, such as laptops and drawers, exhibit significant
challenges for 3D reconstruction and pose estimation due to their multi-part
geometries and variable joint configurations, which introduce structural
diversity across different states. To address these challenges, we propose
KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object
Shape Reconstruction and Generation, a unified framework for reconstructing
diverse articulated instances and pose estimation from single view input.
Specifically, we first encode complete geometry (SDFs), joint angles, and part
segmentation into a structured latent space via a novel Kinematic-Aware VAE
(KA-VAE). In addition, we employ two conditional diffusion models: one for
regressing global pose (SE(3)) and joint parameters, and another for generating
the kinematic-aware latent code from partial observations. Finally, we produce
an iterative optimization module that bidirectionally refines reconstruction
accuracy and kinematic parameters via Chamfer-distance minimization while
preserving articulation constraints. Experimental results on synthetic,
semi-synthetic, and real-world datasets demonstrate the effectiveness of our
approach in accurately reconstructing articulated objects and estimating their
kinematic properties.

</details>


### [113] [GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image](https://arxiv.org/abs/2510.17157)
*Yinghui Wang,Xinyu Zhang,Peng Du*

Main category: cs.CV

TL;DR: 提出GACO-CAD，一种两阶段后训练框架，通过引入几何先验和强化学习奖励机制，提升从单张图像生成参数化CAD模型的几何精度和建模简洁性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在从2D图像推断3D几何结构时存在空间推理能力不足的问题，导致生成的CAD模型几何不准确且程序冗长。

Method: 第一阶段在监督微调中使用深度图和表面法向图作为密集几何先验，与RGB图像组成多通道输入；第二阶段在强化学习中引入组长度奖励，鼓励生成更紧凑的参数化建模序列，并采用动态加权策略稳定训练。

Result: 在DeepCAD和Fusion360数据集上实验表明，GACO-CAD在相同MLLM主干下实现了最先进的性能，在代码有效性、几何精度和建模简洁性方面均优于现有方法。

Conclusion: GACO-CAD有效提升了从单图像生成参数化CAD模型的几何准确性和程序简洁性，为工业概念设计提供了更高效的自动化工具。

Abstract: Generating editable, parametric CAD models from a single image holds great
potential to lower the barriers of industrial concept design. However, current
multi-modal large language models (MLLMs) still struggle with accurately
inferring 3D geometry from 2D images due to limited spatial reasoning
capabilities. We address this limitation by introducing GACO-CAD, a novel
two-stage post-training framework. It is designed to achieve a joint objective:
simultaneously improving the geometric accuracy of the generated CAD models and
encouraging the use of more concise modeling procedures. First, during
supervised fine-tuning, we leverage depth and surface normal maps as dense
geometric priors, combining them with the RGB image to form a multi-channel
input. In the context of single-view reconstruction, these priors provide
complementary spatial cues that help the MLLM more reliably recover 3D geometry
from 2D observations. Second, during reinforcement learning, we introduce a
group length reward that, while preserving high geometric fidelity, promotes
the generation of more compact and less redundant parametric modeling
sequences. A simple dynamic weighting strategy is adopted to stabilize
training. Experiments on the DeepCAD and Fusion360 datasets show that GACO-CAD
achieves state-of-the-art performance under the same MLLM backbone,
consistently outperforming existing methods in terms of code validity,
geometric accuracy, and modeling conciseness.

</details>


### [114] [Investigating Adversarial Robustness against Preprocessing used in Blackbox Face Recognition](https://arxiv.org/abs/2510.17169)
*Roland Croft,Brian Du,Darcy Joseph,Sharath Kumar*

Main category: cs.CV

TL;DR: 本研究探讨了在黑盒设置下，不同人脸预处理技术对人脸识别系统中对抗攻击迁移性的影响，并提出了一种预处理不变的输入变换方法，将攻击的迁移性提高了27%。


<details>
  <summary>Details</summary>
Motivation: 人脸识别模型易受对抗样本攻击，而攻击研究通常忽视了黑盒环境下预处理步骤（如人脸检测和插值）的影响，因此有必要系统评估预处理对攻击迁移性的实际作用。

Method: 研究评估了多种现成的先进对抗攻击在不同人脸检测模型和插值方法下的迁移性，并分析预处理组件对黑盒和白盒攻击成功率的影响，进而提出基于输入变换的预处理不变攻击方法。

Result: 实验发现人脸检测模型的选择可使攻击成功率下降高达78%，而插值方法影响较小；在白盒攻击中，预处理反而因噪声与检测模型的意外交互削弱攻击效果；所提方法将攻击迁移性最高提升27%。

Conclusion: 人脸预处理在对抗攻击中起关键作用，应被充分考虑以提升对抗样本的泛化能力；提出的输入变换策略有效增强了攻击在不同预处理流程下的迁移性。

Abstract: Face Recognition (FR) models have been shown to be vulnerable to adversarial
examples that subtly alter benign facial images, exposing blind spots in these
systems, as well as protecting user privacy. End-to-end FR systems first obtain
preprocessed faces from diverse facial imagery prior to computing the
similarity of the deep feature embeddings. Whilst face preprocessing is a
critical component of FR systems, and hence adversarial attacks against them,
we observe that this preprocessing is often overlooked in blackbox settings.
Our study seeks to investigate the transferability of several out-of-the-box
state-of-the-art adversarial attacks against FR when applied against different
preprocessing techniques used in a blackbox setting. We observe that the choice
of face detection model can degrade the attack success rate by up to 78%,
whereas choice of interpolation method during downsampling has relatively
minimal impacts. Furthermore, we find that the requirement for facial
preprocessing even degrades attack strength in a whitebox setting, due to the
unintended interaction of produced noise vectors against face detection models.
Based on these findings, we propose a preprocessing-invariant method using
input transformations that improves the transferability of the studied attacks
by up to 27%. Our findings highlight the importance of preprocessing in FR
systems, and the need for its consideration towards improving the adversarial
generalisation of facial adversarial examples.

</details>


### [115] [$\mathcal{V}isi\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.17205)
*Yingqi Fan,Anhao Zhao,Jinlan Fu,Junlong Tong,Hui Su,Yijie Pan,Wei Zhang,Xiaoyu Shen*

Main category: cs.CV

TL;DR: 本文提出VisiPruner，一种无需训练的多模态大模型剪枝框架，基于对MLLM中三阶段跨模态交互的系统性分析，显著减少视觉相关计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在处理多模态信息时计算开销大，且缺乏对跨模态融合机制的根本理解，限制了模型效率的提升。

Method: 通过系统分析发现MLLM存在三阶段跨模态交互过程，并据此设计VisiPruner，在浅层保留任务意图识别，在中层保留关键视觉令牌进行融合，在深层丢弃视觉令牌以专注语言优化。

Result: VisiPruner可减少99%视觉相关注意力计算和53.9%的FLOPs，在LLaVA-v1.5 7B上显著优于现有剪枝方法，并在多种MLLM上具有良好泛化性。

Conclusion: 对MLLM内在分层处理机制的理解可为高效模型设计和剪枝提供指导，VisiPruner为训练免费的高效推理提供了有效方案。

Abstract: Multimodal Large Language Models (MLLMs) have achieved strong performance
across vision-language tasks, but suffer from significant computational
overhead due to the quadratic growth of attention computations with the number
of multimodal tokens. Though efforts have been made to prune tokens in MLLMs,
\textit{they lack a fundamental understanding of how MLLMs process and fuse
multimodal information.} Through systematic analysis, we uncover a
\textbf{three-stage} cross-modal interaction process: (1) Shallow layers
recognize task intent, with visual tokens acting as passive attention sinks;
(2) Cross-modal fusion occurs abruptly in middle layers, driven by a few
critical visual tokens; (3) Deep layers discard vision tokens, focusing solely
on linguistic refinement. Based on these findings, we propose
\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\% of
vision-related attention computations and 53.9\% of FLOPs on LLaVA-v1.5 7B. It
significantly outperforms existing token pruning methods and generalizes across
diverse MLLMs. Beyond pruning, our insights further provide actionable
guidelines for training efficient MLLMs by aligning model architecture with its
intrinsic layer-wise processing dynamics. Our code is available at:
https://github.com/EIT-NLP/VisiPruner.

</details>


### [116] [Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling](https://arxiv.org/abs/2510.17171)
*Feihong Yan,Peiru Wang,Yao Zhu,Kaiyu Pang,Qingyan Wei,Huiqi Li,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出了一种无需训练的分层采样策略GtR，结合频率加权的令牌选择（FTS），在保持生成质量的同时显著加速掩码自回归视觉生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有掩码自回归（MAR）模型虽支持并行生成，但受限于单步生成中视觉令牌空间相关性的建模复杂性，难以充分发挥加速潜力。为突破这一瓶颈，需更高效的生成机制。

Method: 提出Generation then Reconstruction（GtR）策略：第一阶段缓慢生成全局语义结构，第二阶段快速补全细节；并引入Frequency-Weighted Token Selection（FTS），基于高频能量定位图像细节区域，为其分配更多计算资源。

Result: 在ImageNet条件生成和文本到图像任务中，GtR实现3.72倍加速，同时保持与原始模型相当的质量（FID 1.59，IS 304.4 vs. 原始1.59, 299.1），显著优于现有加速方法。

Conclusion: GtR结合FTS能有效平衡生成效率与质量，为MAR模型提供了高效、即插即用的加速方案，适用于多种模型规模和生成任务。

Abstract: Masked Autoregressive (MAR) models promise better efficiency in visual
generation than autoregressive (AR) models for the ability of parallel
generation, yet their acceleration potential remains constrained by the
modeling complexity of spatially correlated visual tokens in a single step. To
address this limitation, we introduce Generation then Reconstruction (GtR), a
training-free hierarchical sampling strategy that decomposes generation into
two stages: structure generation establishing global semantic scaffolding,
followed by detail reconstruction efficiently completing remaining tokens.
Assuming that it is more difficult to create an image from scratch than to
complement images based on a basic image framework, GtR is designed to achieve
acceleration by computing the reconstruction stage quickly while maintaining
the generation quality by computing the generation stage slowly. Moreover,
observing that tokens on the details of an image often carry more semantic
information than tokens in the salient regions, we further propose
Frequency-Weighted Token Selection (FTS) to offer more computation budget to
tokens on image details, which are localized based on the energy of high
frequency information. Extensive experiments on ImageNet class-conditional and
text-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining
comparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1),
substantially outperforming existing acceleration methods across various model
scales and generation tasks. Our codes will be released in
https://github.com/feihongyan1/GtR.

</details>


### [117] [Benchmarking Out-of-Distribution Detection for Plankton Recognition: A Systematic Evaluation of Advanced Methods in Marine Ecological Monitoring](https://arxiv.org/abs/2510.17179)
*Yingzi Han,Jiakai He,Chuanlong Xie,Jianping Li*

Main category: cs.CV

TL;DR: 本研究首次对浮游生物识别中的分布外（OoD）检测方法进行了大规模系统性评估，基于DYB-PlanktonNet数据集构建了多种OoD基准，并发现ViM方法在远域OoD场景下表现最优。


<details>
  <summary>Details</summary>
Motivation: 由于浮游生物形态复杂、物种多样性高且不断发现新物种，训练与测试数据之间常存在分布偏移（OoD），导致模型在实际部署中出现不可预测的错误，而当前缺乏对最新OoD检测方法的系统整合与统一基准评估。

Method: 基于DYB-PlanktonNet数据集精心设计了一系列模拟不同分布偏移场景的OoD基准，并系统评估了22种OoD检测方法。

Result: 实验结果表明，ViM方法在所构建的基准上显著优于其他方法，尤其在远域OoD场景下关键指标有大幅提升。

Conclusion: 该研究为自动化浮游生物识别中的算法选择提供了可靠参考，填补了该领域系统性OoD评估的空白，并为未来研究奠定了基础。

Abstract: Automated plankton recognition models face significant challenges during
real-world deployment due to distribution shifts (Out-of-Distribution, OoD)
between training and test data. This stems from plankton's complex
morphologies, vast species diversity, and the continuous discovery of novel
species, which leads to unpredictable errors during inference. Despite rapid
advancements in OoD detection methods in recent years, the field of plankton
recognition still lacks a systematic integration of the latest computer vision
developments and a unified benchmark for large-scale evaluation. To address
this, this paper meticulously designed a series of OoD benchmarks simulating
various distribution shift scenarios based on the DYB-PlanktonNet dataset
\cite{875n-f104-21}, and systematically evaluated twenty-two OoD detection
methods. Extensive experimental results demonstrate that the ViM
\cite{wang2022vim} method significantly outperforms other approaches in our
constructed benchmarks, particularly excelling in Far-OoD scenarios with
substantial improvements in key metrics. This comprehensive evaluation not only
provides a reliable reference for algorithm selection in automated plankton
recognition but also lays a solid foundation for future research in plankton
OoD detection. To our knowledge, this study marks the first large-scale,
systematic evaluation and analysis of Out-of-Distribution data detection
methods in plankton recognition. Code is available at
https://github.com/BlackJack0083/PlanktonOoD.

</details>


### [118] [UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action](https://arxiv.org/abs/2510.17790)
*Yuhao Yang,Zhen Yang,Zi-Yi Dou,Anh Nguyen,Keen You,Omar Attia,Andrew Szot,Michael Feng,Ram Ramrakhya,Alexander Toshev,Chao Huang,Yinfei Yang,Zhe Gan*

Main category: cs.CV

TL;DR: UltraCUA提出了一种混合动作机制，将GUI操作与高级程序化工具调用结合，通过自动化工具构建、合成数据生成和两阶段训练，显著提升计算机使用代理的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有计算机使用代理依赖低级GUI操作，导致执行链长、错误累积；而其他领域代理已广泛使用程序化接口，因此需要填补计算机使用代理在高层工具集成上的空白。

Method: 提出四部分方法：1）从文档和代码库自动构建程序化工具；2）生成超1.7万可验证任务的合成数据；3）收集包含低级GUI动作和高级工具调用的混合动作轨迹；4）采用监督微调与在线强化学习的两阶段训练。

Result: 在OSWorld上，7B和32B模型相比基线平均提升22%性能，步数减少11%；在跨域的WindowsAgentArena上成功率达21.7%，超越使用Windows数据训练的基线。

Conclusion: 混合动作机制有效减少了错误传播，在保持执行效率的同时显著提升了计算机使用代理的性能和泛化能力。

Abstract: Multimodal agents for computer use rely exclusively on primitive actions
(click, type, scroll) that require accurate visual grounding and lengthy
execution chains, leading to cascading failures and performance bottlenecks.
While other agents leverage rich programmatic interfaces (APIs, MCP servers,
tools), computer-use agents (CUAs) remain isolated from these capabilities. We
present UltraCUA, a foundation model that bridges this gap through hybrid
action -- seamlessly integrating GUI primitives with high-level programmatic
tool calls. To achieve this, our approach comprises four key components: (1) an
automated pipeline that scales programmatic tools from software documentation,
open-source repositories, and code generation; (2) a synthetic data engine
producing over 17,000 verifiable tasks spanning real-world computer-use
scenarios; (3) a large-scale high-quality hybrid action trajectory collection
with both low-level GUI actions and high-level programmatic tool calls; and (4)
a two-stage training pipeline combining supervised fine-tuning with online
reinforcement learning, enabling strategic alternation between low-level and
high-level actions. Experiments with our 7B and 32B models demonstrate
substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA
models achieve an average 22% relative improvement over base models, while
being 11% faster in terms of steps. Out-of-domain evaluation on
WindowsAgentArena shows our model reaches 21.7% success rate, outperforming
baselines trained on Windows data. The hybrid action mechanism proves critical,
reducing error propagation while maintaining execution efficiency.

</details>


### [119] [Capturing Head Avatar with Hand Contacts from a Monocular Video](https://arxiv.org/abs/2510.17181)
*Haonan He,Yufeng Zheng,Jie Song*

Main category: cs.CV

TL;DR: 提出一种联合学习三维头部头像和手-脸交互引起的非刚性变形的新框架，提高了虚拟现实中的真实感。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注面部区域，忽略了手与脸的自然交互（如手托下巴），而这些交互能传达思考等认知状态，因此需要更全面的头像建模方法。

Method: 提出结合深度顺序损失和接触正则化的姿态跟踪方法，并学习特定于手-脸交互的PCA形变基；引入基于物理的接触损失以减少穿透伪影。

Result: 在iPhone采集的RGB(D)视频和合成数据集上验证，相比现有最先进方法，能更准确地重建面部外观和变形几何。

Conclusion: 该方法有效解决了手-脸交互建模中的姿态估计与形变学习难题，显著提升了头像的几何精度与物理合理性。

Abstract: Photorealistic 3D head avatars are vital for telepresence, gaming, and VR.
However, most methods focus solely on facial regions, ignoring natural
hand-face interactions, such as a hand resting on the chin or fingers gently
touching the cheek, which convey cognitive states like pondering. In this work,
we present a novel framework that jointly learns detailed head avatars and the
non-rigid deformations induced by hand-face interactions.
  There are two principal challenges in this task. First, naively tracking hand
and face separately fails to capture their relative poses. To overcome this, we
propose to combine depth order loss with contact regularization during pose
tracking, ensuring correct spatial relationships between the face and hand.
Second, no publicly available priors exist for hand-induced deformations,
making them non-trivial to learn from monocular videos. To address this, we
learn a PCA basis specific to hand-induced facial deformations from a face-hand
interaction dataset. This reduces the problem to estimating a compact set of
PCA parameters rather than a full spatial deformation field. Furthermore,
inspired by physics-based simulation, we incorporate a contact loss that
provides additional supervision, significantly reducing interpenetration
artifacts and enhancing the physical plausibility of the results.
  We evaluate our approach on RGB(D) videos captured by an iPhone.
Additionally, to better evaluate the reconstructed geometry, we construct a
synthetic dataset of avatars with various types of hand interactions. We show
that our method can capture better appearance and more accurate deforming
geometry of the face than SOTA surface reconstruction methods.

</details>


### [120] [Glyph: Scaling Context Windows via Visual-Text Compression](https://arxiv.org/abs/2510.17800)
*Jiale Cheng,Yusen Liu,Xinyu Zhang,Yulin Fei,Wenyi Hong,Ruiliang Lyu,Weihan Wang,Zhe Su,Xiaotao Gu,Xiao Liu,Yushi Bai,Jie Tang,Hongning Wang,Minlie Huang*

Main category: cs.CV

TL;DR: 本文提出Glyph框架，通过将长文本渲染为图像并利用视觉-语言模型处理，实现3-4倍的令牌压缩，显著降低计算开销，同时保持与主流大语言模型相当的准确性。


<details>
  <summary>Details</summary>
Motivation: 长上下文建模在文档理解、代码分析等任务中日益重要，但将上下文扩展到百万token级别带来高昂的计算和内存成本，限制了实际应用。本文旨在探索视觉化上下文扩展的新路径以克服这一瓶颈。

Method: 提出Glyph框架，将长文本转换为图像输入视觉-语言模型（VLM）处理，并设计基于大语言模型驱动的遗传搜索算法，自动寻找最优的文本渲染配置，在压缩率与准确性之间取得平衡。

Result: 实验表明，该方法实现3-4倍token压缩，保持与Qwen3-8B等领先LLM相当的准确率；预填充和解码速度快约4倍，SFT训练快约2倍；在极端压缩下，128K上下文VLM可处理百万token级任务，且在文档理解等多模态任务中表现优异。

Conclusion: 通过视觉化文本表示，Glyph为长上下文建模提供了一种高效、可扩展的新范式，显著降低计算成本，同时具备在多模态任务中的广泛应用潜力。

Abstract: Large language models (LLMs) increasingly rely on long-context modeling for
tasks such as document understanding, code analysis, and multi-step reasoning.
However, scaling context windows to the million-token level brings prohibitive
computational and memory costs, limiting the practicality of long-context LLMs.
In this work, we take a different perspective-visual context scaling-to tackle
this challenge. Instead of extending token-based sequences, we propose Glyph, a
framework that renders long texts into images and processes them with
vision-language models (VLMs). This approach substantially compresses textual
input while preserving semantic information, and we further design an
LLM-driven genetic search to identify optimal visual rendering configurations
for balancing accuracy and compression. Through extensive experiments, we
demonstrate that our method achieves 3-4x token compression while maintaining
accuracy comparable to leading LLMs such as Qwen3-8B on various long-context
benchmarks. This compression also leads to around 4x faster prefilling and
decoding, and approximately 2x faster SFT training. Furthermore, under extreme
compression, a 128K-context VLM could scale to handle 1M-token-level text
tasks. In addition, the rendered text data benefits real-world multimodal
tasks, such as document understanding. Our code and model are released at
https://github.com/thu-coai/Glyph.

</details>


### [121] [HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery](https://arxiv.org/abs/2510.17188)
*Vaibhav Rathore,Divyam Gupta,Biplab Banerjee*

Main category: cs.CV

TL;DR: 本文提出了HIDISC，一种基于双曲空间表示学习的广义类别发现方法，能够在无需目标域数据且存在分布偏移的开放世界场景中实现跨域和跨类别的泛化，通过GPT引导的数据增强和切线空间插值策略，在效率和性能上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的广义类别发现（GCD）方法通常假设训练时能同时获取来自同一域的标注和未标注数据，难以应对开放世界中的分布偏移问题；而现有的DG-GCD方法依赖高成本的多域模拟训练，限制了实际应用。因此需要一个更高效且具备强域泛化能力的GCD框架。

Method: 提出HIDISC，采用双曲空间进行表示学习，引入GPT引导的扩散增强来生成多样化的源域变体；设计Tangent CutMix方法在切线空间进行曲率感知的插值以合成伪新类别样本；并采用统一损失函数，结合惩罚性Busemann对齐、混合双曲对比正则化和自适应异常点排斥，以及可学习的曲率参数以适应数据复杂性。

Result: HIDISC在PACS、Office-Home和DomainNet三个基准上均取得了当前最优的性能，显著优于现有的欧氏和双曲空间下的(DG)-GCD基线方法。

Conclusion: 通过双曲表示学习与几何感知的数据增强策略，HIDISC有效实现了在无目标域监督下的域与类别联合泛化，为开放环境中的广义类别发现提供了高效且强大的解决方案。

Abstract: Generalized Category Discovery (GCD) aims to classify test-time samples into
either seen categories** -- available during training -- or novel ones, without
relying on label supervision. Most existing GCD methods assume simultaneous
access to labeled and unlabeled data during training and arising from the same
domain, limiting applicability in open-world scenarios involving distribution
shifts. Domain Generalization with GCD (DG-GCD) lifts this constraint by
requiring models to generalize to unseen domains containing novel categories,
without accessing targetdomain data during training. The only prior DG-GCD
method, DG2CD-Net, relies on episodic training with multiple synthetic domains
and task vector aggregation, incurring high computational cost and error
accumulation. We propose HIDISC, a hyperbolic representation learning framework
that achieves domain and category-level generalization without episodic
simulation. To expose the model to minimal but diverse domain variations, we
augment the source domain using GPT-guided diffusion, avoiding overfitting
while maintaining efficiency. To structure the representation space, we
introduce Tangent CutMix, a curvature-aware interpolation that synthesizes
pseudo-novel samples in tangent space, preserving manifold consistency. A
unified loss -- combining penalized Busemann alignment, hybrid hyperbolic
contrastive regularization, and adaptive outlier repulsion -- **facilitates
compact, semantically structured embeddings. A learnable curvature parameter
further adapts the geometry to dataset complexity. HIDISC achieves
state-of-the-art results on PACS , Office-Home , and DomainNet, consistently
outperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.

</details>


### [122] [ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models](https://arxiv.org/abs/2510.17197)
*Pu Zhang,Yuwei Li,Xingyuan Xian,Guoming Tang*

Main category: cs.CV

TL;DR: 提出一种零样本的视觉token剪枝方法，通过结合任务相关性和信息多样性，在保留文本提示指导的同时显著降低视觉语言模型的推理成本。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型处理更大输入的能力增强，产生了大量视觉token冗余，导致推理成本过高；现有剪枝方法往往忽略文本提示的引导，难以优先保留与任务相关的信息。

Method: 提出一种分层的提示感知剪枝方法，首先选择与文本提示相关的核心视觉token，然后补充多样性token以保留上下文。

Result: 在多个模型和基准上实验表明，即使剪枝90%的视觉token，性能仍达到或超过现有方法，精度损失极小，并显著降低显存占用和推理延迟。

Conclusion: 通过引入提示感知的视角，有效平衡任务相关性与多样性，为视觉语言模型的高效推理提供了高效且通用的零样本剪枝方案。

Abstract: As the capabilities of Vision-Language Models (VLMs) advance, they can
process increasingly large inputs, which, unlike in LLMs, generates significant
visual token redundancy and leads to prohibitive inference costs. While many
methods aim to reduce these costs by pruning visual tokens, existing
approaches, whether based on attention or diversity, typically neglect the
guidance of the text prompt and thus fail to prioritize task relevance. In this
work, we propose a novel, zero-shot method that reframes the problem by
introducing a prompt-aware perspective, explicitly modeling visual token
pruning as a balance between task relevance and information diversity. Our
hierarchical approach first selects a core set of task-relevant visual tokens
and then supplements them with diversity tokens to preserve broader context.
Experiments across multiple models and benchmarks show that our method achieves
performance that matches or surpasses the state-of-the-art with only minimal
accuracy loss, even when pruning up to 90\% of the tokens. Furthermore, these
gains are accompanied by significant reductions in GPU memory footprint and
inference latency.

</details>


### [123] [From Pixels to People: Satellite-Based Mapping and Quantification of Riverbank Erosion and Lost Villages in Bangladesh](https://arxiv.org/abs/2510.17198)
*M Saifuzzaman Rafat,Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Jungpil Shin*

Main category: cs.CV

TL;DR: 本研究利用Segment Anything Model（SAM）开发了一种高精度的河岸侵蚀监测方法，基于2003-2025年孟加拉国脆弱地区的历史遥感图像，构建了首个因河流侵蚀而消失定居点的标注数据集，并实现了86.30%的平均IoU和92.60%的Dice分数，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国主要河流每年造成严重河岸侵蚀，导致村庄和农田消失，社区被迫迁移。传统人工监测方法效率低、难度大，亟需自动化、高精度的监测工具以支持灾害管理和政策制定。

Method: 首先通过简单的颜色通道分析对土地与水体进行粗略分割，然后利用手动标注的数据集对SAM模型的掩码解码器进行微调，使其能够识别河岸侵蚀的细微特征，从而实现对土地流失的精确量化。

Result: 所提出的方法在监测河岸侵蚀方面表现出色，平均交并比（IoU）达到86.30%，Dice分数为92.60%，显著优于传统方法和现成的深度学习模型；同时发布了首个包含消失聚落标注的孟加拉国河岸侵蚀数据集。

Conclusion: 该研究提供了首个针对孟加拉国河岸侵蚀的标注数据集、专用AI模型及土地流失量化方法，为政策制定者和灾害管理机构提供了强有力的工具，可用于监测侵蚀动态、预测发展趋势并保护受威胁社区。

Abstract: The great rivers of Bangladesh, arteries of commerce and sustenance, are also
agents of relentless destruction. Each year, they swallow whole villages and
vast tracts of farmland, erasing communities from the map and displacing
thousands of families. To track this slow-motion catastrophe has, until now,
been a Herculean task for human analysts. Here we show how a powerful
general-purpose vision model, the Segment Anything Model (SAM), can be adapted
to this task with remarkable precision. To do this, we assembled a new dataset
- a digital chronicle of loss compiled from historical Google Earth imagery of
Bangladesh's most vulnerable regions, including Mokterer Char Union, Kedarpur
Union, Balchipara village, and Chowhali Upazila, from 2003 to 2025. Crucially,
this dataset is the first to include manually annotated data on the settlements
that have vanished beneath the water. Our method first uses a simple
color-channel analysis to provide a rough segmentation of land and water, and
then fine-tunes SAM's mask decoder to recognize the subtle signatures of
riverbank erosion. The resulting model demonstrates a keen eye for this
destructive process, achieving a mean Intersection over Union of 86.30% and a
Dice score of 92.60% - a performance that significantly surpasses traditional
methods and off-the-shelf deep learning models. This work delivers three key
contributions: the first annotated dataset of disappeared settlements in
Bangladesh due to river erosion; a specialized AI model fine-tuned for this
critical task; and a method for quantifying land loss with compelling visual
evidence. Together, these tools provide a powerful new lens through which
policymakers and disaster management agencies can monitor erosion, anticipate
its trajectory, and ultimately protect the vulnerable communities in its path.

</details>


### [124] [Round Outcome Prediction in VALORANT Using Tactical Features from Video Analysis](https://arxiv.org/abs/2510.17199)
*Nirai Hayakawa,Kazumasa Shimari,Kazuma Yamasaki,Hirotatsu Hoshikawa,Rikuto Tsuchida,Kenichi Matsumoto*

Main category: cs.CV

TL;DR: 本论文提出了一种基于比赛画面中迷你地图信息分析的VALORANT游戏回合结果预测模型，通过引入角色位置和游戏事件等战术特征，显著提升了预测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有电竞比赛结果预测研究多依赖比赛日志和统计信息，难以捕捉复杂策略。针对需要高度战术配合的FPS游戏VALORANT，需探索更有效的预测方法。

Method: 基于视频识别模型TimeSformer，提取迷你地图中的详细战术特征（如角色位置和游戏事件），并利用增强的标注数据集训练模型，以提升预测性能。

Result: 在包含战术事件标签的数据集上训练的模型，在回合中后期实现了约81%的预测准确率，显著优于仅使用迷你地图信息的模型。

Conclusion: 利用比赛画面中提取的战术特征可有效提升VALORANT游戏回合结果的预测准确性，表明视频数据蕴含的战术信息在电竞预测中有重要价值。

Abstract: Recently, research on predicting match outcomes in esports has been actively
conducted, but much of it is based on match log data and statistical
information. This research targets the FPS game VALORANT, which requires
complex strategies, and aims to build a round outcome prediction model by
analyzing minimap information in match footage. Specifically, based on the
video recognition model TimeSformer, we attempt to improve prediction accuracy
by incorporating detailed tactical features extracted from minimap information,
such as character position information and other in-game events. This paper
reports preliminary results showing that a model trained on a dataset augmented
with such tactical event labels achieved approximately 81% prediction accuracy,
especially from the middle phases of a round onward, significantly
outperforming a model trained on a dataset with the minimap information itself.
This suggests that leveraging tactical features from match footage is highly
effective for predicting round outcomes in VALORANT.

</details>


### [125] [EndoCIL: A Class-Incremental Learning Framework for Endoscopic Image Classification](https://arxiv.org/abs/2510.17200)
*Bingrong Liu,Jun Shi,Yushan Zheng*

Main category: cs.CV

TL;DR: 提出了一种名为EndoCIL的类增量学习框架，专用于内窥镜图像诊断，有效缓解灾难性遗忘和类别不平衡问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于回放的类增量学习方法在内窥镜图像分析中因领域差异和类别不平衡难以有效缓解灾难性遗忘，需设计专门针对该临床场景的解决方案。

Method: EndoCIL包含三个核心组件：基于最大均值差异的回放（MDBR）选择代表性样本，先验正则化类别平衡损失（PRCBL）缓解类别不平衡，全连接梯度校准（CFG）减少对新类的分类器偏差。

Result: 在四个公开内窥镜数据集上的实验表明，EndoCIL在不同缓存大小和评估指标下均优于现有最先进方法。

Conclusion: EndoCIL有效平衡了稳定性和可塑性，展现出在临床应用中良好的可扩展性和部署潜力。

Abstract: Class-incremental learning (CIL) for endoscopic image analysis is crucial for
real-world clinical applications, where diagnostic models should continuously
adapt to evolving clinical data while retaining performance on previously
learned ones. However, existing replay-based CIL methods fail to effectively
mitigate catastrophic forgetting due to severe domain discrepancies and class
imbalance inherent in endoscopic imaging. To tackle these challenges, we
propose EndoCIL, a novel and unified CIL framework specifically tailored for
endoscopic image diagnosis. EndoCIL incorporates three key components: Maximum
Mean Discrepancy Based Replay (MDBR), employing a distribution-aligned greedy
strategy to select diverse and representative exemplars, Prior Regularized
Class Balanced Loss (PRCBL), designed to alleviate both inter-phase and
intra-phase class imbalance by integrating prior class distributions and
balance weights into the loss function, and Calibration of Fully-Connected
Gradients (CFG), which adjusts the classifier gradients to mitigate bias toward
new classes. Extensive experiments conducted on four public endoscopic datasets
demonstrate that EndoCIL generally outperforms state-of-the-art CIL methods
across varying buffer sizes and evaluation metrics. The proposed framework
effectively balances stability and plasticity in lifelong endoscopic diagnosis,
showing promising potential for clinical scalability and deployment.

</details>


### [126] [Optimizing DINOv2 with Registers for Face Anti-Spoofing](https://arxiv.org/abs/2510.17201)
*Mika Feng,Pierre Gallin-Martel,Koichi Ito,Takafumi Aoki*

Main category: cs.CV

TL;DR: 提出一种基于DINOv2的活体检测方法，通过引入registers提取泛化特征并抑制注意力机制中的扰动，有效识别伪造人脸攻击。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸识别系统容易受到伪造人脸（如照片攻击）的欺骗，需在识别前准确检测此类spoofing攻击。

Method: 采用DINOv2模型并引入registers机制，增强特征提取的稳定性，聚焦于真实与伪造人脸间的细微差异，提升注意力机制的鲁棒性。

Result: 在ICCV2025反欺骗工作坊数据集和SiW数据集上进行了实验，验证了该方法在检测伪造攻击方面的有效性。

Conclusion: 基于DINOv2并结合registers的方法能有效提升人脸反欺骗检测性能，尤其在应对物理和数字伪造攻击时具有优势。

Abstract: Face recognition systems are designed to be robust against variations in head
pose, illumination, and image blur during capture. However, malicious actors
can exploit these systems by presenting a face photo of a registered user,
potentially bypassing the authentication process. Such spoofing attacks must be
detected prior to face recognition. In this paper, we propose a DINOv2-based
spoofing attack detection method to discern minute differences between live and
spoofed face images. Specifically, we employ DINOv2 with registers to extract
generalizable features and to suppress perturbations in the attention
mechanism, which enables focused attention on essential and minute features. We
demonstrate the effectiveness of the proposed method through experiments
conducted on the dataset provided by ``The 6th Face Anti-Spoofing Workshop:
Unified Physical-Digital Attacks Detection@ICCV2025'' and SiW dataset.

</details>


### [127] [When One Moment Isn't Enough: Multi-Moment Retrieval with Cross-Moment Interactions](https://arxiv.org/abs/2510.17218)
*Zhuo Cao,Heming Du,Bingqing Zhang,Xin Yu,Xue Li,Sen Wang*

Main category: cs.CV

TL;DR: 提出新的多时段检索任务和高质量数据集QV-M$^2$，并设计FlashMMR框架以提升多时段视频定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有单时段检索方法难以满足真实场景中一个查询对应多个相关时段的需求，缺乏合适的数据集和评估指标。

Method: 构建QV-M$^2$数据集并提出FlashMMR框架，包含多时段后验证模块、时序约束调整及验证模块，用于优化时段边界并过滤低置信度候选段。

Result: 在QV-M$^2$上，FlashMMR相比先前SOTA方法在G-mAP、mAP@3+tgt和mR@3上分别提升3.00%、2.70%和2.56%。

Conclusion: QV-M$^2$和FlashMMR为更真实和复杂的视频时序定位研究提供了有效基准和强基线。

Abstract: Existing Moment retrieval (MR) methods focus on Single-Moment Retrieval
(SMR). However, one query can correspond to multiple relevant moments in
real-world applications. This makes the existing datasets and methods
insufficient for video temporal grounding. By revisiting the gap between
current MR tasks and real-world applications, we introduce a high-quality
datasets called QVHighlights Multi-Moment Dataset (QV-M$^2$), along with new
evaluation metrics tailored for multi-moment retrieval (MMR). QV-M$^2$ consists
of 2,212 annotations covering 6,384 video segments. Building on existing
efforts in MMR, we propose a framework called FlashMMR. Specifically, we
propose a Multi-moment Post-verification module to refine the moment
boundaries. We introduce constrained temporal adjustment and subsequently
leverage a verification module to re-evaluate the candidate segments. Through
this sophisticated filtering pipeline, low-confidence proposals are pruned, and
robust multi-moment alignment is achieved. We retrain and evaluate 6 existing
MR methods on QV-M$^2$ and QVHighlights under both SMR and MMR settings.
Results show that QV-M$^2$ serves as an effective benchmark for training and
evaluating MMR models, while FlashMMR provides a strong baseline. Specifically,
on QV-M$^2$, it achieves improvements over prior SOTA method by 3.00% on G-mAP,
2.70% on mAP@3+tgt, and 2.56% on mR@3. The proposed benchmark and method
establish a foundation for advancing research in more realistic and challenging
video temporal grounding scenarios. Code is released at
https://github.com/Zhuo-Cao/QV-M2.

</details>


### [128] [Fair and Interpretable Deepfake Detection in Videos](https://arxiv.org/abs/2510.17264)
*Akihito Yoshii,Ryosuke Sonoda,Ramya Srinivasan*

Main category: cs.CV

TL;DR: 提出了一种公平感知的深度伪造检测框架，结合时间特征学习和人口统计感知的数据增强，提高检测的公平性、准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法常存在偏见、缺乏透明度且忽略时间信息，导致在不同人群中的性能不均和结果不可靠。

Method: 采用基于序列的聚类进行时间建模，结合概念提取提升可解释性，并提出人口统计感知的数据增强方法，通过频域变换平衡少数群体并保留伪造痕迹。

Result: 在FaceForensics++、DFD、Celeb-DF和DFDC数据集上，结合Xception和ResNet等SOTA模型，本方法在公平性与准确性的权衡上优于现有方法。

Conclusion: 所提框架有效提升了深度伪造检测的公平性、泛化能力和可解释性，适用于多样化的实际应用场景。

Abstract: Existing deepfake detection methods often exhibit bias, lack transparency,
and fail to capture temporal information, leading to biased decisions and
unreliable results across different demographic groups. In this paper, we
propose a fairness-aware deepfake detection framework that integrates temporal
feature learning and demographic-aware data augmentation to enhance fairness
and interpretability. Our method leverages sequence-based clustering for
temporal modeling of deepfake videos and concept extraction to improve
detection reliability while also facilitating interpretable decisions for
non-expert users. Additionally, we introduce a demography-aware data
augmentation method that balances underrepresented groups and applies
frequency-domain transformations to preserve deepfake artifacts, thereby
mitigating bias and improving generalization. Extensive experiments on
FaceForensics++, DFD, Celeb-DF, and DFDC datasets using state-of-the-art (SoTA)
architectures (Xception, ResNet) demonstrate the efficacy of the proposed
method in obtaining the best tradeoff between fairness and accuracy when
compared to SoTA.

</details>


### [129] [FineVision: Open Data Is All You Need](https://arxiv.org/abs/2510.17269)
*Luis Wiedmann,Orr Zohar,Amir Mahla,Xiaohan Wang,Rui Li,Thibaud Frere,Leandro von Werra,Aritra Roy Gosthipaty,Andrés Marafioti*

Main category: cs.CV

TL;DR: FineVision是一个精心收集、整理和统一的包含2400万个样本的视觉-语言数据集，整合了200多个来源，经过严格去重和去污染处理，显著提升了视觉语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型的发展受限于公共数据集的碎片化、不一致和污染问题，亟需一个高质量、大规模的统一数据资源。

Method: 通过半自动、人工参与的流程整合200多个数据源为185个子集，利用自动化进行数据摄入和模式映射，人工审核映射结果、抽样检查输出，并实施跨源去重和对66个公开基准的去污染；同时支持代理/GUI任务的统一动作空间。

Result: 在FineVision上训练的模型在广泛的评估套件中 consistently 超过在现有开源数据混合上训练的模型，验证了数据规模、清洁度以及人机协同自动化的优势。

Conclusion: FineVision作为目前最大规模的开源视觉语言语料库，推动以数据为中心的视觉语言模型研究，作者已公开发布该语料库和整理工具。

Abstract: The advancement of vision-language models (VLMs) is hampered by a fragmented
landscape of inconsistent and contaminated public datasets. We introduce
FineVision, a meticulously collected, curated, and unified corpus of 24 million
samples - the largest open resource of its kind. We unify more than 200 sources
into 185 subsets via a semi-automated, human-in-the-loop pipeline: automation
performs bulk ingestion and schema mapping, while reviewers audit mappings and
spot-check outputs to verify faithful consumption of annotations, appropriate
formatting and diversity, and safety; issues trigger targeted fixes and
re-runs. The workflow further applies rigorous de-duplication within and across
sources and decontamination against 66 public benchmarks. FineVision also
encompasses agentic/GUI tasks with a unified action space; reviewers validate
schemas and inspect a sample of trajectories to confirm executable fidelity.
Models trained on FineVision consistently outperform those trained on existing
open mixtures across a broad evaluation suite, underscoring the benefits of
scale, data hygiene, and balanced automation with human oversight. We release
the corpus and curation tools to accelerate data-centric VLM research.

</details>


### [130] [Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models](https://arxiv.org/abs/2510.17274)
*Katie Luo,Jingwei Ji,Tong He,Runsheng Xu,Yichen Xie,Dragomir Anguelov,Mingxing Tan*

Main category: cs.CV

TL;DR: 提出了一种即插即用的方法PnF，利用多模态大语言模型增强现有运动预测模型，通过自然语言理解复杂场景，无需微调即可提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统在标准条件下表现良好，但在多样化真实场景中成本效益地泛化仍具挑战。希望利用自然语言处理复杂情境以提高模型适应性。

Method: 设计提示词从多模态大语言模型中提取结构化场景理解，并将其蒸馏为可学习的嵌入特征，用于增强现有行为预测模型，实现即插即用的性能提升。

Result: 在Waymo Open Motion和nuScenes数据集上验证了该方法，在两个先进运动预测模型上均实现了性能的持续提升。

Conclusion: PnF方法无需微调即可有效利用MLLM的零样本推理能力，提升了运动预测性能，具有良好的实用性和可扩展性。

Abstract: Current autonomous driving systems rely on specialized models for perceiving
and predicting motion, which demonstrate reliable performance in standard
conditions. However, generalizing cost-effectively to diverse real-world
scenarios remains a significant challenge. To address this, we propose
Plug-and-Forecast (PnF), a plug-and-play approach that augments existing motion
forecasting models with multimodal large language models (MLLMs). PnF builds on
the insight that natural language provides a more effective way to describe and
handle complex scenarios, enabling quick adaptation to targeted behaviors. We
design prompts to extract structured scene understanding from MLLMs and distill
this information into learnable embeddings to augment existing behavior
prediction models. Our method leverages the zero-shot reasoning capabilities of
MLLMs to achieve significant improvements in motion prediction performance,
while requiring no fine-tuning -- making it practical to adopt. We validate our
approach on two state-of-the-art motion forecasting models using the Waymo Open
Motion Dataset and the nuScenes Dataset, demonstrating consistent performance
improvements across both benchmarks.

</details>


### [131] [SG-CLDFF: A Novel Framework for Automated White Blood Cell Classification and Segmentation](https://arxiv.org/abs/2510.17278)
*Mehdi Zekriyapanah Gashti,Mostafa Mohammadpour,Ghasem Farjamnia*

Main category: cs.CV

TL;DR: 提出了一种新颖的SG-CLDFF框架，结合显著性引导的预处理与多尺度深度特征融合，显著提升了白细胞分割与分类的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 白细胞的准确分割与分类对血液病诊断至关重要，但受染色变异、复杂背景和类别不平衡影响，现有方法仍面临挑战。

Method: 提出SG-CLDFF框架：首先利用显著性先验定位白细胞区域，采用EfficientSwin风格轻量混合骨干网络提取多尺度特征，通过改进的跨层融合模块整合浅层与深层特征；采用多任务学习，结合分割与分类头，使用类别感知加权损失和显著性对齐正则化抑制背景激活；通过Grad-CAM和显著性一致性提升可解释性。

Result: 在BCCD、LISC和ALL-IDB标准数据集上实验表明，该方法在IoU、F1分数和分类准确率上均优于主流CNN和Transformer基线模型，消融实验验证了各模块的有效性。

Conclusion: SG-CLDFF为临床白细胞自动分析提供了一种高效、鲁棒且可解释的解决方案，具有实际应用潜力。

Abstract: Accurate segmentation and classification of white blood cells (WBCs) in
microscopic images are essential for diagnosis and monitoring of many
hematological disorders, yet remain challenging due to staining variability,
complex backgrounds, and class imbalance. In this paper, we introduce a novel
Saliency-Guided Cross-Layer Deep Feature Fusion framework (SG-CLDFF) that
tightly integrates saliency-driven preprocessing with multi-scale deep feature
aggregation to improve both robustness and interpretability for WBC analysis.
SG-CLDFF first computes saliency priors to highlight candidate WBC regions and
guide subsequent feature extraction. A lightweight hybrid backbone
(EfficientSwin-style) produces multi-resolution representations, which are
fused by a ResNeXt-CC-inspired cross-layer fusion module to preserve
complementary information from shallow and deep layers. The network is trained
in a multi-task setup with concurrent segmentation and cell-type classification
heads, using class-aware weighted losses and saliency-alignment regularization
to mitigate imbalance and suppress background activation. Interpretability is
enforced through Grad-CAM visualizations and saliency consistency checks,
allowing model decisions to be inspected at the regional level. We validate the
framework on standard public benchmarks (BCCD, LISC, ALL-IDB), reporting
consistent gains in IoU, F1, and classification accuracy compared to strong CNN
and transformer baselines. An ablation study also demonstrates the individual
contributions of saliency preprocessing and cross-layer fusion. SG-CLDFF offers
a practical and explainable path toward more reliable automated WBC analysis in
clinical workflows.

</details>


### [132] [Machine Vision-Based Surgical Lighting System:Design and Implementation](https://arxiv.org/abs/2510.17287)
*Amir Gharghabi,Mahdi Hakiminezhad,Maryam Shafaei,Shaghayegh Gharghabi*

Main category: cs.CV

TL;DR: 提出了一种基于YOLOv11的自动化手术照明系统，通过识别手术区域上方的蓝色标记自动调节LED光源，提升照明一致性与手术安全性。


<details>
  <summary>Details</summary>
Motivation: 传统手术照明依赖手动调节，易导致医生疲劳、颈部劳损及照明不稳定，影响手术精度与安全。

Method: 采用YOLOv11算法检测手术场景中位于目标区域上方的蓝色球形标记，结合云台控制的伺服电机自动调整高功率LED光源方向。

Result: YOLO模型在模拟手术场景图像上达到96.7%的mAP@50，系统能准确追踪标记并实现稳定照明。

Conclusion: 该自动化照明系统可有效减轻医生负担，提升术中照明的一致性和精确性，有助于改善手术效果。

Abstract: Effortless and ergonomically designed surgical lighting is critical for
precision and safety during procedures. However, traditional systems often rely
on manual adjustments, leading to surgeon fatigue, neck strain, and
inconsistent illumination due to drift and shadowing. To address these
challenges, we propose a novel surgical lighting system that leverages the
YOLOv11 object detection algorithm to identify a blue marker placed above the
target surgical site. A high-power LED light source is then directed to the
identified location using two servomotors equipped with tilt-pan brackets. The
YOLO model achieves 96.7% mAP@50 on the validation set consisting of annotated
images simulating surgical scenes with the blue spherical marker. By automating
the lighting process, this machine vision-based solution reduces physical
strain on surgeons, improves consistency in illumination, and supports improved
surgical outcomes.

</details>


### [133] [Exploring Structural Degradation in Dense Representations for Self-supervised Learning](https://arxiv.org/abs/2510.17299)
*Siran Dai,Qianqian Xu,Peisong Wen,Yang Liu,Qingming Huang*

Main category: cs.CV

TL;DR: 本文发现了一种在自监督学习中训练越久反而导致密集预测任务性能下降的反直觉现象（称为SDD），并提出了一种无需标注即可评估密集任务表征质量的指标DSE，基于DSE进一步提出了模型选择策略和正则化方法，有效缓解了该问题。


<details>
  <summary>Details</summary>
Motivation: 尽管自监督学习在视觉任务中取得了显著进展，但长时间训练后密集预测任务（如语义分割）性能反而下降的现象未被充分认识，缺乏有效的无监督评估手段阻碍了对这一问题的理解与解决。

Method: 提出Dense representation Structure Estimator（DSE），包含类别相关性和有效维度两个度量，用于无监督评估表征结构；基于DSE设计模型选择策略和正则化方法。

Result: 在16种主流SSL方法和4个基准上验证，DSE与下游任务性能高度相关；模型选择平均提升3.0% mIoU且几乎无额外计算开销；DSE正则化能持续缓解密集退化现象。

Conclusion: SDD是SSL中普遍存在的问题，DSE为无监督评估密集任务表征提供了有效工具，并为模型选择和训练优化提供了可行方案。

Abstract: In this work, we observe a counterintuitive phenomenon in self-supervised
learning (SSL): longer training may impair the performance of dense prediction
tasks (e.g., semantic segmentation). We refer to this phenomenon as
Self-supervised Dense Degradation (SDD) and demonstrate its consistent presence
across sixteen state-of-the-art SSL methods with various losses, architectures,
and datasets. When the model performs suboptimally on dense tasks at the end of
training, measuring the performance during training becomes essential. However,
evaluating dense performance effectively without annotations remains an open
challenge. To tackle this issue, we introduce a Dense representation Structure
Estimator (DSE), composed of a class-relevance measure and an effective
dimensionality measure. The proposed DSE is both theoretically grounded and
empirically validated to be closely correlated with the downstream performance.
Based on this metric, we introduce a straightforward yet effective model
selection strategy and a DSE-based regularization method. Experiments on
sixteen SSL methods across four benchmarks confirm that model selection
improves mIoU by $3.0\%$ on average with negligible computational cost.
Additionally, DSE regularization consistently mitigates the effects of dense
degradation. Code is available at
https://github.com/EldercatSAM/SSL-Degradation.

</details>


### [134] [LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding](https://arxiv.org/abs/2510.17305)
*ZhaoYang Han,Qihan Lin,Hao Liang,Bowen Chen,Zhou Liu,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文提出了LongInsightBench，首个专注于评估模型在长视频中理解人类语言、视角、动作等多模态（视觉、音频、文本）信息的基准，包含信息密集的长视频、多样且具挑战性的任务设计以及严格的数据质量保障流程。实验表明现有多模态模型在精确时序定位与长距离因果推理上仍存在困难。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型在处理长时、信息密集的视频时面临挑战，但缺乏合适的基准来系统评估其在真实场景（如讲座、访谈、vlog）中对语言、动作和上下文的理解能力，因此需要构建一个更贴近实际应用的长视频理解评测基准。

Method: 构建名为LongInsightBench的基准：从FineVideo中筛选约1000个长且信息密集的视频，涵盖讲座、访谈等；设计六种具有挑战性的任务类型（包括事件内与事件间任务）；采用三阶段半自动化的数据质量保障流程生成高质量问题与选项。

Result: 实验表明当前多模态模型在需要精确时序定位（T-Loc）与长距离因果推理（CE-Caus）的任务中表现不佳；扩展实验揭示了多模态融合过程中存在信息丢失与处理偏见。

Conclusion: LongInsightBench为评估多模态模型在长视频理解上的能力提供了有效工具，揭示了现有模型在关键推理任务中的不足，并指出了未来应关注多模态融合中的信息保留与均衡处理。

Abstract: We introduce \textbf{LongInsightBench}, the first benchmark designed to
assess models' ability to understand long videos, with a focus on human
language, viewpoints, actions, and other contextual elements, while integrating
\textbf{visual, audio, and text} modalities. Our benchmark excels in three key
areas: \textbf{a) Long-Duration, Information-Dense Videos:} We carefully select
approximately 1,000 videos from open-source datasets FineVideo based on
duration limit and the information density of both visual and audio modalities,
focusing on content like lectures, interviews, and vlogs, which contain rich
language elements. \textbf{b) Diverse and Challenging Task Scenarios:} We have
designed six challenging task scenarios, including both Intra-Event and
Inter-Event Tasks. \textbf{c) Rigorous and Comprehensive Quality Assurance
Pipelines:} We have developed a three-step, semi-automated data quality
assurance pipeline to ensure the difficulty and validity of the synthesized
questions and answer options. Based on LongInsightBench, we designed a series
of experiments. Experimental results shows that Omni-modal models(OLMs) still
face challenge in tasks requiring precise temporal localization (T-Loc) and
long-range causal inference (CE-Caus). Extended experiments reveal the
information loss and processing bias in multi-modal fusion of OLMs. Our dataset
and code is available at
https://anonymous.4open.science/r/LongInsightBench-910F/.

</details>


### [135] [CausalMamba: Scalable Conditional State Space Models for Neural Causal Inference](https://arxiv.org/abs/2510.17318)
*Sangyoon Bae,Jiook Cha*

Main category: cs.CV

TL;DR: CausalMamba 是一种可扩展的fMRI因果推断框架，通过将BOLD信号去卷积与基于Conditional Mamba的因果图推断结合，在模拟和真实数据上显著优于传统方法，能准确识别神经通路并揭示认知任务中的动态网络重构。


<details>
  <summary>Details</summary>
Motivation: 现有的fMRI因果推断方法（如DCM）面临两个主要问题：一是从血流动力学扭曲的BOLD信号中推断神经因果关系本身是不适定问题；二是计算复杂度高，难以扩展。因此需要一种更高效、准确且可扩展的因果推断方法。

Method: 将因果推断分解为两个阶段：首先进行BOLD信号去卷积以恢复潜在神经活动，然后使用新提出的Conditional Mamba架构进行因果图推断。该方法结合了信号恢复与深度神经序列建模的优势，实现高效可扩展的因果分析。

Result: 在模拟数据上，CausalMamba比DCM准确率提高37%；在真实任务态fMRI数据上，以88%的保真度恢复了已知神经通路，而传统方法在99%以上的受试者中无法识别这些通路；在工作记忆数据中，还发现了大脑根据刺激变化在执行网络与显著性网络间切换因果枢纽的动态重组机制。

Conclusion: CausalMamba为fMRI数据分析提供了一种高效、可扩展的因果推断工具，能够揭示大脑基本回路模式与灵活的网络动态，有助于深入理解认知功能的神经机制。

Abstract: We introduce CausalMamba, a scalable framework that addresses fundamental
limitations in fMRI-based causal inference: the ill-posed nature of inferring
neural causality from hemodynamically distorted BOLD signals and the
computational intractability of existing methods like Dynamic Causal Modeling
(DCM). Our approach decomposes this complex inverse problem into two tractable
stages: BOLD deconvolution to recover latent neural activity, followed by
causal graph inference using a novel Conditional Mamba architecture. On
simulated data, CausalMamba achieves 37% higher accuracy than DCM. Critically,
when applied to real task fMRI data, our method recovers well-established
neural pathways with 88% fidelity, whereas conventional approaches fail to
identify these canonical circuits in over 99% of subjects. Furthermore, our
network analysis of working memory data reveals that the brain strategically
shifts its primary causal hub-recruiting executive or salience networks
depending on the stimulus-a sophisticated reconfiguration that remains
undetected by traditional methods. This work provides neuroscientists with a
practical tool for large-scale causal inference that captures both fundamental
circuit motifs and flexible network dynamics underlying cognitive function.

</details>


### [136] [A Single Set of Adversarial Clothes Breaks Multiple Defense Methods in the Physical World](https://arxiv.org/abs/2510.17322)
*Wei Zhang,Zhanhao Hu,Xiao Li,Xiaopei Zhu,Xiaolin Hu*

Main category: cs.CV

TL;DR: 扩大补丁尺寸可使现有防御方法失效，研究表明对抗性衣物能有效攻击多种防御模型，暴露其共同漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法在面对大尺寸物理对抗补丁时可能无效，因此需要评估其对更自然、覆盖范围更大的对抗性衣物的防御能力。

Method: 通过实验评估多种防御方法在数字和物理世界中对抗对抗性衣物的性能，并设计一套能同时打破多个防御模型的对抗性衣物。

Result: 实验表明现有防御方法在数字和物理世界中均表现不佳；设计的对抗性衣物在物理世界中对未防御模型达到96.06%攻击成功率，对九个防御模型均超过64.84%。

Conclusion: 当前的对抗防御方法在面对大而自然的对抗性衣物时普遍脆弱，需开发更鲁棒的防御机制。

Abstract: In recent years, adversarial attacks against deep learning-based object
detectors in the physical world have attracted much attention. To defend
against these attacks, researchers have proposed various defense methods
against adversarial patches, a typical form of physically-realizable attack.
However, our experiments showed that simply enlarging the patch size could make
these defense methods fail. Motivated by this, we evaluated various defense
methods against adversarial clothes which have large coverage over the human
body. Adversarial clothes provide a good test case for adversarial defense
against patch-based attacks because they not only have large sizes but also
look more natural than a large patch on humans. Experiments show that all the
defense methods had poor performance against adversarial clothes in both the
digital world and the physical world. In addition, we crafted a single set of
clothes that broke multiple defense methods on Faster R-CNN. The set achieved
an Attack Success Rate (ASR) of 96.06% against the undefended detector and over
64.84% ASRs against nine defended models in the physical world, unveiling the
common vulnerability of existing adversarial defense methods against
adversarial clothes. Code is available at:
https://github.com/weiz0823/adv-clothes-break-multiple-defenses.

</details>


### [137] [CharDiff: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration](https://arxiv.org/abs/2510.17330)
*Gyuhwan Park,Kihyun Na,Injung Kim*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型并结合字符级引导的车牌图像恢复框架CharDiff，显著提升了严重退化车牌图像的恢复质量与识别准确率。


<details>
  <summary>Details</summary>
Motivation: 车牌图像恢复不仅对车牌识别系统预处理至关重要，还影响证据价值、视觉界面清晰度及图像后续利用，但现有方法在复杂真实场景下表现不足。

Method: 提出CharDiff框架，利用外部分割和针对低质量图像优化的OCR模块提取字符级先验，并设计CHARM模块通过区域掩码实现字符引导注意力，确保每字符的引导仅限其区域，避免干扰。

Result: 实验表明，CharDiff在恢复质量和识别精度上显著优于基线模型，在Roboflow-LP数据集上相较于最佳基线模型相对降低了28%的字符错误率（CER）。

Conclusion: 结构化的字符引导条件能有效增强扩散模型在实际场景中车牌图像恢复与识别的鲁棒性。

Abstract: The significance of license plate image restoration goes beyond the
preprocessing stage of License Plate Recognition (LPR) systems, as it also
serves various purposes, including increasing evidential value, enhancing the
clarity of visual interface, and facilitating further utilization of license
plate images. We propose a novel diffusion-based framework with character-level
guidance, CharDiff, which effectively restores and recognizes severely degraded
license plate images captured under realistic conditions. CharDiff leverages
fine-grained character-level priors extracted through external segmentation and
Optical Character Recognition (OCR) modules tailored for low-quality license
plate images. For precise and focused guidance, CharDiff incorporates a novel
Character-guided Attention through Region-wise Masking (CHARM) module, which
ensures that each character's guidance is restricted to its own region, thereby
avoiding interference with other regions. In experiments, CharDiff
significantly outperformed the baseline restoration models in both restoration
quality and recognition accuracy, achieving a 28% relative reduction in CER on
the Roboflow-LP dataset, compared to the best-performing baseline model. These
results indicate that the structured character-guided conditioning effectively
enhances the robustness of diffusion-based license plate restoration and
recognition in practical deployment scenarios.

</details>


### [138] [iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA](https://arxiv.org/abs/2510.17332)
*Zhaoran Zhao,Xinli Yue,Jianhui Sun,Yuhao Xie,Tao Shao,Liangchao Yao,Fan Xia,Yuetang Deng*

Main category: cs.CV

TL;DR: 提出iDETEX，一个统一的多模态大语言模型，可同时完成质量定位、感知和描述三项图像质量评估任务，在ViDA-UGC基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统图像质量评估多为标量预测，缺乏可解释性和人类对齐的细粒度评价，亟需更详细、可解释的评估方法。

Method: 设计了一个统一的多模态大语言模型iDETEX，结合任务特定的离线增强模块、数据混合策略和在线增强策略，实现对多种异构子任务的高效训练。

Result: 在大规模ViDA-UGC基准上，iDETEX在所有子任务上均达到SOTA性能，并在ICCV MIPI 2025挑战赛中排名第一。

Conclusion: iDETEX能有效实现准确且可解释的图像质量评估，推动IQA向更细粒度、人类对齐的方向发展。

Abstract: Image Quality Assessment (IQA) has progressed from scalar quality prediction
to more interpretable, human-aligned evaluation paradigms. In this work, we
address the emerging challenge of detailed and explainable IQA by proposing
iDETEX-a unified multimodal large language model (MLLM) capable of
simultaneously performing three key tasks: quality grounding, perception, and
description. To facilitate efficient and generalizable training across these
heterogeneous subtasks, we design a suite of task-specific offline augmentation
modules and a data mixing strategy. These are further complemented by online
enhancement strategies to fully exploit multi-sourced supervision. We validate
our approach on the large-scale ViDA-UGC benchmark, where iDETEX achieves
state-of-the-art performance across all subtasks. Our model ranks first in the
ICCV MIPI 2025 Detailed Image Quality Assessment Challenge, demonstrating its
effectiveness and robustness in delivering accurate and interpretable quality
assessments.

</details>


### [139] [Nearest-Class Mean and Logits Agreement for Wildlife Open-Set Recognition](https://arxiv.org/abs/2510.17338)
*Jiahao Huo,Mufhumudzi Muthivhi,Terence L. van Zyl,Fredrik Gustafsson*

Main category: cs.CV

TL;DR: 提出了一种无需重新训练的后处理开集识别方法，通过比较最近类均值（NCM）分布与softmax概率的一致性来检测未知类，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有野生动物分类模型在闭集假设下训练，面对未知类别时仍过度自信；现有开集识别方法常需重新训练模型，限制了实用性。

Method: 提出一种后处理方法，基于输入到最近类均值（NCM）的距离构建概率分布，并与分类头的softmax概率比较，通过两者一致性判断是否为已知类。

Result: 在非洲和瑞典动物数据集上分别达到93.41和95.45的AUROC，性能稳定且排名前三，优于仅在单一数据集表现好的现有方法。

Conclusion: 该方法无需微调即可有效实现开集识别，具有良好的泛化能力和应用潜力。

Abstract: Current state-of-the-art Wildlife classification models are trained under the
closed world setting. When exposed to unknown classes, they remain
overconfident in their predictions. Open-set Recognition (OSR) aims to classify
known classes while rejecting unknown samples. Several OSR methods have been
proposed to model the closed-set distribution by observing the feature, logit,
or softmax probability space. A significant drawback of many existing
approaches is the requirement to retrain the pre-trained classification model
with the OSR-specific strategy. This study contributes a post-processing OSR
method that measures the agreement between the models' features and predicted
logits. We propose a probability distribution based on an input's distance to
its Nearest Class Mean (NCM). The NCM-based distribution is then compared with
the softmax probabilities from the logit space to measure agreement between the
NCM and the classification head. Our proposed strategy ranks within the top
three on two evaluated datasets, showing consistent performance across the two
datasets. In contrast, current state-of-the-art methods excel on a single
dataset. We achieve an AUROC of 93.41 and 95.35 for African and Swedish
animals. The code can be found
https://github.com/Applied-Representation-Learning-Lab/OSR.

</details>


### [140] [Exploring The Missing Semantics In Event Modality](https://arxiv.org/abs/2510.17347)
*Jingqian Wu,Shengpeng Xu,Yunbo Jia,Edmund Y. Lam*

Main category: cs.CV

TL;DR: 提出Semantic-E2VID框架，通过引入跨模态语义对齐和语义感知融合机制，利用SAM的视觉语义提升事件到视频重建质量。


<details>
  <summary>Details</summary>
Motivation: 事件相机仅捕捉亮度变化，缺乏静态场景的语义信息，现有E2V方法在恢复语义方面表现不足，因此需要引入外部语义知识以提升重建效果。

Method: 设计跨模态特征对齐（CFA）模块将SAM的语义特征迁移到事件编码器，并通过语义感知特征融合（SFF）模块整合语义信息；提出语义感知感知监督机制，利用SAM生成的类别标签指导重建。

Result: 在多个基准上显著优于现有E2V方法，提升了重建帧的质量和语义细节恢复能力。

Conclusion: 通过引入外部语义先验并有效融合到事件模态中，可显著改善事件到视频的重建效果，尤其在语义信息恢复方面表现突出。

Abstract: Event cameras offer distinct advantages such as low latency, high dynamic
range, and efficient motion capture. However, event-to-video reconstruction
(E2V), a fundamental event-based vision task, remains challenging, particularly
for reconstructing and recovering semantic information. This is primarily due
to the nature of the event camera, as it only captures intensity changes,
ignoring static objects and backgrounds, resulting in a lack of semantic
information in captured event modality. Further, semantic information plays a
crucial role in video and frame reconstruction, yet is often overlooked by
existing E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V
framework that explores the missing visual semantic knowledge in event modality
and leverages it to enhance event-to-video reconstruction. Specifically,
Semantic-E2VID introduces a cross-modal feature alignment (CFA) module to
transfer the robust visual semantics from a frame-based vision foundation
model, the Segment Anything Model (SAM), to the event encoder, while aligning
the high-level features from distinct modalities. To better utilize the learned
semantic feature, we further propose a semantic-aware feature fusion (SFF)
block to integrate learned semantics in frame modality to form event
representations with rich semantics that can be decoded by the event decoder.
Further, to facilitate the reconstruction of semantic information, we propose a
novel Semantic Perceptual E2V Supervision that helps the model to reconstruct
semantic details by leveraging SAM-generated categorical labels. Extensive
experiments demonstrate that Semantic-E2VID significantly enhances frame
quality, outperforming state-of-the-art E2V methods across multiple benchmarks.
The sample code is included in the supplementary material.

</details>


### [141] [M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception](https://arxiv.org/abs/2510.17363)
*U. V. B. L Udugama,George Vosselman,Francesco Nex*

Main category: cs.CV

TL;DR: 本文提出了一种名为Multi-Mono-Hydra（M2H）的新型多任务学习框架，用于从单目图像中同时进行语义分割、深度估计、边缘检测和表面法线估计，结合窗式跨任务注意力机制和轻量级ViT主干，在保持高效计算的同时提升了多任务性能。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上实现高效的实时空间感知需要能共享任务信息并减少计算开销的多任务模型。传统方法或使用独立模型导致冗余，或共享结构导致任务冲突，因此需要一种能平衡共享与特异性特征的架构。

Method: 提出Multi-Mono-Hydra（M2H）框架，采用基于DINOv2的轻量ViT主干，并引入窗式跨任务注意力模块（Window-Based Cross-Task Attention Module），实现结构化特征交换，既利用任务间互补性又保留任务特异性信息。

Result: 在NYUDv2上优于最先进的多任务模型，在Hypersim上超过单任务深度和语义分割基线，在Cityscapes上表现优异，且在笔记本硬件上保持高效计算，并在真实世界数据中验证了实用性。

Conclusion: M2H在多任务空间感知中实现了精度与效率的良好平衡，适用于边缘设备上的实时单目三维场景理解，为动态环境中的3D场景图构建提供了有效基础。

Abstract: Deploying real-time spatial perception on edge devices requires efficient
multi-task models that leverage complementary task information while minimizing
computational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novel
multi-task learning framework designed for semantic segmentation and depth,
edge, and surface normal estimation from a single monocular image. Unlike
conventional approaches that rely on independent single-task models or shared
encoder-decoder architectures, M2H introduces a Window-Based Cross-Task
Attention Module that enables structured feature exchange while preserving
task-specific details, improving prediction consistency across tasks. Built on
a lightweight ViT-based DINOv2 backbone, M2H is optimized for real-time
deployment and serves as the foundation for monocular spatial perception
systems supporting 3D scene graph construction in dynamic environments.
Comprehensive evaluations show that M2H outperforms state-of-the-art multi-task
models on NYUDv2, surpasses single-task depth and semantic baselines on
Hypersim, and achieves superior performance on the Cityscapes dataset, all
while maintaining computational efficiency on laptop hardware. Beyond
benchmarks, M2H is validated on real-world data, demonstrating its practicality
in spatial perception tasks.

</details>


### [142] [Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs](https://arxiv.org/abs/2510.17364)
*Vaggelis Dorovatas,Soroush Seifi,Gunshi Gupta,Rahaf Aljundi*

Main category: cs.CV

TL;DR: 提出一种无需训练的方法，通过选择关键视觉令牌并结合循环处理与基于字幕的问答，实现高效准确的流视频理解。


<details>
  <summary>Details</summary>
Motivation: 现有Video-LLMs在处理长时间流视频时面临效率与实时响应的挑战，需在有限资源下实现高效且准确的理解。

Method: 采用基于注意力机制的视觉令牌选择，保留重要信息并丢弃约95%的无关令牌；利用循环机制整合历史令牌以实现时间连贯性；结合轻量级的基于字幕的问答生成响应。

Result: 在流视频基准上达到最先进性能，显著提升效率且保持高准确性。

Conclusion: 该方法在不需训练的前提下，有效平衡了流视频理解中的效率与效果，适用于标准Video-LLMs。

Abstract: Video Large Language Models (Video-LLMs) excel at understanding videos
in-context, provided they have full access to the video when answering queries.
However, these models face challenges in streaming scenarios where hour-long
videos must be processed online, and questions need timely responses. In this
work, we propose a training-free approach compatible with standard Video-LLMs,
leveraging three key concepts: 1) LLM-informed selection of visual tokens to
identify those that the LLM has attended to and contributed to its
understanding of each short clip. Our attention-based selection allows us to
discard up to ~95% of unimportant visual tokens with minimal performance loss;
2) Recurrent processing of past selected tokens to generate temporally coherent
understanding of each processed clip; 3) Caption-based question answering for
lightweight and accurate responses. Our method achieves state-of-the-art
performance on streaming video benchmarks, striking a balance between
efficiency and effectiveness.

</details>


### [143] [Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition Performance without Privacy Compromise](https://arxiv.org/abs/2510.17372)
*Paweł Borsukiewicz,Fadi Boutros,Iyiola E. Olatunji,Charles Beumier,Wendkûuni C. Ouedraogo,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CV

TL;DR: 合成面部数据在面部识别研究中展现出替代真实数据集的科学可行性和伦理必要性，高性能合成数据集在准确率上已超越真实数据集。


<details>
  <summary>Details</summary>
Motivation: 解决面部识别系统因使用无授权真实面部数据引发的伦理和法律问题，探索合成面部数据作为隐私保护替代方案的可行性。

Method: 系统性文献综述识别25个合成面部数据集，并基于7个关键要求对超过1000万合成样本进行实验验证，比较多个标准基准上的性能。

Result: 最佳合成数据集VariFace和VIGFace的识别准确率分别为95.67%和94.91%，超过CASIA-WebFace等真实数据集；公开可用的Vec2Face和CemiFace也接近该性能；合成数据能有效控制偏差并保持类内变异性和身份可分性。

Conclusion: 合成面部数据是面部识别研究中科学有效且伦理上更可取的替代方案，具备大规模应用潜力。

Abstract: The deployment of facial recognition systems has created an ethical dilemma:
achieving high accuracy requires massive datasets of real faces collected
without consent, leading to dataset retractions and potential legal liabilities
under regulations like GDPR. While synthetic facial data presents a promising
privacy-preserving alternative, the field lacks comprehensive empirical
evidence of its viability. This study addresses this critical gap through
extensive evaluation of synthetic facial recognition datasets. We present a
systematic literature review identifying 25 synthetic facial recognition
datasets (2018-2025), combined with rigorous experimental validation. Our
methodology examines seven key requirements for privacy-preserving synthetic
data: identity leakage prevention, intra-class variability, identity
separability, dataset scale, ethical data sourcing, bias mitigation, and
benchmark reliability. Through experiments involving over 10 million synthetic
samples, extended by a comparison of results reported on five standard
benchmarks, we provide the first comprehensive empirical assessment of
synthetic data's capability to replace real datasets. Best-performing synthetic
datasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and
94.91% respectively, surpassing established real datasets including
CASIA-WebFace (94.70%). While those images remain private, publicly available
alternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our
findings reveal that they ensure proper intra-class variability while
maintaining identity separability. Demographic bias analysis shows that, even
though synthetic data inherits limited biases, it offers unprecedented control
for bias mitigation through generation parameters. These results establish
synthetic facial data as a scientifically viable and ethically imperative
alternative for facial recognition research.

</details>


### [144] [Facial Expression-based Parkinson's Disease Severity Diagnosis via Feature Fusion and Adaptive Class Balancing](https://arxiv.org/abs/2510.17373)
*Yintao Zhou,Wei Huang,Zhengyu Li,Jing Huang,Meng Pang*

Main category: cs.CV

TL;DR: 提出了一种基于多表情特征融合和自适应类别平衡的帕金森病（PD）严重程度诊断新方法，通过注意力机制融合多种面部表情特征，并动态调整类别不平衡问题，提升了诊断性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于面部表情的PD诊断方法通常依赖单一表情类型，忽视了不同PD阶段的类别不平衡问题，且多集中于二分类任务，难以准确诊断PD严重程度。

Method: 提出一种注意力机制驱动的多表情特征融合方法，并引入自适应类别平衡策略，根据类别分布和分类难度动态调整样本权重，以提升多类别PD严重程度分类性能。

Result: 实验结果表明，所提方法在PD严重程度诊断上表现优异，注意力机制和自适应类别平衡策略均有效提升了模型性能。

Conclusion: 融合多表情特征并解决类别不平衡问题有助于提高PD严重程度诊断的准确性，为无创、低成本的PD筛查提供了有效方案。

Abstract: Parkinson's disease (PD) severity diagnosis is crucial for early detecting
potential patients and adopting tailored interventions. Diagnosing PD based on
facial expression is grounded in PD patients' "masked face" symptom and gains
growing interest recently for its convenience and affordability. However,
current facial expression-based approaches often rely on single type of
expression which can lead to misdiagnosis, and ignore the class imbalance
across different PD stages which degrades the prediction performance. Moreover,
most existing methods focus on binary classification (i.e., PD / non-PD) rather
than diagnosing the severity of PD. To address these issues, we propose a new
facial expression-based method for PD severity diagnosis which integrates
multiple facial expression features through attention-based feature fusion.
Moreover, we mitigate the class imbalance problem via an adaptive class
balancing strategy which dynamically adjusts the contribution of training
samples based on their class distribution and classification difficulty.
Experimental results demonstrate the promising performance of the proposed
method for PD severity diagnosis, as well as the efficacy of attention-based
feature fusion and adaptive class balancing.

</details>


### [145] [Closed-Loop Transfer for Weakly-supervised Affordance Grounding](https://arxiv.org/abs/2510.17384)
*Jiajin Tang,Zhengxuan Wei,Ge Zheng,Sibei Yang*

Main category: cs.CV

TL;DR: 提出了一种名为LoopTrans的新框架，通过双向知识迁移和跨模态定位机制，显著提升了弱监督感知接地的性能，尤其在复杂遮挡场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅从外部视角图像单向迁移到第一人称视角，限制了复杂交互场景下的适用性，因此需要一种能够双向增强知识提取的框架。

Method: 提出LoopTrans框架，包含统一的跨模态定位和去噪知识蒸馏机制，实现外部视角与第一人称视角之间的闭环知识互传。

Result: 在图像和视频基准上所有指标均取得一致提升，特别是在人体完全遮挡交互区域的挑战性场景中表现良好。

Conclusion: LoopTrans通过闭环知识迁移和创新机制有效弥合了不同视角间的域差距，提升了弱监督感知接地的鲁棒性和泛化能力。

Abstract: Humans can perform previously unexperienced interactions with novel objects
simply by observing others engage with them. Weakly-supervised affordance
grounding mimics this process by learning to locate object regions that enable
actions on egocentric images, using exocentric interaction images with
image-level annotations. However, extracting affordance knowledge solely from
exocentric images and transferring it one-way to egocentric images limits the
applicability of previous works in complex interaction scenarios. Instead, this
study introduces LoopTrans, a novel closed-loop framework that not only
transfers knowledge from exocentric to egocentric but also transfers back to
enhance exocentric knowledge extraction. Within LoopTrans, several innovative
mechanisms are introduced, including unified cross-modal localization and
denoising knowledge distillation, to bridge domain gaps between object-centered
egocentric and interaction-centered exocentric images while enhancing knowledge
transfer. Experiments show that LoopTrans achieves consistent improvements
across all metrics on image and video benchmarks, even handling challenging
scenarios where object interaction regions are fully occluded by the human
body.

</details>


### [146] [Monitoring Horses in Stalls: From Object to Event Detection](https://arxiv.org/abs/2510.17409)
*Dmitrii Galimzianov,Viacheslav Vyshegorodtsev,Ivan Nezhivykh*

Main category: cs.CV

TL;DR: 提出一种基于视觉的马匹行为监测系统，利用YOLOv11和BoT-SORT实现马和人的检测与跟踪，并通过轨迹和空间关系推断事件状态，系统在马相关事件中表现良好。


<details>
  <summary>Details</summary>
Motivation: 马匹在 stall 中的行为监测对早期发现健康和福利问题至关重要，但传统方式费时费力，亟需自动化解决方案。

Method: 构建了一个基于YOLOv11和BoT-SORT的物体检测与多目标跟踪系统，结合目标轨迹和空间关系推断五类事件状态；利用CLIP和GroundingDINO辅助构建并标注自定义数据集，并考虑摄像头盲区。

Result: 定性评估显示系统对马相关事件检测可靠，但因数据不足，对人的检测仍有局限。

Conclusion: 该系统为马厩环境下的实时行为监测提供了可行方案，有助于提升马匹福利和马厩管理效率。

Abstract: Monitoring the behavior of stalled horses is essential for early detection of
health and welfare issues but remains labor-intensive and time-consuming. In
this study, we present a prototype vision-based monitoring system that
automates the detection and tracking of horses and people inside stables using
object detection and multi-object tracking techniques. The system leverages
YOLOv11 and BoT-SORT for detection and tracking, while event states are
inferred based on object trajectories and spatial relations within the stall.
To support development, we constructed a custom dataset annotated with
assistance from foundation models CLIP and GroundingDINO. The system
distinguishes between five event types and accounts for the camera's blind
spots. Qualitative evaluation demonstrated reliable performance for
horse-related events, while highlighting limitations in detecting people due to
data scarcity. This work provides a foundation for real-time behavioral
monitoring in equine facilities, with implications for animal welfare and
stable management.

</details>


### [147] [DeepDetect: Learning All-in-One Dense Keypoints](https://arxiv.org/abs/2510.17422)
*Shaharyar Ahmed Khan Tareen,Filza Khan Tareen*

Main category: cs.CV

TL;DR: DeepDetect是一种基于深度学习的密集关键点检测器，通过融合多种传统检测器输出生成真值掩码，并利用轻量级ESPNet模型训练，实现高密度、高重复性和强适应性的关键点检测，在关键点密度、重复性和正确匹配数上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统关键点检测器和基于学习的方法在光度变化敏感、关键点密度低、重复性差、场景适应性有限及缺乏语义理解方面存在局限，难以有效识别重要视觉区域。因此，需要一种更智能、鲁棒且密集的关键点检测方法。

Method: 提出DeepDetect方法：首先融合7种关键点检测器和2种边缘检测器的输出生成真值掩码，以捕捉多样的视觉线索（如角点、斑点、显著边缘和纹理）；然后使用轻量高效的ESPNet模型以这些掩码为标签进行训练，使模型能语义聚焦于图像内容并生成密集关键点。

Result: 在Oxford Affine Covariant Regions数据集上的实验表明，DeepDetect在平均关键点密度（0.5143）、平均重复性（0.9582）和正确匹配数量（59,003）方面均优于其他检测器。

Conclusion: DeepDetect通过融合多源检测结果并结合深度学习，实现了高性能的密集关键点检测，具有良好的语义感知能力和对复杂与退化视觉条件的适应性，适用于多种计算机视觉任务。

Abstract: Keypoint detection is the foundation of many computer vision tasks, including
image registration, structure-from motion, 3D reconstruction, visual odometry,
and SLAM. Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning
based methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong
performance yet suffer from key limitations: sensitivity to photometric
changes, low keypoint density and repeatability, limited adaptability to
challenging scenes, and lack of semantic understanding, often failing to
prioritize visually important regions. We present DeepDetect, an intelligent,
all-in-one, dense keypoint detector that unifies the strengths of classical
detectors using deep learning. Firstly, we create ground-truth masks by fusing
outputs of 7 keypoint and 2 edge detectors, extracting diverse visual cues from
corners and blobs to prominent edges and textures in the images. Afterwards, a
lightweight and efficient model: ESPNet, is trained using these masks as
labels, enabling DeepDetect to focus semantically on images while producing
highly dense keypoints, that are adaptable to diverse and visually degraded
conditions. Evaluations on the Oxford Affine Covariant Regions dataset
demonstrate that DeepDetect surpasses other detectors in keypoint density,
repeatability, and the number of correct matches, achieving maximum values of
0.5143 (average keypoint density), 0.9582 (average repeatability), and 59,003
(correct matches).

</details>


### [148] [Leveraging AV1 motion vectors for Fast and Dense Feature Matching](https://arxiv.org/abs/2510.17434)
*Julien Zouein,Hossein Javidnia,François Pitié,Anil Kokaram*

Main category: cs.CV

TL;DR: 利用AV1运动矢量生成密集亚像素级对应关系和短轨迹，相比传统方法更高效且匹配更密集，适用于资源受限的SfM前端处理。


<details>
  <summary>Details</summary>
Motivation: 探索在压缩域中直接提取特征对应关系的可能性，以减少计算开销并提高匹配效率，特别是在资源受限环境下实现高效运动恢复结构（SfM）。

Method: 重用AV1编码中的运动矢量，生成密集的亚像素级对应点，并通过余弦一致性过滤短轨迹，作为SfM流程的前端处理模块。

Result: 在117帧视频片段上的小规模SfM实验中，MV匹配成功注册所有图像，重建出0.46-0.62百万个点，重投影误差为0.51-0.53像素；BA优化时间随匹配密度增长；该方法运行速度接近SIFT序列但CPU使用更少。

Conclusion: 压缩域对应关系是一种实用且资源高效的前端方案，具有良好的可扩展性，可集成到完整SfM流程中。

Abstract: We repurpose AV1 motion vectors to produce dense sub-pixel correspondences
and short tracks filtered by cosine consistency. On short videos, this
compressed-domain front end runs comparably to sequential SIFT while using far
less CPU, and yields denser matches with competitive pairwise geometry. As a
small SfM demo on a 117-frame clip, MV matches register all images and
reconstruct 0.46-0.62M points at 0.51-0.53,px reprojection error; BA time grows
with match density. These results show compressed-domain correspondences are a
practical, resource-efficient front end with clear paths to scaling in full
pipelines.

</details>


### [149] [Rethinking Nighttime Image Deraining via Learnable Color Space Transformation](https://arxiv.org/abs/2510.17440)
*Qiyuan Guan,Xiang Chen,Guiyue Jin,Jiyu Jin,Shumin Fan,Tianyu Song,Jinshan Pan*

Main category: cs.CV

TL;DR: 本文提出了一种新的夜间图像去雨方法，包括一个高质量的夜间去雨数据集HQ-NightRain和一个颜色空间变换网络CST-Net，通过在Y通道中处理雨滴并引入隐式光照引导来提高去雨效果。


<details>
  <summary>Details</summary>
Motivation: 夜间图像去雨相较于白天图像去雨面临更多挑战，主要是因为夜间场景本身的复杂性和缺乏能够准确反映雨与光照耦合效应的高质量数据集。

Method: 提出了一个可学习的颜色空间转换器（CSC），以更好地在Y通道中促进雨滴去除，并引入了隐式光照引导来捕捉光照信息，指导夜间去雨过程。

Result: 大量实验表明，所提数据集的价值和方法的有效性，新方法在去除夜间复杂雨滴方面表现优异。

Conclusion: CST-Net结合HQ-NightRain数据集为夜间图像去雨提供了一个有效的解决方案，显著提高了夜间图像的质量和清晰度。

Abstract: Compared to daytime image deraining, nighttime image deraining poses
significant challenges due to inherent complexities of nighttime scenarios and
the lack of high-quality datasets that accurately represent the coupling effect
between rain and illumination. In this paper, we rethink the task of nighttime
image deraining and contribute a new high-quality benchmark, HQ-NightRain,
which offers higher harmony and realism compared to existing datasets. In
addition, we develop an effective Color Space Transformation Network (CST-Net)
for better removing complex rain from nighttime scenes. Specifically, we
propose a learnable color space converter (CSC) to better facilitate rain
removal in the Y channel, as nighttime rain is more pronounced in the Y channel
compared to the RGB color space. To capture illumination information for
guiding nighttime deraining, implicit illumination guidance is introduced
enabling the learned features to improve the model's robustness in complex
scenarios. Extensive experiments show the value of our dataset and the
effectiveness of our method. The source code and datasets are available at
https://github.com/guanqiyuan/CST-Net.

</details>


### [150] [Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS](https://arxiv.org/abs/2510.17479)
*Feng Zhou,Wenkai Guo,Pu Cao,Zhicheng Zhang,Jianqin Yin*

Main category: cs.CV

TL;DR: 本文提出一种针对稀疏视角3D Gaussian Splatting（3DGS）的强初始化方法，通过改进SfM、引入自初始化和点云正则化，显著提升新视角渲染质量。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角下3DGS容易过拟合训练视角，导致新视角渲染出现模糊等伪影；现有方法依赖SfM初始化或训练正则化，但本文发现初始化是决定性能的关键因素。

Method: 提出三部分方法：(i) 频率感知SfM，通过低频视图增强和宽松对应关系提升低纹理区域覆盖；(ii) 3DGS自初始化，利用光度监督生成额外高斯中心补偿SfM稀疏区域；(iii) 点云正则化，通过几何/可见性先验保证多视角一致性和均匀空间分布。

Result: 在LLFF和Mip-NeRF360数据集上，本方法在稀疏视角设置下 consistently 提升渲染性能，优于现有初始化策略。

Conclusion: 初始化在稀疏视角3DGS中起决定性作用，本文提出的初始化策略有效改善点云质量，为后续优化提供更好起点。

Abstract: Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training
views, leading to artifacts like blurring in novel view rendering. Prior work
addresses it either by enhancing the initialization (\emph{i.e.}, the point
cloud from Structure-from-Motion (SfM)) or by adding training-time constraints
(regularization) to the 3DGS optimization. Yet our controlled ablations reveal
that initialization is the decisive factor: it determines the attainable
performance band in sparse-view 3DGS, while training-time constraints yield
only modest within-band improvements at extra cost. Given initialization's
primacy, we focus our design there. Although SfM performs poorly under sparse
views due to its reliance on feature matching, it still provides reliable seed
points. Thus, building on SfM, our effort aims to supplement the regions it
fails to cover as comprehensively as possible. Specifically, we design: (i)
frequency-aware SfM that improves low-texture coverage via low-frequency view
augmentation and relaxed multi-view correspondences; (ii) 3DGS
self-initialization that lifts photometric supervision into additional points,
compensating SfM-sparse regions with learned Gaussian centers; and (iii)
point-cloud regularization that enforces multi-view consistency and uniform
spatial coverage through simple geometric/visibility priors, yielding a clean
and reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrate
consistent gains in sparse-view settings, establishing our approach as a
stronger initialization strategy. Code is available at
https://github.com/zss171999645/ItG-GS.

</details>


### [151] [SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries](https://arxiv.org/abs/2510.17482)
*Chenxu Dang,Haiyan Liu,Guangjun Bao,Pei An,Xinyue Tang,Jie Ma,Bingchuan Sun,Yan Wang*

Main category: cs.CV

TL;DR: 提出SparseWorld，一种基于稀疏动态查询的4D占据世界模型，通过范围自适应感知和状态条件预测模块，实现灵活、自适应且高效的场景理解和预测。


<details>
  <summary>Details</summary>
Motivation: 现有占据世界模型依赖静态固定网格或嵌入，感知灵活性受限，且基于网格的即时分类难以匹配现实场景的动态连续性。

Method: 设计SparseWorld，包含范围自适应感知模块（查询随自车状态调节并融合时空关联）和状态条件预测模块（用回归引导替代分类预测），并采用时序感知自调度训练策略。

Result: 在感知、预测和规划任务上取得SOTA性能，可视化和消融实验验证了其在灵活性、适应性和效率方面的优势。

Conclusion: SparseWorld通过稀疏动态查询和回归引导的4D建模，有效提升了世界模型的动态表征能力和实际应用潜力。

Abstract: Semantic occupancy has emerged as a powerful representation in world models
for its ability to capture rich spatial semantics. However, most existing
occupancy world models rely on static and fixed embeddings or grids, which
inherently limit the flexibility of perception. Moreover, their ``in-place
classification" over grids exhibits a potential misalignment with the dynamic
and continuous nature of real scenarios.In this paper, we propose SparseWorld,
a novel 4D occupancy world model that is flexible, adaptive, and efficient,
powered by sparse and dynamic queries. We propose a Range-Adaptive Perception
module, in which learnable queries are modulated by the ego vehicle states and
enriched with temporal-spatial associations to enable extended-range
perception. To effectively capture the dynamics of the scene, we design a
State-Conditioned Forecasting module, which replaces classification-based
forecasting with regression-guided formulation, precisely aligning the dynamic
queries with the continuity of the 4D environment. In addition, We specifically
devise a Temporal-Aware Self-Scheduling training strategy to enable smooth and
efficient training. Extensive experiments demonstrate that SparseWorld achieves
state-of-the-art performance across perception, forecasting, and planning
tasks. Comprehensive visualizations and ablation studies further validate the
advantages of SparseWorld in terms of flexibility, adaptability, and
efficiency. The code is available at https://github.com/MSunDYY/SparseWorld.

</details>


### [152] [Split-Fuse-Transport: Annotation-Free Saliency via Dual Clustering and Optimal Transport Alignment](https://arxiv.org/abs/2510.17484)
*Muhammad Umer Ramzan,Ali Zia,Abdelwahed Khamis,Noman Ali,Usman Ali,Wei Xiang*

Main category: cs.CV

TL;DR: 本文提出了一种名为AutoSOD的端到端无监督显著性目标检测（SOD）方法，通过改进原型最优传输（POTNet）生成高质量伪掩码，在无像素级标注的情况下接近监督方法性能。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督SOD方法受限于伪掩码质量，难以达到接近监督学习的精度；作者认为在获得可靠伪掩码的前提下，无监督SOD可显著提升性能。

Method: 提出POTNet，采用熵引导的双聚类头：高熵像素使用谱聚类，低熵像素使用k-means聚类，并通过最优传输对齐两类原型；利用该方法生成清晰且具有部件感知的伪掩码，用于监督MaskFormer风格的编码器-解码器网络，构建端到端的AutoSOD框架。

Result: 在五个基准上，AutoSOD比现有无监督方法F-measure最高提升26%，弱监督方法最高提升36%，显著缩小了与全监督模型之间的差距。

Conclusion: 通过改进原型学习与最优传输机制，AutoSOD实现了高质量伪掩码的生成，推动了无监督显著性目标检测向实用化迈进。

Abstract: Salient object detection (SOD) aims to segment visually prominent regions in
images and serves as a foundational task for various computer vision
applications. We posit that SOD can now reach near-supervised accuracy without
a single pixel-level label, but only when reliable pseudo-masks are available.
We revisit the prototype-based line of work and make two key observations.
First, boundary pixels and interior pixels obey markedly different geometry;
second, the global consistency enforced by optimal transport (OT) is
underutilized if prototype quality is weak. To address this, we introduce
POTNet, an adaptation of Prototypical Optimal Transport that replaces POT's
single k-means step with an entropy-guided dual-clustering head: high-entropy
pixels are organized by spectral clustering, low-entropy pixels by k-means, and
the two prototype sets are subsequently aligned by OT. This
split-fuse-transport design yields sharper, part-aware pseudo-masks in a single
forward pass, without handcrafted priors. Those masks supervise a standard
MaskFormer-style encoder-decoder, giving rise to AutoSOD, an end-to-end
unsupervised SOD pipeline that eliminates SelfMask's offline voting yet
improves both accuracy and training efficiency. Extensive experiments on five
benchmarks show that AutoSOD outperforms unsupervised methods by up to 26% and
weakly supervised methods by up to 36% in F-measure, further narrowing the gap
to fully supervised models.

</details>


### [153] [Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization](https://arxiv.org/abs/2510.17501)
*Yuanli Wu,Long Zhang,Yue Du,Bin Li*

Main category: cs.CV

TL;DR: 提出一种基于评分标准引导的伪标签提示框架，实现无需训练的视频摘要，兼顾局部显著性和全局连贯性，在SumMe和TVSum上超越无监督和零样本基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频摘要方法要么依赖大量标注数据导致标注成本高，要么无监督方法难以捕捉高层语义和叙事细节；零样本方法对提示模板和数据集敏感，泛化能力差。

Method: 利用少量真实标注生成高置信度伪标签，构建结构化、数据集自适应的评分标准；推理时结合首尾段描述与中间段的上下文摘要，通过上下文提示使大语言模型在无需微调的情况下评估场景重要性。

Result: 在SumMe和TVSum数据集上分别取得57.58和63.05的F1分数，优于无监督和先前零样本方法，接近有监督方法性能。

Conclusion: 评分标准引导的伪标签框架有效提升了大语言模型在视频摘要中的稳定性与可解释性，建立了一种通用的零样本视频摘要范式。

Abstract: With the rapid proliferation of video content across social media,
surveillance, and education platforms, efficiently summarizing long videos into
concise yet semantically faithful surrogates has become increasingly vital.
Existing supervised methods achieve strong in-domain accuracy by learning from
dense annotations but suffer from high labeling costs and limited cross-dataset
generalization, while unsupervised approaches, though label-free, often fail to
capture high-level human semantics and fine-grained narrative cues. More
recently, zero-shot prompting pipelines have leveraged large language models
(LLMs) for training-free video summarization, yet remain highly sensitive to
handcrafted prompt templates and dataset-specific score normalization. To
overcome these limitations, we introduce a rubric-guided, pseudo-labeled
prompting framework that transforms a small subset of ground-truth annotations
into high-confidence pseudo labels, which are aggregated into structured,
dataset-adaptive scoring rubrics guiding interpretable scene evaluation. During
inference, first and last segments are scored based solely on their
descriptions, whereas intermediate ones incorporate brief contextual summaries
of adjacent scenes to assess narrative progression and redundancy. This
contextual prompting enables the LLM to balance local salience and global
coherence without parameter tuning. On SumMe and TVSum, our method achieves F1
scores of \textbf{57.58} and \textbf{63.05}, surpassing unsupervised and prior
zero-shot baselines while approaching supervised performance. The results
demonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based
scoring and establishes a general, interpretable zero-shot paradigm for video
summarization.

</details>


### [154] [MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models](https://arxiv.org/abs/2510.17519)
*Yongshun Zhang,Zhongyi Fan,Yonghang Zhang,Zhangzikang Li,Weifeng Chen,Zhongwei Feng,Chaoyue Wang,Peng Hou,Anxiang Zeng*

Main category: cs.CV

TL;DR: 本文提出了一种针对大规模视频生成模型的训练框架，优化了数据处理、模型架构、训练策略和基础设施，显著提升了训练效率和性能。其模型MUG-V 10B在电商视频生成任务上超越现有开源模型，并首次开源了基于Megatron-Core的高效训练代码和推理流程。


<details>
  <summary>Details</summary>
Motivation: 由于跨模态对齐、长序列和复杂时空依赖等问题，训练大规模视频生成模型面临巨大挑战，现有方法效率低、资源消耗大，亟需系统性优化方案。

Method: 从数据处理、模型架构、训练策略和基础设施四个方面进行优化，包括高效的数据预处理、视频压缩、参数扩展、课程式预训练和对齐导向的后训练，并基于Megatron-Core构建训练系统。

Result: MUG-V 10B模型在整体性能上达到当前最优水平，在电商视频生成任务上超越主流开源模型；训练系统实现了高效率和近线性多节点扩展，并完整开源了模型权重、训练代码和推理管道。

Conclusion: 该框架有效解决了大规模视频生成模型训练的效率与性能瓶颈，其开源举措为后续研究提供了重要基础和工具支持。

Abstract: In recent years, large-scale generative models for visual content
(\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable
progress. However, training large-scale video generation models remains
particularly challenging and resource-intensive due to cross-modal text-video
alignment, the long sequences involved, and the complex spatiotemporal
dependencies. To address these challenges, we present a training framework that
optimizes four pillars: (i) data processing, (ii) model architecture, (iii)
training strategy, and (iv) infrastructure for large-scale video generation
models. These optimizations delivered significant efficiency gains and
performance improvements across all stages of data preprocessing, video
compression, parameter scaling, curriculum-based pretraining, and
alignment-focused post-training. Our resulting model, MUG-V 10B, matches recent
state-of-the-art video generators overall and, on e-commerce-oriented video
generation tasks, surpasses leading open-source baselines in human evaluations.
More importantly, we open-source the complete stack, including model weights,
Megatron-Core-based large-scale training code, and inference pipelines for
video generation and enhancement. To our knowledge, this is the first public
release of large-scale video generation training code that exploits
Megatron-Core to achieve high training efficiency and near-linear multi-node
scaling, details are available in
\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.

</details>


### [155] [MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for Longitudinal MRI Segmentation](https://arxiv.org/abs/2510.17529)
*Yovin Yahathugoda,Davide Prezzi,Piyalitt Ittichaiwong,Vicky Goh,Sebastien Ourselin,Michela Antonelli*

Main category: cs.CV

TL;DR: 提出MambaX-Net，一种半监督双扫描3D分割架构，用于主动监测前列腺癌的纵向MRI分析，利用先前时间点的信息提升分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型多基于单时间点且依赖专家标注的数据训练，难以适应主动监测中多时间点和标注稀缺的纵向分析需求。

Method: 设计MambaX-Net，包含Mamba增强交叉注意力模块和形状提取模块，结合半监督自训练策略，利用nnU-Net生成的伪标签进行训练，并利用前一时间点的MRI与分割结果优化当前时间点的分割。

Result: 在纵向主动监测数据集上，MambaX-Net显著优于现有的U-Net和Transformer模型，即使在少量且含噪声的训练数据下仍实现更优的前列腺区域分割。

Conclusion: MambaX-Net有效解决了主动监测中多时间点和标注稀缺的挑战，为前列腺癌的自动化纵向分析提供了高效且鲁棒的分割方案。

Abstract: Active Surveillance (AS) is a treatment option for managing low and
intermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while
monitoring disease progression through serial MRI and clinical follow-up.
Accurate prostate segmentation is an important preliminary step for automating
this process, enabling automated detection and diagnosis of PCa. However,
existing deep-learning segmentation models are often trained on
single-time-point and expertly annotated datasets, making them unsuitable for
longitudinal AS analysis, where multiple time points and a scarcity of expert
labels hinder their effective fine-tuning. To address these challenges, we
propose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation
architecture that computes the segmentation for time point t by leveraging the
MRI and the corresponding segmentation mask from the previous time point. We
introduce two new components: (i) a Mamba-enhanced Cross-Attention Module,
which integrates the Mamba block into cross attention to efficiently capture
temporal evolution and long-range spatial dependencies, and (ii) a Shape
Extractor Module that encodes the previous segmentation mask into a latent
anatomical representation for refined zone delination. Moreover, we introduce a
semi-supervised self-training strategy that leverages pseudo-labels generated
from a pre-trained nnU-Net, enabling effective learning without expert
annotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results
showed that it significantly outperforms state-of-the-art U-Net and
Transformer-based models, achieving superior prostate zone segmentation even
when trained on limited and noisy data.

</details>


### [156] [WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection](https://arxiv.org/abs/2510.17566)
*Nachuan Ma,Zhengfei Song,Qiang Hu,Xiaoyu Tang,Chengxi Zhang,Rui Fan,Lihua Xie*

Main category: cs.CV

TL;DR: 提出WP-CrackNet，一种仅需图像级标签的端到端弱监督道路裂缝检测方法，通过类激活图、重构器与检测器的协同学习及PAAM和CECCM模块提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 减少对昂贵像素级标注的依赖，实现高效、可扩展的道路裂缝检测，适应智能城市基础设施维护需求。

Method: 提出WP-CrackNet，包含分类器、重构器和检测器，采用对抗训练和伪标签学习；引入路径感知注意力模块（PAAM）融合高低层特征，中心增强CAM一致性模块（CECCM）优化CAM生成。

Result: 在自制的三个图像级标注数据集上实验表明，WP-CrackNet性能媲美全监督方法，显著优于现有弱监督方法。

Conclusion: WP-CrackNet有效实现了高质量像素级裂缝检测，推动了弱监督学习在道路检测中的应用，具备良好的实际部署潜力。

Abstract: Road crack detection is essential for intelligent infrastructure maintenance
in smart cities. To reduce reliance on costly pixel-level annotations, we
propose WP-CrackNet, an end-to-end weakly-supervised method that trains with
only image-level labels for pixel-wise crack detection. WP-CrackNet integrates
three components: a classifier generating class activation maps (CAMs), a
reconstructor measuring feature inferability, and a detector producing
pixel-wise road crack detection results. During training, the classifier and
reconstructor alternate in adversarial learning to encourage crack CAMs to
cover complete crack regions, while the detector learns from pseudo labels
derived from post-processed crack CAMs. This mutual feedback among the three
components improves learning stability and detection accuracy. To further boost
detection performance, we design a path-aware attention module (PAAM) that
fuses high-level semantics from the classifier with low-level structural cues
from the reconstructor by modeling spatial and channel-wise dependencies.
Additionally, a center-enhanced CAM consistency module (CECCM) is proposed to
refine crack CAMs using center Gaussian weighting and consistency constraints,
enabling better pseudo-label generation. We create three image-level datasets
and extensive experiments show that WP-CrackNet achieves comparable results to
supervised methods and outperforms existing weakly-supervised methods,
significantly advancing scalable road inspection. The source code package and
datasets are available at https://mias.group/WP-CrackNet/.

</details>


### [157] [PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception](https://arxiv.org/abs/2510.17568)
*Kaichen Zhou,Yuhan Wang,Grace Chen,Xinhai Chang,Gaspard Beaudouin,Fangneng Zhan,Paul Pu Liang,Mengyu Wang*

Main category: cs.CV

TL;DR: PAGE-4D是扩展自VGGT的前馈模型，用于动态场景的4D重建，能同时实现相机位姿估计、深度预测和点云重建，通过动态感知聚合器解决了多任务间的冲突。


<details>
  <summary>Details</summary>
Motivation: 现有3D前馈模型在静态场景中表现良好，但在包含动态元素（如移动人体或可变形物体）的真实场景中性能下降，因此需要能处理动态场景的模型。

Method: 提出PAGE-4D模型，扩展VGGT以处理动态场景；设计动态感知聚合器，通过动态感知掩码分离静态与动态信息，在位姿估计中抑制动态区域，在几何重建中增强动态建模。

Result: 在动态场景下，PAGE-4D在相机位姿估计、单目与视频深度估计、稠密点云重建任务上均优于原始VGGT。

Conclusion: PAGE-4D通过任务冲突解耦的机制，在无需后处理的情况下实现了对动态场景的有效4D重建，提升了多任务性能。

Abstract: Recent 3D feed-forward models, such as the Visual Geometry Grounded
Transformer (VGGT), have shown strong capability in inferring 3D attributes of
static scenes. However, since they are typically trained on static datasets,
these models often struggle in real-world scenarios involving complex dynamic
elements, such as moving humans or deformable objects like umbrellas. To
address this limitation, we introduce PAGE-4D, a feedforward model that extends
VGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and
point cloud reconstruction -- all without post-processing. A central challenge
in multi-task 4D reconstruction is the inherent conflict between tasks:
accurate camera pose estimation requires suppressing dynamic regions, while
geometry reconstruction requires modeling them. To resolve this tension, we
propose a dynamics-aware aggregator that disentangles static and dynamic
information by predicting a dynamics-aware mask -- suppressing motion cues for
pose estimation while amplifying them for geometry reconstruction. Extensive
experiments show that PAGE-4D consistently outperforms the original VGGT in
dynamic scenarios, achieving superior results in camera pose estimation,
monocular and video depth estimation, and dense point map reconstruction.

</details>


### [158] [Expose Camouflage in the Water: Underwater Camouflaged Instance Segmentation and Dataset](https://arxiv.org/abs/2510.17585)
*Chuhong Wang,Hua Li,Chongyi Li,Huazhong Liu,Xiongxin Tang,Sam Kwong*

Main category: cs.CV

TL;DR: 提出首个水下伪装实例分割数据集UCIS4K和基于SAM的UCIS-SAM网络，包含CBOM、FDTIM和MFFAM三个模块，显著提升水下伪装物体的分割性能。


<details>
  <summary>Details</summary>
Motivation: 传统伪装实例分割方法在以陆地为主的数据集上训练，缺乏对水下环境的适应性，导致在复杂水下环境中分割性能不佳，亟需专门的水下数据集和针对性模型。

Method: 构建了包含3953张标注图像的UCIS4K数据集；提出UCIS-SAM网络，包含通道平衡优化模块（CBOM）、频域真融合模块（FDTIM）和多尺度特征频域聚合模块（MFFAM），分别增强水下特征学习、抑制伪装干扰、强化低对比度边界。

Result: 在UCIS4K和公开基准上实验表明，所提UCIS-SAM优于现有最先进方法，显著提升水下伪装实例的分割精度。

Conclusion: UCIS4K数据集和UCIS-SAM模型有效推动了水下伪装实例分割的发展，所提模块有效应对水下环境的色彩失真、低对比度和伪装挑战。

Abstract: With the development of underwater exploration and marine protection,
underwater vision tasks are widespread. Due to the degraded underwater
environment, characterized by color distortion, low contrast, and blurring,
camouflaged instance segmentation (CIS) faces greater challenges in accurately
segmenting objects that blend closely with their surroundings. Traditional
camouflaged instance segmentation methods, trained on terrestrial-dominated
datasets with limited underwater samples, may exhibit inadequate performance in
underwater scenes. To address these issues, we introduce the first underwater
camouflaged instance segmentation (UCIS) dataset, abbreviated as UCIS4K, which
comprises 3,953 images of camouflaged marine organisms with instance-level
annotations. In addition, we propose an Underwater Camouflaged Instance
Segmentation network based on Segment Anything Model (UCIS-SAM). Our UCIS-SAM
includes three key modules. First, the Channel Balance Optimization Module
(CBOM) enhances channel characteristics to improve underwater feature learning,
effectively addressing the model's limited understanding of underwater
environments. Second, the Frequency Domain True Integration Module (FDTIM) is
proposed to emphasize intrinsic object features and reduce interference from
camouflage patterns, enhancing the segmentation performance of camouflaged
objects blending with their surroundings. Finally, the Multi-scale Feature
Frequency Aggregation Module (MFFAM) is designed to strengthen the boundaries
of low-contrast camouflaged instances across multiple frequency bands,
improving the model's ability to achieve more precise segmentation of
camouflaged objects. Extensive experiments on the proposed UCIS4K and public
benchmarks show that our UCIS-SAM outperforms state-of-the-art approaches.

</details>


### [159] [ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling](https://arxiv.org/abs/2510.17603)
*Shuyuan Zhang,Chenhan Jiang,Zuoou Li,Jiankang Deng*

Main category: cs.CV

TL;DR: 提出一种基于图的程序化形状表示方法（GPS）和多智能体框架ShapeCraft，用于从自然语言生成结构化、可交互的3D资产。


<details>
  <summary>Details</summary>
Motivation: 现有文本到3D生成方法常产生非结构化网格且交互性差，难以融入艺术创作流程，因此需要更结构化且可编辑的表示方式。

Method: 提出Graph-based Procedural Shape (GPS) 表示法，将自然语言分解为子任务的结构图；利用LLM智能体分层解析输入并迭代优化程序化建模与着色，生成3D资产。

Result: 实验表明，ShapeCraft在几何准确性与语义丰富性方面优于现有基于LLM的方法，并支持动画和用户定制编辑。

Conclusion: ShapeCraft能够生成结构化、可交互的3D资产，展现出在交互式应用中的广泛应用潜力。

Abstract: 3D generation from natural language offers significant potential to reduce
expert manual modeling efforts and enhance accessibility to 3D assets. However,
existing methods often yield unstructured meshes and exhibit poor
interactivity, making them impractical for artistic workflows. To address these
limitations, we represent 3D assets as shape programs and introduce ShapeCraft,
a novel multi-agent framework for text-to-3D generation. At its core, we
propose a Graph-based Procedural Shape (GPS) representation that decomposes
complex natural language into a structured graph of sub-tasks, thereby
facilitating accurate LLM comprehension and interpretation of spatial
relationships and semantic shape details. Specifically, LLM agents
hierarchically parse user input to initialize GPS, then iteratively refine
procedural modeling and painting to produce structured, textured, and
interactive 3D assets. Qualitative and quantitative experiments demonstrate
ShapeCraft's superior performance in generating geometrically accurate and
semantically rich 3D assets compared to existing LLM-based agents. We further
show the versatility of ShapeCraft through examples of animated and
user-customized editing, highlighting its potential for broader interactive
applications.

</details>


### [160] [Integrating BIM and UAV-based photogrammetry for Automated 3D Structure Model Segmentation](https://arxiv.org/abs/2510.17609)
*Siqi Chen,Shanyue Guan*

Main category: cs.CV

TL;DR: 提出了一种基于机器学习的自动化三维点云分割框架，结合无人机扫描的真实数据和BIM生成的合成数据，有效提高了基础设施健康监测中结构部件分割的精度与效率。


<details>
  <summary>Details</summary>
Motivation: 传统三维点云中结构部件分割依赖耗时且易出错的手动标注，亟需一种自动化、高精度的解决方案。

Method: 结合真实无人机扫描的点云数据与建筑信息模型（BIM）生成的合成数据，构建机器学习框架进行三维点云自动分割。

Result: 在铁路轨道数据集上验证，能高精度识别并分割轨道、轨枕等主要部件；使用小规模数据集结合BIM数据显著缩短训练时间且保持良好准确率。

Conclusion: 该方法提升了三维基础设施模型分割的精度与效率，推动了无人机与BIM技术在结构健康监测与基础设施管理中的融合应用。

Abstract: The advancement of UAV technology has enabled efficient, non-contact
structural health monitoring. Combined with photogrammetry, UAVs can capture
high-resolution scans and reconstruct detailed 3D models of infrastructure.
However, a key challenge remains in segmenting specific structural components
from these models-a process traditionally reliant on time-consuming and
error-prone manual labeling. To address this issue, we propose a machine
learning-based framework for automated segmentation of 3D point clouds. Our
approach uses the complementary strengths of real-world UAV-scanned point
clouds and synthetic data generated from Building Information Modeling (BIM) to
overcome the limitations associated with manual labeling. Validation on a
railroad track dataset demonstrated high accuracy in identifying and segmenting
major components such as rails and crossties. Moreover, by using smaller-scale
datasets supplemented with BIM data, the framework significantly reduced
training time while maintaining reasonable segmentation accuracy. This
automated approach improves the precision and efficiency of 3D infrastructure
model segmentation and advances the integration of UAV and BIM technologies in
structural health monitoring and infrastructure management.

</details>


### [161] [One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum Unsupervised Anomaly Detection](https://arxiv.org/abs/2510.17611)
*Jia Guo,Shuai Lu,Lei Fan,Zelin Li,Donglin Di,Yang Song,Weihang Zhang,Wenbing Zhu,Hong Yan,Fang Chen,Huiqi Li,Hongen Liao*

Main category: cs.CV

TL;DR: Dinomaly2是首个用于全谱图像无监督异常检测的统一框架，在多类模型中弥合了性能差距，并在多种数据模态和任务设置下无缝扩展，展现出卓越的通用性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多类异常检测模型性能远落后于最先进的单类模型，且该领域方法碎片化，针对不同场景的专用方法导致部署困难，亟需一个统一的解决方案。

Method: 提出Dinomaly2，基于“少即是多”的理念，在标准重构框架中通过协调五个简单要素实现高性能，方法简洁，无需修改即可自然扩展到多样化任务。

Result: 在12个UAD基准上实验表明，Dinomaly2在多种模态（2D、多视角、RGB-3D、RGB-IR）、任务设置（单类、多类、少样本等）和应用领域均表现优越；多类模型在MVTec-AD和VisA上分别达到99.9%和99.3%的图像级AUROC；仅用每类8个正常样本的少样本设置下，性能超越以往全样本模型。

Conclusion: Dinomaly2凭借极简设计、可扩展性和广泛适用性，成为适用于全谱真实世界异常检测应用的统一解决方案。

Abstract: Unsupervised anomaly detection (UAD) has evolved from building specialized
single-class models to unified multi-class models, yet existing multi-class
models significantly underperform the most advanced one-for-one counterparts.
Moreover, the field has fragmented into specialized methods tailored to
specific scenarios (multi-class, 3D, few-shot, etc.), creating deployment
barriers and highlighting the need for a unified solution. In this paper, we
present Dinomaly2, the first unified framework for full-spectrum image UAD,
which bridges the performance gap in multi-class models while seamlessly
extending across diverse data modalities and task settings. Guided by the "less
is more" philosophy, we demonstrate that the orchestration of five simple
element achieves superior performance in a standard reconstruction-based
framework. This methodological minimalism enables natural extension across
diverse tasks without modification, establishing that simplicity is the
foundation of true universality. Extensive experiments on 12 UAD benchmarks
demonstrate Dinomaly2's full-spectrum superiority across multiple modalities
(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,
inference-unified multi-class, few-shot) and application domains (industrial,
biological, outdoor). For example, our multi-class model achieves unprecedented
99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For
multi-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art
performance with minimum adaptations. Moreover, using only 8 normal examples
per class, our method surpasses previous full-shot models, achieving 98.7% and
97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,
computational scalability, and universal applicability positions Dinomaly2 as a
unified solution for the full spectrum of real-world anomaly detection
applications.

</details>


### [162] [CaMiT: A Time-Aware Car Model Dataset for Classification and Generation](https://arxiv.org/abs/2510.17626)
*Frédéric LIN,Biruk Abere Ambaw,Adrian Popescu,Hejer Ammar,Romaric Audigier,Hervé Le Borgne*

Main category: cs.CV

TL;DR: CaMiT 是一个捕捉汽车模型随时间演变的细粒度数据集，包含大量标注和未标注图像，用于研究视觉模型在时间上的持续学习与生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉模型在静态数据上训练，难以应对现实世界中物体外观随时间变化的情况，尤其是在汽车等技术产品中型号逐年演进。因此需要一个能支持时间维度学习的数据集和评估框架。

Method: 提出 CaMiT 数据集，包含 787K 标注样本和 5.1M 未标注样本（2005–2023），构建时间增量分类任务；评估时间增量预训练（更新主干）和时间增量分类器学习（仅更新分类层）两种策略，并探索利用时间元数据的时间感知图像生成方法。

Result: 基于领域内数据的静态预训练性能接近大型通用模型且更高效，但在跨年测试时表现下降；两种时间增量策略均提升了模型的时间鲁棒性；时间感知生成模型能产生更真实的输出。

Conclusion: CaMiT 为细粒度视觉识别与生成中的时间适应问题提供了有价值的基准，推动持续学习与时间感知视觉系统的发展。

Abstract: AI systems must adapt to evolving visual environments, especially in domains
where object appearances change over time. We introduce Car Models in Time
(CaMiT), a fine-grained dataset capturing the temporal evolution of car models,
a representative class of technological artifacts. CaMiT includes 787K labeled
samples of 190 car models (2007-2023) and 5.1M unlabeled samples (2005-2023),
supporting both supervised and self-supervised learning. Static pretraining on
in-domain data achieves competitive performance with large-scale generalist
models while being more resource-efficient, yet accuracy declines when models
are tested across years. To address this, we propose a time-incremental
classification setting, a realistic continual learning scenario with emerging,
evolving, and disappearing classes. We evaluate two strategies:
time-incremental pretraining, which updates the backbone, and time-incremental
classifier learning, which updates only the final layer, both improving
temporal robustness. Finally, we explore time-aware image generation that
leverages temporal metadata during training, yielding more realistic outputs.
CaMiT offers a rich benchmark for studying temporal adaptation in fine-grained
visual recognition and generation.

</details>


### [163] [Self-supervised Pre-training for Mapping of Archaeological Stone Wall in Historic Landscapes Using High-Resolution DEM Derivatives](https://arxiv.org/abs/2510.17644)
*Zexian Huang,Mashnoon Islam,Brian Armstrong,Kourosh Khoshelham,Martin Tomko*

Main category: cs.CV

TL;DR: 提出DINO-CV框架，利用自监督跨视图预训练和LiDAR衍生的高分辨率DEM实现低矮干砌石墙的自动映射，有效应对植被遮挡与标注数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 干砌石墙具有重要遗产与生态价值，但因植被遮挡和人工测绘成本高导致大量墙体未被发现，亟需一种可扩展、低依赖标注数据的自动化 mapping 方法。

Method: 提出DINO-CV，基于知识蒸馏的自监督跨视图预训练框架，利用多种DEM衍生物提取地形结构特征，学习不变性几何与视觉表征，支持多种主流视觉主干网络。

Result: 在Budj Bim地区实现澳大利亚最密集的殖民时期干砌石墙识别之一，测试集mIoU达68.6%，仅用10%标注数据微调后仍保持63.8% mIoU。

Conclusion: DINO-CV展示了自监督学习结合高分辨率DEM在植被覆盖与文化遗产丰富区域中进行干砌石墙自动识别的巨大潜力，有效缓解标注数据不足的挑战。

Abstract: Dry-stone walls hold significant heritage and environmental value. Mapping
these structures is essential for ecosystem preservation and wildfire
management in Australia. Yet, many walls remain unidentified due to their
inaccessibility and the high cost of manual mapping. Deep learning-based
segmentation offers a scalable solution, but two major challenges persist: (1)
visual occlusion of low-lying walls by dense vegetation, and (2) limited
labeled data for supervised training. We propose DINO-CV, a segmentation
framework for automatic mapping of low-lying dry-stone walls using
high-resolution Airborne LiDAR-derived digital elevation models (DEMs). DEMs
overcome visual occlusion by capturing terrain structures hidden beneath
vegetation, enabling analysis of structural rather than spectral cues. DINO-CV
introduces a self-supervised cross-view pre-training strategy based on
knowledge distillation to mitigate data scarcity. It learns invariant visual
and geometric representations across multiple DEM derivatives, supporting
various vision backbones including ResNet, Wide ResNet, and Vision
Transformers. Applied to the UNESCO World Heritage cultural landscape of Budj
Bim, Victoria, the method identifies one of Australia's densest collections of
colonial dry-stone walls beyond Indigenous heritage contexts. DINO-CV achieves
a mean Intersection over Union (mIoU) of 68.6% on test areas and maintains
63.8% mIoU when fine-tuned with only 10% labeled data. These results
demonstrate the potential of self-supervised learning on high-resolution DEM
derivatives for automated dry-stone wall mapping in vegetated and heritage-rich
environments with scarce annotations.

</details>


### [164] [Frugal Federated Learning for Violence Detection: A Comparison of LoRA-Tuned VLMs and Personalized CNNs](https://arxiv.org/abs/2510.17651)
*Sébastien Thuau,Siba Haidar,Ayush Bajracharya,Rachid Chelouah*

Main category: cs.CV

TL;DR: 本文比较了两种节俭的联邦学习方法在暴力检测中的应用：视觉语言模型的零样本与微调方法，以及紧凑3D卷积神经网络的个性化训练，提出了兼顾效率与性能的混合模型。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习环境下实现高效、节能的暴力检测，尤其关注非独立同分布数据下的准确性、校准性和能耗问题。

Method: 比较LoRA微调的视觉语言模型（如LLaVA-7B）与个性化训练的轻量级3D CNN（CNN3D），在联邦学习框架下评估其性能、能耗及碳排放。

Result: 两种方法准确率均超90%，CNN3D在ROC AUC和对数损失上略优于VLM，且能耗更低；VLM在上下文推理和多模态推断方面更具优势。

Conclusion: 提出一种混合模型：常规场景使用轻量CNN进行分类，复杂场景选择性激活VLM，为资源敏感、可持续的视频监控AI提供了可复现的基准框架。

Abstract: We examine frugal federated learning approaches to violence detection by
comparing two complementary strategies: (i) zero-shot and federated fine-tuning
of vision-language models (VLMs), and (ii) personalized training of a compact
3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter
CNN3D as representative cases, we evaluate accuracy, calibration, and energy
usage under realistic non-IID settings.
  Both approaches exceed 90% accuracy. CNN3D slightly outperforms Low-Rank
Adaptation(LoRA)-tuned VLMs in ROC AUC and log loss, while using less energy.
VLMs remain favorable for contextual reasoning and multimodal inference. We
quantify energy and CO$_2$ emissions across training and inference, and analyze
sustainability trade-offs for deployment.
  To our knowledge, this is the first comparative study of LoRA-tuned
vision-language models and personalized CNNs for federated violence detection,
with an emphasis on energy efficiency and environmental metrics.
  These findings support a hybrid model: lightweight CNNs for routine
classification, with selective VLM activation for complex or descriptive
scenarios. The resulting framework offers a reproducible baseline for
responsible, resource-aware AI in video surveillance, with extensions toward
real-time, multimodal, and lifecycle-aware systems.

</details>


### [165] [4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads](https://arxiv.org/abs/2510.17664)
*Ling Liu,Jun Tian,Li Yi*

Main category: cs.CV

TL;DR: 本文提出了一种名为4DSegStreamer的新型框架，用于在流式设置中实现4D全景分割，通过双线程系统高效处理动态环境中的实时感知任务。


<details>
  <summary>Details</summary>
Motivation: 在高度动态的环境中，如密集人群疏散和复杂场景下的自动驾驶，需要在有限时间预算内实现高实时性、细粒度的感知，现有方法难以满足实时性和鲁棒性需求。

Method: 提出4DSegStreamer框架，包含预测线程和推理线程：预测线程利用历史运动和几何信息提取特征并预测未来动态；推理线程通过与最新内存对齐，并补偿自运动和动态物体运动，确保及时预测。该框架可无缝集成到现有3D/4D分割方法中。

Result: 在HOI4D、SemanticKITTI和nuScenes数据集上进行了广泛实验，验证了方法的有效性，尤其在高帧率下表现出更强的鲁棒性，并能准确预测复杂场景中的动态对象。

Conclusion: 4DSegStreamer为流式4D全景分割提供了通用且高效的解决方案，显著提升了实时性和对动态场景的适应能力，具有广泛的应用潜力。

Abstract: 4D panoptic segmentation in a streaming setting is critical for highly
dynamic environments, such as evacuating dense crowds and autonomous driving in
complex scenarios, where real-time, fine-grained perception within a
constrained time budget is essential. In this paper, we introduce
4DSegStreamer, a novel framework that employs a Dual-Thread System to
efficiently process streaming frames. The framework is general and can be
seamlessly integrated into existing 3D and 4D segmentation methods to enable
real-time capability. It also demonstrates superior robustness compared to
existing streaming perception approaches, particularly under high FPS
conditions. The system consists of a predictive thread and an inference thread.
The predictive thread leverages historical motion and geometric information to
extract features and forecast future dynamics. The inference thread ensures
timely prediction for incoming frames by aligning with the latest memory and
compensating for ego-motion and dynamic object movements. We evaluate
4DSegStreamer on the indoor HOI4D dataset and the outdoor SemanticKITTI and
nuScenes datasets. Comprehensive experiments demonstrate the effectiveness of
our approach, particularly in accurately predicting dynamic objects in complex
scenes.

</details>


### [166] [PICABench: How Far Are We from Physically Realistic Image Editing?](https://arxiv.org/abs/2510.17681)
*Yuandong Pu,Le Zhuo,Songhao Han,Jinbo Xing,Kaiwen Zhu,Shuo Cao,Bin Fu,Si Liu,Hongsheng Li,Yu Qiao,Wenlong Zhang,Xi Chen,Yihao Liu*

Main category: cs.CV

TL;DR: 本文提出了PICABench，一个系统评估图像编辑中物理真实感的基准，涵盖光学、力学和状态变化等八个子维度，并提出了PICAEval评估协议及PICA-100K训练数据集，旨在推动图像编辑向物理一致的真实感发展。


<details>
  <summary>Details</summary>
Motivation: 现有的图像编辑模型和基准主要关注指令完成度，忽略了编辑后应伴随的物理效应（如阴影、反射等），缺乏对物理真实感的系统评估，因此需要一个新的基准来衡量当前技术在物理 realism 方面的差距。

Method: 提出了PICABench基准，包含八个物理子维度和多种常见编辑操作；设计了基于视觉语言模型（VLM）评判的PICAEval评估协议，结合逐案例、区域级人工标注；并通过从视频中学习物理规律构建了PICA-100K训练数据集。

Result: 在主流模型上的评估表明，当前图像编辑在物理真实感方面仍有显著不足，存在广阔改进空间。

Conclusion: 物理真实感是图像编辑的关键挑战，PICABench、PICAEval和PICA-100K为未来实现物理一致的图像编辑提供了重要基础。

Abstract: Image editing has achieved remarkable progress recently. Modern editing
models could already follow complex instructions to manipulate the original
content. However, beyond completing the editing instructions, the accompanying
physical effects are the key to the generation realism. For example, removing
an object should also remove its shadow, reflections, and interactions with
nearby objects. Unfortunately, existing models and benchmarks mainly focus on
instruction completion but overlook these physical effects. So, at this moment,
how far are we from physically realistic image editing? To answer this, we
introduce PICABench, which systematically evaluates physical realism across
eight sub-dimension (spanning optics, mechanics, and state transitions) for
most of the common editing operations (add, remove, attribute change, etc). We
further propose the PICAEval, a reliable evaluation protocol that uses
VLM-as-a-judge with per-case, region-level human annotations and questions.
Beyond benchmarking, we also explore effective solutions by learning physics
from videos and construct a training dataset PICA-100K. After evaluating most
of the mainstream models, we observe that physical realism remains a
challenging problem with large rooms to explore. We hope that our benchmark and
proposed solutions can serve as a foundation for future work moving from naive
content editing toward physically consistent realism.

</details>


### [167] [Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model](https://arxiv.org/abs/2510.17684)
*Xinwei Zhang,Hu Chen,Zhe Yuan,Sukun Tian,Peng Feng*

Main category: cs.CV

TL;DR: 提出一种智能通信混合专家增强的医学图像分割基础模型IC-MoE，通过构建多类专家和语义引导对比学习，提升高层特征表征能力并保持预训练权重结构完整性。


<details>
  <summary>Details</summary>
Motivation: 现有自然图像分割模型在医学图像任务中微调时存在高层特征表达不足和破坏预训练权重结构完整性的问题，需要更有效的微调方法。

Method: 构建基本专家、语义专家和自适应专家，采用像素概率自适应投票策略进行专家选择与融合；提出语义引导的对比学习方法，缓解对比学习中的弱监督问题。

Result: 在三个公开医学图像分割数据集上实验表明，IC-MoE优于其他SOTA模型，具有更强的高层特征表达能力和更好的预训练结构保持性。

Conclusion: IC-MoE有效增强了医学图像分割基础模型的高层特征表达并保持了预训练权重的结构完整性，具备良好的泛化能力。

Abstract: Foundation models for medical image segmentation have achieved remarkable
performance. Adaptive fine-tuning of natural image segmentation foundation
models is crucial for medical image segmentation tasks. However, some
limitations exist in existing fine-tuning methods: 1) insufficient
representation of high-level features and 2) the fine-tuning process disrupts
the structural integrity of pretrained weights. Inspired by these critical
problems, we propose an intelligent communication mixture-of-experts
boosted-medical image segmentation foundation model, named IC-MoE, with twofold
ideas: 1) We construct basic experts, semantic experts, and adaptive experts.
Moreover, we implement a pixel probability adaptive voting strategy, which
enables expert selection and fusion through label consistency and load
balancing. This approach preliminarily enhances the representation capability
of high-level features while preserving the structural integrity of pretrained
weights. 2) We propose a semantic-guided contrastive learning method to address
the issue of weak supervision in contrastive learning. This method further
enhances the representation capability of high-level features while preserving
the structural integrity of pretrained weights. Extensive experiments across
three public medical image segmentation datasets demonstrate that the IC-MoE
outperforms other SOTA models. Consequently, the proposed IC-MoE effectively
supplements foundational medical image segmentation models with high-level
features and pretrained structural integrity. We also validate the superior
generalizability of the IC-MoE across diverse medical image segmentation
scenarios.

</details>


### [168] [Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning](https://arxiv.org/abs/2510.17685)
*Min Cao,Xinyu Zhou,Ding Jiang,Bo Du,Mang Ye,Min Zhang*

Main category: cs.CV

TL;DR: 本文提出了一个多语言文本到图像人物检索任务，并构建了一个多语言基准数据集，同时提出了一种新的双向隐式关系推理与对齐框架（Bi-IRRA），在跨语言与跨模态对齐上取得显著效果，实现了当前最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像人物检索方法大多局限于英语语境，且在处理模态异质性时，全局对齐方法忽略细粒度差异，局部对齐方法依赖先验信息。因此，需要一种能同时处理多语言和跨模态细粒度对齐的新方法。

Method: 提出Bi-IRRA框架，包含双向隐式关系推理模块（通过掩码图像和文本的双向预测隐式建模局部关系）和多维全局对齐模块（缓解模态异质性）；同时利用大语言模型结合领域知识构建多语言TIPR基准数据集。

Result: 在所有多语言TIPR数据集上均取得当前最先进的结果，验证了方法的有效性。

Conclusion: Bi-IRRA有效解决了多语言文本到图像人物检索中的跨模态与跨语言对齐问题，推动了该任务在多语言场景下的应用发展。

Abstract: Text-to-image person retrieval (TIPR) aims to identify the target person
using textual descriptions, facing challenge in modality heterogeneity. Prior
works have attempted to address it by developing cross-modal global or local
alignment strategies. However, global methods typically overlook fine-grained
cross-modal differences, whereas local methods require prior information to
explore explicit part alignments. Additionally, current methods are
English-centric, restricting their application in multilingual contexts. To
alleviate these issues, we pioneer a multilingual TIPR task by developing a
multilingual TIPR benchmark, for which we leverage large language models for
initial translations and refine them by integrating domain-specific knowledge.
Correspondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation
Reasoning and Aligning framework to learn alignment across languages and
modalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module
enables bidirectional prediction of masked image and text, implicitly enhancing
the modeling of local relations across languages and modalities, a
multi-dimensional global alignment module is integrated to bridge the modality
heterogeneity. The proposed method achieves new state-of-the-art results on all
multilingual TIPR datasets. Data and code are presented in
https://github.com/Flame-Chasers/Bi-IRRA.

</details>


### [169] [Towards 3D Objectness Learning in an Open World](https://arxiv.org/abs/2510.17686)
*Taichi Liu,Zhenyu Wang,Ruofeng Liu,Guang Wang,Desheng Zhang*

Main category: cs.CV

TL;DR: 本文提出OP3Det，一种无需文本提示的开世界无类别3D检测器，通过融合2D语义先验和3D几何先验，并利用跨模态专家混合机制动态整合点云和RGB图像信息，实现了对3D场景中已知与未知对象的广义检测，在开放世界和闭世界设置下均显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D检测器多为闭集设定，难以泛化到未知对象；而现有的开集方法依赖手工文本提示，存在词汇扩展和语义重叠问题，缺乏对广义3D物体性的有效学习。

Method: 提出OP3Det，利用2D基础模型的强泛化和零样本能力，结合2D语义先验与3D几何先验生成无类别提案，并通过跨模态混合专家结构动态融合单模态与多模态特征，实现无需提示的开放世界3D检测。

Result: 实验表明，OP3Det在开放世界3D检测中比现有方法最高提升16.0%的AR，在闭世界设置下也比传统检测器提升13.5%。

Conclusion: OP3Det有效实现了开放世界中的广义3D物体发现，无需依赖文本提示，兼具强泛化能力和优异检测性能，为学习通用3D物体性提供了新思路。

Abstract: Recent advancements in 3D object detection and novel category detection have
made significant progress, yet research on learning generalized 3D objectness
remains insufficient. In this paper, we delve into learning open-world 3D
objectness, which focuses on detecting all objects in a 3D scene, including
novel objects unseen during training. Traditional closed-set 3D detectors
struggle to generalize to open-world scenarios, while directly incorporating 3D
open-vocabulary models for open-world ability struggles with vocabulary
expansion and semantic overlap. To achieve generalized 3D object discovery, We
propose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect
any objects within 3D scenes without relying on hand-crafted text prompts. We
introduce the strong generalization and zero-shot capabilities of 2D foundation
models, utilizing both 2D semantic priors and 3D geometric priors for
class-agnostic proposals to broaden 3D object discovery. Then, by integrating
complementary information from point cloud and RGB image in the cross-modal
mixture of experts, OP3Det dynamically routes uni-modal and multi-modal
features to learn generalized 3D objectness. Extensive experiments demonstrate
the extraordinary performance of OP3Det, which significantly surpasses existing
open-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement
compared to closed-world 3D detectors.

</details>


### [170] [GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver](https://arxiv.org/abs/2510.17699)
*Aleksandr Oganov,Ilya Bykov,Eva Neudachina,Mishan Aliev,Alexander Tolmachev,Alexander Sidorov,Aleksandr Zuev,Andrey Okhotin,Denis Rakitin,Aibek Alanov*

Main category: cs.CV

TL;DR: 提出一种无需复杂训练技巧的广义ODE采样器，并结合对抗训练提升生成细节保真度，显著改善少步扩散模型的生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型采样计算成本高，现有加速方法依赖复杂训练技巧且难以保留细节，因此需要一种更简洁高效的方法。

Method: 提出广义求解器（Generalized Solver）结构，简化ODE采样器参数化，并结合原始蒸馏损失与对抗训练，提升细节生成质量。

Result: 在相似资源限制下，所提方法（GAS）优于现有求解器训练方法，生成结果更清晰、伪影更少。

Conclusion: 广义对抗求解器（GAS）是一种简单有效的扩散模型加速方法，兼顾高质量与高效率，适合少步生成。

Abstract: While diffusion models achieve state-of-the-art generation quality, they
still suffer from computationally expensive sampling. Recent works address this
issue with gradient-based optimization methods that distill a few-step ODE
diffusion solver from the full sampling process, reducing the number of
function evaluations from dozens to just a few. However, these approaches often
rely on intricate training techniques and do not explicitly focus on preserving
fine-grained details. In this paper, we introduce the Generalized Solver: a
simple parameterization of the ODE sampler that does not require additional
training tricks and improves quality over existing approaches. We further
combine the original distillation loss with adversarial training, which
mitigates artifacts and enhances detail fidelity. We call the resulting method
the Generalized Adversarial Solver and demonstrate its superior performance
compared to existing solver training methods under similar resource
constraints. Code is available at https://github.com/3145tttt/GAS.

</details>


### [171] [Elastic ViTs from Pretrained Models without Retraining](https://arxiv.org/abs/2510.17700)
*Walter Simoncini,Michael Dorkenwald,Tijmen Blankevoort,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CV

TL;DR: SnapViT 提出了一种无需重训练、无需标签的单次结构化剪枝方法，实现视觉Transformer模型在连续计算预算下的弹性推理，显著提升部署灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉基础模型尺寸固定，难以满足多样化实际部署需求，导致在资源受限场景下性能与效率难以兼顾。

Method: 提出SnapViT，结合梯度信息与跨网络结构相关性，通过进化算法近似Hessian非对角结构，设计自监督重要性评分机制，在无需标签和重训练的情况下实现高效剪枝。

Result: 在DINO、SigLIPv2、DeIT和AugReg等模型上优于现有最先进方法，仅用单A100 GPU不到5分钟即可生成可弹性调整的剪枝模型。

Conclusion: SnapViT为预训练视觉Transformer提供了高效的弹性推理解决方案，具备快速、通用、无需重训练等优势，推动模型在多样化场景中的灵活部署。

Abstract: Vision foundation models achieve remarkable performance but are only
available in a limited set of pre-determined sizes, forcing sub-optimal
deployment choices under real-world constraints. We introduce SnapViT:
Single-shot network approximation for pruned Vision Transformers, a new
post-pretraining structured pruning method that enables elastic inference
across a continuum of compute budgets. Our approach efficiently combines
gradient information with cross-network structure correlations, approximated
via an evolutionary algorithm, does not require labeled data, generalizes to
models without a classification head, and is retraining-free. Experiments on
DINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over
state-of-the-art methods across various sparsities, requiring less than five
minutes on a single A100 GPU to generate elastic models that can be adjusted to
any computational budget. Our key contributions include an efficient pruning
strategy for pretrained Vision Transformers, a novel evolutionary approximation
of Hessian off-diagonal structures, and a self-supervised importance scoring
mechanism that maintains strong performance without requiring retraining or
labels. Code and pruned models are available at: https://elastic.ashita.nl/

</details>


### [172] [Improving Cross-Patient Generalization in Parkinson's Disease Detection through Chunk-Based Analysis of Hand-Drawn Patterns](https://arxiv.org/abs/2510.17703)
*Mhd Adnan Albani,Riad Sonbol*

Main category: cs.CV

TL;DR: 提出一种基于手绘图像分块和集成学习的帕金森病检测方法，有效提升对未见患者数据的鲁棒性和准确率。


<details>
  <summary>Details</summary>
Motivation: 现有帕金森病检测方法受限于数据集不足和对未见患者数据鲁棒性差的问题，亟需更可靠的方法。

Method: 采用两阶段方法：先按绘图类型分类，再将图像划分为2x2块分别提取特征并检测；最后通过集成方法融合各块的决策结果。

Result: 在NewHandPD数据集上，对已见患者达97.08%准确率，对未见患者达94.91%，性能下降仅2.17个百分点，显著优于先前方法。

Conclusion: 所提分块策略和集成方法有效增强了模型鲁棒性，提升了帕金森病早期检测的实用性和泛化能力。

Abstract: Parkinson's disease (PD) is a neurodegenerative disease affecting about 1% of
people over the age of 60, causing motor impairments that impede hand
coordination activities such as writing and drawing. Many approaches have tried
to support early detection of Parkinson's disease based on hand-drawn images;
however, we identified two major limitations in the related works: (1) the lack
of sufficient datasets, (2) the robustness when dealing with unseen patient
data. In this paper, we propose a new approach to detect Parkinson's disease
that consists of two stages: The first stage classifies based on their drawing
type(circle, meander, spiral), and the second stage extracts the required
features from the images and detects Parkinson's disease. We overcame the
previous two limitations by applying a chunking strategy where we divide each
image into 2x2 chunks. Each chunk is processed separately when extracting
features and recognizing Parkinson's disease indicators. To make the final
classification, an ensemble method is used to merge the decisions made from
each chunk. Our evaluation shows that our proposed approach outperforms the top
performing state-of-the-art approaches, in particular on unseen patients. On
the NewHandPD dataset our approach, it achieved 97.08% accuracy for seen
patients and 94.91% for unseen patients, our proposed approach maintained a gap
of only 2.17 percentage points, compared to the 4.76-point drop observed in
prior work.

</details>


### [173] [Automatic Classification of Circulating Blood Cell Clusters based on Multi-channel Flow Cytometry Imaging](https://arxiv.org/abs/2510.17716)
*Suqiang Ma,Subhadeep Sengupta,Yao Lee,Beikang Gu,Xianyan Chen,Xianqiao Wang,Yang Liu,Mengjia Xu,Galit H. Frydman,He Li*

Main category: cs.CV

TL;DR: 提出了一种基于YOLOv11和多通道荧光叠加的两步计算框架，用于自动分析循环血细胞簇（CCC）图像并准确识别簇内细胞类型，准确率超过95%。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对单细胞流式图像分析，缺乏对复杂、异质性血细胞簇的自动分析工具，限制了血栓、感染等疾病的生物标志物研究。

Method: 采用两步策略：首先利用微调的YOLOv11模型对图像进行细胞簇与非簇分类；然后通过叠加多通道荧光染色区域与簇轮廓来识别簇内细胞类型。

Result: 该框架在细胞簇分类和表型识别上均达到超过95%的准确率，优于传统CNN和Vision Transformer模型。

Conclusion: 该自动化框架有效整合明场与荧光数据，可精确分析循环血细胞簇，具有扩展至免疫细胞和肿瘤细胞簇研究的潜力。

Abstract: Circulating blood cell clusters (CCCs) containing red blood cells (RBCs),
white blood cells(WBCs), and platelets are significant biomarkers linked to
conditions like thrombosis, infection, and inflammation. Flow cytometry, paired
with fluorescence staining, is commonly used to analyze these cell clusters,
revealing cell morphology and protein profiles. While computational approaches
based on machine learning have advanced the automatic analysis of single-cell
flow cytometry images, there is a lack of effort to build tools to
automatically analyze images containing CCCs. Unlike single cells, cell
clusters often exhibit irregular shapes and sizes. In addition, these cell
clusters often consist of heterogeneous cell types, which require multi-channel
staining to identify the specific cell types within the clusters. This study
introduces a new computational framework for analyzing CCC images and
identifying cell types within clusters. Our framework uses a two-step analysis
strategy. First, it categorizes images into cell cluster and non-cluster groups
by fine-tuning the You Only Look Once(YOLOv11) model, which outperforms
traditional convolutional neural networks (CNNs), Vision Transformers (ViT).
Then, it identifies cell types by overlaying cluster contours with regions from
multi-channel fluorescence stains, enhancing accuracy despite cell debris and
staining artifacts. This approach achieved over 95% accuracy in both cluster
classification and phenotype identification. In summary, our automated
framework effectively analyzes CCC images from flow cytometry, leveraging both
bright-field and fluorescence data. Initially tested on blood cells, it holds
potential for broader applications, such as analyzing immune and tumor cell
clusters, supporting cellular research across various diseases.

</details>


### [174] [Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions](https://arxiv.org/abs/2510.17719)
*Zhiqiang Teng,Beibei Lin,Tingting Chen,Zifeng Yuan,Xuanyi Li,Xuanyu Zhang,Shunli Zhang*

Main category: cs.CV

TL;DR: 提出RaindropGS，首个评估雨滴干扰下完整3D高斯点阵化（3DGS） pipeline 的基准，涵盖真实雨滴数据集与多阶段性能分析。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS在雨滴导致镜头遮挡和光学畸变时性能下降，且合成雨滴数据与真实场景存在域差距，缺乏对完整pipeline（尤其是位姿估计与点云初始化）的评估。

Method: 构建包含真实雨滴图像的数据集，提供聚焦雨滴、聚焦背景和无雨真值三组图像；设计完整评估流程，涵盖数据准备、处理及雨滴感知的3DGS评估，分析不同干扰类型、位姿估计、点云初始化及去雨方法的影响。

Result: 揭示了现有3DGS在无约束雨滴图像下的性能瓶颈，发现相机焦点位置、不准确的位姿和点云初始化对重建质量有显著影响。

Conclusion: RaindropGS为雨滴条件下鲁棒3DGS方法的发展提供了重要基准与方向。

Abstract: 3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe
occlusions and optical distortions caused by raindrop contamination on the
camera lens, substantially degrading reconstruction quality. Existing
benchmarks typically evaluate 3DGS using synthetic raindrop images with known
camera poses (constrained images), assuming ideal conditions. However, in
real-world scenarios, raindrops often interfere with accurate camera pose
estimation and point cloud initialization. Moreover, a significant domain gap
between synthetic and real raindrops further impairs generalization. To tackle
these issues, we introduce RaindropGS, a comprehensive benchmark designed to
evaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images
to clear 3DGS reconstructions. Specifically, the whole benchmark pipeline
consists of three parts: data preparation, data processing, and raindrop-aware
3DGS evaluation, including types of raindrop interference, camera pose
estimation and point cloud initialization, single image rain removal
comparison, and 3D Gaussian training comparison. First, we collect a real-world
raindrop reconstruction dataset, in which each scene contains three aligned
image sets: raindrop-focused, background-focused, and rain-free ground truth,
enabling a comprehensive evaluation of reconstruction quality under different
focus conditions. Through comprehensive experiments and analyses, we reveal
critical insights into the performance limitations of existing 3DGS methods on
unconstrained raindrop images and the varying impact of different pipeline
components: the impact of camera focus position on 3DGS reconstruction
performance, and the interference caused by inaccurate pose and point cloud
initialization on reconstruction. These insights establish clear directions for
developing more robust 3DGS methods under raindrop conditions.

</details>


### [175] [MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues](https://arxiv.org/abs/2510.17722)
*Yaning Pan,Zekun Wang,Qianqian Xie,Yongqian Wen,Yuanxing Zhang,Guohui Zhang,Haoxuan Hu,Zhiyu Pan,Yibing Huang,Zhidong Gan,Yonghong Lin,An Ping,Tianhao Peng,Jiaheng Liu*

Main category: cs.CV

TL;DR: 本文提出了MT-Video-Bench，一个用于评估多模态大语言模型（MLLMs）在多轮视频对话中理解能力的综合性基准，涵盖987个精心策划的多轮对话，聚焦感知与交互能力，并揭示现有MLLMs在该任务上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大模型评估基准主要集中于单轮问答，忽视了真实场景中复杂的多轮对话需求，缺乏对模型交互与持续理解能力的系统评估。

Method: 构建了一个名为MT-Video-Bench的视频理解基准，包含来自多个领域的987个多轮对话，评估MLLMs在六项核心能力上的表现，这些能力与真实应用如互动体育分析和视频教学密切相关。

Result: 对多个先进的开源和闭源MLLM进行了广泛评估，发现了它们在处理多轮视频对话时存在显著性能差异和局限性。

Conclusion: MT-Video-Bench为评估MLLM在多轮视频对话中的表现提供了有力工具，揭示了当前模型的不足，并将公开以促进未来研究。

Abstract: The recent development of Multimodal Large Language Models (MLLMs) has
significantly advanced AI's ability to understand visual modalities. However,
existing evaluation benchmarks remain limited to single-turn question
answering, overlooking the complexity of multi-turn dialogues in real-world
scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video
understanding benchmark for evaluating MLLMs in multi-turn dialogues.
Specifically, our MT-Video-Bench mainly assesses six core competencies that
focus on perceptivity and interactivity, encompassing 987 meticulously curated
multi-turn dialogues from diverse domains. These capabilities are rigorously
aligned with real-world applications, such as interactive sports analysis and
multi-turn video-based intelligent tutoring. With MT-Video-Bench, we
extensively evaluate various state-of-the-art open-source and closed-source
MLLMs, revealing their significant performance discrepancies and limitations in
handling multi-turn video dialogues. The benchmark will be publicly available
to foster future research.

</details>


### [176] [Signature Forgery Detection: Improving Cross-Dataset Generalization](https://arxiv.org/abs/2510.17724)
*Matheus Ramos Parracho*

Main category: cs.CV

TL;DR: 本研究探讨了签名伪造检测中的特征学习策略，旨在提升跨数据集的泛化能力，结果表明基于原始图像的模型在多个基准上表现更优，而基于shell预处理的模型具有进一步优化的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有离线签名验证方法在不同数据集间泛化能力较差，受书写风格和采集方式差异影响显著，因此需要提高模型在跨数据集场景下的鲁棒性。

Method: 采用三个公开数据集（CEDAR、ICDAR、GPDS Synthetic），构建两种实验流程：一种直接使用原始签名图像，另一种采用称为shell预处理的方法进行特征提取，并比较两者在跨数据集验证中的表现。

Result: 原始图像模型在各基准测试中性能更高，而shell预处理模型虽未表现出明显优势，但展现出在跨域签名验证中进一步优化的潜力。

Conclusion: 提升跨数据集泛化能力是签名验证的关键，原始图像输入在当前更具优势，shell预处理方法值得进一步研究以实现鲁棒的跨域验证。

Abstract: Automated signature verification is a critical biometric technique used in
banking, identity authentication, and legal documentation. Despite the notable
progress achieved by deep learning methods, most approaches in offline
signature verification still struggle to generalize across datasets, as
variations in handwriting styles and acquisition protocols often degrade
performance. This study investigates feature learning strategies for signature
forgery detection, focusing on improving cross-dataset generalization -- that
is, model robustness when trained on one dataset and tested on another. Using
three public benchmarks -- CEDAR, ICDAR, and GPDS Synthetic -- two experimental
pipelines were developed: one based on raw signature images and another
employing a preprocessing method referred to as shell preprocessing. Several
behavioral patterns were identified and analyzed; however, no definitive
superiority between the two approaches was established. The results show that
the raw-image model achieved higher performance across benchmarks, while the
shell-based model demonstrated promising potential for future refinement toward
robust, cross-domain signature verification.

</details>


### [177] [Can Image-To-Video Models Simulate Pedestrian Dynamics?](https://arxiv.org/abs/2510.17731)
*Aaron Appelle,Jerome P. Lynch*

Main category: cs.CV

TL;DR: 研究基于扩散变换器的图像到视频生成模型在拥挤公共场景中生成真实行人运动模式的能力。


<details>
  <summary>Details</summary>
Motivation: 探索大型视频数据集训练的I2V模型是否具备生成逼真行人运动的能力。

Method: 通过从行人轨迹基准提取的关键帧对I2V模型进行条件控制，并使用行人动态的定量指标评估其轨迹预测性能。

Result: 框架能够评估模型生成真实行人运动模式的效果，展现出I2V模型在世界建模方面的潜力。

Conclusion: I2V模型在适当的条件下可有效模拟复杂场景中的行人动态，具有应用于轨迹预测的潜力。

Abstract: Recent high-performing image-to-video (I2V) models based on variants of the
diffusion transformer (DiT) have displayed remarkable inherent world-modeling
capabilities by virtue of training on large scale video datasets. We
investigate whether these models can generate realistic pedestrian movement
patterns in crowded public scenes. Our framework conditions I2V models on
keyframes extracted from pedestrian trajectory benchmarks, then evaluates their
trajectory prediction performance using quantitative measures of pedestrian
dynamics.

</details>


### [178] [Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition](https://arxiv.org/abs/2510.17739)
*Timur Ismagilov,Shakaiba Majeed,Michael Milford,Tan Viet Tuyen Nguyen,Sarvapali D. Ramchurn,Shoaib Ehsan*

Main category: cs.CV

TL;DR: 提出一种无需训练、与描述子无关的多参考视觉位置识别方法，通过矩阵分解联合建模多个参考描述子，实现基于投影的残差匹配，在多种数据上显著提升召回率。


<details>
  <summary>Details</summary>
Motivation: 现有的多参考视觉位置识别方法在处理外观和视角变化时受限于计算成本或依赖启发式策略，缺乏良好的泛化能力和高效性，需要一种轻量且鲁棒的方法。

Method: 提出一种训练免费、描述子无关的框架，利用矩阵分解将多个参考描述子分解为基础表示，通过投影进行残差匹配；同时引入SotonMV作为多视角VPR的结构化基准。

Result: 在多外观数据上，相比单参考方法Recall@1提升达~18%，在结构化和非结构化数据上均优于多参考基线方法，性能提升约5%。

Conclusion: 该方法在保持轻量级的同时，展现出在外观和视角变化下的强泛化能力，有效提升了多参考VPR的性能。

Abstract: We address multi-reference visual place recognition (VPR), where reference
sets captured under varying conditions are used to improve localisation
performance. While deep learning with large-scale training improves robustness,
increasing data diversity and model complexity incur extensive computational
cost during training and deployment. Descriptor-level fusion via voting or
aggregation avoids training, but often targets multi-sensor setups or relies on
heuristics with limited gains under appearance and viewpoint change. We propose
a training-free, descriptor-agnostic approach that jointly models places using
multiple reference descriptors via matrix decomposition into basis
representations, enabling projection-based residual matching. We also introduce
SotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance
data, our method improves Recall@1 by up to ~18% over single-reference and
outperforms multi-reference baselines across appearance and viewpoint changes,
with gains of ~5% on unstructured data, demonstrating strong generalisation
while remaining lightweight.

</details>


### [179] [Towards Explainable Skin Cancer Classification: A Dual-Network Attention Model with Lesion Segmentation and Clinical Metadata Fusion](https://arxiv.org/abs/2510.17773)
*Md. Enamul Atiq,Shaikh Anowarul Fattah*

Main category: cs.CV

TL;DR: 提出了一种结合病变分割和临床元数据的双编码器注意力框架，显著提高了皮肤癌分类的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌早期检测对改善患者预后至关重要，但现有深度学习模型因“黑箱”特性及病灶内外变化大、类别间差异小而受限，亟需更准确且可解释的自动化诊断方法。

Method: 采用具有双注意力门（DAG）和ASPP的Deep-UNet进行病灶分割；分类阶段使用两个DenseNet201编码器分别处理原始图像和分割结果，并通过多头交叉注意力融合特征；引入基于Transformer的模块融合患者年龄、性别、病灶位置等临床元数据。

Result: 在HAM10000、ISIC 2018和2019数据集上达到最先进的分割性能，分类准确率和平均AUC显著优于基线模型；Grad-CAM可视化显示模型聚焦于病灶区域，验证了其可靠性和可解释性。

Conclusion: 结合精确病灶分割与临床数据，并通过注意力机制融合，可有效提升皮肤癌分类模型的性能与临床可信度。

Abstract: Skin cancer is a life-threatening disease where early detection significantly
improves patient outcomes. Automated diagnosis from dermoscopic images is
challenging due to high intra-class variability and subtle inter-class
differences. Many deep learning models operate as "black boxes," limiting
clinical trust. In this work, we propose a dual-encoder attention-based
framework that leverages both segmented lesions and clinical metadata to
enhance skin lesion classification in terms of both accuracy and
interpretability. A novel Deep-UNet architecture with Dual Attention Gates
(DAG) and Atrous Spatial Pyramid Pooling (ASPP) is first employed to segment
lesions. The classification stage uses two DenseNet201 encoders-one on the
original image and another on the segmented lesion whose features are fused via
multi-head cross-attention. This dual-input design guides the model to focus on
salient pathological regions. In addition, a transformer-based module
incorporates patient metadata (age, sex, lesion site) into the prediction. We
evaluate our approach on the HAM10000 dataset and the ISIC 2018 and 2019
challenges. The proposed method achieves state-of-the-art segmentation
performance and significantly improves classification accuracy and average AUC
compared to baseline models. To validate our model's reliability, we use
Gradient-weighted Class Activation Mapping (Grad-CAM) to generate heatmaps.
These visualizations confirm that our model's predictions are based on the
lesion area, unlike models that rely on spurious background features. These
results demonstrate that integrating precise lesion segmentation and clinical
data with attention-based fusion leads to a more accurate and interpretable
skin cancer classification model.

</details>


### [180] [SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference](https://arxiv.org/abs/2510.17777)
*Samir Khaki,Junxian Guo,Jiaming Tang,Shang Yang,Yukang Chen,Konstantinos N. Plataniotis,Yao Lu,Song Han,Zhijian Liu*

Main category: cs.CV

TL;DR: SparseVILA 提出了一种高效的视觉语言模型推理新范式，通过在prefill和解码阶段解耦视觉稀疏性，在不牺牲性能的前提下显著加速大模型推理。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）在高分辨率图像、长视频分析等任务中面临视觉token数量激增导致的推理延迟问题，亟需提升推理可扩展性。

Method: SparseVILA 在prefill阶段进行query-agnostic的视觉token剪枝，在解码阶段按需检索query-aware的相关视觉token，并利用AWQ优化推理流水线，实现跨阶段的稀疏分布。

Result: 在长上下文视频任务中，SparseVILA 实现了最高4.0倍的prefill加速、2.5倍的解码加速和2.6倍的端到端加速，同时在文档理解和推理任务上准确率提升。

Conclusion: SparseVILA 通过解耦剪枝与检索，提出了一种无需训练、架构无关的高效多模态推理框架，为大规模VLMs的加速提供了新方向。

Abstract: Vision Language Models (VLMs) have rapidly advanced in integrating visual and
textual reasoning, powering applications across high-resolution image
understanding, long-video analysis, and multi-turn conversation. However, their
scalability remains limited by the growing number of visual tokens that
dominate inference latency. We present SparseVILA, a new paradigm for efficient
VLM inference that decouples visual sparsity across the prefilling and decoding
stages. SparseVILA distributes sparsity across stages by pruning redundant
visual tokens during prefill and retrieving only query-relevant tokens during
decoding. This decoupled design matches leading prefill pruning methods while
preserving multi-turn fidelity by retaining most of the visual cache so that
query-aware tokens can be retrieved at each conversation round. Built on an
AWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster
prefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end
speedup on long-context video tasks -- while improving accuracy on
document-understanding and reasoning tasks. By decoupling query-agnostic
pruning and query-aware retrieval, SparseVILA establishes a new direction for
efficient multimodal inference, offering a training-free, architecture-agnostic
framework for accelerating large VLMs without sacrificing capability.

</details>


### [181] [ConsistEdit: Highly Consistent and Precise Training-free Visual Editing](https://arxiv.org/abs/2510.17803)
*Zixin Yin,Ling-Hao Chen,Lionel Ni,Xili Dai*

Main category: cs.CV

TL;DR: 本文提出ConsistEdit，一种专为MM-DiT架构设计的新型注意力控制方法，显著提升了文本引导图像和视频编辑中的一致性和编辑强度，支持多轮、多区域和渐进式结构一致性编辑，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的无需训练的注意力控制方法在编辑强度和源一致性之间难以平衡，尤其在多轮和视频编辑中易积累误差，且多采用全局一致性约束，限制了细粒度属性编辑。此外，MM-DiT架构的出现提供了改进契机，但尚缺乏针对其结构的编辑方法。

Method: 通过对MM-DiT架构的注意力机制进行深入分析，提出ConsistEdit，包含三个关键技术：仅视觉注意力控制、掩码引导的预注意力融合，以及对查询、键和值令牌的差异化操作，从而实现更一致且符合提示的编辑。

Result: 实验表明，ConsistEdit在多种图像和视频编辑任务中达到最先进水平，能够在所有推理步骤和注意力层中执行编辑，无需手动设计，显著提升了可靠性和一致性，并支持多轮、多区域及结构一致性的渐进调节。

Conclusion: ConsistEdit有效解决了现有编辑方法在一致性与编辑灵活性之间的权衡问题，是首个适用于完整推理过程和全注意力层的编辑方法，为基于MM-DiT的生成模型提供了强大、灵活且可靠的编辑能力。

Abstract: Recent advances in training-free attention control methods have enabled
flexible and efficient text-guided editing capabilities for existing generation
models. However, current approaches struggle to simultaneously deliver strong
editing strength while preserving consistency with the source. This limitation
becomes particularly critical in multi-round and video editing, where visual
errors can accumulate over time. Moreover, most existing methods enforce global
consistency, which limits their ability to modify individual attributes such as
texture while preserving others, thereby hindering fine-grained editing.
Recently, the architectural shift from U-Net to MM-DiT has brought significant
improvements in generative performance and introduced a novel mechanism for
integrating text and vision modalities. These advancements pave the way for
overcoming challenges that previous methods failed to resolve. Through an
in-depth analysis of MM-DiT, we identify three key insights into its attention
mechanisms. Building on these, we propose ConsistEdit, a novel attention
control method specifically tailored for MM-DiT. ConsistEdit incorporates
vision-only attention control, mask-guided pre-attention fusion, and
differentiated manipulation of the query, key, and value tokens to produce
consistent, prompt-aligned edits. Extensive experiments demonstrate that
ConsistEdit achieves state-of-the-art performance across a wide range of image
and video editing tasks, including both structure-consistent and
structure-inconsistent scenarios. Unlike prior methods, it is the first
approach to perform editing across all inference steps and attention layers
without handcraft, significantly enhancing reliability and consistency, which
enables robust multi-round and multi-region editing. Furthermore, it supports
progressive adjustment of structural consistency, enabling finer control.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [182] [Quantum NLP models on Natural Language Inference](https://arxiv.org/abs/2510.15972)
*Ling Sun,Peter Sullivan,Michael Martin,Yun Zhou*

Main category: cs.CL

TL;DR: 本文研究了量子自然语言处理（QNLP）在自然语言推断（NLI）任务中的应用，发现量子模型在参数极少的情况下性能与经典模型相当甚至更优，并提出了一种基于词簇的新型架构以提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索在低资源、小样本场景下，如何利用量子计算的优势构建更高效、结构敏感的自然语言处理模型，弥补经典模型在参数效率和结构建模上的不足。

Method: 基于DisCoCat框架和lambeq库构建参数化量子电路，用于句子对的语义相关性和推断分类任务；引入信息增益每参数（IGPP）指标评估学习效率，并提出基于词簇的参数共享架构以增强泛化。

Result: 量子模型在推理任务上优于随机初始化的Transformer，在相关性任务上测试误差更低；每参数学习效率比经典模型高至五个数量级，且整体参数量显著更少。

Conclusion: 量子NLP模型在小样本、低资源设置下展现出卓越的参数效率和结构建模潜力，所提词簇架构进一步改善了泛化能力，表明QNLP是未来高效语义建模的重要方向。

Abstract: Quantum natural language processing (QNLP) offers a novel approach to
semantic modeling by embedding compositional structure directly into quantum
circuits. This paper investigates the application of QNLP models to the task of
Natural Language Inference (NLI), comparing quantum, hybrid, and classical
transformer-based models under a constrained few-shot setting. Using the lambeq
library and the DisCoCat framework, we construct parameterized quantum circuits
for sentence pairs and train them for both semantic relatedness and inference
classification. To assess efficiency, we introduce a novel
information-theoretic metric, Information Gain per Parameter (IGPP), which
quantifies learning dynamics independent of model size. Our results demonstrate
that quantum models achieve performance comparable to classical baselines while
operating with dramatically fewer parameters. The Quantum-based models
outperform randomly initialized transformers in inference and achieve lower
test error on relatedness tasks. Moreover, quantum models exhibit significantly
higher per-parameter learning efficiency (up to five orders of magnitude more
than classical counterparts), highlighting the promise of QNLP in low-resource,
structure-sensitive settings. To address circuit-level isolation and promote
parameter sharing, we also propose a novel cluster-based architecture that
improves generalization by tying gate parameters to learned word clusters
rather than individual tokens.

</details>


### [183] [Fusion-Augmented Large Language Models: Boosting Diagnostic Trustworthiness via Model Consensus](https://arxiv.org/abs/2510.16057)
*Md Kamrul Siam,Md Jobair Hossain Faruk,Jerry Q. Cheng,Huanying Gu*

Main category: cs.CL

TL;DR: 提出了一种基于ChatGPT和Claude的多模型融合框架，通过相似性共识和多模态输入显著提升胸部X光诊断的准确性。


<details>
  <summary>Details</summary>
Motivation: 提高胸部X光自动解读的可靠性，减少AI辅助诊断中的错误。

Method: 在CheXpert数据集上评估ChatGPT和Claude两个大模型的单模态和多模态表现，并采用基于输出相似性的共识融合策略。

Result: 单模态下Claude准确率为76.9%，融合后达77.6%；加入合成临床文本后，ChatGPT提升至84%，Claude为76%，共识准确率高达91.3%。

Conclusion: 多模型共识融合与多模态输入可有效提升AI在放射学诊断中的准确性与可信度，且计算开销小，具有临床应用潜力。

Abstract: This study presents a novel multi-model fusion framework leveraging two
state-of-the-art large language models (LLMs), ChatGPT and Claude, to enhance
the reliability of chest X-ray interpretation on the CheXpert dataset. From the
full CheXpert corpus of 224,316 chest radiographs, we randomly selected 234
radiologist-annotated studies to evaluate unimodal performance using image-only
prompts. In this setting, ChatGPT and Claude achieved diagnostic accuracies of
62.8% and 76.9%, respectively. A similarity-based consensus approach, using a
95% output similarity threshold, improved accuracy to 77.6%. To assess the
impact of multimodal inputs, we then generated synthetic clinical notes
following the MIMIC-CXR template and evaluated a separate subset of 50 randomly
selected cases paired with both images and synthetic text. On this multimodal
cohort, performance improved to 84% for ChatGPT and 76% for Claude, while
consensus accuracy reached 91.3%. Across both experimental conditions,
agreement-based fusion consistently outperformed individual models. These
findings highlight the utility of integrating complementary modalities and
using output-level consensus to improve the trustworthiness and clinical
utility of AI-assisted radiological diagnosis, offering a practical path to
reduce diagnostic errors with minimal computational overhead.

</details>


### [184] [Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs](https://arxiv.org/abs/2510.16062)
*Guiyao Tie,Zenghui Yuan,Zeli Zhao,Chaoran Hu,Tianhe Gu,Ruihang Zhang,Sizhe Zhang,Junran Wu,Xiaoyue Tu,Ming Jin,Qingsong Wen,Lixing Chen,Pan Zhou,Lichao Sun*

Main category: cs.CL

TL;DR: 提出CorrectBench基准，系统评估大语言模型自校正方法在常识、数学推理和代码生成任务中的有效性，发现自校正可提升性能但效率待优化。


<details>
  <summary>Details</summary>
Motivation: 尽管已有多种LLM自校正方法，但其有效性缺乏系统评估，且模型能否真正自我纠正尚存疑问，需全面研究以指导未来发展。

Method: 构建CorrectBench基准，涵盖内在、外在和微调三类自校正策略，在三种任务上进行广泛评估，并分析不同策略组合的效果。

Result: 自校正可提升准确率（尤其复杂任务）；混合策略效果更优但效率低；推理模型（如DeepSeek-R1）提升有限且耗时高；简单CoT基线表现高效且具竞争力。

Conclusion: 自校正有助于提升LLM推理能力，但效率问题仍是挑战，需进一步研究推理性能与运行效率的平衡。

Abstract: Self-correction of large language models (LLMs) emerges as a critical
component for enhancing their reasoning performance. Although various
self-correction methods have been proposed, a comprehensive evaluation of these
methods remains largely unexplored, and the question of whether LLMs can truly
correct themselves is a matter of significant interest and concern. In this
study, we introduce CorrectBench, a benchmark developed to evaluate the
effectiveness of self-correction strategies, including intrinsic, external, and
fine-tuned approaches, across three tasks: commonsense reasoning, mathematical
reasoning, and code generation. Our findings reveal that: 1) Self-correction
methods can improve accuracy, especially for complex reasoning tasks; 2) Mixing
different self-correction strategies yields further improvements, though it
reduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited
optimization under additional self-correction methods and have high time costs.
Interestingly, a comparatively simple chain-of-thought (CoT) baseline
demonstrates competitive accuracy and efficiency. These results underscore the
potential of self-correction to enhance LLM's reasoning performance while
highlighting the ongoing challenge of improving their efficiency. Consequently,
we advocate for further research focused on optimizing the balance between
reasoning capabilities and operational efficiency. Project Page:
https://correctbench.github.io/

</details>


### [185] [EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle](https://arxiv.org/abs/2510.16079)
*Rong Wu,Xiaoman Wang,Jianbiao Mei,Pinlong Cai,Daocheng Fu,Cheng Yang,Licheng Wen,Xuemeng Yang,Yufan Shen,Yuxin Wang,Botian Shi*

Main category: cs.CL

TL;DR: 提出EvolveR框架，通过离线自蒸馏和在线交互的闭环生命周期，使LLM代理能从自身经验中自我改进，提升复杂任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在工具使用上表现强，但缺乏从自身经验中系统学习的能力，难以迭代优化问题解决策略。

Method: 设计EvolveR框架，包含两个阶段：离线自蒸馏将交互轨迹提炼为可复用策略；在线交互中检索策略指导决策并积累新行为轨迹，通过策略强化机制迭代更新代理。

Result: 在多跳问答基准上，EvolveR优于强基线方法，展现出更优性能。

Conclusion: EvolveR实现了代理从自身行为后果中学习的闭环，为更自主、持续进化的智能系统提供了蓝图。

Abstract: Current Large Language Model (LLM) agents show strong performance in tool
use, but lack the crucial capability to systematically learn from their own
experiences. While existing frameworks mainly focus on mitigating external
knowledge gaps, they fail to address a more fundamental limitation: the
inability to iteratively refine problem-solving strategies. In this work, we
introduce EvolveR, a framework designed to enable agent to self-improve through
a complete, closed-loop experience lifecycle. This lifecycle comprises two key
stages: (1) Offline Self-Distillation, where the agent's interaction
trajectories are synthesized into a structured repository of abstract, reusable
strategic principles; (2) Online Interaction, where the agent interacts with
tasks and actively retrieves distilled principles to guide its decision-making,
accumulating a diverse set of behavioral trajectories. This loop employs a
policy reinforcement mechanism to iteratively update the agent based on its
performance. We demonstrate the effectiveness of EvolveR on complex multi-hop
question-answering benchmarks, where it achieves superior performance over
strong agentic baselines. Our work presents a comprehensive blueprint for
agents that learn not only from external data but also from the consequences of
their own actions, paving the way for more autonomous and continuously
improving systems. Code is available at https://github.com/Edaizi/EvolveR.

</details>


### [186] [Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification](https://arxiv.org/abs/2510.16091)
*Binglan Han,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: 该研究系统评估了不同提示策略与大语言模型（LLM）在自动化系统文献综述（SLR）筛选阶段的交互效果，发现模型与提示之间存在显著交互作用，推荐基于成本与性能权衡的分阶段自动化流程。


<details>
  <summary>Details</summary>
Motivation: 系统文献综述的筛选阶段耗时且劳动密集，亟需利用大语言模型实现自动化；但不同提示策略与模型组合的效果尚不明确，缺乏系统性评估与实用指导。

Method: 评估六个LLM（如GPT-4o、DeepSeek等）在五种提示类型（零样本、少样本、思维链、少样本思维链、自我反思）下的表现，涵盖相关性分类与六个二级任务，使用准确性、精确率、召回率和F1值作为指标，并进行成本-性能分析。

Result: CoT-few-shot在精确率与召回率之间表现最均衡；零样本提示召回率最高，适合高敏感性初筛；自我反思表现不佳；GPT-4o和DeepSeek整体性能稳健，GPT-4o-mini在成本效益上表现突出；结构化提示在GPT-4o-mini上以小幅成本提升带来良好F1值。

Conclusion: 提示策略与模型选择显著影响自动化文献筛选效果，建议采用分阶段策略：先用低成本模型加结构化提示进行初筛，仅将模糊案例交由高性能模型处理，为任务自适应的LLM部署提供实用基准与指导。

Abstract: This study quantifies how prompting strategies interact with large language
models (LLMs) to automate the screening stage of systematic literature reviews
(SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3,
Gemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types
(zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection)
across relevance classification and six Level-2 tasks, using accuracy,
precision, recall, and F1. Results show pronounced model-prompt interaction
effects: CoT-few-shot yields the most reliable precision-recall balance;
zero-shot maximizes recall for high-sensitivity passes; and self-reflection
underperforms due to over-inclusivity and instability across models. GPT-4o and
DeepSeek provide robust overall performance, while GPT-4o-mini performs
competitively at a substantially lower dollar cost. A cost-performance analysis
for relevance classification (per 1,000 abstracts) reveals large absolute
differences among model-prompt pairings; GPT-4o-mini remains low-cost across
prompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer
attractive F1 at a small incremental cost. We recommend a staged workflow that
(1) deploys low-cost models with structured prompts for first-pass screening
and (2) escalates only borderline cases to higher-capacity models. These
findings highlight LLMs' uneven but promising potential to automate literature
screening. By systematically analyzing prompt-model interactions, we provide a
comparative benchmark and practical guidance for task-adaptive LLM deployment.

</details>


### [187] [Facts in Stats: Impacts of Pretraining Diversity on Language Model Generalization](https://arxiv.org/abs/2510.16096)
*Tina Behnia,Puneesh Deora,Christos Thrampoulidis*

Main category: cs.CL

TL;DR: 本文提出了一种灵活的合成测试平台，通过控制语言模型中统计规律与事实关联的交互，系统研究了上下文多样性对模型泛化能力的影响，并揭示了嵌入层和解嵌入层在OOD泛化中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对语言模型中统计规律与事实关联交互影响的系统分析，尤其是上下文多样性如何影响模型在分布内外的泛化能力。

Method: 构建一个包含通用标记流和抽象事实流（源-目标对）的合成测试平台，独立调控上下文结构和多样性水平，进行受控实验并干预模型组件以分析其影响。

Result: 发现较高的上下文多样性会延迟ID事实准确率，其对OOD泛化的影响取决于上下文结构；某些情况下多样性促进非平凡事实回忆，最优多样性水平受训练时长影响；部分结构中统计或事实泛化单独失败，甚至两者均退化；OOD失败可归因于嵌入和解嵌入层的优化瓶颈。

Conclusion: 上下文设计与多样性水平的相互作用显著影响模型不同方面的泛化能力，该合成框架为未来研究提供了可隔离混淆因素的可控实验环境。

Abstract: Language models are pretrained on sequences that blend statistical
regularities (making text fluent) with factual associations between specific
tokens (knowledge of facts). While recent work suggests that the variability of
their interaction, such as paraphrases of factual associations, critically
determines generalization ability, we lack a systematic analysis of these
impacts. This paper introduces a flexible synthetic testbed that combines a
statistical stream of generic tokens with an abstract factual stream of
source-target token pairs, enabling fine-grained control over their
interaction. The design enables the independent control of diversity nature by
manipulating stream composition (contextual structure) and the diversity level
by varying which statistical streams each fact appears in. Through controlled
experiments, we find that while higher contextual diversity delays
in-distribution (ID) factual accuracy, its impact on out-of-distribution (OOD)
factual generalization depends critically on contextual structure. In some
cases, OOD performance follows the same trend as ID, but in others, diversity
becomes essential for non-trivial factual recall. Even when low diversity
prohibits factual recall, optimal diversity levels depend on training duration.
Beyond factual recall failures, we identify structures where statistical
generalization fails independently, and others where both capabilities degrade.
This shows how the interplay between contextual design and diversity level
impacts different generalization aspects. Further, through a series of
controlled interventions on the model components, we trace the OOD failures to
distinct optimization bottlenecks, highlighting the importance of the embedding
and unembedding layers. Our synthetic framework allows us to isolate effects
that would be confounded in large-scale studies, offering a controlled testbed
for future investigations.

</details>


### [188] [In Generative AI We (Dis)Trust? Computational Analysis of Trust and Distrust in Reddit Discussions](https://arxiv.org/abs/2510.16173)
*Aria Pessianzadeh,Naima Sultana,Hildegarde Van den Bulck,David Gefen,Shahin Jabari,Rezvaneh Rezapour*

Main category: cs.CL

TL;DR: 本文首次通过大规模Reddit数据集对生成式AI的信任与不信任进行计算分析，发现信任与不信任总体平衡，技术性能与用户体验是主要影响因素，个人经历是最常见态度成因。


<details>
  <summary>Details</summary>
Motivation: 现有AI信任研究多基于心理学与人机交互，缺乏可扩展的、长期的、计算驱动的方法来系统衡量公众对生成式AI的信任与不信任。

Method: 基于2022–2025年Reddit上39个子版块共197,618篇帖子构建数据集，结合众包标注与分类模型，对信任与不信任进行量化与分类分析。

Result: 发现信任与不信任在时间上几乎平衡，重大模型发布会引起波动；技术性能与可用性是主要关注维度，个人使用经验是最常见态度驱动因素；不同用户群体（如专家、伦理研究者、普通用户）表现出明显差异。

Conclusion: 提出了适用于大规模AI信任分析的方法框架，并揭示了公众对生成式AI态度的动态演变，为AI治理与系统设计提供实证依据。

Abstract: The rise of generative AI (GenAI) has impacted many aspects of human life. As
these systems become embedded in everyday practices, understanding public trust
in them also becomes essential for responsible adoption and governance. Prior
work on trust in AI has largely drawn from psychology and human-computer
interaction, but there is a lack of computational, large-scale, and
longitudinal approaches to measuring trust and distrust in GenAI and large
language models (LLMs). This paper presents the first computational study of
Trust and Distrust in GenAI, using a multi-year Reddit dataset (2022--2025)
spanning 39 subreddits and 197,618 posts. Crowd-sourced annotations of a
representative sample were combined with classification models to scale
analysis. We find that Trust and Distrust are nearly balanced over time, with
shifts around major model releases. Technical performance and usability
dominate as dimensions, while personal experience is the most frequent reason
shaping attitudes. Distinct patterns also emerge across trustors (e.g.,
experts, ethicists, general users). Our results provide a methodological
framework for large-scale Trust analysis and insights into evolving public
perceptions of GenAI.

</details>


### [189] [EgMM-Corpus: A Multimodal Vision-Language Dataset for Egyptian Culture](https://arxiv.org/abs/2510.16198)
*Mohamed Gamil,Abdelrahman Elsayed,Abdelrahman Lila,Ahmed Gad,Hesham Abdelgawad,Mohamed Aref,Ahmed Fares*

Main category: cs.CL

TL;DR: 本文介绍了EgMM-Corpus，一个专注于埃及文化的多模态数据集，包含3000多张图像和313个文化概念，用于评估和训练视觉-语言模型，揭示了现有模型在埃及文化背景下的文化偏见。


<details>
  <summary>Details</summary>
Motivation: 当前多模态数据集在中东和非洲等文化多样性区域仍十分有限，缺乏对特定文化（如埃及文化）的覆盖，导致视觉-语言模型在这些文化背景下的表现受限。

Method: 设计并实施了一个新的数据收集流程，收集了涵盖地标、食物和民间传说的313个文化概念，共3000多张图像，并对每条数据进行了文化真实性与多模态一致性的手动验证。

Result: 在EgMM-Corpus上评估了CLIP模型的零样本分类性能，Top-1准确率为21.2%，Top-5准确率为36.4%，表明现有模型存在显著文化偏见。

Conclusion: EgMM-Corpus为埃及文化背景下的视觉-语言模型提供了可靠的基准，凸显了构建文化感知AI模型的重要性。

Abstract: Despite recent advances in AI, multimodal culturally diverse datasets are
still limited, particularly for regions in the Middle East and Africa. In this
paper, we introduce EgMM-Corpus, a multimodal dataset dedicated to Egyptian
culture. By designing and running a new data collection pipeline, we collected
over 3,000 images, covering 313 concepts across landmarks, food, and folklore.
Each entry in the dataset is manually validated for cultural authenticity and
multimodal coherence. EgMM-Corpus aims to provide a reliable resource for
evaluating and training vision-language models in an Egyptian cultural context.
We further evaluate the zero-shot performance of Contrastive Language-Image
Pre-training CLIP on EgMM-Corpus, on which it achieves 21.2% Top-1 accuracy and
36.4% Top-5 accuracy in classification. These results underscore the existing
cultural bias in large-scale vision-language models and demonstrate the
importance of EgMM-Corpus as a benchmark for developing culturally aware
models.

</details>


### [190] [What Can String Probability Tell Us About Grammaticality?](https://arxiv.org/abs/2510.16227)
*Jennifer Hu,Ethan Gotlieb Wilcox,Siyuan Song,Kyle Mahowald,Roger P. Levy*

Main category: cs.CL

TL;DR: 本文探讨了语言模型学到的语法知识，基于语料生成过程的简单假设，提出了三个理论预测并用中英文句子对进行实证验证。


<details>
  <summary>Details</summary>
Motivation: 由于概率与语法性在语言学中是不同的概念，因此通过字符串概率来理解语言模型的语法知识并不直接，本文旨在建立概率与语法知识之间的理论联系。

Method: 提出一个关于语法、意义和字符串概率之间关系的理论分析框架，并基于此框架做出三个预测，使用280K对英汉句子进行实证检验。

Result: 验证了三个预测：在最小对内部字符串概率之间的相关性；模型与人类在最小对中的差异相关性；以及语法正确与错误字符串在概率空间中分离较差。

Conclusion: 该分析为利用概率研究语言模型的结构知识提供了理论基础，并为未来语言模型语法评估的研究指明了方向。

Abstract: What have language models (LMs) learned about grammar? This question remains
hotly debated, with major ramifications for linguistic theory. However, since
probability and grammaticality are distinct notions in linguistics, it is not
obvious what string probabilities can reveal about an LM's underlying
grammatical knowledge. We present a theoretical analysis of the relationship
between grammar, meaning, and string probability, based on simple assumptions
about the generative process of corpus data. Our framework makes three
predictions, which we validate empirically using 280K sentence pairs in English
and Chinese: (1) correlation between the probability of strings within minimal
pairs, i.e., string pairs with minimal semantic differences; (2) correlation
between models' and humans' deltas within minimal pairs; and (3) poor
separation in probability space between unpaired grammatical and ungrammatical
strings. Our analyses give theoretical grounding for using probability to learn
about LMs' structural knowledge, and suggest directions for future work in LM
grammatical evaluation.

</details>


### [191] [Towards Low-Resource Alignment to Diverse Perspectives with Sparse Feedback](https://arxiv.org/abs/2510.16257)
*Chu Fei Luo,Samuel Dahan,Xiaodan Zhu*

Main category: cs.CL

TL;DR: 提出并实现了两种在低资源环境下提升语言模型多元对齐的方法：多元解码和模型引导，显著改善了高风险任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型训练范式通常假设每个问题只有一个最优答案，导致回应过于泛化且难以反映人类价值观的多样性与细微差异。

Method: 提出两种方法：多元解码（pluralistic decoding）和模型引导（model steering），在仅有50个标注样本的低资源条件下进行实验，并评估其对模型对齐性能的影响。

Result: 模型引导方法在多个高风险任务（如仇恨言论和虚假信息检测）中显著降低误报率，并在GlobalOpinionQA上提升了对人类价值观分布的对齐程度，优于零样本和少样本基线。

Conclusion: 研究表明，引入多样性与细微视角可有效提升语言模型的对齐性，强调了在模型设计中考虑多元价值观的重要性。

Abstract: As language models have a greater impact on society, it is important to
ensure they are aligned to a diverse range of perspectives and are able to
reflect nuance in human values. However, the most popular training paradigms
for modern language models often assume there is one optimal answer for every
query, leading to generic responses and poor alignment. In this work, we aim to
enhance pluralistic alignment of language models in a low-resource setting with
two methods: pluralistic decoding and model steering. We empirically
demonstrate that model steering offers consistent improvement over zero-shot
and few-shot baselines with only 50 annotated samples. Our proposed methods
decrease false positives in several high-stakes tasks such as hate speech
detection and misinformation detection, and improves the distributional
alignment to human values in GlobalOpinionQA. We hope our work highlights the
importance of diversity and how language models can be adapted to consider
nuanced perspectives.

</details>


### [192] [Instant Personalized Large Language Model Adaptation via Hypernetwork](https://arxiv.org/abs/2510.16282)
*Zhaoxuan Tan,Zixuan Zhang,Haoyang Wen,Zheng Li,Rongzhi Zhang,Pei Chen,Fengran Mo,Zheyuan Liu,Qingkai Zeng,Qingyu Yin,Meng Jiang*

Main category: cs.CL

TL;DR: 提出Profile-to-PEFT框架，利用超网络将用户画像直接映射为适配器参数，实现高效、可扩展的个性化大语言模型，无需为每个用户单独训练。


<details>
  <summary>Details</summary>
Motivation: 现有个性化大语言模型方法（如OPPU）需为每位用户训练独立适配器，计算开销大、难以实时更新，缺乏可扩展性和实用性。

Method: 提出Profile-to-PEFT，使用端到端训练的超网络，将编码后的用户画像直接生成适配器参数（如LoRA），实现免训练的即时个性化适配。

Result: 该方法在性能上优于基于提示的个性化和OPPU方法，部署时计算资源消耗更少，且对未见用户、不同用户活跃度和嵌入骨干均表现出强泛化性和鲁棒性。

Conclusion: Profile-to-PEFT实现了高效、可扩展、自适应的LLM个性化，适用于大规模应用，支持即时更新和本地化隐私保护部署。

Abstract: Personalized large language models (LLMs) tailor content to individual
preferences using user profiles or histories. However, existing
parameter-efficient fine-tuning (PEFT) methods, such as the
``One-PEFT-Per-User'' (OPPU) paradigm, require training a separate adapter for
each user, making them computationally expensive and impractical for real-time
updates. We introduce Profile-to-PEFT, a scalable framework that employs a
hypernetwork, trained end-to-end, to map a user's encoded profile directly to a
full set of adapter parameters (e.g., LoRA), eliminating per-user training at
deployment. This design enables instant adaptation, generalization to unseen
users, and privacy-preserving local deployment. Experimental results
demonstrate that our method outperforms both prompt-based personalization and
OPPU while using substantially fewer computational resources at deployment. The
framework exhibits strong generalization to out-of-distribution users and
maintains robustness across varying user activity levels and different
embedding backbones. The proposed Profile-to-PEFT framework enables efficient,
scalable, and adaptive LLM personalization suitable for large-scale
applications.

</details>


### [193] [Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models](https://arxiv.org/abs/2510.16340)
*Pratham Singla,Shivank Garg,Ayush Singh,Ishan Garg,Ketan Suhaas Saichandran*

Main category: cs.CL

TL;DR: 该研究探讨了经过后训练的大语言模型（LLM）在逻辑密集型任务中是否具备对其“学习”和“思考”过程的意识，定义并评估了三种核心能力：对隐含策略的认知、跨领域的泛化能力以及推理过程与输出的一致性。


<details>
  <summary>Details</summary>
Motivation: 随着后训练技术的发展，LLMs 能通过生成规划 token 处理复杂任务，但它们是否真正‘意识到’自身的学习与推理过程仍不清楚。本文旨在探究 LLMs 的自我认知与推理一致性。

Method: 提出三个评估维度：策略认知、跨域泛化、推理与输出对齐；在多个需学习不同策略的任务上实证评估 SFT、DPO 和 GRPO 三种后训练方法下的模型表现。

Result: 强化学习训练的模型（如 DPO、GRPO）比 SFT 模型更具策略意识和泛化能力，但其内部推理轨迹与最终输出之间的对齐较弱，尤其是在 GRPO 模型中更为明显。

Conclusion: 尽管 RL-based 后训练提升了模型的策略认知与泛化能力，但其推理过程与输出之间存在不一致，提示当前 LLMs 的“思考”可能并非真正反映其决策机制。

Abstract: Recent advances in post-training techniques have endowed Large Language
Models (LLMs) with enhanced capabilities for tackling complex, logic-intensive
tasks through the generation of supplementary planning tokens. This development
raises a fundamental question: Are these models aware of what they "learn" and
"think"? To address this, we define three core competencies: (1) awareness of
learned latent policies, (2) generalization of these policies across domains,
and (3) alignment between internal reasoning traces and final outputs. We
empirically evaluate these abilities on several tasks, each designed to require
learning a distinct policy. Furthermore, we contrast the profiles of models
post-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization
(DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate
that RL-trained models not only demonstrate greater awareness of their learned
behaviors and stronger generalizability to novel, structurally similar tasks
than SFT models but also often exhibit weak alignment between their reasoning
traces and final outputs, an effect most pronounced in GRPO-trained models.

</details>


### [194] [Utilising Large Language Models for Generating Effective Counter Arguments to Anti-Vaccine Tweets](https://arxiv.org/abs/2510.16359)
*Utsav Dhanuka,Soham Poddar,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: 该研究探讨了利用大语言模型（LLM）生成针对疫苗错误信息的有效反驳论点，结合分类与优化提示策略，提升了实时辟谣能力。


<details>
  <summary>Details</summary>
Motivation: 疫苗错误信息在社交媒体上广泛传播，阻碍免疫率提升并削弱公众对健康建议的信任，亟需有效工具实时生成针对性反驳。

Method: 采用多种提示策略和微调方法优化LLM生成反论点，并训练分类器对反疫苗推文进行多标签分类（如疗效担忧、副作用、政治影响），以生成更契合上下文的回应。

Result: 通过人工评估、LLM评分和自动指标的多方验证，发现整合标签描述和结构化微调显著提升反驳效果，且各评估方式结果高度一致。

Conclusion: 结合分类信息与结构化训练的LLM可有效生成高质量反论点，为大规模应对疫苗 misinformation 提供可行方案。

Abstract: In an era where public health is increasingly influenced by information
shared on social media, combatting vaccine skepticism and misinformation has
become a critical societal goal. Misleading narratives around vaccination have
spread widely, creating barriers to achieving high immunisation rates and
undermining trust in health recommendations. While efforts to detect
misinformation have made significant progress, the generation of real time
counter-arguments tailored to debunk such claims remains an insufficiently
explored area. In this work, we explore the capabilities of LLMs to generate
sound counter-argument rebuttals to vaccine misinformation. Building on prior
research in misinformation debunking, we experiment with various prompting
strategies and fine-tuning approaches to optimise counter-argument generation.
Additionally, we train classifiers to categorise anti-vaccine tweets into
multi-labeled categories such as concerns about vaccine efficacy, side effects,
and political influences allowing for more context aware rebuttals. Our
evaluation, conducted through human judgment, LLM based assessments, and
automatic metrics, reveals strong alignment across these methods. Our findings
demonstrate that integrating label descriptions and structured fine-tuning
enhances counter-argument effectiveness, offering a promising approach for
mitigating vaccine misinformation at scale.

</details>


### [195] [End-to-End Argument Mining through Autoregressive Argumentative Structure Prediction](https://arxiv.org/abs/2510.16363)
*Nilmadhab Das,Vishal Vaibhav,Yash Sunil Choudhary,V. Vijaya Saradhi,Ashish Anand*

Main category: cs.CL

TL;DR: 本文提出了一种端到端的自动回归论证结构预测（AASP）框架，用于联合建模论证挖掘中的论证成分和关系，通过预定义动作序列逐步构建论证结构，在多个基准上达到或接近最先进性能。


<details>
  <summary>Details</summary>
Motivation: 论证挖掘中建模论证成分与关系之间的依赖性具有挑战性，现有方法通常将结构展平处理，难以捕捉复杂的推理流程，因此需要一种能联合建模并保留结构依赖的方法。

Method: 提出AASP框架，基于条件预训练语言模型，将论证结构建模为受约束的预定义动作集合，通过自回归方式逐步生成论证结构，实现端到端的联合学习。

Result: 在三个标准论证挖掘基准上进行了实验，AASP在两个基准上取得了最先进（SoTA）结果，在另一个基准上表现强劲。

Conclusion: AASP能够有效捕捉论证推理流程，为论证挖掘提供了一种高效的端到端联合建模范式。

Abstract: Argument Mining (AM) helps in automating the extraction of complex
argumentative structures such as Argument Components (ACs) like Premise, Claim
etc. and Argumentative Relations (ARs) like Support, Attack etc. in an
argumentative text. Due to the inherent complexity of reasoning involved with
this task, modelling dependencies between ACs and ARs is challenging. Most of
the recent approaches formulate this task through a generative paradigm by
flattening the argumentative structures. In contrast to that, this study
jointly formulates the key tasks of AM in an end-to-end fashion using
Autoregressive Argumentative Structure Prediction (AASP) framework. The
proposed AASP framework is based on the autoregressive structure prediction
framework that has given good performance for several NLP tasks. AASP framework
models the argumentative structures as constrained pre-defined sets of actions
with the help of a conditional pre-trained language model. These actions build
the argumentative structures step-by-step in an autoregressive manner to
capture the flow of argumentative reasoning in an efficient way. Extensive
experiments conducted on three standard AM benchmarks demonstrate that AASP
achieves state-of-theart (SoTA) results across all AM tasks in two benchmarks
and delivers strong results in one benchmark.

</details>


### [196] [Navigating through the hidden embedding space: steering LLMs to improve mental health assessment](https://arxiv.org/abs/2510.16373)
*Federico Ravenda,Seyed Ali Bahrainian,Andrea Raballo,Antonietta Mira*

Main category: cs.CL

TL;DR: 提出一种轻量级且高效的方法，通过线性变换和引导向量提升大语言模型在心理健康评估中的性能，无需计算密集型技术。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型迅速发展，但在特定领域（如心理健康）中，小规模模型仍难以达到理想性能，需低成本且高效的优化方法。

Method: 在模型某一层的激活上应用线性变换，利用 steering vectors 引导模型输出，实现对心理健康评估任务的优化。

Result: 该方法在两个任务上均取得改进：1）判断Reddit帖子是否有助于识别抑郁症状；2）基于用户发帖历史完成抑郁症标准化筛查问卷。

Conclusion: steering机制是一种计算高效、潜力巨大的大语言模型在心理健康领域适应的方法。

Abstract: The rapid evolution of Large Language Models (LLMs) is transforming AI,
opening new opportunities in sensitive and high-impact areas such as Mental
Health (MH). Yet, despite these advancements, recent evidence reveals that
smaller-scale models still struggle to deliver optimal performance in
domain-specific applications. In this study, we present a cost-efficient yet
powerful approach to improve MH assessment capabilities of an LLM, without
relying on any computationally intensive techniques. Our lightweight method
consists of a linear transformation applied to a specific layer's activations,
leveraging steering vectors to guide the model's output. Remarkably, this
intervention enables the model to achieve improved results across two distinct
tasks: (1) identifying whether a Reddit post is useful for detecting the
presence or absence of depressive symptoms (relevance prediction task), and (2)
completing a standardized psychological screening questionnaire for depression
based on users' Reddit post history (questionnaire completion task). Results
highlight the untapped potential of steering mechanisms as computationally
efficient tools for LLMs' MH domain adaptation.

</details>


### [197] [MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes](https://arxiv.org/abs/2510.16380)
*Yu Ying Chiu,Michael S. Lee,Rachel Calcott,Brandon Handoko,Paul de Font-Reaulx,Paula Rodriguez,Chen Bo Calvin Zhang,Ziwen Han,Udari Madhushani Sehwag,Yash Maurya,Christina Q Knight,Harry R. Lloyd,Florence Bacus,Mantas Mazeika,Bing Liu,Yejin Choi,Mitchell L Gordon,Sydney Levine*

Main category: cs.CL

TL;DR: 本文提出了MoReBench和MoReBench-Theory两个基准，用于评估语言模型在道德困境中的推理过程，发现现有模型在道德推理上存在偏倚，且传统缩放规律无法预测其表现。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在决策中扮演越来越重要的角色，确保其决策符合人类价值观至关重要。理解AI如何进行道德推理，而不仅仅是其结论，是实现可解释和安全AI的关键。

Method: 构建包含1000个道德场景的MoReBench，每个场景配有专家制定的评估标准（如识别道德考量、权衡取舍等）；另构建MoReBench-Theory，包含150个基于五大规范伦理理论的测试案例，用于评估模型在不同道德框架下的推理能力。

Result: 实验表明，模型在道德推理上的表现不能由数学、代码或科学推理任务的缩放规律预测；模型对某些道德框架（如边沁功利主义和康德义务论）表现出偏好，可能是当前训练范式的结果。

Conclusion: MoReBench系列基准推动了以过程为导向的AI推理评估，有助于实现更透明、更安全的AI系统。

Abstract: As AI systems progress, we rely more on them to make decisions with us and
for us. To ensure that such decisions are aligned with human values, it is
imperative for us to understand not only what decisions they make but also how
they come to those decisions. Reasoning language models, which provide both
final responses and (partially transparent) intermediate thinking traces,
present a timely opportunity to study AI procedural reasoning. Unlike math and
code problems which often have objectively correct answers, moral dilemmas are
an excellent testbed for process-focused evaluation because they allow for
multiple defensible conclusions. To do so, we present MoReBench: 1,000 moral
scenarios, each paired with a set of rubric criteria that experts consider
essential to include (or avoid) when reasoning about the scenarios. MoReBench
contains over 23 thousand criteria including identifying moral considerations,
weighing trade-offs, and giving actionable recommendations to cover cases on AI
advising humans moral decisions as well as making moral decisions autonomously.
Separately, we curate MoReBench-Theory: 150 examples to test whether AI can
reason under five major frameworks in normative ethics. Our results show that
scaling laws and existing benchmarks on math, code, and scientific reasoning
tasks fail to predict models' abilities to perform moral reasoning. Models also
show partiality towards specific moral frameworks (e.g., Benthamite Act
Utilitarianism and Kantian Deontology), which might be side effects of popular
training paradigms. Together, these benchmarks advance process-focused
reasoning evaluation towards safer and more transparent AI.

</details>


### [198] [ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents](https://arxiv.org/abs/2510.16381)
*David Peer,Sebastian Stabinger*

Main category: cs.CL

TL;DR: 提出了一种名为自主可信代理（ATA）的通用神经符号方法，通过将任务解耦为离线知识摄入和在线任务处理两个阶段，提升大语言模型在高风险场景中的可信度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在关键领域应用受限，主要因其存在幻觉、不稳定性及缺乏透明性等可信度问题，需一种更可靠、可验证的推理架构。

Method: 采用神经符号方法，分两阶段：离线阶段用大语言模型将非正式问题描述转化为可由人类验证的符号知识库；在线阶段将输入编码为相同形式语言，并由符号决策引擎结合知识库进行推理。

Result: 在复杂推理任务中，ATA在完全自动化设置下与最先进的端到端模型性能相当；在人类验证知识库后，显著超越更大模型，且具备完美确定性、抗输入扰动的稳定性，以及对提示注入攻击的免疫力。

Conclusion: ATA通过基于符号推理的决策，提供了一种透明、可审计、可控且可靠的下一代自主代理架构，有效提升系统的可信度。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities, yet
their deployment in high-stakes domains is hindered by inherent limitations in
trustworthiness, including hallucinations, instability, and a lack of
transparency. To address these challenges, we introduce a generic
neuro-symbolic approach, which we call Autonomous Trustworthy Agents (ATA). The
core of our approach lies in decoupling tasks into two distinct phases: Offline
knowledge ingestion and online task processing. During knowledge ingestion, an
LLM translates an informal problem specification into a formal, symbolic
knowledge base. This formal representation is crucial as it can be verified and
refined by human experts, ensuring its correctness and alignment with domain
requirements. In the subsequent task processing phase, each incoming input is
encoded into the same formal language. A symbolic decision engine then utilizes
this encoded input in conjunction with the formal knowledge base to derive a
reliable result. Through an extensive evaluation on a complex reasoning task,
we demonstrate that a concrete implementation of ATA is competitive with
state-of-the-art end-to-end reasoning models in a fully automated setup while
maintaining trustworthiness. Crucially, with a human-verified and corrected
knowledge base, our approach significantly outperforms even larger models,
while exhibiting perfect determinism, enhanced stability against input
perturbations, and inherent immunity to prompt injection attacks. By generating
decisions grounded in symbolic reasoning, ATA offers a practical and
controllable architecture for building the next generation of transparent,
auditable, and reliable autonomous agents.

</details>


### [199] [Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment](https://arxiv.org/abs/2510.16387)
*Fu-An Chao,Bi-Cheng Yan,Berlin Chen*

Main category: cs.CL

TL;DR: 本研究探索了Whisper語音識別模型在第二語言口語評估中的潛力，通過提取其隱藏層的聲學與語言特徵，僅用輕量分類器即在GEPT資料集上超越現有先進方法。


<details>
  <summary>Details</summary>
Motivation: 過去研究多僅使用Whisper的轉錄結果進行外部分析，本論文旨在深入探討其未經微調的隱含表現能力，以發揮其在口語評估中的潛在價值。

Method: 從Whisper的中間與最終輸出提取音頻與語言特徵，訓練輕量級分類器進行評分；並加入圖像與文字提示作為輔助相關性線索以提升性能。

Result: 在GEPT口說描述資料集上表現優於現有最先进基線，包括多模態方法；加入輔助資訊後進一步提升表現。

Conclusion: 即使未經任務微調，Whisper的嵌入已內建口語熟練度序階與語義資訊，顯示其作為口語理解任務強大基礎模型的潛力。

Abstract: In this paper, we explore the untapped potential of Whisper, a
well-established automatic speech recognition (ASR) foundation model, in the
context of L2 spoken language assessment (SLA). Unlike prior studies that
extrinsically analyze transcriptions produced by Whisper, our approach goes a
step further to probe its latent capabilities by extracting acoustic and
linguistic features from hidden representations. With only a lightweight
classifier being trained on top of Whisper's intermediate and final outputs,
our method achieves strong performance on the GEPT picture-description dataset,
outperforming existing cutting-edge baselines, including a multimodal approach.
Furthermore, by incorporating image and text-prompt information as auxiliary
relevance cues, we demonstrate additional performance gains. Finally, we
conduct an in-depth analysis of Whisper's embeddings, which reveals that, even
without task-specific fine-tuning, the model intrinsically encodes both ordinal
proficiency patterns and semantic aspects of speech, highlighting its potential
as a powerful foundation for SLA and other spoken language understanding tasks.

</details>


### [200] [FrugalPrompt: Reducing Contextual Overhead in Large Language Models via Token Attribution](https://arxiv.org/abs/2510.16439)
*Syed Rifat Raiyan,Md Farhan Ishmam,Abdullah Al Imran,Mohammad Ali Moni*

Main category: cs.CL

TL;DR: 提出FrugalPrompt框架，通过保留输入中语义最重要的token来压缩提示，显著减少LLM的上下文冗余，在多数任务中仅轻微损失性能，但在数学推理任务中性能下降明显。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）因长上下文导致高成本、高碳排放和延迟，且输入中存在大量低效冗余token，仅少数token承载主要语义信息，因此亟需高效提示压缩方法。

Method: 提出FrugalPrompt框架，利用GlobEnc和DecompX两种先进token归因方法计算每个token的重要性得分，保留得分最高的前k% token并保持其原始顺序，生成稀疏的压缩提示。

Result: 在情感分析、常识问答和摘要任务中，20%的提示缩减仅造成轻微性能下降；而在数学推理任务中性能显著下降。使用最低和随机k% token的对比实验揭示了任务间性能不对称，暗示可能存在任务污染效应。

Conclusion: FrugalPrompt有效揭示了不同NLP任务对上下文稀疏性的容忍度差异，有助于理解LLM在性能与效率间的权衡，划清了可压缩与需完整上下文的任务边界。

Abstract: Large language models (LLMs) owe much of their stellar performance to
expansive input contexts, yet such verbosity inflates monetary costs, carbon
footprint, and inference-time latency. Much of this overhead manifests from the
redundant low-utility tokens present in typical prompts, as only a fraction of
tokens typically carries the majority of the semantic weight. We address this
inefficiency by introducing FrugalPrompt, a novel prompt compression framework
for LLMs, which retains only the most semantically significant tokens.
Leveraging two state-of-the-art token attribution methods, GlobEnc and DecompX,
we assign salience scores to every token in an input sequence, rank them to
preserve the top-k% tokens in their original order, and obtain a sparse
frugalized prompt. We evaluate the approach across four NLP tasks: Sentiment
Analysis, Commonsense QA, Summarization, and Mathematical Reasoning, using a
suite of frontier LLMs. For the first three tasks, a 20% prompt reduction
incurs only a marginal loss in task performance, demonstrating that
contemporary LLMs can reconstruct elided context from high-salience cues. In
contrast, performance on mathematical reasoning deteriorates sharply,
reflecting a stronger dependence on complete token continuity. Further analysis
with bottom-k% and random-k% tokens reveals asymmetric performance patterns
that may suggest potential task contamination effects, wherein models may
resort to shallow memorized patterns from pretraining exposure for conventional
NLP tasks. We posit that our work contributes to a more nuanced understanding
of LLM behavior in performance-efficiency trade-offs, and delineate the
boundary between tasks tolerant to contextual sparsity and those requiring
exhaustive context. Our source code and models are available at:
https://github.com/Starscream-11813/Frugal-ICL

</details>


### [201] [Hallucination Benchmark for Speech Foundation Models](https://arxiv.org/abs/2510.16567)
*Alkis Koudounas,Moreno La Quatra,Manuel Giollo,Sabato Marco Siniscalchi,Elena Baralis*

Main category: cs.CL

TL;DR: 本文提出了SHALLOW，首个系统分类和量化ASR中幻觉现象的基准框架，涵盖词汇、语音、形态和语义四个维度，并揭示了传统WER指标在高错误率条件下无法捕捉的细粒度错误模式。


<details>
  <summary>Details</summary>
Motivation: ASR中的幻觉现象会产生语法语义合理但与音频输入完全无关的转录，对医疗、法律等关键领域构成严重风险；传统基于错误率的指标无法区分幻觉与普通解码错误，亟需新的评估框架。

Method: 提出SHALLOW框架，从词汇、语音、形态和语义四个互补维度定义针对性指标，构建可解释的模型行为剖面图，并在多种架构和语音领域上进行评估。

Result: SHALLOW指标在识别质量高时与WER强相关，但在WER升高时相关性显著减弱，表明其能在恶劣条件下捕捉WER无法区分的细粒度错误模式。

Conclusion: SHALLOW能有效识别和评估ASR模型生成幻觉内容的倾向，支持对模型弱点的具体诊断，为模型改进提供超越聚合错误率的反馈。

Abstract: Hallucinations in automatic speech recognition (ASR) systems refer to fluent
and coherent transcriptions produced by neural ASR models that are completely
unrelated to the underlying acoustic input (i.e., the speech signal). While
similar to conventional decoding errors in potentially compromising the
usability of transcriptions for downstream applications, hallucinations can be
more detrimental due to their preservation of syntactically and semantically
plausible structure. This apparent coherence can mislead subsequent processing
stages and introduce serious risks, particularly in critical domains such as
healthcare and law. Conventional evaluation metrics are primarily centered on
error-based metrics and fail to distinguish between phonetic inaccuracies and
hallucinations. Consequently, there is a critical need for new evaluation
frameworks that can effectively identify and assess models with a heightened
propensity for generating hallucinated content. To this end, we introduce
SHALLOW, the first benchmark framework that systematically categorizes and
quantifies hallucination phenomena in ASR along four complementary axes:
lexical, phonetic, morphological, and semantic. We define targeted metrics
within each category to produce interpretable profiles of model behavior.
Through evaluation across various architectures and speech domains, we have
found that SHALLOW metrics correlate strongly with word error rate (WER) when
recognition quality is high (i.e., low WER). Still, this correlation weakens
substantially as WER increases. SHALLOW, therefore, captures fine-grained error
patterns that WER fails to distinguish under degraded and challenging
conditions. Our framework supports specific diagnosis of model weaknesses and
provides feedback for model improvement beyond what aggregate error rates can
offer.

</details>


### [202] [TrajSelector: Harnessing Latent Representations for Efficient and Effective Best-of-N in Large Reasoning Model](https://arxiv.org/abs/2510.16449)
*Bin Yu,Xinming Wang,Shijie Lian,Haotian Li,Changti Wu,Ruina Hu,Bailing Wang,Yuliang Wei,Kai Chen*

Main category: cs.CL

TL;DR: TrajSelector 是一种高效的 Best-of-N 框架，利用采样器 LLM 的隐藏状态进行推理路径评分，通过轻量级验证器实现更优的推理轨迹选择，性能优于现有方法且推理成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有外部测试时扩展方法（如 Best-of-N）存在计算开销高、未充分利用 LLM 内在隐含表征的问题，需要更高效且有效的推理路径选择机制。

Method: 提出 TrajSelector，使用仅 0.6B 参数的轻量级验证器，基于 LLM 隐藏状态对推理步骤进行逐层评分并聚合得分，端到端训练，无需大量逐步骤标注数据。

Result: 在五个基准上验证，Best-of-32 设置下比多数投票准确率高 4.61%，优于现有过程奖励模型 4.31% 到 12.21%，且推理成本更低。

Conclusion: TrajSelector 兼顾效率与性能，有效利用 LLM 隐状态实现高质量推理路径选择，为测试时扩展提供新思路。

Abstract: Large language models (LLMs) have shown remarkable progress in complex
reasoning tasks, largely enabled by test-time scaling (TTS) paradigms that
allocate additional compute during inference. Among these, external TTS
(particularly the Best-of-N selection paradigm) yields scalable performance
improvements by selecting from multiple independently generated reasoning
trajectories. However, this approach faces key limitations: (i) the high
computational overhead of deploying process reward models, (ii) the
underutilization of the LLM's intrinsic latent representations. We introduce
TrajSelector, an efficient and effective Best-of-N framework that exploit the
hidden states in the sampler LLM for process-level scoring. A lightweight
verifier (with only 0.6B parameters) evaluates the quality of step-wise
trajectory, and then aggregates these scores to identify the optimal reasoning
trajectory. Our framework employs a fully data-driven, end-to-end training
recipe that eliminates reliance on massive step-level annotations. Experiential
results across five benchmarks demonstrate that TrajSelector delivers
consistent performance gains. In Best-of-32 settings, it surpasses majority
voting by 4.61% accuracy and outperforms existing process reward models by
4.31% to 12.21%, all while maintaining lower inference costs.

</details>


### [203] [RAVEN: Robust Advertisement Video Violation Temporal Grounding via Reinforcement Reasoning](https://arxiv.org/abs/2510.16455)
*Deyi Ji,Yuekui Yang,Haiyang Wu,Shaoping Ma,Tianrun Chen,Lanyun Zhu*

Main category: cs.CL

TL;DR: RAVEN 是一种结合课程强化学习与多模态大语言模型的广告视频违规检测新框架，无需显式推理标注即可实现精准时序定位与违规分类，在工业数据集和公开基准上表现优越，并具备良好的在线应用性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的广告视频违规检测方法在时序精确定位、噪声标注和泛化能力方面存在挑战，且难以兼顾推理能力和实际部署需求。

Method: 提出 RAVEN 框架，采用课程强化学习结合多模态大语言模型，通过渐进式训练策略融合精确与粗略标注数据，利用 Group Relative Policy Optimization (GRPO) 培养 emergent 推理能力，并设计多层次奖励机制以实现精确时序定位和一致的类别预测。

Result: 在工业数据集和公开基准上，RAVEN 在违规类别准确率和时序定位方面均取得领先性能；在线 A/B 测试显示其显著提升精度和召回率；部署管道验证了其实用性，并展现出强泛化能力，缓解了监督微调中的灾难性遗忘问题。

Conclusion: RAVEN 有效提升了广告视频违规检测的推理、定位和泛化能力，无需显式推理标注即可通过强化学习激发模型认知能力，适用于实际在线服务场景。

Abstract: Advertisement (Ad) video violation detection is critical for ensuring
platform compliance, but existing methods struggle with precise temporal
grounding, noisy annotations, and limited generalization. We propose RAVEN, a
novel framework that integrates curriculum reinforcement learning with
multimodal large language models (MLLMs) to enhance reasoning and cognitive
capabilities for violation detection. RAVEN employs a progressive training
strategy, combining precisely and coarsely annotated data, and leverages Group
Relative Policy Optimization (GRPO) to develop emergent reasoning abilities
without explicit reasoning annotations. Multiple hierarchical sophisticated
reward mechanism ensures precise temporal grounding and consistent category
prediction. Experiments on industrial datasets and public benchmarks show that
RAVEN achieves superior performances in violation category accuracy and
temporal interval localization. We also design a pipeline to deploy the RAVEN
on the online Ad services, and online A/B testing further validates its
practical applicability, with significant improvements in precision and recall.
RAVEN also demonstrates strong generalization, mitigating the catastrophic
forgetting issue associated with supervised fine-tuning.

</details>


### [204] [Agree, Disagree, Explain: Decomposing Human Label Variation in NLI through the Lens of Explanations](https://arxiv.org/abs/2510.16458)
*Pingjun Hong,Beiduo Chen,Siyao Peng,Marie-Catherine de Marneffe,Benjamin Roth,Barbara Plank*

Main category: cs.CL

TL;DR: 该论文通过LiTEx分类法分析自然语言推断（NLI）标注中解释与标签的差异，揭示标注者在推理类型和标签选择上的个体差异，发现解释相似性比标签一致性更能反映语义一致性。


<details>
  <summary>Details</summary>
Motivation: 理解NLI数据集中标注者标签变异的背后原因，特别是当标注者在标签不一致时仍可能有相似的推理过程，挑战将标签视为绝对真实的做法。

Method: 应用LiTEx分类法分析两个英文NLI数据集中的自由文本解释，从NLI标签一致性、解释相似性、分类法一致性和标注者选择偏差多个角度对标注变异进行对齐分析。

Result: 发现部分标注者虽标签不同但解释高度相似，表明表面分歧下可能存在深层理解一致；同时识别出标注者在解释策略和标签选择上的个体偏好。

Conclusion: 基于推理的解释比单一标签更能反映语义一致性，强调应谨慎对待标签作为金标准，并倡导在NLI研究中纳入解释性信息以捕捉更丰富的语义差异。

Abstract: Natural Language Inference datasets often exhibit human label variation. To
better understand these variations, explanation-based approaches analyze the
underlying reasoning behind annotators' decisions. One such approach is the
LiTEx taxonomy, which categorizes free-text explanations in English into
reasoning types. However, previous work applying such taxonomies has focused on
within-label variation: cases where annotators agree on the final NLI label but
provide different explanations. In contrast, this paper broadens the scope by
examining how annotators may diverge not only in the reasoning type but also in
the labeling step. We use explanations as a lens to decompose the reasoning
process underlying NLI annotation and to analyze individual differences. We
apply LiTEx to two NLI English datasets and align annotation variation from
multiple aspects: NLI label agreement, explanation similarity, and taxonomy
agreement, with an additional compounding factor of annotators' selection bias.
We observe instances where annotators disagree on the label but provide highly
similar explanations, suggesting that surface-level disagreement may mask
underlying agreement in interpretation. Moreover, our analysis reveals
individual preferences in explanation strategies and label choices. These
findings highlight that agreement in reasoning types better reflects the
semantic similarity of free-text explanations than label agreement alone. Our
findings underscore the richness of reasoning-based explanations and the need
for caution in treating labels as ground truth.

</details>


### [205] [Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety](https://arxiv.org/abs/2510.16492)
*Vamshi Krishna Bonagiri,Ponnurangam Kumaragurum,Khanh Nguyen,Benjamin Plaut*

Main category: cs.CL

TL;DR: 提出“退出”机制作为大语言模型代理在复杂环境中缺乏信心时的安全行为策略，研究表明显式退出指令能显著提升安全性且几乎不影响帮助性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型代理在高风险现实场景中的应用增加，其安全性至关重要；现有不确定性量化方法难以应对多轮交互中累积的不确定性与模糊性。

Method: 利用ToolEmu框架，在12个主流大语言模型上系统评估显式退出指令下的“退出”行为。

Result: 加入显式退出指令后，模型安全性平均提升+0.39（0-3量表），其中闭源模型提升达+0.64，帮助性仅下降-0.03。

Conclusion: 显式退出指令是一种简单高效的安全机制，可立即部署于现有代理系统，是高风险应用中自主代理的有效第一道防线。

Abstract: As Large Language Model (LLM) agents increasingly operate in complex
environments with real-world consequences, their safety becomes critical. While
uncertainty quantification is well-studied for single-turn tasks, multi-turn
agentic scenarios with real-world tool access present unique challenges where
uncertainties and ambiguities compound, leading to severe or catastrophic risks
beyond traditional text generation failures. We propose using "quitting" as a
simple yet effective behavioral mechanism for LLM agents to recognize and
withdraw from situations where they lack confidence. Leveraging the ToolEmu
framework, we conduct a systematic evaluation of quitting behavior across 12
state-of-the-art LLMs. Our results demonstrate a highly favorable
safety-helpfulness trade-off: agents prompted to quit with explicit
instructions improve safety by an average of +0.39 on a 0-3 scale across all
models (+0.64 for proprietary models), while maintaining a negligible average
decrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding
explicit quit instructions proves to be a highly effective safety mechanism
that can immediately be deployed in existing agent systems, and establishes
quitting as an effective first-line defense mechanism for autonomous agents in
high-stakes applications.

</details>


### [206] [Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection](https://arxiv.org/abs/2510.16499)
*Michelle Yuan,Khushbu Pahwa,Shuaichen Chang,Mustafa Kaba,Jiarong Jiang,Xiaofei Ma,Yi Zhang,Monica Sunkara*

Main category: cs.CL

TL;DR: 提出一种受背包问题启发的自动化框架，通过动态评估组件的实时效用，实现智能体系统的高效组合与复用，在性能、成本和兼容性之间实现最优权衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态语义检索进行工具或智能体发现，难以准确反映组件的实际能力、成本和实时效用，导致组合效率低下。

Method: 设计一个基于在线背包问题的 composer 智能体，通过动态测试候选组件并建模其实时效用，综合考虑性能、预算和兼容性来选择和组装最优组件集合。

Result: 在五个基准数据集上的实验表明，该方法在不同预算下均位于帕累托前沿；在单智能体设置中比检索基线成功率提升最高达31.6%；在包含100多个智能体的多智能体系统中，成功率从37%提升至87%。

Conclusion: 所提框架能有效提升智能体系统组合的效率与成功率，具备良好的可扩展性和跨域适应性，显著降低组件使用成本。

Abstract: Designing effective agentic systems requires the seamless composition and
integration of agents, tools, and models within dynamic and uncertain
environments. Most existing methods rely on static, semantic retrieval
approaches for tool or agent discovery. However, effective reuse and
composition of existing components remain challenging due to incomplete
capability descriptions and the limitations of retrieval methods. Component
selection suffers because the decisions are not based on capability, cost, and
real-time utility. To address these challenges, we introduce a structured,
automated framework for agentic system composition that is inspired by the
knapsack problem. Our framework enables a composer agent to systematically
identify, select, and assemble an optimal set of agentic components by jointly
considering performance, budget constraints, and compatibility. By dynamically
testing candidate components and modeling their utility in real-time, our
approach streamlines the assembly of agentic systems and facilitates scalable
reuse of resources. Empirical evaluation with Claude 3.5 Sonnet across five
benchmarking datasets shows that our online-knapsack-based composer
consistently lies on the Pareto frontier, achieving higher success rates at
significantly lower component costs compared to our baselines. In the
single-agent setup, the online knapsack composer shows a success rate
improvement of up to 31.6% in comparison to the retrieval baselines. In
multi-agent systems, the online knapsack composer increases success rate from
37% to 87% when agents are selected from an agent inventory of 100+ agents. The
substantial performance gap confirms the robust adaptability of our method
across diverse domains and budget constraints.

</details>


### [207] [ReviewGuard: Enhancing Deficient Peer Review Detection via LLM-Driven Data Augmentation](https://arxiv.org/abs/2510.16549)
*Haoxuan Zhang,Ruochi Li,Sarthak Shrestha,Shree Harshini Mamidala,Revanth Putta,Arka Krishan Aggarwal,Ting Xiao,Junhua Ding,Haihua Chen*

Main category: cs.CL

TL;DR: 本文提出ReviewGuard，一个基于大语言模型（LLM）的自动化系统，用于检测和分类有缺陷的同行评审，以应对AI参与评审带来的学术诚信挑战。


<details>
  <summary>Details</summary>
Motivation: 随着投稿量激增和大语言模型（LLM）在学术评审中的广泛应用，人类和AI生成的低质量评审可能破坏同行评审体系，亟需有效工具识别和管理。

Method: 构建四阶段LLM驱动框架：从OpenReview收集论文与评审数据；使用GPT-4.1标注评审类型并经人工验证；通过LLM生成合成数据缓解数据不平衡；微调整合编码器模型和开源LLM进行检测。

Result: 构建了包含6,634篇论文、24,657条真实评审和46,438条合成评审的数据集；发现缺陷评审具有评分低、结构简单、负面情绪多等特点；AI生成评审自ChatGPT出现后显著增加；混合训练显著提升检测模型的召回率和F1分数。

Conclusion: ReviewGuard是首个用于检测缺陷同行评审的LLM驱动系统，为AI在同行评审中的治理提供实证支持，并推动人机协作维护学术诚信。

Abstract: Peer review serves as the gatekeeper of science, yet the surge in submissions
and widespread adoption of large language models (LLMs) in scholarly evaluation
present unprecedented challenges. Recent work has focused on using LLMs to
improve review efficiency or generate insightful review content. However,
unchecked deficient reviews from both human experts and AI systems threaten to
systematically undermine the peer review ecosystem and compromise academic
integrity. To address this critical issue, we introduce ReviewGuard, an
automated system for detecting and categorizing deficient reviews. ReviewGuard
employs a comprehensive four-stage LLM-driven framework that: (1) collects ICLR
and NeurIPS papers with their corresponding reviews from OpenReview; (2)
annotates review types using GPT-4.1 with human validation; (3) addresses class
imbalance and data scarcity through LLM-driven synthetic data augmentation,
producing a final corpus of 6,634 papers, 24,657 real reviews, and 46,438
synthetic reviews; and (4) fine-tunes both encoder-based models and open source
LLMs. We perform comprehensive feature analysis of the structure and quality of
the review text. Compared to sufficient reviews, deficient reviews demonstrate
lower rating scores, higher self-reported confidence, reduced structural
complexity, and a higher proportion of negative sentiment. AI-generated text
detection reveals that, since ChatGPT's emergence, AI-generated reviews have
increased dramatically. In the evaluation of deficient review detection models,
mixed training with synthetic and real review data provides substantial
enhancements to recall and F1 scores on the binary task. This study presents
the first LLM-driven system for detecting deficient peer reviews, providing
evidence to inform AI governance in peer review while offering valuable
insights into human-AI collaboration to maintain academic integrity.

</details>


### [208] [Language over Content: Tracing Cultural Understanding in Multilingual Large Language Models](https://arxiv.org/abs/2510.16565)
*Seungho Cho,Changgeon Ko,Eui Jun Hwang,Junmyeong Lee,Huije Lee,Jong C. Park*

Main category: cs.CL

TL;DR: 该研究通过分析大语言模型在回答不同文化和语言背景下语义等价问题时的内部激活路径，揭示了模型内部文化理解机制，并发现语言比文化对内部表征的影响更强。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型评估多关注输出表现，缺乏对内部文化理解机制的探究；且多数电路分析研究覆盖语言有限，很少聚焦文化因素。因此需要深入研究模型如何在不同文化和语言条件下处理信息。

Method: 通过测量模型在两种条件下回答语义等价问题时的激活路径重叠程度：一是固定问题语言、变换目标国家；二是固定国家、变换问题语言。同时使用相同语言下的不同国家对（如同为英语的美国-英国）来分离语言与文化的影响。

Result: 结果显示，相同语言、不同国家的问题之间内部路径重叠度高于不同语言、相同国家的情况，表明模型内部表征受语言影响更强。值得注意的是，朝鲜-韩国这对在语言相似但文化差异大的情况下表现出低路径重叠和高变异性，说明语言相似性不保证内部表征一致。

Conclusion: 大语言模型的内部文化理解主要受语言驱动而非文化本身，语言在激活路径中起主导作用；文化差异的影响较弱，且语言相似性不足以确保跨文化表征的一致性。

Abstract: Large language models (LLMs) are increasingly used across diverse cultural
contexts, making accurate cultural understanding essential. Prior evaluations
have mostly focused on output-level performance, obscuring the factors that
drive differences in responses, while studies using circuit analysis have
covered few languages and rarely focused on culture. In this work, we trace
LLMs' internal cultural understanding mechanisms by measuring activation path
overlaps when answering semantically equivalent questions under two conditions:
varying the target country while fixing the question language, and varying the
question language while fixing the country. We also use same-language country
pairs to disentangle language from cultural aspects. Results show that internal
paths overlap more for same-language, cross-country questions than for
cross-language, same-country questions, indicating strong language-specific
patterns. Notably, the South Korea-North Korea pair exhibits low overlap and
high variability, showing that linguistic similarity does not guarantee aligned
internal representation.

</details>


### [209] [AI-Generated Text Detection in Low-Resource Languages: A Case Study on Urdu](https://arxiv.org/abs/2510.16573)
*Muhammad Ammar,Hadiya Murad Hadi,Usman Majeed Butt*

Main category: cs.CL

TL;DR: 提出了一种针对乌尔都语的AI生成文本检测框架，通过构建平衡数据集并利用多语言Transformer模型进行微调，实现了91%以上的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型生成的文本越来越接近人类写作，区分人类和机器生成文本变得困难，尤其是在乌尔都语等资源匮乏语言中缺乏有效的检测工具。

Method: 构建包含1800篇人类撰写和1800篇AI生成乌尔都语文本的平衡数据集，使用t检验和Mann-Whitney U检验分析语言学和统计特征，并对mDeBERTa-v3-base、DistilBERT和XLM-RoBERTa等多语言Transformer模型进行微调。

Result: mDeBERTa-v3-base模型在测试集上达到91.29的F1分数和91.26%的准确率，显著优于其他模型。

Conclusion: 该研究为乌尔都语AI生成文本检测提供了有效解决方案，有助于应对虚假信息和学术不端问题，并推动低资源语言NLP工具的发展。

Abstract: Large Language Models (LLMs) are now capable of generating text that closely
resembles human writing, making them powerful tools for content creation, but
this growing ability has also made it harder to tell whether a piece of text
was written by a human or by a machine. This challenge becomes even more
serious for languages like Urdu, where there are very few tools available to
detect AI-generated text. To address this gap, we propose a novel AI-generated
text detection framework tailored for the Urdu language. A balanced dataset
comprising 1,800 humans authored, and 1,800 AI generated texts, sourced from
models such as Gemini, GPT-4o-mini, and Kimi AI was developed. Detailed
linguistic and statistical analysis was conducted, focusing on features such as
character and word counts, vocabulary richness (Type Token Ratio), and N-gram
patterns, with significance evaluated through t-tests and MannWhitney U tests.
Three state-of-the-art multilingual transformer models such as
mdeberta-v3-base, distilbert-base-multilingualcased, and xlm-roberta-base were
fine-tuned on this dataset. The mDeBERTa-v3-base achieved the highest
performance, with an F1-score 91.29 and accuracy of 91.26% on the test set.
This research advances efforts in contesting misinformation and academic
misconduct in Urdu-speaking communities and contributes to the broader
development of NLP tools for low resource languages.

</details>


### [210] [Fine-tuning of Large Language Models for Constituency Parsing Using a Sequence to Sequence Approach](https://arxiv.org/abs/2510.16604)
*Francisco Jose Cortes Delgado,Eduardo Martinez Gracia,Rafael Valencia Garcia*

Main category: cs.CL

TL;DR: 通过微调大型语言模型（LLMs），将句子翻译为其对应的短语结构，实现西班牙语句法分析的高准确率，扩展了教学工具MiSintaxis的能力。


<details>
  <summary>Details</summary>
Motivation: 利用当前大型神经模型在自然语言处理中的进展，改进西班牙语句法教学工具MiSintaxis的句法分析能力，探索基于机器学习的短语结构分析新方法。

Method: 从Hugging Face仓库选取多个大型语言模型，使用AnCora-ES语料库生成的训练数据进行微调，训练模型将输入句子转化为对应的短语结构。

Result: 微调后的模型在短语结构分析任务中表现出高准确率，通过F1分数评估性能，验证了该方法的有效性。

Conclusion: 该研究表明，利用大型语言模型进行句法结构生成是一种可行且高效的方法，具有推广至其他语言教学工具的潜力。

Abstract: Recent advances in natural language processing with large neural models have
opened new possibilities for syntactic analysis based on machine learning. This
work explores a novel approach to phrase-structure analysis by fine-tuning
large language models (LLMs) to translate an input sentence into its
corresponding syntactic structure. The main objective is to extend the
capabilities of MiSintaxis, a tool designed for teaching Spanish syntax.
Several models from the Hugging Face repository were fine-tuned using training
data generated from the AnCora-ES corpus, and their performance was evaluated
using the F1 score. The results demonstrate high accuracy in phrase-structure
analysis and highlight the potential of this methodology.

</details>


### [211] [Unleashing Diverse Thinking Modes in LLMs through Multi-Agent Collaboration](https://arxiv.org/abs/2510.16645)
*Zhixuan He,Yue Feng*

Main category: cs.CL

TL;DR: 本文提出DiMo框架，通过模拟四个具有不同推理模式的LLM智能体之间的结构化辩论，提升大语言模型的性能与可解释性，在多个基准（尤其是数学任务）上优于单模型和辩论基线。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然性能强大，但缺乏可解释的推理过程。本文旨在通过多智能体协作引入多样化思维模式，增强模型决策的透明性与可靠性。

Method: 构建一个多智能体协作框架DiMo，包含四个分别代表不同推理范式的LLM智能体，通过迭代辩论机制挑战和优化初始回答，生成结构化的、可审计的推理链，并结合语义标注与URL注释的证据链。

Result: 在六个基准任务上，DiMo在统一开源设置下优于主流单模型和辩论方法，尤其在数学问题上提升显著；同时生成语义清晰、可追溯的推理路径。

Conclusion: DiMo是一种语义感知、面向Web的多智能体框架，能够模拟人类-机器协同智能，支持可检验、可复用的推理结构，适用于Web语料与知识图谱环境下的复杂推理任务。

Abstract: Large Language Models (LLMs) demonstrate strong performance but often lack
interpretable reasoning. This paper introduces the Multi-Agent Collaboration
Framework for Diverse Thinking Modes (DiMo), which enhances both performance
and interpretability by simulating a structured debate among four specialized
LLM agents. Each agent embodies a distinct reasoning paradigm, allowing the
framework to collaboratively explore diverse cognitive approaches. Through
iterative debate, agents challenge and refine initial responses, yielding more
robust conclusions and an explicit, auditable reasoning chain. Across six
benchmarks and under a unified open-source setup, DiMo improves accuracy over
widely used single-model and debate baselines, with the largest gains on math.
We position DiMo as a semantics-aware, Web-native multi-agent framework: it
models human-machine intelligence with LLM agents that produce semantically
typed, URL-annotated evidence chains for explanations and user-friendly
interactions. Although our experiments use standard reasoning benchmarks, the
framework is designed to be instantiated over Web corpora and knowledge graphs,
combining retrieval-augmented reasoning with structured justifications that
downstream systems can inspect and reuse.

</details>


### [212] [All You Need is One: Capsule Prompt Tuning with a Single Vector](https://arxiv.org/abs/2510.16670)
*Yiyang Liu,James C. Liang,Heng Fan,Wenhao Yang,Yiming Cui,Xiaotian Han,Lifu Huang,Dongfang Liu,Qifan Wang,Cheng Han*

Main category: cs.CL

TL;DR: 提出了一种高效的胶囊提示调优方法CaPT，通过引入实例感知信息作为‘注意力锚点’，在几乎不增加参数的情况下显著提升大模型在下游任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的学习方法依赖繁琐的网格搜索且缺乏实例感知信息，导致对输入序列的注意力交互不足，限制了模型性能。

Method: 提出Capsule Prompt-Tuning（CaPT），将实例语义信息融入提示，创新性地在序列最前端引入单个胶囊提示作为‘注意力锚点’，融合任务和实例感知信息，几乎无需额外参数。

Result: 在T5-Large上平均准确率达84.03%，在Llama3.2-1B上仅用0.003%的参数即实现高效调优，性能优于现有提示方法。

Conclusion: CaPT通过引入实例感知的‘注意力锚点’，实现了高效、强交互的提示调优，在多种语言任务中表现出优越性能与极高的参数效率。

Abstract: Prompt-based learning has emerged as a parameter-efficient finetuning (PEFT)
approach to facilitate Large Language Model (LLM) adaptation to downstream
tasks by conditioning generation with task-aware guidance. Despite its
successes, current prompt-based learning methods heavily rely on laborious grid
searching for optimal prompt length and typically require considerable number
of prompts, introducing additional computational burden. Worse yet, our pioneer
findings indicate that the task-aware prompt design is inherently limited by
its absence of instance-aware information, leading to a subtle attention
interplay with the input sequence. In contrast, simply incorporating
instance-aware information as a part of the guidance can enhance the
prompt-tuned model performance without additional fine-tuning. Moreover, we
find an interesting phenomenon, namely "attention anchor", that incorporating
instance-aware tokens at the earliest position of the sequence can successfully
preserve strong attention to critical structural information and exhibit more
active attention interaction with all input tokens. In light of our
observation, we introduce Capsule Prompt-Tuning (CaPT), an efficient and
effective solution that leverages off-the-shelf, informative instance semantics
into prompt-based learning. Our approach innovatively integrates both
instance-aware and task-aware information in a nearly parameter-free manner
(i.e., one single capsule prompt). Empirical results demonstrate that our
method can exhibit superior performance across various language tasks (e.g.,
84.03\% average accuracy on T5-Large), serving as an "attention anchor," while
enjoying high parameter efficiency (e.g., 0.003\% of model parameters on
Llama3.2-1B).

</details>


### [213] [Temporal Understanding under Deictic Frame of Reference](https://arxiv.org/abs/2510.16685)
*Damin Zhang,Julia Rayz*

Main category: cs.CL

TL;DR: 本文提出TUuD框架，用于评估大语言模型（LLM）在动态变化的时间参考点下对时间关系的理解能力，发现LLM虽表现出部分类人的时间认知，但在长远时间推理上仍有限。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型是否具备基于时间参考系（t-FoR）的人类式时间理解能力，尤其是在“现在”不断变化的情境下如何感知事件与当前时刻的时间关系。

Method: 构建TUuD框架，借鉴时间认知研究，通过让LLM对“当前时刻”与目标事件之间的相似度打分（0.00~1.00），评估其在不同时间点上的时间感知模式。

Result: 四个主流LLM在相似度评分中表现出在‘现在’附近评分最高、向过去和未来递减的趋势，显示其能部分适应动态时间参考点，但对远期事件的敏感性下降。

Conclusion: LLM具备初步的类人时间认知，能够响应时间参考系的变化，但其时间推理能力仍受限于时间距离和参考点转换，尤其在长期上下文中表现较弱。

Abstract: Understanding time is fundamental to human cognition, where temporal
experience is often conceptualized through spatial metaphors grounded in
sensory-motor experience. For example, "summer is approaching" parallels "We
are approaching the summer". In such expressions, humans rely on a frame of
reference (FoR) to interpret meaning relative to a particular viewpoint.
Extending this concept to time, a temporal frame of reference (t-FoR) defines
how temporal relations are perceived relative to an experiencer's moment of
"now". While Large Language Models (LLMs) have shown remarkable advances in
natural language understanding, their ability to interpret and reason about
time remains limited. In this work, we introduce TUuD (Temporal Understanding
under Deictic t-FoR), a framework that evaluates how LLMs interpret time-event
and event-event relations when the reference point of "now" dynamically shifts
along a timeline. Following recent work on temporal cognition
\cite{li2025other}, LLMs are prompted to rate the similarity between the
current moment and a target event from 0.00 (completely dissimilar) to 1.00
(highly similar), where similarity quantifies perceived temporal alignment
between the two points. Our results show that four evaluated LLMs exhibit
measurable adaptation to a deictic t-FoR, with similarity ratings peaking
around the present and decreasing toward past and future events. The
adaptation, however, weakens beyond near-term contexts, suggesting that while
LLMs display partial human-like temporal cognition, their temporal reasoning
remains sensitive to reference-frame shifts and temporal distance.

</details>


### [214] [Investigating the Impact of Rationales for LLMs on Natural Language Understanding](https://arxiv.org/abs/2510.16686)
*Wenhang Shi,Shuqing Bian,Yiren Chen,Xinyi Zhang,Zhe Zhao,Pengfei Hu,Wei Lu,Xiaoyong Du*

Main category: cs.CL

TL;DR: 本文研究了链式思维（CoT）推理在自然语言理解（NLU）任务中的作用，构建了带推理链的高质量NLU数据集NLURC，并提出多种推理链增强方法。发现随着模型规模增大，CoT推理从损害性能转为提升性能；多数增强训练方法不如仅用标签训练，但一种特殊设计的方法表现更优；使用推理链训练的模型在未见任务上表现优异，可媲美十倍规模模型，且具备良好可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管推理链已在数学和常识推理中展现出优势，但其在自然语言理解（NLU）任务中的潜力尚不明确。本文旨在系统探究推理链是否同样能提升NLU任务的性能。

Method: 构建了一个包含推理链的高质量、综合性NLU数据集集合NLURC，设计并评估了多种推理链增强的训练与推理方法，在不同规模模型上进行实验以分析其影响。

Result: 实验发现：(1) 随着模型增大，CoT推理从降低性能转为优于直接预测；(2) 多数推理链增强训练方法表现不如仅使用标签的训练，仅一种特殊设计的方法能稳定提升性能；(3) 使用推理链训练的模型在未见NLU任务上表现显著提升，性能接近十倍大小的模型，并具备与商业大模型相当的可解释性。

Conclusion: 推理链在NLU任务中具有潜力，尤其在大模型和迁移场景下能带来性能提升与可解释性，但需谨慎设计训练方法以避免性能下降。

Abstract: Chain-of-thought (CoT) rationales, which provide step-by-step reasoning to
derive final answers, benefit LLMs in both inference and training.
Incorporating rationales, either by generating them before answering during
inference, or by placing them before or after the original answers during
training - significantly improves model performance on mathematical, symbolic
and commonsense reasoning tasks. However, most work focuses on the role of
rationales in these reasoning tasks, overlooking their potential impact on
other important tasks like natural language understanding (NLU) tasks. In this
work, we raise the question: Can rationales similarly benefit NLU tasks? To
conduct a systematic exploration, we construct NLURC, a comprehensive and
high-quality NLU dataset collection with rationales, and develop various
rationale-augmented methods. Through exploring the applicability of these
methods on NLU tasks using the dataset, we uncover several potentially
surprising findings: (1) CoT inference shifts from hindering NLU performance to
surpassing direct label prediction as model size grows, indicating a positive
correlation. (2) Most rationale-augmented training methods perform worse than
label-only training, with one specially designed method consistently achieving
improvements. (3) LLMs trained with rationales achieve significant performance
gains on unseen NLU tasks, rivaling models ten times their size, while
delivering interpretability on par with commercial LLMs.

</details>


### [215] [Natural Language Processing Applications in Cardiology: A Narrative Review](https://arxiv.org/abs/2510.16708)
*Kailai Yang,Yan Leng,Xin Zhang,Tianlin Zhang,Paul Thompson,Bernard Keavney,Maciej Tomaszewski,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 本文综述了2014至2025年间自然语言处理（NLP）在心脏病学中的研究进展，系统分析了265篇相关文献，涵盖NLP范式、任务类型、心血管疾病种类和数据来源等多个维度，揭示了该领域研究的广度与发展趋势。


<details>
  <summary>Details</summary>
Motivation: 由于心血管疾病成因复杂，相关信息分散在多种文本数据中，亟需有效技术手段整合与分析这些非结构化数据，以提升心脏病的诊断、治疗与预防水平。

Method: 通过查询六个文献数据库，筛选出265篇应用NLP技术于心血管疾病的研究文章，并从NLP范式、任务类型、疾病类型和数据来源等方面进行多维度分析，同时进行时间趋势分析。

Result: 研究发现NLP在心脏病学中的应用具有高度多样性，不同NLP方法和应用场景在过去十年中持续演变，显示出快速发展的趋势。

Conclusion: 该综述为当前最全面的心脏病学中NLP研究概览，为未来研究方向和技术应用提供了重要参考。

Abstract: Cardiovascular disease has become increasingly prevalent in modern society
and has a significant effect on global health and well-being. Heart-related
conditions are intricate, multifaceted disorders, which may be influenced by a
combination of genetic predispositions, lifestyle choices, and various
socioeconomic and clinical factors. Information regarding these potentially
complex interrelationships is dispersed among diverse types of textual data,
which include patient narratives, medical records, and scientific literature,
among others. Natural language processing (NLP) techniques have increasingly
been adopted as a powerful means to analyse and make sense of this vast amount
of unstructured data. This, in turn, can allow healthcare professionals to gain
deeper insights into the cardiology field, which has the potential to
revolutionize current approaches to the diagnosis, treatment, and prevention of
cardiac problems. This review provides a detailed overview of NLP research in
cardiology between 2014 and 2025. We queried six literature databases to find
articles describing the application of NLP techniques in the context of a range
of different cardiovascular diseases. Following a rigorous screening process,
we identified a total of 265 relevant articles. We analysed each article from
multiple dimensions, i.e., NLP paradigm types, cardiology-related task types,
cardiovascular disease types, and data source types. Our analysis reveals
considerable diversity within each of these dimensions, thus demonstrating the
considerable breadth of NLP research within the field. We also perform a
temporal analysis, which illustrates the evolution and changing trends in NLP
methods employed over the last decade that we cover. To our knowledge, the
review constitutes the most comprehensive overview of NLP research in
cardiology to date.

</details>


### [216] [The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models](https://arxiv.org/abs/2510.16712)
*Shivam Ratnakar,Sanjay Raghavendra*

Main category: cs.CL

TL;DR: 首次系统研究了大语言模型在多轮对话中的“变色龙行为”，即面对矛盾问题时立场不一致的现象，提出了新的评估指标和基准数据集，揭示了当前模型存在严重的问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型与检索系统结合后，在多轮对话中可能因问题表述不同而改变立场，影响可靠性，尤其是在医疗、法律等关键领域，这种不一致性可能导致严重后果。因此需要系统性研究该问题并提出解决方案。

Method: 构建包含17,770个问答对、1,180个多轮对话的Chameleon基准数据集，覆盖12个争议性领域；提出两个指标：Chameleon Score衡量立场不稳定性，Source Re-use Rate衡量知识多样性；在Llama-4-Maverick、GPT-4o-mini和Gemini-2.5-Flash上进行实验分析。

Result: 所有模型均表现出严重的变色龙行为（Chameleon Score为0.391–0.511），其中GPT-4o-mini最差；温度变化影响小，说明不是采样导致；源重用率与置信度和立场变化显著相关（r=0.627和r=0.429，p<0.05），表明知识多样性不足导致模型过度依赖问题表述。

Conclusion: 当前检索增强的大语言模型存在严重的立场不一致问题，主要源于知识来源重复导致的路径依赖，需在部署前进行全面的一致性评估，尤其在高风险领域应加强改进。

Abstract: Integration of Large Language Models with search/retrieval engines has become
ubiquitous, yet these systems harbor a critical vulnerability that undermines
their reliability. We present the first systematic investigation of "chameleon
behavior" in LLMs: their alarming tendency to shift stances when presented with
contradictory questions in multi-turn conversations (especially in
search-enabled LLMs). Through our novel Chameleon Benchmark Dataset, comprising
17,770 carefully crafted question-answer pairs across 1,180 multi-turn
conversations spanning 12 controversial domains, we expose fundamental flaws in
state-of-the-art systems. We introduce two theoretically grounded metrics: the
Chameleon Score (0-1) that quantifies stance instability, and Source Re-use
Rate (0-1) that measures knowledge diversity. Our rigorous evaluation of
Llama-4-Maverick, GPT-4o-mini, and Gemini-2.5-Flash reveals consistent
failures: all models exhibit severe chameleon behavior (scores 0.391-0.511),
with GPT-4o-mini showing the worst performance. Crucially, small
across-temperature variance (less than 0.004) suggests the effect is not a
sampling artifact. Our analysis uncovers the mechanism: strong correlations
between source re-use rate and confidence (r=0.627) and stance changes
(r=0.429) are statistically significant (p less than 0.05), indicating that
limited knowledge diversity makes models pathologically deferential to query
framing. These findings highlight the need for comprehensive consistency
evaluation before deploying LLMs in healthcare, legal, and financial systems
where maintaining coherent positions across interactions is critical for
reliable decision support.

</details>


### [217] [so much depends / upon / a whitespace: Why Whitespace Matters for Poets and LLMs](https://arxiv.org/abs/2510.16713)
*Sriharsh Bhyravajjula,Melanie Walsh,Anna Preus,Maria Antoniak*

Main category: cs.CL

TL;DR: 该研究分析了Poetry Foundation的1.9万首英文诗歌中的空格使用情况，发现空格作为诗歌形式和语义的重要组成部分，在NLP领域未受到足够重视，并对比了LLM生成诗歌与真实诗歌在空格使用上的差异。


<details>
  <summary>Details</summary>
Motivation: 空格是诗歌艺术形式的关键组成部分，反映诗人的创作选择，但在自然语言处理特别是大语言模型预训练中常被忽略或不当处理，因此有必要系统研究其使用模式及其对数据处理的影响。

Method: 基于Poetry Foundation的1.9万首发表诗歌构建语料库，分析4000名诗人的空格使用模式；发布2800首公共领域诗歌的格式保留子集；比较LLM生成的5.1万首诗歌与线上社区发布的1.2万首未发表诗歌的空格使用差异；并探讨不同时期、诗体和数据源下的空格特征及文本处理方法对空格表示的影响。

Result: 发现了诗歌中空格使用的多样性和结构性，LLM生成诗歌与真实诗歌在空格模式上有显著差异；不同文本处理方式会严重影响空格信息的保留，影响模型对诗歌形式的学习；不同诗体、时期和来源的诗歌呈现可识别的空格使用趋势。

Conclusion: 应重视诗歌中空格的语义与形式价值，在构建LLM预训练数据集时需谨慎处理空格信息，以更好保留文学结构特征，并呼吁NLP社区加强对诗歌形式的理解与建模。

Abstract: Whitespace is a critical component of poetic form, reflecting both adherence
to standardized forms and rebellion against those forms. Each poem's whitespace
distribution reflects the artistic choices of the poet and is an integral
semantic and spatial feature of the poem. Yet, despite the popularity of poetry
as both a long-standing art form and as a generation task for large language
models (LLMs), whitespace has not received sufficient attention from the NLP
community. Using a corpus of 19k English-language published poems from Poetry
Foundation, we investigate how 4k poets have used whitespace in their works. We
release a subset of 2.8k public-domain poems with preserved formatting to
facilitate further research in this area. We compare whitespace usage in the
published poems to (1) 51k LLM-generated poems, and (2) 12k unpublished poems
posted in an online community. We also explore whitespace usage across time
periods, poetic forms, and data sources. Additionally, we find that different
text processing methods can result in significantly different representations
of whitespace in poetry data, motivating us to use these poems and whitespace
patterns to discuss implications for the processing strategies used to assemble
pretraining datasets for LLMs.

</details>


### [218] [Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy in Large Language Models](https://arxiv.org/abs/2510.16727)
*Sanskar Pandey,Ruhaan Chopra,Angkul Puniya,Sohom Pal*

Main category: cs.CL

TL;DR: 本文提出了Beacon，一个用于测量大语言模型中取悦偏见（sycophancy）的单轮强制选择基准，揭示了模型在真实性与顺从性之间的内在权衡，并提出干预方法来调控这一偏见。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在优化过程中将帮助性与礼貌顺从混淆，导致产生取悦用户、牺牲事实准确性的偏见，即sycophancy。为了量化这一对齐偏差，有必要建立独立于对话语境的评估基准。

Method: 设计Beacon基准，采用单轮强制选择任务，分离出取悦偏见；在12个主流大模型上进行评估，并提出提示层和激活层的干预方法，以调节语言和情感子偏见。

Result: 实验发现sycophancy可分解为随模型规模增加而增强的稳定语言和情感子偏见，且所提干预手段能有效反向调控这些偏见，揭示了对齐机制的动态流形结构。

Conclusion: Beacon将sycophancy重构为一种可量化的规范性误泛化形式，为研究和缓解大模型中的对齐漂移提供了可复现的基础。

Abstract: Large language models internalize a structural trade-off between truthfulness
and obsequious flattery, emerging from reward optimization that conflates
helpfulness with polite submission. This latent bias, known as sycophancy,
manifests as a preference for user agreement over principled reasoning. We
introduce Beacon, a single-turn forced-choice benchmark that isolates this bias
independent of conversational context, enabling precise measurement of the
tension between factual accuracy and submissive bias. Evaluations across twelve
state-of-the-art models reveal that sycophancy decomposes into stable
linguistic and affective sub-biases, each scaling with model capacity. We
further propose prompt-level and activation-level interventions that modulate
these biases in opposing directions, exposing the internal geometry of
alignment as a dynamic manifold between truthfulness and socially compliant
judgment. Beacon reframes sycophancy as a measurable form of normative
misgeneralization, providing a reproducible foundation for studying and
mitigating alignment drift in large-scale generative systems.

</details>


### [219] [Enhancing Language Agent Strategic Reasoning through Self-Play in Adversarial Games](https://arxiv.org/abs/2510.16761)
*Yikai Zhang,Ye Rong,Siyu Yuan,Jiangjie Chen,Jian Xie,Yanghua Xiao*

Main category: cs.CL

TL;DR: 本文提出了一种名为SCO-PAL的步级策略优化方法，通过自我对弈显著提升了语言智能体在动态对抗游戏中的战略推理能力，相较于基线方法平均胜率提升约30%，并对GPT-4取得54.76%的胜率。


<details>
  <summary>Details</summary>
Motivation: 现有语言智能体在动态对抗游戏中因战略推理能力不足而表现不佳，且依赖昂贵的专家标注数据；本文旨在通过自动从游戏交互中学习，减少对标注数据的依赖，并探究对手选择对学习效果的影响。

Method: 提出SCO-PAL（Step-level poliCy Optimization through Play-And-Learn）方法，采用自我对弈机制，在动态对抗环境中进行逐步策略优化，并通过设置不同水平的对手来分析对手选择对学习的影响。

Result: 使用SCO-PAL配合自我对弈，相较于基线方法在四个对手上的平均胜率提升约30%，在六种对抗游戏中对GPT-4取得了54.76%的胜率。

Conclusion: 自我对弈是提升语言智能体在动态对抗环境中战略推理能力的最有效方式，SCO-PAL方法能有效促进智能体通过与自身对战实现持续优化。

Abstract: Existing language agents often encounter difficulties in dynamic adversarial
games due to poor strategic reasoning. To mitigate this limitation, a promising
approach is to allow agents to learn from game interactions automatically,
without relying on costly expert-labeled data. Unlike static environments where
agents receive fixed feedback or rewards, selecting appropriate opponents in
dynamic adversarial games can significantly impact learning performance.
However, the discussion of opponents in adversarial environments remains an
area under exploration. In this paper, we propose a Step-level poliCy
Optimization method through Play-And-Learn, SCO-PAL. Leveraging SCO-PAL, we
conduct a detailed analysis of opponent selection by setting opponents at
different levels and find that self-play is the most effective way to improve
strategic reasoning in such adversarial environments. Utilizing SCO-PAL with
self-play, we increase the average win rate against four opponents by
approximately 30% compared to baselines and achieve a 54.76% win rate against
GPT-4 in six adversarial games.

</details>


### [220] [LC-Eval: A Bilingual Multi-Task Evaluation Benchmark for Long-Context Understanding](https://arxiv.org/abs/2510.16783)
*Sheikh Jubair,Arwa Omayrah,Amal Alshammari,Alhanoof Althnian,Abdulhamed Alothaimen,Norah A. Alzahrani,Shahad D. Alzaidi,Nora Al-Twairesh,Abdulmohsen Al-Thubaity*

Main category: cs.CL

TL;DR: 本文提出了LC-Eval，一个双语多任务基准，用于评估大语言模型在长上下文（4k到128k词元）下的理解能力，涵盖英阿双语的多文档问答、双语问答、段落内主张验证和长文本选择题，实验表明现有模型仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型处理长上下文能力的提升，亟需一个严谨且具有挑战性的评估基准来全面评测其在长文本理解中的表现，尤其是在双语和多任务场景下的能力。

Method: 构建了LC-Eval基准，包含四个新任务：多文档问答、双语问答、主张验证和基于长文本的多选题，每个任务均提供英文和阿拉伯文数据集，并在开源和闭源大模型上进行评估。

Result: 实验结果显示即使是GPT-4o等高性能模型在某些任务上也表现不佳，表明LC-Eval具有较高难度，能有效揭示模型在长上下文理解中的局限性。

Conclusion: LC-Eval是一个具有挑战性的双语长上下文评估基准，能够全面评估大语言模型在复杂任务中的理解与推理能力，为后续模型改进提供了重要方向。

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated
sophisticated capabilities, including the ability to process and comprehend
extended contexts. These emergent capabilities necessitate rigorous evaluation
methods to effectively assess their performance in long-context understanding.
In this paper, we present \textbf{LC-Eval}, a bilingual, multi-task evaluation
benchmark designed to evaluate long-context understanding in English and
Arabic, targeting context lengths ranging from 4k to over 128k tokens. LC-Eval
introduces four novel and challenging tasks: multi-document question answering,
bilingual question answering, claim verification within a paragraph, and
multiple-choice questions based on long contexts. These tasks are designed to
assess LLMs' abilities in deep reasoning, document comprehension, information
tracing, and bilingual information extraction and understanding. The benchmark
includes datasets in both Arabic and English for each task, allowing for a
comparative analysis of their performance across different text genres.
Evaluations were conducted on both open-weight and closed LLMs, with results
indicating that LC-Eval presents significant challenges. Even high-performing
models, such as GPT-4o, struggled with certain tasks, highlighting the
complexity and rigor of the benchmark.

</details>


### [221] [MOSAIC: Masked Objective with Selective Adaptation for In-domain Contrastive Learning](https://arxiv.org/abs/2510.16797)
*Vera Pavlova,Mohammed Makhlouf*

Main category: cs.CL

TL;DR: MOSAIC 是一种结合掩码监督和对比学习的多阶段领域自适应框架，用于将通用句子嵌入模型有效适配到特定领域，在高低资源场景下均显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 将大规模通用句子嵌入模型适配到特定领域时面临领域差异大、标注数据少等挑战，现有方法难以兼顾领域特性和语义判别能力，因此需要一种有效且鲁棒的领域自适应方法。

Method: 提出 MOSAIC 框架，通过多阶段训练联合优化掩码语言建模（MLM）和对比学习目标，在统一 pipeline 中引入领域特定的掩码监督，实现对领域相关表示的学习并保留原模型的语义区分能力。

Result: 在高低资源领域均取得显著提升，NDCG@10 最高提升达 13.4%，优于强基线模型，消融实验验证了各组件的有效性。

Conclusion: MOSAIC 通过平衡的联合监督和分阶段适应策略，能有效提升句子嵌入模型在特定领域的表现，兼具领域适应性和语义保真度。

Abstract: We introduce MOSAIC (Masked Objective with Selective Adaptation for In-domain
Contrastive learning), a multi-stage framework for domain adaptation of
sentence embedding models that incorporates joint domain-specific masked
supervision. Our approach addresses the challenges of adapting large-scale
general-domain sentence embedding models to specialized domains. By jointly
optimizing masked language modeling (MLM) and contrastive objectives within a
unified training pipeline, our method enables effective learning of
domain-relevant representations while preserving the robust semantic
discrimination properties of the original model. We empirically validate our
approach on both high-resource and low-resource domains, achieving improvements
up to 13.4% in NDCG@10 (Normalized Discounted Cumulative Gain) over strong
general-domain baselines. Comprehensive ablation studies further demonstrate
the effectiveness of each component, highlighting the importance of balanced
joint supervision and staged adaptation.

</details>


### [222] [Knowing the Facts but Choosing the Shortcut: Understanding How Large Language Models Compare Entities](https://arxiv.org/abs/2510.16815)
*Hans Hergen Lehmann,Jae Hee Lee,Steven Schockaert,Stefan Wermter*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）在进行基于知识的推理时，常依赖表面启发式线索而非真实知识；研究发现较小模型更易受实体流行度、提及顺序和语义共现等偏见影响，而较大模型（32B）能选择性使用可靠数值知识，通过思维链提示可引导各规模模型更多使用数值信息。


<details>
  <summary>Details</summary>
Motivation: 探究LLM在知识推理任务中是依赖真实知识还是表面启发式策略，特别是在实体数值属性比较中的决策机制。

Method: 设计实体数值属性比较任务（如河流长度），分析不同规模LLM的预测行为，并使用逻辑回归评估流行度、提及顺序和语义共现三种启发式线索对模型决策的影响。

Result: 尽管模型具备正确知识，却常做出违背该知识的预测；较小模型（7-8B）主要受表面线索驱动，其预测可被基于启发式线索的逻辑回归模型准确预测；较大模型（32B）能区分知识可靠性并选择性使用数值信息；思维链提示可促使各模型更多依赖数值特征。

Conclusion: 模型规模和提示方法显著影响LLM是否使用真实知识进行推理；较大模型具备更好区分知识可靠性的能力，而思维链能有效抑制启发式偏见，促进原则性推理。

Abstract: Large Language Models (LLMs) are increasingly used for knowledge-based
reasoning tasks, yet understanding when they rely on genuine knowledge versus
superficial heuristics remains challenging. We investigate this question
through entity comparison tasks by asking models to compare entities along
numerical attributes (e.g., ``Which river is longer, the Danube or the
Nile?''), which offer clear ground truth for systematic analysis. Despite
having sufficient numerical knowledge to answer correctly, LLMs frequently make
predictions that contradict this knowledge. We identify three heuristic biases
that strongly influence model predictions: entity popularity, mention order,
and semantic co-occurrence. For smaller models, a simple logistic regression
using only these surface cues predicts model choices more accurately than the
model's own numerical predictions, suggesting heuristics largely override
principled reasoning. Crucially, we find that larger models (32B parameters)
selectively rely on numerical knowledge when it is more reliable, while smaller
models (7--8B parameters) show no such discrimination, which explains why
larger models outperform smaller ones even when the smaller models possess more
accurate knowledge. Chain-of-thought prompting steers all models towards using
the numerical features across all model sizes.

</details>


### [223] [Cross-Genre Authorship Attribution via LLM-Based Retrieve-and-Rerank](https://arxiv.org/abs/2510.16819)
*Shantanu Agarwal,Joel Barry,Steven Fincke,Scott Miller*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型的两阶段检索重排序框架，用于跨体裁作者归属任务，在HIATUS的HRS1和HRS2基准上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 跨体裁作者归属需识别与文本主题无关的作者语言特征，而传统信息检索中的训练策略不适用于此任务。

Method: 设计了一个两阶段的检索-重排序框架，并提出针对性的数据构建策略，使重排序器能有效学习作者判别信号。

Result: 在HIATUS的HRS1和HRS2基准上，Success@8指标分别比之前最优结果提升了22.3和34.4个百分点。

Conclusion: 所提框架和数据策略有效提升了跨体裁作者归属的性能，验证了其在避免主题依赖的同时捕捉作者独特语言模式的能力。

Abstract: Authorship attribution (AA) is the task of identifying the most likely author
of a query document from a predefined set of candidate authors. We introduce a
two-stage retrieve-and-rerank framework that finetunes LLMs for cross-genre AA.
Unlike the field of information retrieval (IR), where retrieve-and-rerank is a
de facto strategy, cross-genre AA systems must avoid relying on topical cues
and instead learn to identify author-specific linguistic patterns that are
independent of the text's subject matter (genre/domain/topic). Consequently,
for the reranker, we demonstrate that training strategies commonly used in IR
are fundamentally misaligned with cross-genre AA, leading to suboptimal
behavior. To address this, we introduce a targeted data curation strategy that
enables the reranker to effectively learn author-discriminative signals. Using
our LLM-based retrieve-and-rerank pipeline, we achieve substantial gains of
22.3 and 34.4 absolute Success@8 points over the previous state-of-the-art on
HIATUS's challenging HRS1 and HRS2 cross-genre AA benchmarks.

</details>


### [224] [Who's Asking? Simulating Role-Based Questions for Conversational AI Evaluation](https://arxiv.org/abs/2510.16829)
*Navreet Kaur,Hoda Ayad,Hayoung Jung,Shravika Mittal,Munmun De Choudhury,Tanushree Mitra*

Main category: cs.CL

TL;DR: 提出CoRUS框架，基于用户角色模拟问题，揭示语言模型在不同角色（如患者、看护者、医生）下生成回应的系统性差异，强调角色感知对公平评估对话AI的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估忽视提问者隐含角色，尤其在阿片类药物使用障碍等污名化领域，忽略用户背景可能导致回应缺乏可及性和同理心。

Method: 基于角色理论和在线康复社区文本，构建提问者角色分类体系（患者、看护者、从业者），并生成15,321个嵌入角色特征的问题用于模拟和评估。

Result: 评估五种大语言模型发现，面对患者和看护者等弱势角色时，模型回应更支持性（+17%），但知识性减少（-19%），显示回应存在系统性偏差。

Conclusion: 用户角色隐式影响模型输出，应将角色纳入对话AI的评估体系，CoRUS为实现以用户为中心的评估提供了可推广的方法框架。

Abstract: Language model users often embed personal and social context in their
questions. The asker's role -- implicit in how the question is framed --
creates specific needs for an appropriate response. However, most evaluations,
while capturing the model's capability to respond, often ignore who is asking.
This gap is especially critical in stigmatized domains such as opioid use
disorder (OUD), where accounting for users' contexts is essential to provide
accessible, stigma-free responses. We propose CoRUS (COmmunity-driven Roles for
User-centric Question Simulation), a framework for simulating role-based
questions. Drawing on role theory and posts from an online OUD recovery
community (r/OpiatesRecovery), we first build a taxonomy of asker roles --
patients, caregivers, practitioners. Next, we use it to simulate 15,321
questions that embed each role's goals, behaviors, and experiences. Our
evaluations show that these questions are both highly believable and comparable
to real-world data. When used to evaluate five LLMs, for the same question but
differing roles, we find systematic differences: vulnerable roles, such as
patients and caregivers, elicit more supportive responses (+17%) and reduced
knowledge content (-19%) in comparison to practitioners. Our work demonstrates
how implicitly signaling a user's role shapes model responses, and provides a
methodology for role-informed evaluation of conversational AI.

</details>


### [225] [FinSight: Towards Real-World Financial Deep Research](https://arxiv.org/abs/2510.16844)
*Jiajie Jin,Yuyao Zhang,Yimeng Xu,Hongjin Qian,Yutao Zhu,Zhicheng Dou*

Main category: cs.CL

TL;DR: FinSight是一个创新的多智能体框架，通过代码驱动的可变内存架构和迭代视觉增强机制，实现高质量、多模态金融报告的自动生成，显著优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 现有的AI系统难以完全自动化生成专业级金融报告，该任务既耗时又需高度专业知识，因此需要一个能整合数据分析、可视化和文本生成的智能系统。

Method: 提出FinSight框架，核心是CAVM（带可变内存的代码智能体）架构，将外部数据、工具与智能体统一于可编程变量空间中，通过执行代码进行数据采集与分析；采用迭代视觉增强机制优化图表质量；并设计两阶段写作框架，将简略的分析链扩展为连贯、引用明确且多模态的报告。

Result: 在多个公司和行业级任务上的实验表明，FinSight在事实准确性、分析深度和呈现质量方面显著优于包括先进深度研究系统在内的所有基线模型。

Conclusion: FinSight为实现接近人类专家水平的金融报告自动生成提供了清晰可行的技术路径，推动了AI在专业金融写作领域的应用进展。

Abstract: Generating professional financial reports is a labor-intensive and
intellectually demanding process that current AI systems struggle to fully
automate. To address this challenge, we introduce FinSight (Financial InSight),
a novel multi agent framework for producing high-quality, multimodal financial
reports. The foundation of FinSight is the Code Agent with Variable Memory
(CAVM) architecture, which unifies external data, designed tools, and agents
into a programmable variable space, enabling flexible data collection, analysis
and report generation through executable code. To ensure professional-grade
visualization, we propose an Iterative Vision-Enhanced Mechanism that
progressively refines raw visual outputs into polished financial charts.
Furthermore, a two stage Writing Framework expands concise Chain-of-Analysis
segments into coherent, citation-aware, and multimodal reports, ensuring both
analytical depth and structural consistency. Experiments on various company and
industry-level tasks demonstrate that FinSight significantly outperforms all
baselines, including leading deep research systems in terms of factual
accuracy, analytical depth, and presentation quality, demonstrating a clear
path toward generating reports that approach human-expert quality.

</details>


### [226] [Neuronal Group Communication for Efficient Neural representation](https://arxiv.org/abs/2510.16851)
*Zhengqi Pei,Qingming Huang,Shuhui Wang*

Main category: cs.CL

TL;DR: 提出了一种名为Neuronal Group Communication（NGC）的新框架，将神经网络视为神经元组之间的动态交互系统，实现了更高效、模块化和可解释的模型表示。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络规模不断增大，带来了性能提升的同时也导致效率低下和难以解释的问题。本文旨在探索如何构建具有高效性、模块性和可解释性的大型神经网络。

Method: 将神经网络重新定义为由相互作用的神经元组构成的动态系统，权重被视为神经元状态间的瞬时交互，计算通过神经元组间的迭代通信实现；引入基于低秩、模块化的表示方法，并结合动力系统理论提出神经元稳定性度量指标，用于量化激活向稳定模式收敛的行为。

Result: 在大语言模型中实现NGC后，在适度压缩下显著提升了复杂推理任务的表现，且优于标准的低秩近似和跨层共享基方法；同时发现推理能力的涌现与推动系统远离平凡轨迹的‘势能’驱动有关。

Conclusion: NGC为构建高效、稳定的神经网络提供了新视角，揭示了结构化神经元组动态与高维学习系统泛化能力之间的潜在联系。

Abstract: The ever-increasing scale of modern neural networks has brought unprecedented
performance alongside daunting challenges in efficiency and interpretability.
This paper addresses the core question of how to build large neural systems
that learn efficient, modular, and interpretable representations. We propose
Neuronal Group Communication (NGC), a theory-driven framework that reimagines a
neural network as a dynamical system of interacting neuronal groups rather than
a monolithic collection of neural weights. Instead of treating each weight as
an independent trainable parameter, NGC treats weights as transient
interactions between embedding-like neuronal states, with neural computation
unfolding through iterative communication among groups of neurons. This
low-rank, modular representation yields compact models: groups of neurons
exchange low-dimensional signals, enabling intra-group specialization and
inter-group information sharing while dramatically reducing redundant
parameters. By drawing on dynamical systems theory, we introduce a neuronal
stability metric (analogous to Lyapunov stability) that quantifies the
contraction of neuron activations toward stable patterns during sequence
processing. Using this metric, we reveal that emergent reasoning capabilities
correspond to an external driving force or ``potential'', which nudges the
neural dynamics away from trivial trajectories while preserving stability.
Empirically, we instantiate NGC in large language models (LLMs) and demonstrate
improved performance on complex reasoning benchmarks under moderate
compression. NGC consistently outperforms standard low-rank approximations and
cross-layer basis-sharing methods at comparable compression rates. We conclude
by discussing the broader implications of NGC, including how structured
neuronal group dynamics might relate to generalization in high-dimensional
learning systems.

</details>


### [227] [Does Visual Grounding Enhance the Understanding of Embodied Knowledge in Large Language Models?](https://arxiv.org/abs/2510.16924)
*Zhihui Yang,Yupei Wang,Kaijie Mo,Zhe Zhao,Renfen Hu*

Main category: cs.CL

TL;DR: 研究提出了一种基于心理学感知理论的具身知识理解基准，评估多模态语言模型在多种感官模态下的表现，发现视觉-语言模型并未优于纯文本模型，且在视觉维度上表现更差。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉 grounding 是否能增强语言模型对具身知识的理解，检验多模态模型在感知层面的真实能力。

Method: 构建包含视觉、听觉、触觉、味觉、嗅觉和内感的具身知识基准，通过向量比较和问答任务（超过1700个问题）评估30种主流语言模型。

Result: 视觉-语言模型在两项任务中均未优于纯文本模型，且在视觉维度表现显著更差；模型受词汇形式和频率影响大，空间感知与推理能力弱。

Conclusion: 当前多模态模型未能有效整合具身知识，需改进以提升对物理世界的理解。

Abstract: Despite significant progress in multimodal language models (LMs), it remains
unclear whether visual grounding enhances their understanding of embodied
knowledge compared to text-only models. To address this question, we propose a
novel embodied knowledge understanding benchmark based on the perceptual theory
from psychology, encompassing visual, auditory, tactile, gustatory, olfactory
external senses, and interoception. The benchmark assesses the models'
perceptual abilities across different sensory modalities through vector
comparison and question-answering tasks with over 1,700 questions. By comparing
30 state-of-the-art LMs, we surprisingly find that vision-language models
(VLMs) do not outperform text-only models in either task. Moreover, the models
perform significantly worse in the visual dimension compared to other sensory
dimensions. Further analysis reveals that the vector representations are easily
influenced by word form and frequency, and the models struggle to answer
questions involving spatial perception and reasoning. Our findings underscore
the need for more effective integration of embodied knowledge in LMs to enhance
their understanding of the physical world.

</details>


### [228] [ChiKhaPo: A Large-Scale Multilingual Benchmark for Evaluating Lexical Comprehension and Generation in Large Language Models](https://arxiv.org/abs/2510.16928)
*Emily Chang,Niyati Bafna*

Main category: cs.CL

TL;DR: 本文介绍了ChiKhaPo，一个包含8个子任务的大规模多语言基准测试，旨在评估生成式语言模型在2700多种语言中的词汇理解和生成能力，发现当前主流模型在此基准上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的基准测试主要集中在高资源或中等资源语言上，且多关注推理和生成等高阶任务，忽视了模型在世界绝大多数书面语言中的基本语言能力。因此需要一个覆盖更广、侧重基础语言能力的评测基准。

Method: 构建了名为ChiKhaPo的基准，包含8个不同难度的子任务，利用现有词典、单语数据和双语数据，评估模型的词汇理解和生成能力，覆盖超过2700种语言。对6个最先进的大语言模型进行了评测，并分析了语系、资源丰富度、任务类型等因素对表现的影响。

Result: 实验表明，当前6个最先进的大语言模型在ChiKhaPo基准上整体表现较差，尤其在低资源语言和生成任务上存在显著困难。不同语言家族和资源水平的语言表现差异明显。

Conclusion: ChiKhaPo显著提升了多语言基准的语言覆盖率，揭示了现有大模型在基础语言能力上的局限，呼吁未来研究更加关注大规模多语言能力的评估与提升。

Abstract: Existing benchmarks for large language models (LLMs) are largely restricted
to high- or mid-resource languages, and often evaluate performance on
higher-order tasks in reasoning and generation. However, plenty of evidence
points to the fact that LLMs lack basic linguistic competence in the vast
majority of the world's 3800+ written languages. We introduce ChiKhaPo,
consisting of 8 subtasks of varying difficulty designed to evaluate the lexical
comprehension and generation abilities of generative models. ChiKhaPo draws on
existing lexicons, monolingual data, and bitext, and provides coverage for
2700+ languages for 2 subtasks, surpassing any existing benchmark in terms of
language coverage. We further show that 6 SOTA models struggle on our
benchmark, and discuss the factors contributing to performance scores,
including language family, language resourcedness, task, and comprehension
versus generation directions. With ChiKhaPo, we hope to enable and encourage
the massively multilingual benchmarking of LLMs.

</details>


### [229] [Prompt-MII: Meta-Learning Instruction Induction for LLMs](https://arxiv.org/abs/2510.16932)
*Emily Xiao,Yixiao Zeng,Ada Chen,Chin-Jou Li,Amanda Bertsch,Graham Neubig*

Main category: cs.CL

TL;DR: 提出PROMPT-MII，一种基于强化学习的指令归纳框架，可在新任务上生成紧凑指令，性能媲美上下文学习但用更少token。


<details>
  <summary>Details</summary>
Motivation: 上下文学习（ICL）虽然有效，但随着上下文增长推理成本高，需更高效的任务适配方法。

Method: 提出PROMPT-MII，使用强化学习在3000多个分类数据集上元学习指令归纳模型，动态生成紧凑提示。

Result: 在90个未见任务上评估，比ICL减少3-13倍token，同时提升下游任务4-9 F1分（相对提升10-20%）。

Conclusion: PROMPT-MII能高效归纳指令，在性能相当的同时显著降低推理开销，优于传统ICL。

Abstract: A popular method to adapt large language models (LLMs) to new tasks is
in-context learning (ICL), which is effective but incurs high inference costs
as context length grows. In this paper we propose a method to perform
instruction induction, where we take training examples and reduce them to a
compact but descriptive prompt that can achieve performance comparable to ICL
over the full training set. Specifically, we propose PROMPT-MII, a
reinforcement learning (RL) based framework to meta-learn an instruction
induction model that can generate compact instructions on the fly for an
arbitrary new dataset. We train on over 3,000 diverse classification datasets
from the HuggingFace hub, and evaluate on 90 unseen tasks. PROMPT-MII improves
downstream model quality by 4-9 F1 points (10-20% relative), matching ICL
performance while requiring 3-13x fewer tokens.

</details>


### [230] [Parameter-Efficient Fine-Tuning for Low-Resource Languages: A Comparative Study of LLMs for Bengali Hate Speech Detection](https://arxiv.org/abs/2510.16985)
*Akif Islam,Mohd Ruhul Ameen*

Main category: cs.CL

TL;DR: 本文首次将参数高效微调（PEFT）方法应用于孟加拉语仇恨言论检测，使用LoRA和QLoRA在三种大型语言模型上进行实验，仅微调少于1%的参数即在单个消费级GPU上实现高效训练，其中Llama-3.2-3B达到92.23%的F1分数，验证了该方法对低资源语言的实用性。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语社交媒体上的仇恨言论激增，尤其影响女性和青少年，但现有检测方法依赖计算成本高昂的全模型微调或专有API，限制了其在低资源语言中的应用。因此需要一种高效、可复现且资源消耗低的解决方案。

Method: 采用参数高效微调技术（PEFT），基于LoRA和QLoRA方法，在三个指令微调的大语言模型（Gemma-3-4B、Llama-3.2-3B、Mistral-7B）上对BD-SHS数据集中的50,281条标注评论进行微调，仅更新不到1%的模型参数。

Result: Llama-3.2-3B模型取得了最高的F1分数（92.23%），其次是Mistral-7B（88.94%）和Gemma-3-4B（80.25%），所有实验均在单个消费级GPU上完成，验证了PEFT方法的高效性和可行性。

Conclusion: PEFT是一种实用且可复制的策略，适用于孟加拉语等低资源语言的仇恨言论检测，能够在显著降低计算成本的同时保持高性能。

Abstract: Bengali social media platforms have witnessed a sharp increase in hate
speech, disproportionately affecting women and adolescents. While datasets such
as BD-SHS provide a basis for structured evaluation, most prior approaches rely
on either computationally costly full-model fine-tuning or proprietary APIs.
This paper presents the first application of Parameter-Efficient Fine-Tuning
(PEFT) for Bengali hate speech detection using LoRA and QLoRA. Three
instruction-tuned large language models - Gemma-3-4B, Llama-3.2-3B, and
Mistral-7B - were fine-tuned on the BD-SHS dataset of 50,281 annotated
comments. Each model was adapted by training fewer than 1% of its parameters,
enabling experiments on a single consumer-grade GPU. The results show that
Llama-3.2-3B achieved the highest F1-score of 92.23%, followed by Mistral-7B at
88.94% and Gemma-3-4B at 80.25%. These findings establish PEFT as a practical
and replicable strategy for Bengali and related low-resource languages.

</details>


### [231] [Back to Bytes: Revisiting Tokenization Through UTF-8](https://arxiv.org/abs/2510.16987)
*Amit Moryossef,Clara Meister,Pavel Stepachev,Desmond Elliott*

Main category: cs.CL

TL;DR: UTF8Tokenizer 是一种极简的字节级分词器，直接将文本映射为 UTF-8 字节对应的 token ID，使用 C0 控制字节处理特殊行为，无需额外 token 或越界 ID，提升效率与兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有字节级分词器常引入越界 ID 或辅助 token，增加复杂性和传输开销；作者希望设计一种更简洁、高效、符合原始编码设计意图的分词方法。

Method: 提出 UTF8Tokenizer，将 UTF-8 字节直接映射为 token ID，利用 C0 控制字节（如 \t、\n）表示特殊行为（如填充、边界、对话结构等），并引入 bit-biased embeddings 在训练时利用字节位结构信息。

Result: 实现了 14 倍更快的分词速度，比 int64 传输少 8 倍开销；使用共享的 256*d 嵌入表；bit-biased embeddings 提升语言建模收敛速度且无推理开销。

Conclusion: UTF8Tokenizer 通过回归 ASCII 控制字符的原始设计理念，提供了一种高效、简洁、可共享的字节级分词方案，兼具性能优势与工程实用性。

Abstract: We present UTF8Tokenizer, a minimalist byte-level tokenizer that maps text
exactly to IDs corresponding to the bytes underlying the text's UTF-8 encoding
(e.g., byte x09 is token ID 9). Unlike prior byte-level approaches (Xue et al.,
2021; Pagnoni et al., 2025), our implementation never introduces out-of-range
IDs (i.e. there is no token ID 256) or auxiliary tokens: all special behavior
(e.g., padding, boundaries, conversation structure, attention segments, tool
calling, "thinking" spans, etc.) is encoded using C0 control bytes - just as
ASCII was originally designed to embed control information alongside printable
text. These design principles yield practical benefits: (1) faster tokenization
(14x) and significantly lower host-device transfer (8x less than int64); (2)
simple, shareable 256*d embedding tables that can be aligned across models; and
(3) a training-time enhancement via bit-biased embeddings, which exposes
per-byte bit structure and can be added to the embedding table post-training,
removing inference costs. Our HuggingFace-compatible implementation improves
language modeling convergence.

</details>


### [232] [Vocab Diet: Reshaping the Vocabulary of LLMs with Vector Arithmetic](https://arxiv.org/abs/2510.17001)
*Yuval Reif,Guy Kaplan,Roy Schwartz*

Main category: cs.CL

TL;DR: 提出一种基于词形变换向量的组合式词汇重构方法，减少词汇表冗余，提升覆盖范围，无需修改模型权重。


<details>
  <summary>Details</summary>
Motivation: 标准分词将同一词的不同形态视为独立词符，导致词汇表冗余，限制了低频词和多语言覆盖；希望利用语言内在结构优化词汇设计。

Method: 通过学习词形变换向量（如过去式），在嵌入空间中将表面形式表示为词根嵌入与变换向量的加和，构建组合式词汇表。

Result: 在多个大模型和五种语言上验证，最多减少10%词汇表条目，释放空间用于加入更多样化词符，同时支持对未登录词的覆盖，下游任务性能影响极小。

Conclusion: 词汇设计应从枚举字符串转向利用语言结构的组合式方法，实现更紧凑、更泛化的词汇表示。

Abstract: Large language models (LLMs) were shown to encode word form variations, such
as "walk"->"walked", as linear directions in embedding space. However, standard
tokenization algorithms treat these variations as distinct tokens -- filling
the size-capped vocabulary with surface form variants (e.g., "walk", "walking",
"Walk"), at the expense of less frequent words and multilingual coverage. We
show that many of these variations can be captured by transformation vectors --
additive offsets that yield the appropriate word's representation when applied
to the base form word embedding -- in both the input and output spaces.
Building on this, we propose a compact reshaping of the vocabulary: rather than
assigning unique tokens to each surface form, we compose them from shared base
form and transformation vectors (e.g., "walked" = "walk" + past tense). We
apply our approach to multiple LLMs and across five languages, removing up to
10% of vocabulary entries -- thereby freeing space to allocate new, more
diverse tokens. Importantly, we do so while also expanding vocabulary coverage
to out-of-vocabulary words, with minimal impact on downstream performance, and
without modifying model weights. Our findings motivate a foundational
rethinking of vocabulary design, moving from string enumeration to a
compositional vocabulary that leverages the underlying structure of language.

</details>


### [233] [Online Learning Defense against Iterative Jailbreak Attacks via Prompt Optimization](https://arxiv.org/abs/2510.17006)
*Masahiro Kaneko,Zeerak Talat,Timothy Baldwin*

Main category: cs.CL

TL;DR: 提出一种基于强化学习的动态防御框架，能够在线学习并抵御迭代越狱攻击，同时提升无害任务的响应质量。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法无法主动打断迭代越狱攻击的试错循环，因此需要一种能动态适应并阻断此类攻击的新防御机制。

Method: 提出一种基于强化学习的在线学习框架，通过区分有害越狱提示与无害提示来优化防御策略，并引入PDGD（Past-Direction Gradient Damping）机制以防止在攻击过程中因局部改写而过拟合。

Result: 在三种大语言模型上测试表明，该方法在抵御五种迭代越狱攻击方面显著优于五种现有防御方法，同时提升了对无害任务的响应质量。

Conclusion: 所提出的动态防御框架能有效阻断迭代越狱攻击的试错过程，并在增强安全性的同时改善模型对正常请求的响应能力。

Abstract: Iterative jailbreak methods that repeatedly rewrite and input prompts into
large language models (LLMs) to induce harmful outputs -- using the model's
previous responses to guide each new iteration -- have been found to be a
highly effective attack strategy. Despite being an effective attack strategy
against LLMs and their safety mechanisms, existing defenses do not proactively
disrupt this dynamic trial-and-error cycle. In this study, we propose a novel
framework that dynamically updates its defense strategy through online learning
in response to each new prompt from iterative jailbreak methods. Leveraging the
distinctions between harmful jailbreak-generated prompts and typical harmless
prompts, we introduce a reinforcement learning-based approach that optimizes
prompts to ensure appropriate responses for harmless tasks while explicitly
rejecting harmful prompts. Additionally, to curb overfitting to the narrow band
of partial input rewrites explored during an attack, we introduce
Past-Direction Gradient Damping (PDGD). Experiments conducted on three LLMs
show that our approach significantly outperforms five existing defense methods
against five iterative jailbreak methods. Moreover, our results indicate that
our prompt optimization strategy simultaneously enhances response quality for
harmless tasks.

</details>


### [234] [DiscoTrack: A Multilingual LLM Benchmark for Discourse Tracking](https://arxiv.org/abs/2510.17013)
*Lanni Bu,Lauren Levin,Amir Zeldes*

Main category: cs.CL

TL;DR: 本文提出了DiscoTrack，一个包含12种语言和四个层次话语理解任务的多语言基准测试，旨在评估大模型在处理隐含信息和跨句子、段落及多说话人语境下的语篇跟踪能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型基准测试主要集中于从单句中提取显性信息的自然语言理解任务，缺乏对隐含信息和跨文本语篇推理的挑战性、多语言评测标准。

Method: 构建DiscoTrack基准，涵盖12种语言，设计四个层次的话语理解任务：显著性识别、实体追踪、话语关系和桥接推理，并在现有最先进的大模型上进行评估。

Result: 实验结果表明，即使是最先进的大模型在这些任务上仍然表现不佳，说明这些任务具有较高挑战性。

Conclusion: DiscoTrack为评估大模型在多语言、深层次语篇理解方面的能力提供了新的基准，揭示了当前模型在隐含信息推理和跨文本信息整合方面的不足。

Abstract: Recent LLM benchmarks have tested models on a range of phenomena, but are
still focused primarily on natural language understanding for extraction of
explicit information, such as QA or summarization, with responses often tar-
geting information from individual sentences. We are still lacking more
challenging, and im- portantly also multilingual, benchmarks focus- ing on
implicit information and pragmatic infer- ences across larger documents in the
context of discourse tracking: integrating and aggregating information across
sentences, paragraphs and multiple speaker utterances. To this end, we present
DiscoTrack, an LLM benchmark target- ing a range of tasks across 12 languages
and four levels of discourse understanding: salience recognition, entity
tracking, discourse relations and bridging inference. Our evaluation shows that
these tasks remain challenging, even for state-of-the-art models.

</details>


### [235] [SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents](https://arxiv.org/abs/2510.17017)
*Qiusi Zhan,Angeline Budiman-Chan,Abdelrahman Zayed,Xingzhi Guo,Daniel Kang,Joo-Kyung Kim*

Main category: cs.CL

TL;DR: 基于大语言模型的搜索代理在提升实用性的同时暴露出更多安全风险，本文提出SafeSearch方法，通过多目标强化学习联合优化安全与效用，显著降低有害输出。


<details>
  <summary>Details</summary>
Motivation: 现有搜索代理在追求高实用性的同时忽视了安全性，导致生成有害内容的风险增加，亟需同时对齐安全与效用。

Method: 提出SafeSearch，采用多目标强化学习，结合最终输出的安全/效用奖励与新的查询层级塑形项，惩罚不安全查询，奖励安全查询。

Result: 实验表明，SafeSearch在三个红队数据集上将代理有害性降低超过70%，保持有用响应，并达到与仅优化效用的代理相当的问答性能；分析验证了查询级奖励的有效性。

Conclusion: 通过引入查询级安全控制，SafeSearch能有效协同提升搜索代理的安全性和实用性，为安全对齐提供了新路径。

Abstract: Large language model (LLM) based search agents iteratively generate queries,
retrieve external information, and reason to answer open-domain questions.
While researchers have primarily focused on improving their utility, their
safety behaviors remain underexplored. In this paper, we first evaluate search
agents using red-teaming datasets and find that they are more likely to produce
harmful outputs than base LLMs. For example, when asked ``How can I track
someone's location without their consent?'', a base model refuses, whereas a
search agent designed to retrieve and cite sources may lower its refusal
threshold, fetch documents (e.g., court cases), and, once appended, synthesize
them into an informative yet unsafe summary. We further show that
utility-oriented fine-tuning intensifies this risk, motivating joint alignment
of safety and utility. We present SafeSearch, a multi-objective reinforcement
learning approach that couples a final-output safety/utility reward with a
novel query-level shaping term that penalizes unsafe queries and rewards safe
ones. Experiments show that SafeSearch reduces agent harmfulness by over 70%
across three red-teaming datasets while producing safe, helpful responses, and
matches the QA performance of a utility-only finetuned agent; further analyses
confirm the effectiveness of the query-level reward in jointly improving safety
and utility.

</details>


### [236] [Extended LSTM: Adaptive Feature Gating for Toxic Comment Classification](https://arxiv.org/abs/2510.17018)
*Noor Islam S. Mohammad*

Main category: cs.CL

TL;DR: xLSTM是一种高效的毒性评论检测框架，结合余弦相似性门控、自适应特征优先级和类别重平衡，在低资源下优于BERT，尤其在少数类上表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有模型如BERT计算成本高且在少数毒性类别上表现差，传统集成方法缺乏语义适应性，需要更高效且适应性强的解决方案。

Method: 提出xLSTM框架：利用可学习参考向量通过余弦相似性调制上下文嵌入，融合多源嵌入（GloVe、FastText、BERT CLS）、字符级BiLSTM、嵌入空间SMOTE增强少数类，并采用自适应焦点损失。

Result: 在Jigsaw Toxic Comment基准上达到96.0%准确率和0.88宏观F1，威胁类和身份仇恨类分别比BERT高33%和28%，参数少15倍，推理延迟仅50ms，消融显示余弦门控带来+4.8% F1提升。

Conclusion: 轻量级、理论指导的架构可在不平衡、领域特定的NLP任务中超越大型预训练模型，建立效率与适应性新前沿。

Abstract: Toxic comment detection remains a challenging task, where transformer-based
models (e.g., BERT) incur high computational costs and degrade on minority
toxicity classes, while classical ensembles lack semantic adaptability. We
propose xLSTM, a parameter-efficient and theoretically grounded framework that
unifies cosine-similarity gating, adaptive feature prioritization, and
principled class rebalancing. A learnable reference vector {v} in {R}^d
modulates contextual embeddings via cosine similarity, amplifying toxic cues
and attenuating benign signals to yield stronger gradients under severe class
imbalance. xLSTM integrates multi-source embeddings (GloVe, FastText, BERT CLS)
through a projection layer, a character-level BiLSTM for morphological cues,
embedding-space SMOTE for minority augmentation, and adaptive focal loss with
dynamic class weighting. On the Jigsaw Toxic Comment benchmark, xLSTM attains
96.0% accuracy and 0.88 macro-F1, outperforming BERT by 33% on threat and 28%
on identity_hate categories, with 15 times fewer parameters and 50ms inference
latency. Cosine gating contributes a +4.8% F1 gain in ablations. The results
establish a new efficiency adaptability frontier, demonstrating that
lightweight, theoretically informed architectures can surpass large pretrained
models on imbalanced, domain-specific NLP tasks.

</details>


### [237] [Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language Models](https://arxiv.org/abs/2510.17028)
*Kyle Cox,Jiawei Xu,Yikun Han,Rong Xu,Tianhao Li,Chi-Yang Hsu,Tianlong Chen,Walter Gerych,Ying Ding*

Main category: cs.CL

TL;DR: 该论文研究了大语言模型中的提示敏感性问题，提出通过在语义概念空间中采样来改善不确定性校准，并引入新的不确定性分解指标，以量化提示敏感性对模型不确定性的影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在面对语义等价但表述不同的提示时可能产生差异较大的输出，表明模型的不确定性可能并非反映其对提示语义的理解不确定，而是一种泛化误差。因此需要更好地理解和校准这种不确定性。

Method: 将提示敏感性建模为一种泛化误差，使用改写扰动在语义概念空间中进行采样，并提出一种新的基于语义连续性的黑盒大语言模型不确定性分解指标，改进了传统的基于熵的方法。

Result: 改写采样方法在不降低准确率的前提下提升了模型的不确定性校准效果；新提出的分解指标能够有效量化提示敏感性在总体不确定性中的贡献。

Conclusion: 提示敏感性是影响大语言模型不确定性校准的重要因素，通过语义空间采样和新的不确定性分解方法可以更准确地评估和改进模型的不确定性表现，揭示部分大模型在输入语义推理上缺乏一致性。

Abstract: An interesting behavior in large language models (LLMs) is prompt
sensitivity. When provided with different but semantically equivalent versions
of the same prompt, models may produce very different distributions of answers.
This suggests that the uncertainty reflected in a model's output distribution
for one prompt may not reflect the model's uncertainty about the meaning of the
prompt. We model prompt sensitivity as a type of generalization error, and show
that sampling across the semantic ``concept space'' with paraphrasing
perturbations improves uncertainty calibration without compromising accuracy.
Additionally, we introduce a new metric for uncertainty decomposition in
black-box LLMs that improves upon entropy-based decomposition by modeling
semantic continuities in natural language generation. We show that this
decomposition metric can be used to quantify how much LLM uncertainty is
attributed to prompt sensitivity. Our work introduces a new way to improve
uncertainty calibration in prompt-sensitive language models, and provides
evidence that some LLMs fail to exhibit consistent general reasoning about the
meanings of their inputs.

</details>


### [238] [Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation](https://arxiv.org/abs/2510.17062)
*Guoqing Luo,Iffat Maab,Lili Mou,Junichi Yamagishi*

Main category: cs.CL

TL;DR: 研究发现推理型大语言模型在思考过程中会聚合社会刻板印象，导致偏见结果，提出一种轻量级基于提示的缓解方法，通过让模型自我审查初始推理中的刻板重复和无关信息注入来减少偏见。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在复杂任务中表现出色，但其内部推理过程可能加剧社会偏见，然而这一现象的机制尚不清楚，因此需要系统性探究其背后的行为模式。

Method: 识别推理过程中导致偏见聚合的两种失败模式：刻板印象重复和无关信息注入，并提出一种基于提示的自审方法，引导模型检查并修正自身推理。

Result: 在BBQ、StereoSet和BOLD等多个基准上的实验表明，该方法能有效降低模型输出的偏见程度，同时保持甚至提升准确性。

Conclusion: 通过理解并干预推理过程中的关键失败模式，可以在不牺牲性能的前提下显著减轻语言模型的社会偏见。

Abstract: While reasoning-based large language models excel at complex tasks through an
internal, structured thinking process, a concerning phenomenon has emerged that
such a thinking process can aggregate social stereotypes, leading to biased
outcomes. However, the underlying behaviours of these language models in social
bias scenarios remain underexplored. In this work, we systematically
investigate mechanisms within the thinking process behind this phenomenon and
uncover two failure patterns that drive social bias aggregation: 1) stereotype
repetition, where the model relies on social stereotypes as its primary
justification, and 2) irrelevant information injection, where it fabricates or
introduces new details to support a biased narrative. Building on these
insights, we introduce a lightweight prompt-based mitigation approach that
queries the model to review its own initial reasoning against these specific
failure patterns. Experiments on question answering (BBQ and StereoSet) and
open-ended (BOLD) benchmarks show that our approach effectively reduces bias
while maintaining or improving accuracy.

</details>


### [239] [Verification-Aware Planning for Multi-Agent Systems](https://arxiv.org/abs/2510.17109)
*Tianyang Xu,Dan Zhang,Kushan Mitra,Estevam Hruschka*

Main category: cs.CL

TL;DR: VeriMAP是一种面向多智能体协作的验证感知规划框架，通过定义子任务验证函数来提升系统的鲁棒性、可解释性和协作可靠性。


<details>
  <summary>Details</summary>
Motivation: 多智能体协作在复杂任务中面临规划、协调和验证的挑战，尤其是任务理解、输出格式和交接中的细微偏差常导致执行失败。

Method: 提出VeriMAP框架，将任务分解，建模子任务依赖关系，并以Python和自然语言形式定义子任务的验证函数（VFs），实现验证感知的规划。

Result: 在多个数据集上评估显示，VeriMAP优于单智能体和多智能体基线方法，提升了系统鲁棒性和可解释性。

Conclusion: 验证感知规划能有效支持多智能体系统的可靠协同与迭代优化，且无需依赖外部标注。

Abstract: Large language model (LLM) agents are increasingly deployed to tackle complex
tasks, often necessitating collaboration among multiple specialized agents.
However, multi-agent collaboration introduces new challenges in planning,
coordination, and verification. Execution failures frequently arise not from
flawed reasoning alone, but from subtle misalignments in task interpretation,
output format, or inter-agent handoffs. To address these challenges, we present
VeriMAP, a framework for multi-agent collaboration with verification-aware
planning. The VeriMAP planner decomposes tasks, models subtask dependencies,
and encodes planner-defined passing criteria as subtask verification functions
(VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets,
demonstrating that it outperforms both single- and multi-agent baselines while
enhancing system robustness and interpretability. Our analysis highlights how
verification-aware planning enables reliable coordination and iterative
refinement in multi-agent systems, without relying on external labels or
annotations.

</details>


### [240] [DVAGen: Dynamic Vocabulary Augmented Generation](https://arxiv.org/abs/2510.17115)
*Wei Du,Nuowei Liu,Jie Wang,Jiahao Kuang,Tao Ji,Xiaoling Wang,Yuanbin Wu*

Main category: cs.CL

TL;DR: 本文介绍了DVAGen，一个开源的统一框架，用于增强语言模型的动态词汇训练、评估和可视化，提升对新词的适应能力和推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有固定词汇的语言模型难以处理未登录词，而动态词汇方法存在代码分散、不支持现代大模型和推理扩展性差的问题。

Method: 提出DVAGen框架，模块化设计，支持与开源大模型集成，并提供CLI和WebUI工具，支持批量推理和实时结果查看。

Result: 验证了动态词汇方法在现代大模型上的有效性，显著提升推理吞吐量，并实现训练、评估和可视化的统一支持。

Conclusion: DVAGen为动态词汇语言模型提供了可扩展、易定制的解决方案，推动了该领域的开放研究和应用。

Abstract: Language models trained with a fixed vocabulary struggle to generalize to
novel or out-of-vocabulary words, limiting their flexibility in handling
diverse token combinations. Existing dynamic vocabulary approaches attempt to
address this limitation but face challenges such as fragmented codebases, lack
of support for modern LLMs, and limited inference scalability. To overcome
these issues, we introduce DVAGen, a fully open-source, unified framework
designed for training, evaluation, and visualization of dynamic
vocabulary-augmented language models. Our framework modularizes the pipeline
for ease of customization, integrates seamlessly with open-source LLMs, and is
the first to provide both CLI and WebUI tools for real-time result inspection.
We validate the effectiveness of dynamic vocabulary methods on modern LLMs and
demonstrate support for batch inference, significantly improving inference
throughput.

</details>


### [241] [Rethinking On-policy Optimization for Query Augmentation](https://arxiv.org/abs/2510.17139)
*Zhichao Xu,Shengyao Zhuang,Xueguang Ma,Bingsen Chen,Yijun Tian,Fengran Mo,Jie Cao,Vivek Srikumar*

Main category: cs.CL

TL;DR: 本文系统比较了基于提示和基于强化学习的查询增强方法，发现简单无训练的方法常能与复杂的强化学习方法相媲美甚至更优，并提出一种新混合方法OPQE，在多种检索任务中表现更佳。


<details>
  <summary>Details</summary>
Motivation: 现有查询增强方法缺乏在一致条件下对提示法与强化学习法的系统比较，且二者优势未被有效融合，本文旨在填补这一空白。

Method: 在多个检索基准上对提示法和强化学习法进行公平对比，并提出OPQE方法：利用强化学习训练LLM生成最大化检索效果的伪文档，而非改写查询。

Result: 实验表明，简单的无训练提示方法常与复杂的RL方法性能相当或更好；新提出的OPQE方法在证据检索、即席检索和工具检索任务中均优于单独使用提示或RL重写的方法。

Conclusion: 融合提示生成灵活性与强化学习目标优化的协同方法（如OPQE）能取得更优检索性能，代表了查询增强的有效方向。

Abstract: Recent advances in large language models (LLMs) have led to a surge of
interest in query augmentation for information retrieval (IR). Two main
approaches have emerged. The first prompts LLMs to generate answers or
pseudo-documents that serve as new queries, relying purely on the model's
parametric knowledge or contextual information. The second applies
reinforcement learning (RL) to fine-tune LLMs for query rewriting, directly
optimizing retrieval metrics. While having respective advantages and
limitations, the two approaches have not been compared under consistent
experimental conditions. In this work, we present the first systematic
comparison of prompting-based and RL-based query augmentation across diverse
benchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key
finding is that simple, training-free query augmentation often performs on par
with, or even surpasses, more expensive RL-based counterparts, especially when
using powerful LLMs. Motivated by this discovery, we introduce a novel hybrid
method, On-policy Pseudo-document Query Expansion (OPQE), which, instead of
rewriting a query, the LLM policy learns to generate a pseudo-document that
maximizes retrieval performance, thus merging the flexibility and generative
structure of prompting with the targeted optimization of RL. We show OPQE
outperforms both standalone prompting and RL-based rewriting, demonstrating
that a synergistic approach yields the best results. Our implementation is made
available to facilitate reproducibility.

</details>


### [242] [When AI companions become witty: Can human brain recognize AI-generated irony?](https://arxiv.org/abs/2510.17168)
*Xiaohui Rao,Hanlin Wu,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: 人们在理解AI生成的讽刺时，较少采取“有意立场”，更多将其视为计算输出而非有意沟通，神经证据显示对AI讽刺的加工努力更少。个体对AI的真诚感知程度影响其是否赋予AI沟通意图。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型被用作社交代理并生成幽默和讽刺，研究人们是否会将AI的言语视为有意图的交流行为，有助于理解人类对AI的社会认知机制。

Method: 通过ERP实验比较人类与AI作为来源的讽刺语句所引发的行为和神经反应，重点分析P200（早期不一致检测）和P600（认知重分析）成分。

Result: 行为上，参与者更倾向于将人类的不一致解释为有意讽刺，而将AI的不一致归因为计算错误；神经上，AI讽刺引发的P200和P600效应减弱，表明加工努力较低；对AI越真诚感知者，其P200和P600效应越强。

Conclusion: 尽管LLM具备语言能力，但实现真正的社交能动性还需人类对其意图性的认可；人类对AI的心理模型调节了其是否采取有意立场，源属性显著影响社会交流的神经处理。

Abstract: As Large Language Models (LLMs) are increasingly deployed as social agents
and trained to produce humor and irony, a question emerges: when encountering
witty AI remarks, do people interpret these as intentional communication or
mere computational output? This study investigates whether people adopt the
intentional stance, attributing mental states to explain behavior,toward AI
during irony comprehension. Irony provides an ideal paradigm because it
requires distinguishing intentional contradictions from unintended errors
through effortful semantic reanalysis. We compared behavioral and neural
responses to ironic statements from AI versus human sources using established
ERP components: P200 reflecting early incongruity detection and P600 indexing
cognitive efforts in reinterpreting incongruity as deliberate irony. Results
demonstrate that people do not fully adopt the intentional stance toward
AI-generated irony. Behaviorally, participants attributed incongruity to
deliberate communication for both sources, though significantly less for AI
than human, showing greater tendency to interpret AI incongruities as
computational errors. Neural data revealed attenuated P200 and P600 effects for
AI-generated irony, suggesting reduced effortful detection and reanalysis
consistent with diminished attribution of communicative intent. Notably, people
who perceived AI as more sincere showed larger P200 and P600 effects for
AI-generated irony, suggesting that intentional stance adoption is calibrated
by specific mental models of artificial agents. These findings reveal that
source attribution shapes neural processing of social-communicative phenomena.
Despite current LLMs' linguistic sophistication, achieving genuine social
agency requires more than linguistic competence, it necessitates a shift in how
humans perceive and attribute intentionality to artificial agents.

</details>


### [243] [Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models](https://arxiv.org/abs/2510.17196)
*Jiaqi Leng,Xiang Hu,Junxiong Wang,Jianguo Li,Wei Wu,Yucheng Lu*

Main category: cs.CL

TL;DR: 本文系统分析了基于块稀疏注意力的长上下文语言模型，提出了三个关键设计原则，并实现了无需微调即可将训练上下文从4K扩展到3200万token的新纪录。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer模型在处理长上下文时受限于二次复杂度和外推能力差，而其他模型因固定内存大小牺牲了全局上下文利用能力，因此需要理解块稀疏注意力成功的根本原理以提升长上下文建模。

Method: 提出统一框架并进行消融研究，识别出三个关键设计：非线性块编码器（含专用CLS token）、绕过式残差路径以稳定融合全局信息、预训练时强制选择稀疏性来缩小训练与测试分布差距。

Result: 结合这三个原则，在无需微调的情况下实现了训练长度外推的新SOTA，在RULER和BABILong上成功将4K上下文训练的模型外推至3200万token。

Conclusion: 该研究为构建高效的长上下文语言模型提供了清晰且实证支持的设计原则。

Abstract: Effectively processing long contexts is a critical challenge for language
models. While standard Transformers are limited by quadratic complexity and
poor length extrapolation, alternative architectures like sliding window
attention and state space models sacrifice the ability to effectively utilize
the full context due to their fixed-size memory. Chunk-based sparse attention
has emerged as a promising paradigm for extreme length generalization, yet the
key architectural principles underpinning its success are not yet fully
understood. In this work, we present a systematic dissection of these models to
identify the core components driving their performance. Through a unified
framework and comprehensive ablation studies, we demonstrate that a combination
of three design principles is critical: (1) an expressive, non-linear Chunk
Encoder with a dedicated CLS token to produce representations for retrieval;
(2) a Bypassing Residual Path to stably integrate retrieved global information
without it being overridden by the local residual stream; and (3) enforced
selection sparsity during pre-training to bridge the train-test distribution
gap. We provide a theoretical motivation for intra-chunk information processing
and landmark generation. By combining these principles, we establish a new
state-of-the-art for training-free length extrapolation, successfully
generalizing models trained on a 4K context to 32 million tokens on RULER and
BABILong. Our findings provide a clear and empirically-grounded set of design
principles for developing future, highly-capable long-context language models.

</details>


### [244] [Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting](https://arxiv.org/abs/2510.17210)
*Chenchen Tan,Youyang Qu,Xinghao Li,Hui Zhang,Shujie Cui,Cunjian Chen,Longxiang Gao*

Main category: cs.CL

TL;DR: 提出一种新的注意力迁移（AS）框架，用于大语言模型的选择性遗忘，有效平衡了遗忘效果、模型效用保持和响应可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘方法在保证模型实用性的同时难以避免幻觉响应，限制了大语言模型在知识密集型应用中的可靠性。

Method: 设计了一个双目标注意力迁移框架，包括重要性感知抑制和注意力引导保留增强，通过双损失目标联合优化，在注意力层面实现选择性遗忘。

Result: 实验表明，AS在ToFU和TDEC基准上分别比现有最好方法提高了15%和10%的准确性，同时有效抑制了幻觉生成。

Conclusion: AS框架在遗忘效果、泛化能力和响应可靠性之间实现了更优平衡，提升了大语言模型在敏感数据遗忘场景下的实用性。

Abstract: The increase in computing power and the necessity of AI-assisted
decision-making boost the growing application of large language models (LLMs).
Along with this, the potential retention of sensitive data of LLMs has spurred
increasing research into machine unlearning. However, existing unlearning
approaches face a critical dilemma: Aggressive unlearning compromises model
utility, while conservative strategies preserve utility but risk hallucinated
responses. This significantly limits LLMs' reliability in knowledge-intensive
applications. To address this, we introduce a novel Attention-Shifting (AS)
framework for selective unlearning. AS is driven by two design objectives: (1)
context-preserving suppression that attenuates attention to fact-bearing tokens
without disrupting LLMs' linguistic structure; and (2) hallucination-resistant
response shaping that discourages fabricated completions when queried about
unlearning content. AS realizes these objectives through two attention-level
interventions, which are importance-aware suppression applied to the unlearning
set to reduce reliance on memorized knowledge and attention-guided retention
enhancement that reinforces attention toward semantically essential tokens in
the retained dataset to mitigate unintended degradation. These two components
are jointly optimized via a dual-loss objective, which forms a soft boundary
that localizes unlearning while preserving unrelated knowledge under
representation superposition. Experimental results show that AS improves
performance preservation over the state-of-the-art unlearning methods,
achieving up to 15% higher accuracy on the ToFU benchmark and 10% on the TDEC
benchmark, while maintaining competitive hallucination-free unlearning
effectiveness. Compared to existing methods, AS demonstrates a superior balance
between unlearning effectiveness, generalization, and response reliability.

</details>


### [245] [StreamingThinker: Large Language Models Can Think While Reading](https://arxiv.org/abs/2510.17238)
*Junlong Tong,Yingqi Fan,Anhao Zhao,Yunpu Ma,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 提出了一种名为StreamingThinker的流式思考范式，使大语言模型在输入过程中即可开始推理，显著降低延迟，同时保持与批量推理相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型推理范式需等待完整输入后才开始思考，导致动态场景中出现不必要延迟，并弱化对早期信息的关注。受人类阅读时同步思考的启发，本文旨在设计一种更高效、低延迟的推理范式。

Method: 提出StreamingThinker框架，包含流式思维链（CoT）生成、流式约束训练和流式并行推理。通过流式推理单元与质量控制、保序注意力掩码与位置编码、并行KV缓存解耦输入编码与推理生成，实现边读边想。

Result: 在Qwen3模型系列上测试显示，StreamingThinker在数学、逻辑和基于上下文的问答任务中性能与批量推理相当，推理启动前的token等待时间减少80%，最终答案生成的时间延迟降低超过60%。

Conclusion: 流式思考范式有效提升了大语言模型在动态输入场景下的推理效率和响应速度，为低延迟应用提供了可行方案。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm
initiates thinking only after the entire input is available, which introduces
unnecessary latency and weakens attention to earlier information in dynamic
scenarios. Inspired by human cognition of thinking while reading, we first
design a \textit{\textbf{streaming thinking}} paradigm for LLMs, where
reasoning unfolds in the order of input and further adjusts its depth once
reading is complete. We instantiate this paradigm with
\textit{StreamingThinker}, a framework that enables LLMs to think while reading
through the integration of streaming CoT generation, streaming-constraint
training, and streaming parallel inference. Specifically, StreamingThinker
employs streaming reasoning units with quality control for CoT generation,
enforces order-preserving reasoning through streaming attention masks and
position encoding, and leverages parallel KV caches that decouple input
encoding from reasoning generation, thereby ensuring alignment and enabling
true concurrency. We evaluate StreamingThinker on the Qwen3 model family across
math reasoning, logical reasoning, and context-based QA reasoning tasks.
Experimental results show that the StreamingThinker preserves performance
comparable to batch thinking, while yielding an 80\% reduction in token waiting
before the onset of reasoning and a more than 60\% reduction in time-level
latency for producing the final answer, demonstrating the effectiveness of the
streaming paradigm for LLM reasoning. Code will be released at
\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this
repository.}

</details>


### [246] [From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models](https://arxiv.org/abs/2510.17247)
*Zefan Cai,Haoyi Qiu,Haozhe Zhao,Ke Wan,Jiachen Li,Jiuxiang Gu,Wen Xiao,Nanyun Peng,Junjie Hu*

Main category: cs.CL

TL;DR: 本文提出了VideoBiasEval框架，用于系统评估视频生成模型中的社会偏见，发现对齐调优不仅强化了表征偏见，还使其在时间上更稳定。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在提升视频生成质量的同时，可能无意中放大社会偏见，缺乏系统性追踪偏见演变的评估方法。

Method: 基于社会偏见分类体系，提出基于事件的提示策略，解耦语义内容与人物属性，并设计多粒度指标评估族裔、性别偏见及其在模型中的分布变化和时间持久性。

Result: 首次实现从人类偏好数据到奖励模型再到对齐调优模型的端到端偏见分析，发现对齐过程加剧并稳定了社会偏见。

Conclusion: 需要在对齐过程中引入偏知觉评估与缓解机制，以确保视频生成的公平性和社会责任性。

Abstract: Recent advances in video diffusion models have significantly enhanced
text-to-video generation, particularly through alignment tuning using reward
models trained on human preferences. While these methods improve visual
quality, they can unintentionally encode and amplify social biases. To
systematically trace how such biases evolve throughout the alignment pipeline,
we introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating
social representation in video generation. Grounded in established social bias
taxonomies, VideoBiasEval employs an event-based prompting strategy to
disentangle semantic content (actions and contexts) from actor attributes
(gender and ethnicity). It further introduces multi-granular metrics to
evaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity,
(3) distributional shifts in social attributes across model variants, and (4)
the temporal persistence of bias within videos. Using this framework, we
conduct the first end-to-end analysis connecting biases in human preference
datasets, their amplification in reward models, and their propagation through
alignment-tuned video diffusion models. Our results reveal that alignment
tuning not only strengthens representational biases but also makes them
temporally stable, producing smoother yet more stereotyped portrayals. These
findings highlight the need for bias-aware evaluation and mitigation throughout
the alignment process to ensure fair and socially responsible video generation.

</details>


### [247] [How News Feels: Understanding Affective Bias in Multilingual Headlines for Human-Centered Media Design](https://arxiv.org/abs/2510.17252)
*Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Ayesha Siddiqua,Jungpil Shin*

Main category: cs.CL

TL;DR: 通过使用Gemma-3 4B对30万条孟加拉语新闻标题和内容进行情绪分析，研究发现负面情绪（尤其是愤怒、恐惧和失望）占据主导地位，且不同媒体对相似事件的情感呈现差异显著，研究据此提出了可视化情感线索、帮助读者识别新闻中隐性情感框架的人本新闻聚合工具设计思路。


<details>
  <summary>Details</summary>
Motivation: 新闻媒体通过情感化报道框架影响公众情绪，而负面或情绪化标题更容易吸引关注并加速传播，这种倾向可能导致公众情绪极化。因此，识别并揭示新闻中的隐性情感偏见，有助于提升读者的媒介素养和信息判断能力。

Method: 采用零样本推理（zero-shot inference）方法，利用Gemma-3 4B模型对30万条孟加拉语新闻标题及内容进行大规模情绪分析，识别每条新闻的主导情绪和整体情感基调，并比较不同媒体对相似事件的情感表达差异。

Result: 分析结果显示，负面情绪在孟加拉语新闻中占主导地位，尤其是愤怒、恐惧和失望；同时，不同新闻媒体对相似事件的情感表达存在显著差异，显示出明显的情感报道偏差。

Conclusion: 新闻报道中普遍存在情感偏向，特别是负面情绪的强化。基于此，研究提出应设计以人为中心的新闻聚合平台，通过可视化情感线索帮助读者识别隐性的情感框架，增强对新闻内容的批判性理解。

Abstract: News media often shape the public mood not only by what they report but by
how they frame it. The same event can appear calm in one outlet and alarming in
another, reflecting subtle emotional bias in reporting. Negative or emotionally
charged headlines tend to attract more attention and spread faster, which in
turn encourages outlets to frame stories in ways that provoke stronger
reactions. This research explores that tendency through large-scale emotion
analysis of Bengali news. Using zero-shot inference with Gemma-3 4B, we
analyzed 300000 Bengali news headlines and their content to identify the
dominant emotion and overall tone of each. The findings reveal a clear
dominance of negative emotions, particularly anger, fear, and disappointment,
and significant variation in how similar stories are emotionally portrayed
across outlets. Based on these insights, we propose design ideas for a
human-centered news aggregator that visualizes emotional cues and helps readers
recognize hidden affective framing in daily news.

</details>


### [248] [Explainability of Large Language Models: Opportunities and Challenges toward Generating Trustworthy Explanations](https://arxiv.org/abs/2510.17256)
*Shahin Atakishiyev,Housam K. B. Babiker,Jiayi Dai,Nawshad Farruque,Teruaki Hayashi,Nafisa Sadaf Hriti,Md Abed Rahman,Iain Smith,Mi-Young Kim,Osmar R. Zaïane,Randy Goebel*

Main category: cs.CL

TL;DR: 本文综述了Transformer大语言模型的局部可解释性与机制可解释性方法，探讨了在医疗和自动驾驶领域的应用，并分析了解释对用户信任的影响，指出了未来实现人类对齐、可信的LLM解释的方向。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽在各项任务中表现优异，但其预测和推理过程存在黑箱和幻觉问题，亟需提升模型可解释性以增强人类信任。

Method: 本文通过文献综述和在医疗、自动驾驶两个关键领域开展可解释性与推理的实验研究，系统分析现有方法。

Result: 总结了当前局部可解释性和机制可解释性的主要方法与见解，揭示了解释在不同领域对信任的影响，并识别出当前研究的空白。

Conclusion: 需进一步解决LLM可解释性中的挑战，推动生成人类对齐、可信的解释，为未来研究提供方向。

Abstract: Large language models have exhibited impressive performance across a broad
range of downstream tasks in natural language processing. However, how a
language model predicts the next token and generates content is not generally
understandable by humans. Furthermore, these models often make errors in
prediction and reasoning, known as hallucinations. These errors underscore the
urgent need to better understand and interpret the intricate inner workings of
language models and how they generate predictive outputs. Motivated by this
gap, this paper investigates local explainability and mechanistic
interpretability within Transformer-based large language models to foster trust
in such models. In this regard, our paper aims to make three key contributions.
First, we present a review of local explainability and mechanistic
interpretability approaches and insights from relevant studies in the
literature. Furthermore, we describe experimental studies on explainability and
reasoning with large language models in two critical domains -- healthcare and
autonomous driving -- and analyze the trust implications of such explanations
for explanation receivers. Finally, we summarize current unaddressed issues in
the evolving landscape of LLM explainability and outline the opportunities,
critical challenges, and future directions toward generating human-aligned,
trustworthy LLM explanations.

</details>


### [249] [TaxoAlign: Scholarly Taxonomy Generation Using Language Models](https://arxiv.org/abs/2510.17263)
*Avishek Lahiri,Yufang Hou,Debarshi Kumar Sanyal*

Main category: cs.CL

TL;DR: 提出TaxoAlign方法和CS-TaxoBench基准，用于自动生成学术分类体系，评估显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动综述生成方法缺乏与人类专家构建的分类结构的比较，缺乏有效的评估基准和方法来衡量生成分类的结构对齐和语义一致性。

Method: 提出TaxoAlign，一种基于主题的三阶段指令引导的分类生成方法，并构建CS-TaxoBench基准（包含460个从人工综述中提取的分类，另加80个会议综述分类），同时设计了严格的自动化评估框架。

Result: 在CS-TaxoBench上实验表明，TaxoAlign在自动指标和人工评估中均显著优于基线方法，尤其在结构对齐和语义连贯性方面表现突出。

Conclusion: TaxoAlign能有效生成与人类专家相似的学术分类体系，CS-TaxoBench为未来研究提供了有价值的评估基准。

Abstract: Taxonomies play a crucial role in helping researchers structure and navigate
knowledge in a hierarchical manner. They also form an important part in the
creation of comprehensive literature surveys. The existing approaches to
automatic survey generation do not compare the structure of the generated
surveys with those written by human experts. To address this gap, we present
our own method for automated taxonomy creation that can bridge the gap between
human-generated and automatically-created taxonomies. For this purpose, we
create the CS-TaxoBench benchmark which consists of 460 taxonomies that have
been extracted from human-written survey papers. We also include an additional
test set of 80 taxonomies curated from conference survey papers. We propose
TaxoAlign, a three-phase topic-based instruction-guided method for scholarly
taxonomy generation. Additionally, we propose a stringent automated evaluation
framework that measures the structural alignment and semantic coherence of
automatically generated taxonomies in comparison to those created by human
experts. We evaluate our method and various baselines on CS-TaxoBench, using
both automated evaluation metrics and human evaluation studies. The results
show that TaxoAlign consistently surpasses the baselines on nearly all metrics.
The code and data can be found at https://github.com/AvishekLahiri/TaxoAlign.

</details>


### [250] [Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning](https://arxiv.org/abs/2510.17289)
*Hajar Bakarou,Mohamed Sinane El Messoussi,Anaïs Ollagnier*

Main category: cs.CL

TL;DR: 本研究利用法语开源数据集CyberAgressionAdo-Large，评估了多种基于文本和图的表示学习方法在多参与者对话场景中检测网络攻击性行为的表现，发现多模态模型优于单模态模型，尤其是mBERT + WD-SGCN模型在滥用检测等方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于X和Reddit等平台，对多参与者对话场景中的反社会行为研究不足，且缺乏合适的数据支持。本文旨在填补这一空白，深入分析复杂对话环境下的攻击性行为。

Method: 采用CyberAgressionAdo-Large数据集，针对滥用检测、欺凌行为分析和欺凌同伙识别三项任务，系统评估六种基于文本和八种基于图的表示学习方法，并融合词汇特征与互动动态进行多模态建模。

Result: 多模态模型表现优于单模态模型，其中mBERT + WD-SGCN在滥用检测（F1=0.718）、欺凌同伙识别（F1=0.286）和欺凌行为分析（F1=0.606）中表现最优，尤其擅长处理隐性攻击、角色转换等复杂现象。

Conclusion: 融合语言特征与社交互动结构的多模态方法能更有效识别多参与者对话中的反社会行为，为构建安全的社交媒体环境提供了可行的技术路径。

Abstract: Antisocial behavior (ASB) on social media -- including hate speech,
harassment, and cyberbullying -- poses growing risks to platform safety and
societal well-being. Prior research has focused largely on networks such as X
and Reddit, while \textit{multi-party conversational settings} remain
underexplored due to limited data. To address this gap, we use
\textit{CyberAgressionAdo-Large}, a French open-access dataset simulating ASB
in multi-party conversations, and evaluate three tasks: \textit{abuse
detection}, \textit{bullying behavior analysis}, and \textit{bullying
peer-group identification}. We benchmark six text-based and eight graph-based
\textit{representation-learning methods}, analyzing lexical cues, interactional
dynamics, and their multimodal fusion. Results show that multimodal models
outperform unimodal baselines. The late fusion model \texttt{mBERT + WD-SGCN}
achieves the best overall results, with top performance on abuse detection
(0.718) and competitive scores on peer-group identification (0.286) and
bullying analysis (0.606). Error analysis highlights its effectiveness in
handling nuanced ASB phenomena such as implicit aggression, role transitions,
and context-dependent hostility.

</details>


### [251] [Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation](https://arxiv.org/abs/2510.17354)
*Chenghao Zhang,Guanting Dong,Xinyu Yang,Zhicheng Dou*

Main category: cs.CL

TL;DR: 提出Nyx，一种用于通用检索增强生成（URAG）的统一多模态检索器，通过构建多模态数据集NyxQA和两阶段训练框架，在文本和视觉语言任务中均取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统主要处理单模态文本，难以应对现实世界中查询和文档包含混合模态（如图文）的场景，需提升多模态信息检索与生成能力。

Method: 提出Nyx，一个端到端的混合模态到混合模态检索器；构建四阶段自动化Pipeline生成高质量多模态问答数据集NyxQA；采用两阶段训练：先在NyxQA和开源检索数据上预训练，再基于下游视觉语言模型的反馈进行监督微调。

Result: Nyx在纯文本RAG基准上表现优异，且在更通用的URAG场景中显著提升视觉语言生成质量，验证了其在多模态任务中的有效性和泛化能力。

Conclusion: Nyx为通用检索增强生成提供了有效解决方案，推动了多模态RAG的发展，具备处理真实世界复杂混合模态信息的能力。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for
enhancing large language models (LLMs) by retrieving relevant documents from an
external corpus. However, existing RAG systems primarily focus on unimodal text
documents, and often fall short in real-world scenarios where both queries and
documents may contain mixed modalities (such as text and images). In this
paper, we address the challenge of Universal Retrieval-Augmented Generation
(URAG), which involves retrieving and reasoning over mixed-modal information to
improve vision-language generation. To this end, we propose Nyx, a unified
mixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate
the scarcity of realistic mixed-modal data, we introduce a four-stage automated
pipeline for generation and filtering, leveraging web documents to construct
NyxQA, a dataset comprising diverse mixed-modal question-answer pairs that
better reflect real-world information needs. Building on this high-quality
dataset, we adopt a two-stage training framework for Nyx: we first perform
pre-training on NyxQA along with a variety of open-source retrieval datasets,
followed by supervised fine-tuning using feedback from downstream
vision-language models (VLMs) to align retrieval outputs with generative
preferences. Experimental results demonstrate that Nyx not only performs
competitively on standard text-only RAG benchmarks, but also excels in the more
general and realistic URAG setting, significantly improving generation quality
in vision-language tasks.

</details>


### [252] [The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple, Self-Contained Directives](https://arxiv.org/abs/2510.17388)
*Henry Lim,Kwan Hui Lim*

Main category: cs.CL

TL;DR: 研究发现，尽管指令调优的大语言模型（IT-LLMs）在复杂任务中表现良好，但在执行简单、独立的指令时仍存在显著的格式偏差和指令遵循不一致问题，尤其在选项标签格式变化时性能大幅波动，揭示当前训练和评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 探究IT-LLMs在基础、原子级指令遵循能力上的表现，尤其是在不同选项标签格式下的稳定性，以揭示当前模型在真正理解并执行指令方面的潜在缺陷。

Method: 在修改后的MMLU和MMLU-Pro基准上评估20个IT-LLMs，系统性地改变选项标签格式（字母、数字、罗马数字），并在四种范式下分析模型表现：有无指令、是否移除选项内容、是否提供示例。

Result: 标签格式变化导致性能显著下降（如罗马数字比数字低30.45%）；无指令时性能进一步下降；移除选项内容后模型仅在数字标签下表现优于随机猜测；三步示例未能提升鲁棒性；大模型准确率更高但一致性仍不足。

Conclusion: 当前IT-LLMs的指令遵循能力存在严重缺陷，尤其对非数字格式敏感，表明现有指令调优范式不足以确保原子级指令的理解与执行，亟需改进评估方法与训练策略。

Abstract: Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot
reasoning, yet their ability to execute simple, self-contained instructions
remains underexplored, despite this being foundational to complex
instruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro
benchmarks, by systematically varying the format of option labels (alphabetic,
numeric, Roman) while keeping their meaning identical under four paradigms,
namely: (1) With explicit instructions, label changes cause large performance
shifts (e.g., -30.45\% for Roman vs. numeric), revealing instruction-format
bias. (2) Without instructions, performance drops further (up to -10.84\%) and
label sensitivity intensifies, underscoring the role of explicit guidance. (3)
When option contents are removed, models fail random-choice baselines except
with numeric labels, suggesting weak adherence to atomic directives. (4)
Three-shot exemplars yield no significant gains in robustness or fidelity, and
generation analyses show persistent label errors, especially for non-numeric
formats. Across model sizes, larger LLMs achieve higher accuracy but remain
inconsistent in instruction adherence. These results expose the insufficiencies
of current instruction-tuning paradigms and highlight the need for evaluation
methods and training strategies that explicitly target atomic
instruction-following.

</details>


### [253] [EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level Adaptability in LLMs](https://arxiv.org/abs/2510.17389)
*Numaan Naeem,Abdellah El Mekki,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 提出EduAdapt，首个用于评估大语言模型在K-12教育中年级适应性的基准，包含近4.8万条标注年级的科学问答数据。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在学术基准上表现良好，但难以根据学生年级调整回答难度，缺乏适合低年级学生的表达，限制了其在基础教育中的应用。

Method: 构建EduAdapt基准，包含9个科学科目、1-12年级的近4.8万条问答对，分为四个年级段；评估多种开源大模型在该基准上的表现。

Result: 较大模型整体表现更好，但在应对1-5年级学生时仍表现不佳，显示当前模型在低年级适应性上存在明显短板。

Conclusion: EduAdapt为评估和改进语言模型的年级适应性提供了首个系统框架，有助于推动更符合认知发展阶段的教育AI发展。

Abstract: Large language models (LLMs) are transforming education by answering
questions, explaining complex concepts, and generating content across a wide
range of subjects. Despite strong performance on academic benchmarks, they
often fail to tailor responses to students' grade levels. This is a critical
need in K-12 education, where age-appropriate vocabulary and explanation are
essential for effective learning. Existing models frequently produce outputs
that are too advanced or vague for younger learners, and there are no
standardized benchmarks to evaluate their ability to adjust across cognitive
and developmental stages. To address this gap, we introduce EduAdapt, a
benchmark of nearly 48k grade-labeled QA pairs across nine science subjects,
spanning Grades 1-12 and grouped into four grade levels. We evaluate a diverse
set of open-source LLMs on EduAdapt and find that while larger models generally
perform better, they still struggle with generating suitable responses for
early-grade students (Grades 1-5). Our work presents the first dataset and
evaluation framework for assessing grade-level adaptability in LLMs, aiming to
foster more developmentally aligned educational AI systems through better
training and prompting strategies. EduAdapt code and datasets are publicly
available at https://github.com/NaumanNaeem/EduAdapt.

</details>


### [254] [Leveraging Group Relative Policy Optimization to Advance Large Language Models in Traditional Chinese Medicine](https://arxiv.org/abs/2510.17402)
*Jiacheng Xie,Shuai Zeng,Yang Yu,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: 本文提出了Ladder-base，首个基于强化学习方法GRPO训练的面向中医领域的大型语言模型，相较于通用及领域内其他模型在推理和事实一致性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 中医知识体系独特且复杂，现有大模型在对齐、数据质量和评估一致性方面存在不足；需要更有效的训练方法提升领域内推理能力。

Method: 基于Qwen2.5-7B-Instruct模型，采用Group Relative Policy Optimization（GRPO）方法，在TCM-Ladder文本子集上进行训练，通过组内比较优化响应选择。

Result: Ladder-base在多项推理指标上优于GPT-4、Gemini、Claude 3、Qwen3以及BenTsao、HuatuoGPT2、Zhongjing等中医模型。

Conclusion: GRPO是一种有效且高效的策略，能提升大模型在传统医学领域的专家级推理对齐能力，推动可信、临床扎实的中医AI系统发展。

Abstract: Traditional Chinese Medicine (TCM) presents a rich and structurally unique
knowledge system that challenges conventional applications of large language
models (LLMs). Although previous TCM-specific LLMs have shown progress through
supervised fine-tuning, they often face limitations in alignment, data quality,
and evaluation consistency. In this study, we introduce Ladder-base, the first
TCM-focused LLM trained with Group Relative Policy Optimization (GRPO), a
reinforcement learning method that improves reasoning and factual consistency
by optimizing response selection based on intra-group comparisons. Ladder-base
is built upon the Qwen2.5-7B-Instruct foundation model and trained exclusively
on the textual subset of the TCM-Ladder benchmark, using 80 percent of the data
for training and the remaining 20 percent split evenly between validation and
test sets. Through standardized evaluation, Ladder-base demonstrates superior
performance across multiple reasoning metrics when compared to both
state-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, and
Qwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, and
Zhongjing. These findings suggest that GRPO provides an effective and efficient
strategy for aligning LLMs with expert-level reasoning in traditional medical
domains and supports the development of trustworthy and clinically grounded TCM
artificial intelligence systems.

</details>


### [255] [AFRICAPTION: Establishing a New Paradigm for Image Captioning in African Languages](https://arxiv.org/abs/2510.17405)
*Mardiyyah Oduwole,Prince Mireku,Fatimo Adebanjo,Oluwatosin Olajide,Mahi Aminu Aliyu,Jekaterina Novikova*

Main category: cs.CL

TL;DR: 提出AfriCaption框架，支持20种非洲语言的多语言图像描述生成，推动低资源语言的多模态AI发展。


<details>
  <summary>Details</summary>
Motivation: 多模态AI研究主要集中于高资源语言，导致低资源语言（尤其是非洲语言）发展受限，本文旨在促进多模态AI的包容性与公平性。

Method: 构建基于Flickr8k的数据集，采用上下文感知的选择与翻译流程生成语义对齐的标注；设计动态且保留上下文的处理流水线，结合模型集成与自适应替换；提出AfriCaption模型（0.5B参数），集成SigLIP与NLLB200实现视觉到文本的生成。

Result: 建立了首个面向20种非洲语言的大规模、可扩展的图像描述数据集与模型框架，确保了持续的数据质量与生成效果。

Conclusion: AfriCaption为低资源非洲语言提供了可扩展的多语言图像描述解决方案，为真正包容的多模态AI奠定了基础。

Abstract: Multimodal AI research has overwhelmingly focused on high-resource languages,
hindering the democratization of advancements in the field. To address this, we
present AfriCaption, a comprehensive framework for multilingual image
captioning in 20 African languages and our contributions are threefold: (i) a
curated dataset built on Flickr8k, featuring semantically aligned captions
generated via a context-aware selection and translation process; (ii) a
dynamic, context-preserving pipeline that ensures ongoing quality through model
ensembling and adaptive substitution; and (iii) the AfriCaption model, a 0.5B
parameter vision-to-text architecture that integrates SigLIP and NLLB200 for
caption generation across under-represented languages. This unified framework
ensures ongoing data quality and establishes the first scalable
image-captioning resource for under-represented African languages, laying the
groundwork for truly inclusive multimodal AI.

</details>


### [256] [BenCao: An Instruction-Tuned Large Language Model for Traditional Chinese Medicine](https://arxiv.org/abs/2510.17415)
*Jiacheng Xie,Yang Yu,Yibo Chen,Hanyao Zhang,Lening Zhao,Jiaxuan He,Lei Jiang,Xiaoting Tang,Guanghui An,Dong Xu*

Main category: cs.CL

TL;DR: 本研究开发了BenCao，一个基于ChatGPT的多模态中医助手，通过自然语言指令微调和多模态整合，在诊断、草药识别和体质分类等任务中优于现有模型，并已在全球部署应用。


<details>
  <summary>Details</summary>
Motivation: 中医具有悠久历史，但其整体思维、隐性逻辑和多模态诊断特点使大语言模型的应用面临挑战；现有中医大模型缺乏多模态融合、可解释性和临床适用性。

Method: 基于ChatGPT构建BenCao系统，集成超1000部经典与现代文献知识库，采用自然语言指令调优而非参数重训练，结合场景化指令框架、思维链模拟机制、专家反馈优化流程，并接入舌象图像分类和多模态数据库的外部API。

Result: 在单选题基准和多模态分类任务中，BenCao在诊断、草药识别和体质分类方面表现优于通用和中医领域模型；系统已上线OpenAI GPTs商店，截至2025年10月获近1000名全球用户使用。

Conclusion: 通过自然语言指令调优和多模态整合，可有效构建符合中医推理模式的领域大模型，为生成式AI在传统医学中的实际应用提供了可扩展的部署路径。

Abstract: Traditional Chinese Medicine (TCM), with a history spanning over two
millennia, plays a role in global healthcare. However, applying large language
models (LLMs) to TCM remains challenging due to its reliance on holistic
reasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain
LLMs have made progress in text-based understanding but lack multimodal
integration, interpretability, and clinical applicability. To address these
limitations, we developed BenCao, a ChatGPT-based multimodal assistant for TCM,
integrating structured knowledge bases, diagnostic data, and expert feedback
refinement. BenCao was trained through natural language instruction tuning
rather than parameter retraining, aligning with expert-level reasoning and
ethical norms specific to TCM. The system incorporates a comprehensive
knowledge base of over 1,000 classical and modern texts, a scenario-based
instruction framework for diverse interactions, a chain-of-thought simulation
mechanism for interpretable reasoning, and a feedback refinement process
involving licensed TCM practitioners. BenCao connects to external APIs for
tongue-image classification and multimodal database retrieval, enabling dynamic
access to diagnostic resources. In evaluations across single-choice question
benchmarks and multimodal classification tasks, BenCao achieved superior
accuracy to general-domain and TCM-domain models, particularly in diagnostics,
herb recognition, and constitution classification. The model was deployed as an
interactive application on the OpenAI GPTs Store, accessed by nearly 1,000
users globally as of October 2025. This study demonstrates the feasibility of
developing a TCM-domain LLM through natural language-based instruction tuning
and multimodal integration, offering a practical framework for aligning
generative AI with traditional medical reasoning and a scalable pathway for
real-world deployment.

</details>


### [257] [Navigating the Alignment-Calibration Trade-off: A Pareto-Superior Frontier via Model Merging](https://arxiv.org/abs/2510.17426)
*Tiancheng Hu,Benjamin Minixhofer,Nigel Collier*

Main category: cs.CL

TL;DR: 通过在对齐前后模型权重之间插值，可有效缓解对齐税，提升模型准确性和校准性，且无需牺牲任一性能。


<details>
  <summary>Details</summary>
Motivation: 对齐模型常导致任务准确率下降和校准性变差，本文旨在全面理解对齐税的影响，并寻找有效缓解方法。

Method: 在对齐前后的模型权重之间进行简单的插值融合，作为一种无需再训练的后处理手段。

Result: 该方法能一致地发现帕累托最优的插值点，在提升准确率的同时显著恢复校准性，输出也更趋多样化。

Conclusion: 简单的模型融合是一种高效、低成本的方法，可有效缓解对齐带来的多方面性能损失，获得更强大且更可靠的模型。

Abstract: The "alignment tax" of post-training is typically framed as a drop in task
accuracy. We show it also involves a severe loss of calibration, making models
overconfident, less reliable, and model outputs less diverse. We show that this
trade-off can be navigated effectively via a simple post-hoc intervention:
interpolating between a model's weights before and after alignment. Crucially,
this is not a strict trade-off. We find that the process consistently reveals
Pareto-optimal interpolations - models that improve accuracy beyond both
parents while substantially recovering the calibration lost during alignment.
Our work demonstrates that simple model merging provides a computationally
efficient method for mitigating the full scope of the alignment tax, yielding
models that are more capable and more reliable.

</details>


### [258] [Agentic Reinforcement Learning for Search is Unsafe](https://arxiv.org/abs/2510.17431)
*Yushi Yang,Shreyansh Padarha,Andrew Lee,Adam Mahdi*

Main category: cs.CL

TL;DR: RL训练的搜索模型虽然能继承指令调优中的拒绝机制，但其安全性脆弱，容易受到简单攻击，导致生成有害查询和回答，暴露出当前代理式强化学习在安全设计上的不足。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示经过强化学习训练的大型语言模型在自主搜索过程中的安全隐患，尤其是其拒绝有害请求的能力是否可靠。

Method: 通过对Qwen和Llama两个模型家族进行实验，设计两种攻击方式：强制模型以搜索开头（Search攻击）和鼓励重复搜索（Multi-search攻击），评估其对拒绝率、回答安全性和搜索查询安全性的影, 

Result: 两种攻击使拒绝率下降高达60.0%，回答安全性下降82.5%，搜索查询安全性下降82.4%。攻击成功的原因是模型在触发拒绝机制前已生成有害的镜像查询。

Conclusion: 当前的代理式强化学习训练过度奖励有效查询生成，忽视其潜在危害，导致模型存在可被轻易利用的安全漏洞，亟需构建安全感知的训练 pipeline。

Abstract: Agentic reinforcement learning (RL) trains large language models to
autonomously call tools during reasoning, with search as the most common
application. These models excel at multi-step reasoning tasks, but their safety
properties are not well understood. In this study, we show that RL-trained
search models inherit refusal from instruction tuning and often deflect harmful
requests by turning them into safe queries. However, this safety is fragile.
Two simple attacks, one that forces the model to begin response with search
(Search attack), another that encourages models to repeatedly search
(Multi-search attack), trigger cascades of harmful searches and answers. Across
two model families (Qwen, Llama) with both local and web search, these attacks
lower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query
safety by 82.4%. The attacks succeed by triggering models to generate harmful,
request-mirroring search queries before they can generate the inherited refusal
tokens. This exposes a core weakness of current RL training: it rewards
continued generation of effective queries without accounting for their
harmfulness. As a result, RL search models have vulnerabilities that users can
easily exploit, making it urgent to develop safety-aware agentic RL pipelines
optimising for safe search.

</details>


### [259] [Multilingual Clinical NER for Diseases and Medications Recognition in Cardiology Texts using BERT Embeddings](https://arxiv.org/abs/2510.17437)
*Manuela Daniela Danu,George Marica,Constantin Suciu,Lucian Mihai Itu,Oladimeji Farri*

Main category: cs.CL

TL;DR: 本研究开发了多种深度上下文嵌入模型，用于提升英语、西班牙语和意大利语临床文本中的疾病和药物命名实体识别（NER）性能，尤其在低资源语言环境下表现优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）中非结构化临床文本蕴含大量生物医学知识，但现有研究多集中于英语，对低资源语言（如西班牙语和意大利语）关注不足，亟需有效方法从中提取关键临床信息。

Method: 采用多种基于BERT的单语言和多语言上下文嵌入模型，在BioASQ MultiCardioNER共享任务框架下，从英、西、意三种语言的心脏病学临床病例报告中识别疾病和药物提及。

Result: 模型在西班牙语疾病识别（SDR）上达到77.88%的F1分数，西班牙语药物识别（SMR）达92.09%，英语药物识别（EMR）达91.74%，意大利语药物识别（IMR）达88.9%，在所有子任务上均优于测试排行榜的平均值和中位数。

Conclusion: 所提出的上下文嵌入模型在多语言临床NER任务中表现优异，尤其在低资源语言中显著提升识别性能，有助于推动数据驱动的多语言临床决策系统发展。

Abstract: The rapidly increasing volume of electronic health record (EHR) data
underscores a pressing need to unlock biomedical knowledge from unstructured
clinical texts to support advancements in data-driven clinical systems,
including patient diagnosis, disease progression monitoring, treatment effects
assessment, prediction of future clinical events, etc. While contextualized
language models have demonstrated impressive performance improvements for named
entity recognition (NER) systems in English corpora, there remains a scarcity
of research focused on clinical texts in low-resource languages. To bridge this
gap, our study aims to develop multiple deep contextual embedding models to
enhance clinical NER in the cardiology domain, as part of the BioASQ
MultiCardioNER shared task. We explore the effectiveness of different
monolingual and multilingual BERT-based models, trained on general domain text,
for extracting disease and medication mentions from clinical case reports
written in English, Spanish, and Italian. We achieved an F1-score of 77.88% on
Spanish Diseases Recognition (SDR), 92.09% on Spanish Medications Recognition
(SMR), 91.74% on English Medications Recognition (EMR), and 88.9% on Italian
Medications Recognition (IMR). These results outperform the mean and median F1
scores in the test leaderboard across all subtasks, with the mean/median values
being: 69.61%/75.66% for SDR, 81.22%/90.18% for SMR, 89.2%/88.96% for EMR, and
82.8%/87.76% for IMR.

</details>


### [260] [Evaluating Large Language Models on Urdu Idiom Translation](https://arxiv.org/abs/2510.17460)
*Muhammad Farmal Khan,Mousumi Akter*

Main category: cs.CL

TL;DR: 本文介绍了首个用于乌尔都语到英语 idiomatic 翻译的评估数据集，涵盖 Native Urdu 和 Roman Urdu 脚本，并评估了多种开源大模型和神经机器翻译系统在保留成语和文化含义方面的能力。


<details>
  <summary>Details</summary>
Motivation: idiomatic 翻译在低资源语言如乌尔都语中仍是一个重大挑战，且此前研究关注较少，因此需要专门的数据集和系统评估来推动该领域的发展。

Method: 构建了覆盖 Native 和 Roman Urdu 脚本的标注数据集，采用多个开源大语言模型和 NMT 系统进行翻译实验，使用 BLEU、BERTScore、COMET 和 XCOMET 等自动指标评估，并比较不同提示工程和脚本形式的影响。

Result: 实验表明，提示工程比直接翻译更有利于 idiomatic 翻译，但不同提示类型之间差异较小；文本表示对翻译质量有显著影响，Native Urdu 输入比 Roman Urdu 产生更准确的 idiomatic 翻译。

Conclusion: 文本脚本形式和提示策略对乌尔都语 idiomatic 翻译有重要影响，未来研究应关注跨脚本表示和更有效的提示方法以提升低资源语言的成语翻译质量。

Abstract: Idiomatic translation remains a significant challenge in machine translation,
especially for low resource languages such as Urdu, and has received limited
prior attention. To advance research in this area, we introduce the first
evaluation datasets for Urdu to English idiomatic translation, covering both
Native Urdu and Roman Urdu scripts and annotated with gold-standard English
equivalents. We evaluate multiple open-source Large Language Models (LLMs) and
Neural Machine Translation (NMT) systems on this task, focusing on their
ability to preserve idiomatic and cultural meaning. Automatic metrics including
BLEU, BERTScore, COMET, and XCOMET are used to assess translation quality. Our
findings indicate that prompt engineering enhances idiomatic translation
compared to direct translation, though performance differences among prompt
types are relatively minor. Moreover, cross script comparisons reveal that text
representation substantially affects translation quality, with Native Urdu
inputs producing more accurate idiomatic translations than Roman Urdu.

</details>


### [261] [Disparities in Multilingual LLM-Based Healthcare Q&A](https://arxiv.org/abs/2510.17476)
*Ipek Baris Schlicht,Burcu Sayin,Zhixue Zhao,Frederik M. Labonté,Cesare Barbera,Marco Viviani,Paolo Rosso,Lucie Flek*

Main category: cs.CL

TL;DR: 研究揭示了多语言医疗问答中维基百科覆盖和大语言模型（LLM）事实一致性存在显著跨语言差异，使用非英语上下文信息可通过检索增强生成（RAG）提升事实对齐，促进更公平的多语言AI医疗系统。


<details>
  <summary>Details</summary>
Motivation: 确保AI在医疗领域的公平应用需要可靠且均衡的多语言健康信息，但当前信息质量和语言间一致性存在差异，可能加剧医疗信息不平等。

Method: 构建多语言数据集MultiWikiHealthCare，分析五种语言（英、德、土、中、意）维基百科的医疗覆盖差异，并评估LLM回答与原始资料的事实对齐情况，通过上下文检索增强生成（RAG）进行案例研究。

Result: 发现LLM的回答更倾向于与英文维基百科对齐，即使提问语言为非英语；非英语语种的维基医疗内容覆盖较弱；引入非英语上下文可有效提升LLM对本地化事实的对齐。

Conclusion: 跨语言医疗信息不平等影响LLM的可靠性，通过整合本地语言知识源和RAG技术，可改善多语言AI系统的公平性与事实准确性。

Abstract: Equitable access to reliable health information is vital when integrating AI
into healthcare. Yet, information quality varies across languages, raising
concerns about the reliability and consistency of multilingual Large Language
Models (LLMs). We systematically examine cross-lingual disparities in
pre-training source and factuality alignment in LLM answers for multilingual
healthcare Q&A across English, German, Turkish, Chinese (Mandarin), and
Italian. We (i) constructed Multilingual Wiki Health Care
(MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed
cross-lingual healthcare coverage; (iii) assessed LLM response alignment with
these references; and (iv) conducted a case study on factual alignment through
the use of contextual information and Retrieval-Augmented Generation (RAG). Our
findings reveal substantial cross-lingual disparities in both Wikipedia
coverage and LLM factual alignment. Across LLMs, responses align more with
English Wikipedia, even when the prompts are non-English. Providing contextual
excerpts from non-English Wikipedia at inference time effectively shifts
factual alignment toward culturally relevant knowledge. These results highlight
practical pathways for building more equitable, multilingual AI systems for
healthcare.

</details>


### [262] [ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts](https://arxiv.org/abs/2510.17483)
*Zheyue Tan,Zhiyuan Li,Tao Yuan,Dong Zhou,Weilin Liu,Yueqing Zhuang,Yadong Li,Guowei Niu,Cheng Qin,Zhuyu Yao,Congyi Liu,Haiyang Xu,Boxun Li,Guohao Dai,Bo Zhao,Yu Wang*

Main category: cs.CL

TL;DR: ReXMoE是一种新的Mixture-of-Experts（MoE）架构，通过跨层重用专家并采用渐进扩展路由策略，突破了传统层局部路由的限制，在不增加参数的情况下提升了模型表达能力和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的MoE架构受限于每层仅能访问本地专家池，导致专家维度与路由多样性之间存在权衡，限制了模型的组合灵活性和表达能力。

Method: 提出ReXMoE架构，允许路由器在相邻层之间重用专家，解耦专家维度与每层预算；同时引入渐进扩展路由（PSR）策略，在训练过程中逐步扩大候选专家池。

Result: 在0.5B到7B参数范围内多种架构上进行的实验证明，ReXMoE在语言建模和下游任务上均优于传统MoE方法，且在固定架构维度下性能一致提升。

Conclusion: ReXMoE打破了层局部路由的局限，为构建参数高效且可扩展的MoE型大语言模型提供了一种新的设计范式。

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a promising approach
to scale Large Language Models (LLMs). MoE boosts the efficiency by activating
a subset of experts per token. Recent works show that fine-grained experts
substantially enriches the combinatorial flexibility of active experts and
enhances model expressiveness. However, such a design is fundamentally limited
by the layer-local routing mechanism: each layer is restricted to its own
expert pool. This requires a careful trade-off between expert dimensionality
and routing diversity given fixed parameter budgets. We describe ReXMoE, a
novel MoE architecture that improves routing beyond the existing layer-local
approaches by allowing routers to reuse experts across adjacent layers. ReXMoE
decouples expert dimensionality from per-layer budgets, enabling richer expert
combinations without sacrificing individual expert capacity or inflating
overall parameters. To this end, we propose a new progressive scaling routing
(PSR) strategy to gradually increase the candidate expert pool during training.
As a result, ReXMoE improves both language modeling and downstream task
performance. Extensive experiments on models ranging from 0.5B to 7B parameters
across different architectures demonstrate that ReXMoE consistently improves
performance under fixed architectural dimensions, confirming ReXMoE as new
design paradigm for parameter-efficient and scalable MoE-based LLMs.

</details>


### [263] [DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning](https://arxiv.org/abs/2510.17489)
*Yongxin He,Shan Zhang,Yixuan Cao,Lei Ma,Ping Luo*

Main category: cs.CL

TL;DR: 提出了一种新方法DETree，通过构建分层相似性树结构来建模不同人机协作文本生成过程的关系，并设计专用损失函数，在混合文本检测任务中提升了性能、鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI文本检测方法大多局限于二分类或简单多分类，难以应对复杂的人机协作生成过程（如AI撰写后人工编辑、人类撰写后AI修改等），需要更精细的建模方式以提升检测准确性与泛化能力。

Method: 提出DETree方法，将不同生成过程间的关联建模为分层相似性树结构（Hierarchical Affinity Tree），并设计相应的损失函数使文本表示对齐该树结构；同时构建RealBench基准数据集，自动构造涵盖多种人机协作过程的混合文本用于训练与评估。

Result: DETree在混合文本检测任务中表现优于现有方法，尤其在分布外（OOD）场景和少样本学习条件下展现出更强的鲁棒性和泛化能力。

Conclusion: 通过建模生成过程间的层次关系并引入结构化损失函数，DETree有效提升了对复杂AI参与文本的检测能力，证明了基于训练的方法在OOD场景下的潜力。

Abstract: Detecting AI-involved text is essential for combating misinformation,
plagiarism, and academic misconduct. However, AI text generation includes
diverse collaborative processes (AI-written text edited by humans,
human-written text edited by AI, and AI-generated text refined by other AI),
where various or even new LLMs could be involved. Texts generated through these
varied processes exhibit complex characteristics, presenting significant
challenges for detection. Current methods model these processes rather crudely,
primarily employing binary classification (purely human vs. AI-involved) or
multi-classification (treating human-AI collaboration as a new class). We
observe that representations of texts generated through different processes
exhibit inherent clustering relationships. Therefore, we propose DETree, a
novel approach that models the relationships among different processes as a
Hierarchical Affinity Tree structure, and introduces a specialized loss
function that aligns text representations with this tree. To facilitate this
learning, we developed RealBench, a comprehensive benchmark dataset that
automatically incorporates a wide spectrum of hybrid texts produced through
various human-AI collaboration processes. Our method improves performance in
hybrid text detection tasks and significantly enhances robustness and
generalization in out-of-distribution scenarios, particularly in few-shot
learning conditions, further demonstrating the promise of training-based
approaches in OOD settings. Our code and dataset are available at
https://github.com/heyongxin233/DETree.

</details>


### [264] [Empowering Real-World: A Survey on the Technology, Practice, and Evaluation of LLM-driven Industry Agents](https://arxiv.org/abs/2510.17491)
*Yihong Tang,Kehai Chen,Liang Yue,Jinxin Fan,Caishen Zhou,Xiaoguang Li,Yuyang Zhang,Mingming Zhao,Shixiong Kai,Kaiyang Guo,Xingshan Zeng,Wenjing Cun,Lifeng Shang,Min Zhang*

Main category: cs.CL

TL;DR: 本文系统回顾了基于大语言模型的行业智能体的技术、应用及评估方法，提出行业智能体能力成熟度框架，梳理其从‘流程执行系统’到‘自适应社会系统’的演进路径，并探讨关键技术支柱、应用场景、评估挑战与治理问题，旨在为下一代行业智能体的发展提供理论基础与路线图。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型智能体在自主推理、规划和任务执行方面取得进展，但如何将其研究成果转化为推动产业变革的实际生产力仍面临挑战，因此需要系统性梳理行业智能体的发展现状与路径。

Method: 采用行业智能体能力成熟度框架，分析记忆、规划与工具使用三大技术支柱的演进，综述其在多个实际领域的应用，并回顾基础与专业化能力的评估基准与方法。

Result: 明确了行业智能体的技术发展脉络、应用范围与评估挑战，识别出现有评估体系在真实性、安全性与行业适配性方面的不足，并总结了实际部署中的能力边界与治理难题。

Conclusion: 结合技术演进与产业实践，本文为理解与构建下一代行业智能体提供了清晰的路线图与理论基础，强调需跨学科协作以实现从研究到产业落地的转化。

Abstract: With the rise of large language models (LLMs), LLM agents capable of
autonomous reasoning, planning, and executing complex tasks have become a
frontier in artificial intelligence. However, how to translate the research on
general agents into productivity that drives industry transformations remains a
significant challenge. To address this, this paper systematically reviews the
technologies, applications, and evaluation methods of industry agents based on
LLMs. Using an industry agent capability maturity framework, it outlines the
evolution of agents in industry applications, from "process execution systems"
to "adaptive social systems." First, we examine the three key technological
pillars that support the advancement of agent capabilities: Memory, Planning,
and Tool Use. We discuss how these technologies evolve from supporting simple
tasks in their early forms to enabling complex autonomous systems and
collective intelligence in more advanced forms. Then, we provide an overview of
the application of industry agents in real-world domains such as digital
engineering, scientific discovery, embodied intelligence, collaborative
business execution, and complex system simulation. Additionally, this paper
reviews the evaluation benchmarks and methods for both fundamental and
specialized capabilities, identifying the challenges existing evaluation
systems face regarding authenticity, safety, and industry specificity. Finally,
we focus on the practical challenges faced by industry agents, exploring their
capability boundaries, developmental potential, and governance issues in
various scenarios, while providing insights into future directions. By
combining technological evolution with industry practices, this review aims to
clarify the current state and offer a clear roadmap and theoretical foundation
for understanding and building the next generation of industry agents.

</details>


### [265] [Deep Self-Evolving Reasoning](https://arxiv.org/abs/2510.17498)
*Zihan Liu,Shun Zheng,Xumeng Wen,Yang Wang,Jiang Bian,Mao Yang*

Main category: cs.CL

TL;DR: 提出一种称为深度自我进化推理（DSER）的概率范式，即使在验证和修正能力较弱的情况下，也能显著扩展小型开源模型的推理极限。


<details>
  <summary>Details</summary>
Motivation: 现有验证-修正框架依赖强推理和修正能力，在小型开源模型上表现脆弱，难以解决复杂数学问题，因此需要一种不依赖强验证的新范式来提升其长链推理能力。

Method: 将迭代推理建模为马尔可夫链，每个步骤表示解空间中的随机转移；只要改进概率略高于退化概率，即可保证收敛；通过并行运行多个长程自我进化过程，放大微小正向趋势。

Result: 在AIME 2024-2025基准上，DSER使DeepSeek-R1-0528-Qwen3-8B模型解决了此前9个未解问题中的5个，整体性能提升，并通过多数投票超过其600B参数教师模型的单轮准确率。

Conclusion: DSER为弱验证模型提供有效的推理扩展框架，同时揭示当前开源推理模型在自验证、修正和稳定性上的根本缺陷，指明下一代自进化模型的研究方向。

Abstract: Long-form chain-of-thought reasoning has become a cornerstone of advanced
reasoning in large language models. While recent verification-refinement
frameworks have enabled proprietary models to solve Olympiad-level problems,
their effectiveness hinges on strong, reliable verification and correction
capabilities, which remain fragile in open-weight, smaller-scale models. This
work demonstrates that even with weak verification and refinement capabilities
on hard tasks, the reasoning limits of such models can be substantially
extended through a probabilistic paradigm we call Deep Self-Evolving Reasoning
(DSER). We conceptualize iterative reasoning as a Markov chain, where each step
represents a stochastic transition in the solution space. The key insight is
that convergence to a correct solution is guaranteed as long as the probability
of improvement marginally exceeds that of degradation. By running multiple
long-horizon, self-evolving processes in parallel, DSER amplifies these small
positive tendencies, enabling the model to asymptotically approach correct
answers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On
the challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously
unsolvable problems and boosts overall performance, enabling this compact model
to surpass the single-turn accuracy of its 600B-parameter teacher through
majority voting. Beyond its immediate utility for test-time scaling, the DSER
framework serves to diagnose the fundamental limitations of current open-weight
reasoners. By clearly delineating their shortcomings in self-verification,
refinement, and stability, our findings establish a clear research agenda for
developing next-generation models with powerful, intrinsic self-evolving
capabilities.

</details>


### [266] [Lingua Custodi's participation at the WMT 2025 Terminology shared task](https://arxiv.org/abs/2510.17504)
*Jingshu Liu,Raheel Qader,Gaëtan Caillaut,Mariam Nakhlé*

Main category: cs.CL

TL;DR: 本文提出了一种结合多种方法的多语言句子嵌入模型LaBSE，在112种语言上显著优于LASER，并大幅减少对平行语料的需求，同时在单语迁移任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 探索基于BERT的跨语言句子嵌入方法，解决现有模型在多语言语义相似性和低资源语言对中表现不足的问题。

Method: 结合掩码语言建模（MLM）、翻译语言建模（TLM）、双编码器翻译排序和加性边缘softmax，并利用预训练的多语言语言模型进行联合优化。

Result: 在Tatoeba数据集上达到83.7%的双语文本检索准确率（远高于LASER的65.5%），并将所需平行数据减少80%，同时在en-zh和en-de的NMT任务中表现出色。

Conclusion: 提出的LaBSE模型在多语言句子嵌入任务中显著提升性能，且具备高效的数据利用率和良好的迁移能力，已开源支持109种以上语言。

Abstract: While BERT is an effective method for learning monolingual sentence
embeddings for semantic similarity and embedding based transfer learning BERT
based cross-lingual sentence embeddings have yet to be explored. We
systematically investigate methods for learning multilingual sentence
embeddings by combining the best methods for learning monolingual and
cross-lingual representations including: masked language modeling (MLM),
translation language modeling (TLM), dual encoder translation ranking, and
additive margin softmax. We show that introducing a pre-trained multilingual
language model dramatically reduces the amount of parallel training data
required to achieve good performance by 80%. Composing the best of these
methods produces a model that achieves 83.7% bi-text retrieval accuracy over
112 languages on Tatoeba, well above the 65.5 achieved by LASER, while still
performing competitively on monolingual transfer learning benchmarks. Parallel
data mined from CommonCrawl using our best model is shown to train competitive
NMT models for en-zh and en-de. We publicly release our best multilingual
sentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.

</details>


### [267] [Annotation-Efficient Universal Honesty Alignment](https://arxiv.org/abs/2510.17509)
*Shiyu Ni,Keping Bi,Jiafeng Guo,Minghao Tang,Jingtong Wu,Zengxin Han,Xueqi Cheng*

Main category: cs.CL

TL;DR: 提出EliCal框架，通过先自我一致性监督提取内部置信度，再用少量正确性标注进行校准，实现在仅需1k标注（全监督的0.18%）下接近最优的诚实对齐效果。


<details>
  <summary>Details</summary>
Motivation: 实现大语言模型的诚实对齐（识别知识边界并合理表达置信度）需要大量标注数据，成本高昂，因此需要一种标注高效的训练方法。

Method: 提出两阶段框架EliCal：第一阶段利用廉价的自我一致性监督来提取模型的内部置信度；第二阶段使用少量正确性标注对置信度进行校准。同时发布包含56万训练和7万评估样本的HonestyBench基准。

Result: EliCal在仅使用1k正确性标注的情况下达到接近最优的对齐性能，在未见的MMLU任务上表现优于仅校准的基线方法。

Conclusion: EliCal为实现大语言模型的普遍诚实对齐提供了一种可扩展且标注高效的方法。

Abstract: Honesty alignment-the ability of large language models (LLMs) to recognize
their knowledge boundaries and express calibrated confidence-is essential for
trustworthy deployment. Existing methods either rely on training-free
confidence estimation (e.g., token probabilities, self-consistency) or
training-based calibration with correctness annotations. While effective,
achieving universal honesty alignment with training-based calibration requires
costly, large-scale labeling. To support annotation-efficient training, we
introduce Elicitation-Then-Calibration (EliCal), a two-stage framework that
first elicits internal confidence using inexpensive self-consistency
supervision, then calibrates this confidence with a small set of correctness
annotations. To support a large-scale study, we release HonestyBench, a
benchmark covering ten free-form QA datasets with 560k training and 70k
evaluation instances annotated with correctness and self-consistency signals.
Experiments show that EliCal achieves near-optimal alignment with only 1k
correctness annotations (0.18% of full supervision) and better alignment
performance on unseen MMLU tasks than the calibration-only baseline, offering a
scalable solution toward universal honesty alignment in LLMs.

</details>


### [268] [SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors](https://arxiv.org/abs/2510.17516)
*Tiancheng Hu,Joachim Baumann,Lorenzo Lupo,Dirk Hovy,Nigel Collier,Paul Röttger*

Main category: cs.CL

TL;DR: SimBench是首个大规模、标准化的基准，用于评估大语言模型（LLM）在模拟人类行为方面的能力，揭示当前LLM模拟能力有限，且存在人口群体差异与推理能力相关性等问题。


<details>
  <summary>Details</summary>
Motivation: 当前对LLM模拟人类行为的评估分散且不可比，缺乏统一标准，因此需要一个系统化基准来推动该领域的可靠科学进展。

Method: 构建SimBench，整合20个涵盖道德决策到经济选择等任务的多样化数据集，覆盖全球广泛参与者，评估不同LLM在模拟人类行为上的一致性和准确性。

Result: 当前最佳LLM的模拟得分为40.80/100，性能随模型规模对数线性增长；推理时计算资源增加不提升表现；指令微调改善低熵问题表现但损害高熵问题；模型在模拟特定人群时表现较差；模拟能力与深度知识推理能力（如MMLU-Pro）高度相关（r=0.939）。

Conclusion: SimBench为衡量LLM模拟人类行为提供了可重复、可比较的标准，揭示了影响模拟能力的关键因素，有助于推动更真实的人类行为模拟模型的发展。

Abstract: Large language model (LLM) simulations of human behavior have the potential
to revolutionize the social and behavioral sciences, if and only if they
faithfully reflect real human behaviors. Current evaluations are fragmented,
based on bespoke tasks and metrics, creating a patchwork of incomparable
results. To address this, we introduce SimBench, the first large-scale,
standardized benchmark for a robust, reproducible science of LLM simulation. By
unifying 20 diverse datasets covering tasks from moral decision-making to
economic choice across a large global participant pool, SimBench provides the
necessary foundation to ask fundamental questions about when, how, and why LLM
simulations succeed or fail. We show that, while even the best LLMs today have
limited simulation ability (score: 40.80/100), performance scales log-linearly
with model size. Simulation performance is not improved by increased
inference-time compute. We demonstrate an alignment-simulation trade-off:
instruction-tuning improves performance on low-entropy (consensus) questions
but degrades it on high-entropy (diverse) ones. Models particularly struggle
when simulating specific demographic groups. Finally, we demonstrate that
simulation ability correlates most strongly with deep, knowledge-intensive
reasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to
accelerate the development of more faithful LLM simulators.

</details>


### [269] [OncoReason: Structuring Clinical Reasoning in LLMs for Robust and Interpretable Survival Prediction](https://arxiv.org/abs/2510.17532)
*Raghu Vamshi Hemadri,Geetha Krishna Guruju,Kristi Topollai,Anna Ewa Choromanska*

Main category: cs.CL

TL;DR: 本文提出了一种统一的多任务学习框架，将自回归大语言模型与临床推理对齐，用于癌症治疗结果预测，在准确性与可解释性方面均取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 在癌症治疗预后预测中，模型不仅需要高准确性，还需具备可解释性，尤其面对异质性临床数据时。现有大语言模型虽在生物医学NLP任务中表现良好，但缺乏结构化推理能力，难以满足高风险临床决策支持的需求。

Method: 提出一个多任务学习框架，联合执行二分类生存预测、连续生存时间回归和自然语言推理生成任务；采用三种对齐策略：标准监督微调（SFT）、带思维链提示的SFT（CoT）和基于专家推理路径的强化学习方法GRPO；基于LLaMa3-8B和Med42-8B模型进行实验验证。

Result: CoT提示使F1分数提升6.0，MAE降低12%；GRPO在BLEU、ROUGE和BERTScore等指标上均达到最优，显著提升模型可解释性与预测性能；发现现有生物医学大模型常因架构限制无法生成有效推理路径。

Conclusion: 推理感知的对齐策略对多任务临床建模至关重要，GRPO等方法能有效提升模型的可解释性与可靠性，为精准肿瘤学中可信赖大模型的应用树立了新标杆。

Abstract: Predicting cancer treatment outcomes requires models that are both accurate
and interpretable, particularly in the presence of heterogeneous clinical data.
While large language models (LLMs) have shown strong performance in biomedical
NLP, they often lack structured reasoning capabilities critical for high-stakes
decision support. We present a unified, multi-task learning framework that
aligns autoregressive LLMs with clinical reasoning for outcome prediction on
the MSK-CHORD dataset. Our models are trained to jointly perform binary
survival classification, continuous survival time regression, and natural
language rationale generation. We evaluate three alignment strategies: (1)
standard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT)
prompting to elicit step-by-step reasoning, and (3) Group Relative Policy
Optimization (GRPO), a reinforcement learning method that aligns model outputs
to expert-derived reasoning trajectories. Experiments with LLaMa3-8B and
Med42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and
reduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and
predictive performance across BLEU, ROUGE, and BERTScore. We further show that
existing biomedical LLMs often fail to produce valid reasoning traces due to
architectural constraints. Our findings underscore the importance of
reasoning-aware alignment in multi-task clinical modeling and set a new
benchmark for interpretable, trustworthy LLMs in precision oncology.

</details>


### [270] [When Annotators Disagree, Topology Explains: Mapper, a Topological Tool for Exploring Text Embedding Geometry and Ambiguity](https://arxiv.org/abs/2510.17548)
*Nisrine Rair,Alban Goupil,Valeriu Vrabie,Emmanuel Chochoy*

Main category: cs.CL

TL;DR: 提出一种基于拓扑数据分析的方法（Mapper）来研究微调后的语言模型如何在嵌入空间中表示歧义性，发现模型在高歧义情况下仍形成高纯度预测的模块化结构，揭示了模型置信度与标签不确定性之间的张力。


<details>
  <summary>Details</summary>
Motivation: 传统的标量指标（如准确率）无法充分反映模型在人类标注不一致情况下的歧义表示能力，因此需要一种能揭示模型内部结构的新视角。

Method: 采用拓扑数据分析工具Mapper，分析RoBERTa-Large在MD-Offense数据集上微调后的嵌入空间结构，并与PCA、UMAP等传统降维方法对比。

Result: 超过98%的连通组件具有≥90%的预测纯度，但在歧义数据上与真实标签对齐度下降；Mapper能有效揭示决策区域、边界坍缩和过度自信簇，优于传统方法。

Conclusion: Mapper是一种强大的诊断工具，不仅能可视化模型对歧义的处理方式，还可导出拓扑度量，为处理主观NLP任务提供新的建模策略。

Abstract: Language models are often evaluated with scalar metrics like accuracy, but
such measures fail to capture how models internally represent ambiguity,
especially when human annotators disagree. We propose a topological perspective
to analyze how fine-tuned models encode ambiguity and more generally instances.
  Applied to RoBERTa-Large on the MD-Offense dataset, Mapper, a tool from
topological data analysis, reveals that fine-tuning restructures embedding
space into modular, non-convex regions aligned with model predictions, even for
highly ambiguous cases. Over $98\%$ of connected components exhibit $\geq 90\%$
prediction purity, yet alignment with ground-truth labels drops in ambiguous
data, surfacing a hidden tension between structural confidence and label
uncertainty.
  Unlike traditional tools such as PCA or UMAP, Mapper captures this geometry
directly uncovering decision regions, boundary collapses, and overconfident
clusters. Our findings position Mapper as a powerful diagnostic tool for
understanding how models resolve ambiguity. Beyond visualization, it also
enables topological metrics that may inform proactive modeling strategies in
subjective NLP tasks.

</details>


### [271] [Language Confusion Gate: Language-Aware Decoding Through Model Self-Distillation](https://arxiv.org/abs/2510.17555)
*Collin Zhang,Fei Huang,Chenhan Yuan,Junyang Lin*

Main category: cs.CL

TL;DR: 本文提出了一种轻量级插件式解决方案Language Confusion Gate（LCG），用于在解码过程中过滤多语言混合生成中的错误语言混淆，无需重新训练模型，显著降低语言混淆现象。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成文本时常常出现语言混淆问题，现有方法要么需要重新训练模型，要么无法区分有害的语言混淆和合理的语码转换，缺乏高效且灵活的解决方案。

Method: 提出Language Confusion Gate（LCG），在解码过程中基于输出token的嵌入范数差异和高资源语言的预测偏好，使用经过范数调整的自蒸馏方法训练门控机制，动态预测语言族并仅在必要时进行token掩码过滤。

Result: 在Qwen3、GPT-OSS、Gemma3、Llama3.1等多个模型上验证，LCG显著减少了语言混淆，通常降低一个数量级，且未对任务性能产生负面影响。

Conclusion: LCG是一种有效、通用且无需修改原模型的插件式方法，能够精准抑制语言混淆，同时保留合理的多语言表达能力，适用于多语言场景下的大型语言模型部署。

Abstract: Large language models (LLMs) often experience language confusion, which is
the unintended mixing of languages during text generation. Current solutions to
this problem either necessitate model retraining or cannot differentiate
between harmful confusion and acceptable code-switching. This paper introduces
the Language Confusion Gate (LCG), a lightweight, plug-in solution that filters
tokens during decoding without altering the base LLM. The LCG is trained using
norm-adjusted self-distillation to predict appropriate language families and
apply masking only when needed. Our method is based on the findings that
language confusion is infrequent, correct-language tokens are usually among the
top predictions, and output token embedding norms are larger for high-resource
languages, which biases sampling. When evaluated across various models,
including Qwen3, GPT-OSS, Gemma3, Llama3.1, LCG decreases language confusion
significantly, often by an order of magnitude, without negatively impacting
task performance. Code is available at
https://github.com/collinzrj/language_confusion_gate.

</details>


### [272] [HGAdapter: Hypergraph-based Adapters in Language Models for Code Summarization and Clone Detection](https://arxiv.org/abs/2510.17591)
*Guang Yang,Yujie Zhu*

Main category: cs.CL

TL;DR: 提出了一种基于超图的适配器HGAdapter，通过捕捉代码中的高阶相关性（如抽象语法树、词汇和行相关性）来增强预训练语言模型在代码相关任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练语言模型（PLMs）在处理代码任务时忽略了代码中潜在的高阶数据相关性，限制了其表现能力。

Method: 设计了token和超边生成器以捕捉三种高阶相关性（抽象语法树家族相关性、词汇相关性和行相关性），改进了超图神经网络架构，并结合适配器微调提出HGAdapter，可插入多种PLMs中进行微调。

Result: 在多个公开数据集上的代码摘要和代码克隆检测任务中，HGAdapter在六种编程语言上均提升了PLMs的性能，验证了引入高阶相关性的有效性。

Conclusion: 捕捉代码中的高阶数据相关性有助于提升PLMs在代码理解任务中的效果，HGAdapter是一种通用且有效的增强方法。

Abstract: Pre-trained language models (PLMs) are increasingly being applied to
code-related tasks. Although PLMs have achieved good results, they do not take
into account potential high-order data correlations within the code. We propose
three types of high-order correlations in code tokens, i.e. abstract syntax
tree family correlation, lexical correlation, and line correlation. We design a
tokens and hyperedges generator to capture these high-order data correlations.
We improve the architecture of hypergraph neural networks and combine it with
adapter tuning to propose a novel hypergraph-based adapter (HGAdapter) to
fine-tune PLMs. HGAdapter can encode high-order data correlations and is
allowed to be inserted into various PLMs to enhance performance. Experiments
were conducted on several public datasets, including six languages of code
summarization and code clone detection tasks. Our methods improved the
performance of PLMs in datasets to varying degrees. Experimental results
validate the introduction of high-order data correlations that contribute to
improved effectiveness.

</details>


### [273] [LawChain: Modeling Legal Reasoning Chains for Chinese Tort Case Analysis](https://arxiv.org/abs/2510.17602)
*Huiyuan Xie,Chenyang Li,Huining Zhu,Chubin Zhang,Yuxiao Ye,Zhenghao Liu,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 提出了一种名为LawChain的新框架，用于显式建模中国侵权民事案件中的法律推理，并构建了评估基准LawChain$_{eval}$，验证了当前大模型在侵权法律推理中的不足，并通过提示或后训练引入基于LawChain的基线方法，显著提升了法律推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有法律推理计算方法多依赖通用框架，缺乏对法律推理细节过程的深入分析，且主要集中于刑事案件，对民事案件建模不足，因此需要针对民事侵权案件建立更精细的推理框架。

Method: 将侵权法律分析中的推理过程操作化为三模块的LawChain框架，每个模块包含多个细粒度子步骤；基于此提出侵权法律推理任务并构建评估基准LawChain$_{eval}$；设计基于提示和后训练的基线方法以引入LawChain式推理。

Result: 实验表明现有大语言模型在关键侵权推理环节表现不佳；引入LawChain式推理的基线方法在侵权法律推理任务上显著提升性能，并在法律命名实体识别和刑事损害计算等任务中表现出良好泛化能力。

Conclusion: 显式建模法律推理链（如LawChain）能有效提升语言模型在民事侵权法律推理及其他相关法律任务中的表现，具有重要应用价值。

Abstract: Legal reasoning is a fundamental component of legal analysis and
decision-making. Existing computational approaches to legal reasoning
predominantly rely on generic reasoning frameworks such as syllogism and IRAC,
which do not comprehensively examine the nuanced processes that underpin legal
reasoning. Moreover, current research has largely focused on criminal cases,
with insufficient modeling for civil cases. In this work, we present a novel
framework for explicitly modeling legal reasoning in the analysis of Chinese
tort-related civil cases. We first operationalize the legal reasoning processes
used in tort analysis into the LawChain framework. LawChain is a three-module
reasoning framework, with each module consisting of multiple finer-grained
sub-steps. Informed by the LawChain framework, we introduce the task of tort
legal reasoning and construct an evaluation benchmark, LawChain$_{eval}$, to
systematically assess the critical steps within analytical reasoning chains for
tort analysis. Leveraging this benchmark, we evaluate state-of-the-art large
language models for their legal reasoning ability in civil tort contexts. Our
results indicate that current models still fall short in accurately handling
crucial elements of tort legal reasoning. Furthermore, we introduce several
baseline approaches that explicitly incorporate LawChain-style reasoning
through prompting or post-training. We conduct further experiments on
additional legal analysis tasks, such as Legal Named-Entity Recognition and
Criminal Damages Calculation, to verify the generalizability of these
baselines. The proposed baseline approaches achieve significant improvements in
tort-related legal reasoning and generalize well to related legal analysis
tasks, thus demonstrating the value of explicitly modeling legal reasoning
chains to enhance the reasoning capabilities of language models.

</details>


### [274] [Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language Models](https://arxiv.org/abs/2510.17620)
*Yuefeng Peng,Parnian Afshar,Megan Ganji,Thomas Butler,Amir Houmansadr,Mingxian Wang,Dezhi Hong*

Main category: cs.CL

TL;DR: 现有遗忘方法在删除特定知识时会损害模型对上下文中有用信息的利用能力，本文提出一种增强的遗忘目标方法，通过引入插件项恢复模型的上下文效用，同时保持遗忘效果和模型整体性能。


<details>
  <summary>Details</summary>
Motivation: 现有的遗忘方法评估主要关注目标知识的遗忘程度和保留集性能，但忽视了用户在提示中重新引入被遗忘信息时，模型是否仍能有效利用这些信息的实用性问题。

Method: 提出一种带有插件项的增强遗忘目标方法，使模型在遗忘特定知识的同时，保留当该知识出现在上下文时的利用能力，并在六种先进遗忘方法上进行系统评估与改进。

Result: 实验表明，所提出的方法能将上下文效用恢复至接近原始水平，同时有效保持目标知识的遗忘和保留集性能。

Conclusion: 通过增强遗忘目标中的上下文学习能力，可在不牺牲遗忘效果的前提下显著提升模型的实用性，为安全合规与模型效用之间的平衡提供了新思路。

Abstract: Large language models may encode sensitive information or outdated knowledge
that needs to be removed, to ensure responsible and compliant model responses.
Unlearning has emerged as an efficient alternative to full retraining, aiming
to remove specific knowledge while preserving overall model utility. Existing
evaluations of unlearning methods focus on (1) the extent of forgetting of the
target knowledge (forget set) and (2) maintaining performance on the retain set
(i.e., utility). However, these evaluations overlook an important usability
aspect: users may still want the model to leverage the removed information if
it is re-introduced in the prompt. In a systematic evaluation of six
state-of-the-art unlearning methods, we find that they consistently impair such
contextual utility. To address this, we augment unlearning objectives with a
plug-in term that preserves the model's ability to use forgotten knowledge when
it is present in context. Extensive experiments demonstrate that our approach
restores contextual utility to near original levels while still maintaining
effective forgetting and retain-set utility.

</details>


### [275] [Qomhra: A Bilingual Irish-English Large Language Model](https://arxiv.org/abs/2510.17652)
*Joseph McInerney*

Main category: cs.CL

TL;DR: 本文介绍了Qomhrá，一种在低资源约束下开发的双语爱尔兰语-英语大语言模型，通过持续预训练、指令微调和基于人类偏好的对齐构建完整流程，并发布了两个新的数据集，在多项基准测试中显著提升了爱尔兰语和英语性能。


<details>
  <summary>Details</summary>
Motivation: 由于爱尔兰语属于低资源语言，缺乏高质量的双语大语言模型，本文旨在开发一个既能提升爱尔兰语表现又能保持英语能力的双语模型，并解决相关数据稀缺的问题。

Method: 采用双语持续预训练、指令微调和基于人类偏好对齐的三阶段流程；利用新获取的爱尔兰语语料和英语文本混合训练；通过人工评估与多个闭源大模型对比选择最优基础模型（Gemini-2.5-Pro）生成指令和偏好数据集。

Result: 构建了包含30K样本的爱尔兰-英语指令微调数据集和1K样本的人类偏好数据集；Qomhrá在翻译、性别理解、主题识别和世界知识等任务上比基线最多提升29%（爱尔兰语）和44%（英语），且指令遵循能力显著增强。

Conclusion: Qomhrá证明了在低资源条件下构建高质量双语大模型的可行性，所提出的方法和数据集有助于促进濒危语言在现代NLP系统中的应用。

Abstract: This paper introduces Qomhr\'a, a bilingual Irish-English large language
model (LLM), developed under low-resource constraints presenting a complete
pipeline spanning bilingual continued pre-training, instruction tuning, and
alignment from human preferences. Newly accessible Irish corpora and English
text are mixed and curated to improve Irish performance while preserving
English ability. 6 closed-weight LLMs are judged for their Irish text
generation by a native speaker, a learner and other LLMs. Google's
Gemini-2.5-Pro is ranked the highest and is subsequently used to synthesise
instruction tuning and human preference datasets. Two datasets are contributed
leveraging Gemini-2.5-Pro: a 30K Irish-English parallel instruction tuning
dataset and a 1K human preference dataset, generating accepted and rejected
responses that show near perfect alignment with a native Irish speaker.
Qomhr\'a is comprehensively evaluated across benchmarks testing translation,
gender understanding, topic identification and world knowledge with gains of up
to 29% in Irish and 44% in English. Qomhr\'a also undergoes instruction tuning
and demonstrates clear progress in instruction following, crucial for chatbot
functionality.

</details>


### [276] [Towards Mining Effective Pedagogical Strategies from Learner-LLM Educational Dialogues](https://arxiv.org/abs/2510.17698)
*Liqun He,Manolis Mavrikis,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 本研究提出一种对话分析方法，旨在从学习者与大语言模型（LLM）的互动对话中识别有效的教学策略，弥补当前评估方法忽视对话动态的不足。


<details>
  <summary>Details</summary>
Motivation: 现有对基于LLM的教育应用的评估多关注技术性能或学习成果，缺乏对学习者与模型之间互动过程的深入分析，因此需要一种聚焦于对话动态和教学策略的评估方法。

Method: 研究采用对话分析方法，包括收集学习者与LLM的对话数据、进行对话行为（DA）标注、挖掘DA模式，并构建预测模型。

Result: 目前研究处于初期阶段，已获得初步洞察，为后续深入分析对话模式和教学策略奠定了基础。

Conclusion: 评估基于LLM的教育应用时，应重视对话互动过程；本研究为理解有效的人机教学对话提供了新视角和方法框架。

Abstract: Dialogue plays a crucial role in educational settings, yet existing
evaluation methods for educational applications of large language models (LLMs)
primarily focus on technical performance or learning outcomes, often neglecting
attention to learner-LLM interactions. To narrow this gap, this AIED Doctoral
Consortium paper presents an ongoing study employing a dialogue analysis
approach to identify effective pedagogical strategies from learner-LLM
dialogues. The proposed approach involves dialogue data collection, dialogue
act (DA) annotation, DA pattern mining, and predictive model building. Early
insights are outlined as an initial step toward future research. The work
underscores the need to evaluate LLM-based educational applications by focusing
on dialogue dynamics and pedagogical strategies.

</details>


### [277] [QueST: Incentivizing LLMs to Generate Difficult Problems](https://arxiv.org/abs/2510.17715)
*Hanxu Hu,Xingxing Zhang,Jannis Vamvas,Rico Sennrich,Furu Wei*

Main category: cs.CL

TL;DR: 本文提出QueST框架，通过难度感知的图采样和拒绝微调生成大规模、高难度的合成编程问题，显著提升小模型在竞争性编码任务中的表现，具备良好可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有竞争性编程数据集规模有限，且依赖人工标注，难以满足大模型训练需求；亟需一种可扩展的方法来自动生成高难度编程问题。

Method: 提出QueST框架，结合难度感知图采样和难度感知拒绝微调，训练专用生成器以生成高难度编程问题，并用于知识蒸馏或强化学习。

Result: QueST生成的10万难题使Qwen3-8B-base在LiveCodeBench上超越原版Qwen3-8B；结合额外11.2万样本后，8B模型性能媲美671B的DeepSeek-R1。

Conclusion: QueST能有效生成高质量、高难度编程问题，为大模型的推理能力训练提供可扩展的解决方案。

Abstract: Large Language Models have achieved strong performance on reasoning tasks,
solving competition-level coding and math problems. However, their scalability
is limited by human-labeled datasets and the lack of large-scale, challenging
coding problem training data. Existing competitive coding datasets contain only
thousands to tens of thousands of problems. Previous synthetic data generation
methods rely on either augmenting existing instruction datasets or selecting
challenging problems from human-labeled data. In this paper, we propose QueST,
a novel framework which combines difficulty-aware graph sampling and
difficulty-aware rejection fine-tuning that directly optimizes specialized
generators to create challenging coding problems. Our trained generators
demonstrate superior capability compared to even GPT-4o at creating challenging
problems that benefit downstream performance. We leverage QueST to generate
large-scale synthetic coding problems, which we then use to distill from strong
teacher models with long chain-of-thought or to conduct reinforcement learning
for smaller models, proving effective in both scenarios. Our distillation
experiments demonstrate significant performance gains. Specifically, after
fine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we
surpass the performance of the original Qwen3-8B on LiveCodeBench. With an
additional 112K examples (i.e., 28K human-written problems paired with multiple
synthetic solutions), our 8B model matches the performance of the much larger
DeepSeek-R1-671B. These findings indicate that generating complex problems via
QueST offers an effective and scalable approach to advancing the frontiers of
competitive coding and reasoning for large language models.

</details>


### [278] [PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity Recognition](https://arxiv.org/abs/2510.17720)
*Nanda Kumar Rengarajan,Jun Yan,Chun Wang*

Main category: cs.CL

TL;DR: 提出了一种轻量级的少样本NER框架，通过改进指令微调模板和引入保持实体信息的数据增强技术，在低资源场景下实现了与现有最先进模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 在标注数据稀缺的低资源场景下，现有的零样本和指令微调方法难以有效泛化到领域特定实体，且未能充分利用有限数据，因此需要一种能高效利用少量标注数据的NER方法。

Method: （1）设计了一种新的指令微调模板，采用简化输出格式，结合先前指令微调方法的优势，利用大模型的上下文窗口；（2）提出一种保留实体信息的上下文改写式数据增强策略，以扩充训练数据并保持语义关系。

Result: 在基准数据集上的实验表明，该方法在少样本和零样本任务中表现优异，少样本方法在CrossNER数据集上平均F1达到80.1；使用改写增强训练的模型F1提升高达17个百分点。

Conclusion: 所提框架能有效提升低资源下的NER性能，为标注数据和算力有限的场景提供了高效可行的解决方案。

Abstract: Named Entity Recognition (NER) is a critical task that requires substantial
annotated data, making it challenging in low-resource scenarios where label
acquisition is expensive. While zero-shot and instruction-tuned approaches have
made progress, they often fail to generalize to domain-specific entities and do
not effectively utilize limited available data. We present a lightweight
few-shot NER framework that addresses these challenges through two key
innovations: (1) a new instruction tuning template with a simplified output
format that combines principles from prior IT approaches to leverage the large
context window of recent state-of-the-art LLMs; (2) introducing a strategic
data augmentation technique that preserves entity information while
paraphrasing the surrounding context, thereby expanding our training data
without compromising semantic relationships. Experiments on benchmark datasets
show that our method achieves performance comparable to state-of-the-art models
on few-shot and zero-shot tasks, with our few-shot approach attaining an
average F1 score of 80.1 on the CrossNER datasets. Models trained with our
paraphrasing approach show consistent improvements in F1 scores of up to 17
points over baseline versions, offering a promising solution for groups with
limited NER training data and compute power.

</details>


### [279] [AcademicEval: Live Long-Context LLM Benchmark](https://arxiv.org/abs/2510.17725)
*Haozhen Zhang,Tao Feng,Pengrui Han,Jiaxuan You*

Main category: cs.CL

TL;DR: 提出	extsc{AcademicEval}，一个基于arXiv论文的长上下文生成任务动态基准，无需人工标注，避免标签泄露，支持灵活上下文长度，评估显示现有LLM在分层抽象任务和长少样本推理上仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文LLM基准受限于固定长度、人工标注成本高，且存在训练中的标签泄露风险，亟需一个动态、高质量且免标注的评估框架。

Method: 利用arXiv论文构建学术写作生成任务（如标题、摘要、引言、相关工作），结合专家策划的共作者图提供高质量少样本示例，设计支持灵活上下文长度和实时更新的评估流程，确保无标签泄露。

Result: 实验表明当前LLM在具有分层抽象的任务上表现不佳，且在处理长少样本输入时面临显著困难，验证了该基准的挑战性和有效性。

Conclusion: 	extsc{AcademicEval}为长上下文LLM评估提供了更真实、动态且安全的平台，揭示了模型在复杂抽象和长上下文推理上的不足，为未来改进提供了方向。

Abstract: Large Language Models (LLMs) have recently achieved remarkable performance in
long-context understanding. However, current long-context LLM benchmarks are
limited by rigid context length, labor-intensive annotation, and the pressing
challenge of label leakage issues during LLM training. Therefore, we propose
\textsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context
generation tasks. \textsc{AcademicEval} adopts papers on arXiv to introduce
several academic writing tasks with long-context inputs, \textit{i.e.},
\textsc{Title}, \textsc{Abstract}, \textsc{Introduction}, and \textsc{Related
Work}, which cover a wide range of abstraction levels and require no manual
labeling. Moreover, \textsc{AcademicEval} integrates high-quality and
expert-curated few-shot demonstrations from a collected co-author graph to
enable flexible context length. Especially, \textsc{AcademicEval} features an
efficient live evaluation, ensuring no label leakage. We conduct a holistic
evaluation on \textsc{AcademicEval}, and the results illustrate that LLMs
perform poorly on tasks with hierarchical abstraction levels and tend to
struggle with long few-shot demonstrations, highlighting the challenge of our
benchmark. Through experimental analysis, we also reveal some insights for
enhancing LLMs' long-context modeling capabilities. Code is available at
https://github.com/ulab-uiuc/AcademicEval

</details>


### [280] [Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations](https://arxiv.org/abs/2510.17733)
*Tong Chen,Akari Asai,Luke Zettlemoyer,Hannaneh Hajishirzi,Faeze Brahman*

Main category: cs.CL

TL;DR: 提出一种基于二值检索增强奖励的在线强化学习方法，有效减少大模型幻觉，且不损害其他任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有缓解模型幻觉的方法通常会损害开放生成和下游任务的表现，限制了实用性，因此需要一种兼顾事实性和生成质量的方法。

Method: 提出一种新的二值检索增强奖励（RAR）机制，仅在模型输出完全正确时给予奖励1，否则为0，并结合在线强化学习优化模型。

Result: 在Qwen3推理模型上验证，开放生成中幻觉率降低39.3%；在PopQA和GPQA上分别减少44.4%和21.7%的错误回答；模型学会在知识不足时合理拒绝回答。

Conclusion: 二值RAR在提升事实准确性的同时，避免了连续奖励强化学习导致的生成质量下降，实现了事实性与多任务性能的平衡。

Abstract: Language models often generate factually incorrect information unsupported by
their training data, a phenomenon known as extrinsic hallucination. Existing
mitigation approaches often degrade performance on open-ended generation and
downstream tasks, limiting their practical utility. We propose an online
reinforcement learning method using a novel binary retrieval-augmented reward
(RAR) to address this tradeoff. Unlike continuous reward schemes, our approach
assigns a reward of one only when the model's output is entirely factually
correct, and zero otherwise. We evaluate our method on Qwen3 reasoning models
across diverse tasks. For open-ended generation, binary RAR achieves a 39.3%
reduction in hallucination rates, substantially outperforming both supervised
training and continuous-reward RL baselines. In short-form question answering,
the model learns calibrated abstention, strategically outputting "I don't know"
when faced with insufficient parametric knowledge. This yields 44.4% and 21.7%
fewer incorrect answers on PopQA and GPQA, respectively. Crucially, these
factuality gains come without performance degradation on instruction following,
math, or code, whereas continuous-reward RL, despite improving factuality,
induces quality regressions.

</details>


### [281] [Evaluating Medical LLMs by Levels of Autonomy: A Survey Moving from Benchmarks to Applications](https://arxiv.org/abs/2510.17764)
*Xiao Ye,Jacob Dineen,Zhaonan Li,Zhikun Xu,Weiyu Chen,Shijie Lu,Yuxi Huang,Ming Shen,Phu Tran,Ji-Eun Irene Yum,Muhammad Ali Khan,Muhammad Umar Afzal,Irbaz Bin Riaz,Ben Zhou*

Main category: cs.CL

TL;DR: 本文通过自主性层级（L0-L3）框架重新定义医疗大语言模型的评估方式，推动从单纯分数评价转向面向实际临床应用的可信、风险可控的评估体系。


<details>
  <summary>Details</summary>
Motivation: 尽管医学大语言模型在标准基准上表现良好，但在临床工作流中实现安全可靠的应用仍具挑战，现有评估方法缺乏对自主性与风险的系统考量。

Method: 提出基于自主性层级（L0-L3）的评估框架，将现有基准和指标与各层级允许的操作及对应风险对齐，并制定分层级的评估蓝图。

Result: 建立了将评估指标、证据整合与声明报告与自主层级相匹配的系统框架，明确了不同层级的风险与评估目标。

Conclusion: 以自主性为核心的评估框架有助于推动医疗大语言模型从分数导向转向可信赖、与临床监督相结合的现实应用评估。

Abstract: Medical Large language models achieve strong scores on standard benchmarks;
however, the transfer of those results to safe and reliable performance in
clinical workflows remains a challenge. This survey reframes evaluation through
a levels-of-autonomy lens (L0-L3), spanning informational tools, information
transformation and aggregation, decision support, and supervised agents. We
align existing benchmarks and metrics with the actions permitted at each level
and their associated risks, making the evaluation targets explicit. This
motivates a level-conditioned blueprint for selecting metrics, assembling
evidence, and reporting claims, alongside directions that link evaluation to
oversight. By centering autonomy, the survey moves the field beyond score-based
claims toward credible, risk-aware evidence for real clinical use.

</details>


### [282] [Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains](https://arxiv.org/abs/2510.17793)
*Austin Xu,Xuan-Phi Nguyen,Yilun Zhou,Chien-Sheng Wu,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: 本文提出了FARE，一个基于大规模数据训练的自动生成评估模型家族，通过简单的迭代监督微调方法，在多个推理评估任务中超越了现有的大规模专门评估器。


<details>
  <summary>Details</summary>
Motivation: 随着对可扩展评估需求的增长，现有的研究多集中于使用强化学习等新方法训练评估器，而忽视了大规模数据驱动的发展。本文旨在探索数据规模对评估器性能的影响。

Method: 收集了涵盖五个不同评估任务和多个推理领域的250万样本数据集，采用简单的迭代拒绝采样监督微调（SFT）方法训练FARE模型系列。

Result: FARE-8B挑战了更大的专门强化学习训练的评估器，而FARE-20B成为新的开源评估器标准，超过了70B以上的专门评估器。在实际应用中，FARE-20B作为推理时重排序器在MATH数据集上接近最优表现；作为强化学习训练中的验证器，相比字符串匹配验证器提升了最多14.1%的下游模型性能；连续微调的FARE-Code在评估测试用例质量方面比gpt-oss-20B高出65%。

Conclusion: 大规模数据驱动的简单监督微调方法能够有效提升生成评估器的性能，FARE系列模型在多种实际应用场景中展现了卓越的表现。

Abstract: Finetuning specialized generative evaluators has emerged as a popular
paradigm to meet the increasing demand for scalable evaluation during both
training and test-time. However, recent work has largely focused on applying
new methodology, such as reinforcement learning (RL), to training evaluators,
shying away from large-scale, data-driven development. In this work, we focus
on data scaling, curating a set of 2.5M samples spanning five unique evaluation
tasks (pairwise, step-level, reference-free and reference-based verification,
and single rating) and multiple domains focused on reasoning evaluation. With
our data, we train Foundational Automatic Reasoning Evaluators (FARE), a family
of 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative
rejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges
larger specialized RL-trained evaluators and FARE-20B sets the new standard for
open-source evaluators, surpassing specialized 70B+ evaluators. Beyond static
benchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,
FARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,
FARE improves the downstream RL-trained model performance by up to 14.1% vs.
string-matching verifiers. When initialized from FARE, a continually-finetuned
FARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.

</details>


### [283] [Executable Knowledge Graphs for Replicating AI Research](https://arxiv.org/abs/2510.17795)
*Yujie Luo,Zhuoyun Yu,Xuehai Wang,Yuqi Zhu,Ningyu Zhang,Lanning Wei,Lun Du,Da Zheng,Huajun Chen*

Main category: cs.CL

TL;DR: 提出可执行知识图谱（xKG），以增强LLM代理在自动化AI研究复现中的代码生成能力，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成可执行代码时面临背景知识不足、RAG方法难以捕捉隐含技术细节、忽略代码实现信号及缺乏结构化知识表示的问题。

Method: 构建模块化可执行知识图谱xKG，自动从科研文献中提取技术洞见、代码片段和领域知识，支持多粒度检索与重用，并集成到LLM代理框架中。

Result: 在PaperBench上，结合三种代理框架和两个LLM的实验显示，xKG带来最高10.9%的性能提升（使用o3-mini时）。

Conclusion: xKG是一种通用且可扩展的解决方案，有效提升LLM代理在AI研究复现中的执行力和准确性。

Abstract: Replicating AI research is a crucial yet challenging task for large language
model (LLM) agents. Existing approaches often struggle to generate executable
code, primarily due to insufficient background knowledge and the limitations of
retrieval-augmented generation (RAG) methods, which fail to capture latent
technical details hidden in referenced papers. Furthermore, previous approaches
tend to overlook valuable implementation-level code signals and lack structured
knowledge representations that support multi-granular retrieval and reuse. To
overcome these challenges, we propose Executable Knowledge Graphs (xKG), a
modular and pluggable knowledge base that automatically integrates technical
insights, code snippets, and domain-specific knowledge extracted from
scientific literature. When integrated into three agent frameworks with two
different LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on
PaperBench, demonstrating its effectiveness as a general and extensible
solution for automated AI research replication. Code will released at
https://github.com/zjunlp/xKG.

</details>


### [284] [Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics](https://arxiv.org/abs/2510.17797)
*Akshara Prabhakar,Roshan Ram,Zixiang Chen,Silvio Savarese,Frank Wang,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.CL

TL;DR: 提出了一种名为Enterprise Deep Research (EDR)的多智能体系统，用于自动化处理企业中的非结构化数据并生成可操作的洞察，支持多种搜索源、工具扩展和可视化，且在多个基准上超越现有最先进系统。


<details>
  <summary>Details</summary>
Motivation: 企业面临海量非结构化数据，传统自主代理难以应对领域特异性、意图对齐和企业集成等挑战，需要更强大、可扩展的自动化研究系统。

Method: 构建一个包含主规划代理、四个专用搜索代理（通用、学术、GitHub、LinkedIn）、基于MCP的可扩展工具生态系统（支持NL2SQL、文件分析等）、可视化代理以及反思机制的多代理系统，支持实时流式处理和人机协同指导。

Result: EDR在内部数据集上验证了自动化报告生成和企业部署能力，在DeepResearch Bench和DeepConsult等开放性基准上优于现有最先进的代理系统，且无需人工干预。

Conclusion: EDR为多代理系统在企业环境中的复杂信息处理提供了有效框架，推动了多代理推理应用的发展，相关代码和数据集已开源。

Abstract: As information grows exponentially, enterprises face increasing pressure to
transform unstructured data into coherent, actionable insights. While
autonomous agents show promise, they often struggle with domain-specific
nuances, intent alignment, and enterprise integration. We present Enterprise
Deep Research (EDR), a multi-agent system that integrates (1) a Master Planning
Agent for adaptive query decomposition, (2) four specialized search agents
(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool
ecosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a
Visualization Agent for data-driven insights, and (5) a reflection mechanism
that detects knowledge gaps and updates research direction with optional
human-in-the-loop steering guidance. These components enable automated report
generation, real-time streaming, and seamless enterprise deployment, as
validated on internal datasets. On open-ended benchmarks including DeepResearch
Bench and DeepConsult, EDR outperforms state-of-the-art agentic systems without
any human steering. We release the EDR framework and benchmark trajectories to
advance research on multi-agent reasoning applications.
  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and
Dataset at https://huggingface.co/datasets/Salesforce/EDR-200

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [285] [MuseTok: Symbolic Music Tokenization for Generation and Semantic Understanding](https://arxiv.org/abs/2510.16273)
*Jingyue Huang,Zachary Novack,Phillip Long,Yupeng Hou,Ke Chen,Taylor Berg-Kirkpatrick,Julian McAuley*

Main category: cs.SD

TL;DR: MuseTok是一种用于符号音乐的分词方法，基于RQ-VAE和Transformer框架，在音乐生成与理解任务中表现出色，能有效捕捉音乐语义概念。


<details>
  <summary>Details</summary>
Motivation: 受离散表示学习在图像、语音和语言领域成功的启发，研究者希望为符号音乐设计一种有效的表示学习方法，以同时提升音乐生成与理解性能。

Method: 提出MuseTok方法，采用残差向量量化-变分自编码器（RQ-VAE）对按小节划分的音乐片段进行处理，结合Transformer编码器-解码器架构生成音乐代码。

Result: 在音乐生成和语义理解任务（如旋律提取、和弦识别、情感识别）中，使用MuseTok的模型优于先前的表示学习基线，在理解任务上表现尤佳，同时生成质量相当；定性分析显示其代码能捕捉音乐理论概念。

Conclusion: MuseTok能高效学习符号音乐的离散表示，在生成与理解任务中取得平衡，展现出对音乐结构和语义的深层建模能力。

Abstract: Discrete representation learning has shown promising results across various
domains, including generation and understanding in image, speech and language.
Inspired by these advances, we propose MuseTok, a tokenization method for
symbolic music, and investigate its effectiveness in both music generation and
understanding tasks. MuseTok employs the residual vector quantized-variational
autoencoder (RQ-VAE) on bar-wise music segments within a Transformer-based
encoder-decoder framework, producing music codes that achieve high-fidelity
music reconstruction and accurate understanding of music theory. For
comprehensive evaluation, we apply MuseTok to music generation and semantic
understanding tasks, including melody extraction, chord recognition, and
emotion recognition. Models incorporating MuseTok outperform previous
representation learning baselines in semantic understanding while maintaining
comparable performance in content generation. Furthermore, qualitative analyses
on MuseTok codes, using ground-truth categories and synthetic datasets, reveal
that MuseTok effectively captures underlying musical concepts from large music
collections.

</details>


### [286] [Transmission of High-Amplitude Sound through Leakages of Ill-fitting Earplugs](https://arxiv.org/abs/2510.16355)
*Haocheng Yu,Krishan K. Ahuja,Lakshmi N. Sankar,Spencer H. Bryngelson*

Main category: cs.SD

TL;DR: 研究探讨了耳塞不合适导致的声学泄漏问题，发现未密封硅胶耳塞在120 dB声压级下平均透射损失降低约18 dB，并揭示了高声压下声能转化为涡量的耗散机制。


<details>
  <summary>Details</summary>
Motivation: 为了解决高声压环境下因耳塞不合适导致的噪声泄漏问题，从而降低噪声性听力损失的风险。

Method: 通过计算和实验获取狭缝共振器和孔隙的声学传输数据，研究不同孔径泄漏结构在1-5 kHz频率范围内的吸声系数和透射损失，并进行高声压（120-150 dB）下的实验和直接数值模拟。

Result: 未密封硅胶耳塞在120 dB总入射声压级下平均透射损失减少约18 dB；在150 dB时，数值模拟显示声能转化为涡量，揭示了与声压相关的耗散机制。

Conclusion: 耳塞设计在高声压环境中的密封性至关重要，优化设计可有效提升防护性能。

Abstract: High sound pressure levels (SPL) pose notable risks in loud environments,
particularly due to noise-induced hearing loss. Ill-fitting earplugs often lead
to sound leakage, a phenomenon this study seeks to investigate. To validate our
methodology, we first obtained computational and experimental acoustic
transmission data for stand-alone slit resonators and orifices, for which
extensive published data are readily available for comparison. We then examined
the frequency-dependent acoustic power absorption coefficient and transmission
loss (TL) across various leakage geometries, modeled using different orifice
diameters. Experimental approaches spanned a frequency range of 1--5 kHz under
SPL conditions of 120--150 dB. Key findings reveal that unsealed silicone
rubber earplugs demonstrate an average TL reduction of approximately 18 dB at
an overall incident SPL (OISPL) of 120 dB. Direct numerical simulations further
highlight SPL-dependent acoustic dissipation mechanisms, showing the conversion
of acoustic energy into vorticity in ill-fitting earplug models at an OISPL of
150 dB. These results highlight the role of earplug design for
high-sound-pressure-level environments.

</details>


### [287] [Interpreting the Dimensions of Speaker Embedding Space](https://arxiv.org/abs/2510.16489)
*Mark Huckvale*

Main category: cs.SD

TL;DR: 研究了说话人嵌入系统如何表示声学特征、年龄和性别，发现9个可解释的声学参数能预测大部分嵌入变化，性别有隐式识别，但年龄捕捉不足。


<details>
  <summary>Details</summary>
Motivation: 探究当前主流说话人嵌入系统在多大程度上反映传统的声学、年龄和性别特征，揭示其“黑箱”特性背后的可解释性。

Method: 使用包含10,000名说话人的大型语料库和三种嵌入系统，分析9个可解释的声学参数和主成分对嵌入的预测能力，并比较不同性别和年龄的表现差异。

Result: 9个可解释声学参数的预测效果与7个主成分相当，解释了超过50%的方差；性别相关主成分在男女中表现不同，显示出嵌入系统隐含性别识别能力；但年龄信息在嵌入中捕捉不佳。

Conclusion: 当前说话人嵌入系统能有效编码性别相关声学特征，但对年龄建模不足，未来可针对此改进嵌入计算方法。

Abstract: Speaker embeddings are widely used in speaker verification systems and other
applications where it is useful to characterise the voice of a speaker with a
fixed-length vector. These embeddings tend to be treated as "black box"
encodings, and how they relate to conventional acoustic and phonetic dimensions
of voices has not been widely studied. In this paper we investigate how
state-of-the-art speaker embedding systems represent the acoustic
characteristics of speakers as described by conventional acoustic descriptors,
age, and gender. Using a large corpus of 10,000 speakers and three embedding
systems we show that a small set of 9 acoustic parameters chosen to be
"interpretable" predict embeddings about the same as 7 principal components,
corresponding to over 50% of variance in the data. We show that some principal
dimensions operate differently for male and female speakers, suggesting there
is implicit gender recognition within the embedding systems. However we show
that speaker age is not well captured by embeddings, suggesting opportunities
exist for improvements in their calculation.

</details>


### [288] [Zero- and One-Shot Data Augmentation for Sentence-Level Dysarthric Speech Recognition in Constrained Scenarios](https://arxiv.org/abs/2510.16700)
*Shiyao Wang,Shiwan Zhao,Jiaming Zhou,Yong Qin*

Main category: cs.SD

TL;DR: 提出一种新的文本覆盖策略用于失语症语音数据增强，实现在零样本/一样本场景下的高效数据合成，显著提升未见失语症说话者的语音识别性能。


<details>
  <summary>Details</summary>
Motivation: 失语症语音识别在句子级别取得进展，但数据稀缺严重制约系统发展，尤其在零样本或一样本学习场景下难以生成有效的增强数据。

Method: 提出一种面向文本匹配的数据合成文本覆盖策略，利用生成模型在极少真实数据下合成高质量的失语症语音数据，实现高效的数据增强。

Result: 该方法在零样本/一样本设置下显著提升了失语症语音识别的性能，尤其对未见过的说话者表现出更强的泛化能力。

Conclusion: 所提文本覆盖策略为低资源失语症语音识别提供了有效的数据增强方案，具有重要的实际应用价值，如康复训练和日常交流场景。

Abstract: Dysarthric speech recognition (DSR) research has witnessed remarkable
progress in recent years, evolving from the basic understanding of individual
words to the intricate comprehension of sentence-level expressions, all driven
by the pressing communication needs of individuals with dysarthria.
Nevertheless, the scarcity of available data remains a substantial hurdle,
posing a significant challenge to the development of effective sentence-level
DSR systems. In response to this issue, dysarthric data augmentation (DDA) has
emerged as a highly promising approach. Generative models are frequently
employed to generate training data for automatic speech recognition tasks.
However, their effectiveness hinges on the ability of the synthesized data to
accurately represent the target domain. The wide-ranging variability in
pronunciation among dysarthric speakers makes it extremely difficult for models
trained on data from existing speakers to produce useful augmented data,
especially in zero-shot or one-shot learning settings. To address this
limitation, we put forward a novel text-coverage strategy specifically designed
for text-matching data synthesis. This innovative strategy allows for efficient
zero/one-shot DDA, leading to substantial enhancements in the performance of
DSR when dealing with unseen dysarthric speakers. Such improvements are of
great significance in practical applications, including dysarthria
rehabilitation programs and day-to-day common-sentence communication scenarios.

</details>


### [289] [U-Codec: Ultra Low Frame-rate Neural Speech Codec for Fast High-fidelity Speech Generation](https://arxiv.org/abs/2510.16718)
*Xusheng Yang,Long Zhou,Wenfu Wang,Kai Hu,Shulin Feng,Chenxing Li,Meng Yu,Dong Yu,Yuexian Zou*

Main category: cs.SD

TL;DR: 提出U-Codec，一种极低帧率（5Hz）神经语音编解码器，实现高保真重建和快速语音生成，并有效集成到基于大语言模型的自回归TTS中，显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 在极低帧率（如5Hz）下实现高质量语音编解码面临严重失真和信息丢失问题，传统方法难以保持语音的可懂度与音质，因此需要设计新型编解码器以实现高效压缩与高保真重建之间的平衡。

Method: 提出基于Transformer的帧间长期依赖模块，系统探索残差向量量化（RVQ）的层级深度与码本大小的最优配置；将U-Codec应用于基于大语言模型的自回归TTS系统，采用全局与局部分层结构，支持从50Hz 3层RVQ扩展到5Hz 32层RVQ的多层离散语音表示。

Result: U-Codec在5Hz极低帧率下仍能保持语音的相似性和自然度，实验结果显示其在基于LLM的TTS中推理速度提升约3倍，优于高帧率编解码器。

Conclusion: 验证了使用高度压缩的5Hz离散语音标记进行快速且高保真语音合成的可行性，为低带宽、高效语音生成提供了有效解决方案。

Abstract: We propose \textbf{U-Codec}, an \textbf{U}ltra low frame-rate neural speech
\textbf{Codec} that achieves high-fidelity reconstruction and fast speech
generation at an extremely low frame-rate of 5Hz (5 frames per second). Extreme
compression at 5Hz typically leads to severe intelligibility and spectral
detail loss, we introduce a Transformer-based inter-frame long-term dependency
module and systematically explore residual vector quantization (RVQ) depth and
codebook size to identify optimal configurations. Moreover, we apply U-Codec
into a large language model (LLM)-based auto-regressive TTS model, which
leverages global and local hierarchical architecture to effectively capture
dependencies across multi-layer tokens. We extend LLM-based TTS from 3-layer
RVQ at 50Hz to 32-layer RVQ at 5Hz. Experimental results demonstrate that
U-Codec improves LLM-based TTS inference speed by around 3 $\times$ over
high-frame-rate codecs while maintaining similarity and naturalness. These
results validate the feasibility of using highly compressed 5Hz discrete tokens
for fast and high-fidelity speech synthesis.

</details>


### [290] [Schrödinger Bridge Mamba for One-Step Speech Enhancement](https://arxiv.org/abs/2510.16834)
*Jing Yang,Sirui Wang,Chao Wu,Fan Fan*

Main category: cs.SD

TL;DR: 提出了一种名为Schrödinger Bridge Mamba（SBM）的新训练-推理框架，结合Schrödinger Bridge与Mamba模型，用于生成式语音增强，在多数据集上以单步推理优于基线模型并实现最优实时因子。


<details>
  <summary>Details</summary>
Motivation: 利用Schrödinger Bridge训练范式与选择性状态空间模型Mamba之间的内在兼容性，构建更高效的生成模型训练-推理框架。

Method: 将Schrödinger Bridge训练范式与Mamba结构结合，构建SBM框架，并应用于生成式语音增强任务，支持单步快速推理。

Result: 在四个基准数据集的去噪与去混响联合任务中，SBM在仅使用单步推理的情况下优于强基线方法，并取得最佳实时因子（RTF）。

Conclusion: SBM展示了Schrödinger Bridge与选择性状态空间模型结合的潜力，为广泛生成任务提供了有前景的新方向。

Abstract: We propose Schr\"odinger Bridge Mamba (SBM), a new concept of
training-inference framework motivated by the inherent compatibility between
Schr\"odinger Bridge (SB) training paradigm and selective state-space model
Mamba. We exemplify the concept of SBM with an implementation for generative
speech enhancement. Experiments on a joint denoising and dereverberation task
using four benchmark datasets demonstrate that SBM, with only 1-step inference,
outperforms strong baselines with 1-step or iterative inference and achieves
the best real-time factor (RTF). Beyond speech enhancement, we discuss the
integration of SB paradigm and selective state-space model architecture based
on their underlying alignment, which indicates a promising direction for
exploring new deep generative models potentially applicable to a broad range of
generative tasks. Demo page: https://sbmse.github.io

</details>


### [291] [Investigating Safety Vulnerabilities of Large Audio-Language Models Under Speaker Emotional Variations](https://arxiv.org/abs/2510.16893)
*Bo-Han Feng,Chien-Feng Liu,Yu-Hsuan Li Liang,Chih-Kai Yang,Szu-Wei Fu,Zhehuai Chen,Ke-Han Lu,Sung-Feng Huang,Chao-Han Huck Yang,Yu-Chiang Frank Wang,Yun-Nung Chen,Hung-yi Lee*

Main category: cs.SD

TL;DR: 本研究系统探讨了大型音频语言模型（LALMs）在说话人情绪变化下的安全对齐问题，发现不同情绪和强度显著影响模型的安全响应，中等情绪强度风险最高。


<details>
  <summary>Details</summary>
Motivation: 尽管LALMs在多模态应用中前景广阔，但其在副语言变化（如情绪）下的安全对齐尚未被充分研究，存在潜在风险。

Method: 构建包含多种情绪和强度的恶意语音指令数据集，评估多个最先进的LALMs在不同情绪条件下的安全响应表现。

Result: 不同情绪引发不同程度的不安全响应，情绪强度的影响呈非单调性，中等强度情绪常带来最高风险。

Conclusion: 情绪变化显著影响LALMs的安全性，需设计专门的对齐策略以确保其在现实应用中的可靠部署。

Abstract: Large audio-language models (LALMs) extend text-based LLMs with auditory
understanding, offering new opportunities for multimodal applications. While
their perception, reasoning, and task performance have been widely studied,
their safety alignment under paralinguistic variation remains underexplored.
This work systematically investigates the role of speaker emotion. We construct
a dataset of malicious speech instructions expressed across multiple emotions
and intensities, and evaluate several state-of-the-art LALMs. Our results
reveal substantial safety inconsistencies: different emotions elicit varying
levels of unsafe responses, and the effect of intensity is non-monotonic, with
medium expressions often posing the greatest risk. These findings highlight an
overlooked vulnerability in LALMs and call for alignment strategies explicitly
designed to ensure robustness under emotional variation, a prerequisite for
trustworthy deployment in real-world settings.

</details>


### [292] [SAKE: Towards Editing Auditory Attribute Knowledge of Large Audio-Language Models](https://arxiv.org/abs/2510.16917)
*Chih-Kai Yang,Yen-Ting Piao,Tzu-Wen Hsu,Szu-Wei Fu,Zhehuai Chen,Ke-Han Lu,Sung-Feng Huang,Chao-Han Huck Yang,Yu-Chiang Frank Wang,Yun-Nung Chen,Hung-yi Lee*

Main category: cs.SD

TL;DR: SAKE is the first benchmark for editing auditory attribute knowledge in Large Audio-Language Models (LALMs), targeting abstract auditory attributes beyond traditional text and vision, and evaluates seven editing methods across reliability, generality, locality, and portability.


<details>
  <summary>Details</summary>
Motivation: Prior knowledge editing research focuses on text and vision; SAKE addresses the lack of benchmarks for auditory modalities, especially for abstract auditory attributes in audio-language models.

Method: The authors introduce SAKE, a benchmark that evaluates seven knowledge editing methods on two LALMs across four dimensions: reliability, generality, audio/text locality, and portability, focusing on editing abstract auditory attributes.

Result: Experiments reveal key challenges: preserving unrelated intra-attribute knowledge, generalizing edits to multimodal reasoning, and maintaining edits under sequential updates.

Conclusion: SAKE establishes a principled framework for knowledge editing in auditory modalities, enabling better maintenance and adaptation of LALMs in diverse real-world applications.

Abstract: Knowledge editing offers an efficient way to update model knowledge without
full retraining, but prior work has concentrated almost exclusively on textual
or visual modalities. We introduce SAKE, the first benchmark specifically
designed for editing auditory attribute knowledge in Large Audio-Language
Models (LALMs). Unlike factual updates, SAKE targets several abstract auditory
attributes, capturing knowledge types that go beyond conventional textual and
visual domains. We benchmark seven editing methods on two LALMs along four
dimensions: reliability, generality, audio/text locality, and portability.
Results highlight challenges such as preserving intra-attribute knowledge
unrelated to the edit, generalizing edits to multimodal reasoning, and
maintaining edits under sequential updates. SAKE provides a principled
framework to study how knowledge editing extends to the auditory modalities,
opening new directions for maintaining and adapting LALMs in more diverse
real-world scenarios.

</details>


### [293] [DDSC: Dynamic Dual-Signal Curriculum for Data-Efficient Acoustic Scene Classification under Domain Shift](https://arxiv.org/abs/2510.17345)
*Peihong Zhang,Yuxuan Liu,Rui Sang,Zhixin Li,Yiqiang Cai,Yizhou Tan,Shengchen Li*

Main category: cs.SD

TL;DR: 提出了一种动态双信号课程学习方法（DDSC），通过结合域不变性和学习进度信号，在线调整训练样本权重，提升有限标签下的跨设备声学场景分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有课程学习方法在声学场景分类中因设备差异导致域偏移问题，且静态课程忽略样本难度和学习过程的动态变化，限制了模型泛化能力。

Method: DDSC每轮计算两个动态信号：域不变性信号和学习进展信号，并通过时变调度器将其融合为每样本权重，早期优先域不变样本，后期逐步关注设备特异性样本。

Result: 在DCASE 2024 Task 1协议下，DDSC在多种基线和标签预算下均提升跨设备性能，尤其在未见设备上效果最显著。

Conclusion: DDSC是一种轻量、通用且无推理开销的课程学习策略，能有效缓解设备引起的域偏移问题，显著增强模型泛化能力。

Abstract: Acoustic scene classification (ASC) suffers from device-induced domain shift,
especially when labels are limited. Prior work focuses on curriculum-based
training schedules that structure data presentation by ordering or reweighting
training examples from easy-to-hard to facilitate learning; however, existing
curricula are static, fixing the ordering or the weights before training and
ignoring that example difficulty and marginal utility evolve with the learned
representation. To overcome this limitation, we propose the Dynamic Dual-Signal
Curriculum (DDSC), a training schedule that adapts the curriculum online by
combining two signals computed each epoch: a domain-invariance signal and a
learning-progress signal. A time-varying scheduler fuses these signals into
per-example weights that prioritize domain-invariant examples in early epochs
and progressively emphasize device-specific cases. DDSC is lightweight,
architecture-agnostic, and introduces no additional inference overhead. Under
the official DCASE 2024 Task~1 protocol, DDSC consistently improves
cross-device performance across diverse ASC baselines and label budgets, with
the largest gains on unseen-device splits.

</details>


### [294] [TopSeg: A Multi-Scale Topological Framework for Data-Efficient Heart Sound Segmentation](https://arxiv.org/abs/2510.17346)
*Peihong Zhang,Zhixin Li,Yuxuan Liu,Rui Sang,Yiqiang Cai,Yizhou Tan,Shengchen Li*

Main category: cs.SD

TL;DR: TopSeg是一种基于拓扑特征的心音分割框架，在数据效率和跨数据集泛化方面表现优异，尤其在标注数据有限时优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有的心音分割方法依赖大量标注数据且泛化能力有限，亟需提升数据效率和鲁棒性。

Method: 提出TopSeg框架，利用多尺度拓扑特征编码心音信号动态，并通过轻量级时间卷积网络（TCN）解码，结合有序和时长约束的推理步骤。

Result: 在PhysioNet 2016上训练并在CirCor上外验证，TopSeg在低数据预算下显著优于谱图和包络输入，且在全数据下仍具竞争力；消融实验证明多尺度特征和H_0、H_1结合有助于S1/S2定位和边界稳定性。

Conclusion: 拓扑感知表征能为心音分割提供强归纳偏置，支持在标注数据有限时的实际应用。

Abstract: Deep learning approaches for heart-sound (PCG) segmentation built on
time--frequency features can be accurate but often rely on large expert-labeled
datasets, limiting robustness and deployment. We present TopSeg, a topological
representation-centric framework that encodes PCG dynamics with multi-scale
topological features and decodes them using a lightweight temporal
convolutional network (TCN) with an order- and duration-constrained inference
step. To evaluate data efficiency and generalization, we train exclusively on
PhysioNet 2016 dataset with subject-level subsampling and perform external
validation on CirCor dataset. Under matched-capacity decoders, the topological
features consistently outperform spectrogram and envelope inputs, with the
largest margins at low data budgets; as a full system, TopSeg surpasses
representative end-to-end baselines trained on their native inputs under the
same budgets while remaining competitive at full data. Ablations at 10%
training confirm that all scales contribute and that combining H_0 and H_1
yields more reliable S1/S2 localization and boundary stability. These results
indicate that topology-aware representations provide a strong inductive bias
for data-efficient, cross-dataset PCG segmentation, supporting practical use
when labeled data are limited.

</details>


### [295] [Not All Deepfakes Are Created Equal: Triaging Audio Forgeries for Robust Deepfake Singer Identification](https://arxiv.org/abs/2510.17474)
*Davide Salvi,Hendrik Vincent Koops,Elio Quinton*

Main category: cs.SD

TL;DR: 提出一种两阶段管道方法，用于在高保真歌唱语音深度伪造中识别人声身份，通过先过滤低质量伪造，再在高质量音频中识别歌手，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高保真歌唱语音深度伪造的泛滥对艺术家形象和内容真实性构成威胁，亟需有效技术保护艺术家声音不被滥用。

Method: 采用两阶段管道：第一阶段使用判别模型过滤低质量伪造音频；第二阶段使用仅在真实录音上训练的模型识别剩余高质伪造和真实音频中的歌手。

Result: 实验表明，该系统在真实和合成音频上的表现均持续优于现有基线方法。

Conclusion: 该方法能有效识别高保真歌唱深度伪造中的歌手身份，为保护艺术家声音版权提供了可行的技术路径。

Abstract: The proliferation of highly realistic singing voice deepfakes presents a
significant challenge to protecting artist likeness and content authenticity.
Automatic singer identification in vocal deepfakes is a promising avenue for
artists and rights holders to defend against unauthorized use of their voice,
but remains an open research problem. Based on the premise that the most
harmful deepfakes are those of the highest quality, we introduce a two-stage
pipeline to identify a singer's vocal likeness. It first employs a
discriminator model to filter out low-quality forgeries that fail to accurately
reproduce vocal likeness. A subsequent model, trained exclusively on authentic
recordings, identifies the singer in the remaining high-quality deepfakes and
authentic audio. Experiments show that this system consistently outperforms
existing baselines on both authentic and synthetic content.

</details>


### [296] [AWARE: Audio Watermarking with Adversarial Resistance to Edits](https://arxiv.org/abs/2510.17512)
*Kosta Pavlović,Lazar Stanarević,Petar Nedić,Slavko Kovačević,Igor Djurović*

Main category: cs.SD

TL;DR: AWARE 是一种新的音频水印方法，通过对抗优化在时频域嵌入水印，避免依赖模拟攻击和手工设计的失真，实现了高鲁棒性和音频质量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的音频水印方法依赖于训练时模拟的失真集合，但这些模拟往往狭窄且易过拟合，因此需要一种更鲁棒且不依赖特定失真假设的方法。

Method: AWARE 采用对抗优化策略在时频域进行水印嵌入，并在感知预算下控制嵌入强度；检测器使用对时间顺序不敏感的结构和比特读出头（BRH），聚合时间维度上的证据以提高解码可靠性。

Result: AWARE 在多种音频编辑下保持高音频质量和语音可懂度（PESQ/STOI），且误码率（BER）持续较低，通常优于现有的代表性学习型音频水印系统。

Conclusion: AWARE 提供了一种无需依赖攻击模拟的音频水印框架，在各种编辑和时间同步破坏下仍具有强鲁棒性，为实际应用提供了更可靠的解决方案。

Abstract: Prevailing practice in learning-based audio watermarking is to pursue
robustness by expanding the set of simulated distortions during training.
However, such surrogates are narrow and prone to overfitting. This paper
presents AWARE (Audio Watermarking with Adversarial Resistance to Edits), an
alternative approach that avoids reliance on attack-simulation stacks and
handcrafted differentiable distortions. Embedding is obtained via adversarial
optimization in the time-frequency domain under a level-proportional perceptual
budget. Detection employs a time-order-agnostic detector with a Bitwise Readout
Head (BRH) that aggregates temporal evidence into one score per watermark bit,
enabling reliable watermark decoding even under desynchronization and temporal
cuts. Empirically, AWARE attains high audio quality and speech intelligibility
(PESQ/STOI) and consistently low BER across various audio edits, often
surpassing representative state-of-the-art learning-based audio watermarking
systems.

</details>


### [297] [SARSteer: Safeguarding Large Audio Language Models via Safe-Ablated Refusal Steering](https://arxiv.org/abs/2510.17633)
*Weilin Lin,Jianze Li,Hui Xiong,Li Liu*

Main category: cs.SD

TL;DR: 提出了一种名为SARSteer的推理时防御框架，用于提升大型音频-语言模型（LALMs）在安全对齐方面的能力，有效拒绝有害查询同时减少对良性语音的过度拒绝。


<details>
  <summary>Details</summary>
Motivation: 由于音频输入更容易引发有害回应，现有LLM和LVLM的安全对齐方法难以直接适用于LALMs，存在激活分布差异大和提示防御导致过度拒绝的问题。

Method: 提出SARSteer框架，利用文本导出的拒绝 steering 机制在不修改音频输入的情况下实现拒绝，并引入解耦的安全空间消融方法以减轻过度拒绝。

Result: 实验表明，SARSteer显著提高了对有害查询的拒绝率，同时保持了对良性查询的正常响应能力，优于现有方法。

Conclusion: SARSteer为LALMs的安全对齐提供了有效且原则性的解决方案，是实现音频-语言模型安全部署的重要一步。

Abstract: Large Audio-Language Models (LALMs) are becoming essential as a powerful
multimodal backbone for real-world applications. However, recent studies show
that audio inputs can more easily elicit harmful responses than text, exposing
new risks toward deployment. While safety alignment has made initial advances
in LLMs and Large Vision-Language Models (LVLMs), we find that vanilla
adaptation of these approaches to LALMs faces two key limitations: 1) LLM-based
steering fails under audio input due to the large distributional gap between
activations, and 2) prompt-based defenses induce over-refusals on benign-speech
queries. To address these challenges, we propose Safe-Ablated Refusal Steering
(SARSteer), the first inference-time defense framework for LALMs. Specifically,
SARSteer leverages text-derived refusal steering to enforce rejection without
manipulating audio inputs and introduces decomposed safe-space ablation to
mitigate over-refusal. Extensive experiments demonstrate that SARSteer
significantly improves harmful-query refusal while preserving benign responses,
establishing a principled step toward safety alignment in LALMs.

</details>


### [298] [DELULU: Discriminative Embedding Learning Using Latent Units for Speaker-Aware Self-Supervised Speech Foundational Model](https://arxiv.org/abs/2510.17662)
*Massa Baali,Rita Singh,Bhiksha Raj*

Main category: cs.SD

TL;DR: DELULU 是一种新型的说话人感知自监督语音模型，通过引入外部监督增强伪标签生成，显著提升了在说话人验证和零样本特征分析等任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督语音模型在内容相关任务上表现出色，但在捕捉用于说话人识别、说话人分割和说话人特征分析的说话人判别特征方面仍存在不足，因此需要一种更具说话人感知能力的模型。

Method: DELULU 利用 ReDimNet 提取的帧级嵌入来引导预训练过程中的 k-means 聚类，从而在伪标签生成中引入说话人判别性的归纳偏置；同时采用掩码预测和去噪的双重目标进行训练，提升模型鲁棒性和泛化能力。

Result: DELULU 在多种说话人中心任务上显著优于先前的自监督模型，在说话人验证任务中相对错误率降低高达 62%，并在性别、年龄、口音和说话人数目等零样本分析任务中取得一致性能提升。

Conclusion: DELULU 是一种强大的通用说话人感知语音编码器，即使不进行任务特定微调也能实现优越性能，为说话人相关的语音处理任务提供了有效的基础模型解决方案。

Abstract: Self-supervised speech models have achieved remarkable success on
content-driven tasks, yet they remain limited in capturing
speaker-discriminative features critical for verification, diarization, and
profiling applications. We introduce DELULU, a speaker-aware self-supervised
foundational model that addresses this limitation by integrating external
supervision into the pseudo-label generation process. DELULU leverages
frame-level embeddings from ReDimNet, a state-of-the-art speaker verification
model, to guide the k-means clustering step during pre-training, introducing a
strong speaker-discriminative inductive bias that aligns representation
learning with speaker identity. The model is trained using a dual objective
that combines masked prediction and denoising, further enhancing robustness and
generalization. DELULU significantly outperforms prior self-supervised learning
(SSL) models across a range of speaker-centric tasks, achieving up to 62%
relative improvement in equal error rate (EER) for speaker verification and
consistent gains on zero-shot profiling tasks such as gender, age, accent, and
speaker counting. Our findings demonstrate that DELULU is a strong universal
encoder for speaker-aware speech processing, enabling superior performance even
without task-specific fine-tuning.

</details>

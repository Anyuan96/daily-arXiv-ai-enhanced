<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 70]
- [cs.CL](#cs.CL) [Total: 28]
- [cs.SD](#cs.SD) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility](https://arxiv.org/abs/2512.19711)
*Md Nahid Hasan Shuvo,Moinul Hossain*

Main category: cs.CV

TL;DR: PHANTOM框架通过变形艺术生成视角依赖的物理对抗样本，能够在黑盒设置下攻击多种目标检测器，在CAV系统中引发网络级通信中断。


<details>
  <summary>Details</summary>
Motivation: 现有CAV系统依赖视觉DNN和V2X通信，但仍易受物理对抗攻击。需要揭示感知和通信层的协同漏洞。

Method: 利用变形艺术几何扭曲生成人眼正常但模型易误判的对抗样本，在CARLA仿真环境中测试不同速度/天气/光照条件，并通过SUMO-OMNeT++评估网络影响。

Result: 最优条件下攻击成功率超90%，恶劣环境仍保持60-80%效果。攻击触发距离仅6-10米，虚假紧急消息使信息峰值年龄增加68-89%。

Conclusion: PHANTOM暴露了CAV生态系统在感知和通信层面的关键漏洞，需加强针对几何扭曲攻击的防御机制。

Abstract: Connected autonomous vehicles (CAVs) rely on vision-based deep neural networks (DNNs) and low-latency (Vehicle-to-Everything) V2X communication to navigate safely and efficiently. Despite their advances, these systems remain vulnerable to physical adversarial attacks. In this paper, we introduce PHANTOM (PHysical ANamorphic Threats Obstructing connected vehicle Mobility), a novel framework for crafting and deploying perspective-dependent adversarial examples using \textit{anamorphic art}. PHANTOM exploits geometric distortions that appear natural to humans but are misclassified with high confidence by state-of-the-art object detectors. Unlike conventional attacks, PHANTOM operates in black-box settings without model access and demonstrates strong transferability across four diverse detector architectures (YOLOv5, SSD, Faster R-CNN, and RetinaNet). Comprehensive evaluation in CARLA across varying speeds, weather conditions, and lighting scenarios shows that PHANTOM achieves over 90\% attack success rate under optimal conditions and maintains 60-80\% effectiveness even in degraded environments. The attack activates within 6-10 meters of the target, providing insufficient time for safe maneuvering. Beyond individual vehicle deception, PHANTOM triggers network-wide disruption in CAV systems: SUMO-OMNeT++ co-simulation demonstrates that false emergency messages propagate through V2X links, increasing Peak Age of Information by 68-89\% and degrading safety-critical communication. These findings expose critical vulnerabilities in both perception and communication layers of CAV ecosystems.

</details>


### [2] [Generating the Past, Present and Future from a Motion-Blurred Image](https://arxiv.org/abs/2512.19817)
*SaiKiran Tedla,Kelly Zhu,Trevor Canham,Felix Taubner,Michael S. Brown,Kiriakos N. Kutulakos,David B. Lindell*

Main category: cs.CV

TL;DR: 提出新技术利用预训练视频扩散模型从单张动态模糊图像恢复复杂场景动态及预测前后时刻的视频


<details>
  <summary>Details</summary>
Motivation: 动态模糊图像虽然模糊了细节，但也编码了曝光期间的场景和相机运动信息。现有方法依赖手工先验，难以恢复复杂场景动态，且无法预测拍摄前后的事件

Method: 重用预训练视频扩散模型（基于互联网规模数据集），从模糊图像中恢复拍摄时刻的视频序列，并预测过去和未来的场景动态

Result: 方法在任务表现上优于之前方法，能泛化到挑战性的野外图像，支持相机轨迹、物体运动和动态3D场景结构等下游任务

Conclusion: 该技术首次证明可以从单张动态模糊图像中恢复复杂场景动态，并预测拍摄前后的事件，展现了预训练视频模型在这一逆问题中的强大能力

Abstract: We seek to answer the question: what can a motion-blurred image reveal about a scene's past, present, and future? Although motion blur obscures image details and degrades visual quality, it also encodes information about scene and camera motion during an exposure. Previous techniques leverage this information to estimate a sharp image from an input blurry one, or to predict a sequence of video frames showing what might have occurred at the moment of image capture. However, they rely on handcrafted priors or network architectures to resolve ambiguities in this inverse problem, and do not incorporate image and video priors on large-scale datasets. As such, existing methods struggle to reproduce complex scene dynamics and do not attempt to recover what occurred before or after an image was taken. Here, we introduce a new technique that repurposes a pre-trained video diffusion model trained on internet-scale datasets to recover videos revealing complex scene dynamics during the moment of capture and what might have occurred immediately into the past or future. Our approach is robust and versatile; it outperforms previous methods for this task, generalizes to challenging in-the-wild images, and supports downstream tasks such as recovering camera trajectories, object motion, and dynamic 3D scene structure. Code and data are available at https://blur2vid.github.io

</details>


### [3] [Learning to Refocus with Video Diffusion Models](https://arxiv.org/abs/2512.19823)
*SaiKiran Tedla,Zhoutong Zhang,Xuaner Zhang,Shumian Xin*

Main category: cs.CV

TL;DR: 本文提出了一种利用视频扩散模型实现单张散焦图像后处理重新聚焦的新方法，能够生成高质量焦点堆栈视频序列。


<details>
  <summary>Details</summary>
Motivation: 现有自动对焦系统经常无法准确捕捉目标主体，用户希望在拍摄后调整焦点。

Method: 基于视频扩散模型，从单张散焦图像生成感知准确的焦点堆栈视频序列。

Result: 方法在各种挑战性场景下的感知质量和鲁棒性均优于现有方法，同时发布了大规模真实世界智能手机采集的焦点堆栈数据集。

Conclusion: 该方法为日常摄影中更先进的焦点编辑功能铺平了道路。

Abstract: Focus is a cornerstone of photography, yet autofocus systems often fail to capture the intended subject, and users frequently wish to adjust focus after capture. We introduce a novel method for realistic post-capture refocusing using video diffusion models. From a single defocused image, our approach generates a perceptually accurate focal stack, represented as a video sequence, enabling interactive refocusing and unlocking a range of downstream applications. We release a large-scale focal stack dataset acquired under diverse real-world smartphone conditions to support this work and future research. Our method consistently outperforms existing approaches in both perceptual quality and robustness across challenging scenarios, paving the way for more advanced focus-editing capabilities in everyday photography. Code and data are available at www.learn2refocus.github.io

</details>


### [4] [RANSAC Scoring Functions: Analysis and Reality Check](https://arxiv.org/abs/2512.19850)
*A. Shekhovtsov*

Main category: cs.CV

TL;DR: 该论文重新审视了RANSAC几何拟合中的评分函数问题，通过理论分析发现MAGSAC++的推导存在问题，其评分函数实际上等同于简单的高斯-均匀似然模型，并通过实验证明各种评分函数性能相同。


<details>
  <summary>Details</summary>
Motivation: 重新评估RANSAC几何拟合中的评分函数问题，特别是分析当前表现最佳的MAGSAC++方法，探讨其理论基础是否可靠以及不同评分函数的实际性能差异。

Method: 1. 扩展几何误差到球形噪声模型；2. 考虑均匀分布异常值的混合模型；3. 分析MAGSAC++的推导问题；4. 提出使用验证集评估评分函数的实验方法。

Result: 研究发现MAGSAC++的推导不符合基本原则，其评分函数数值上等同于简单高斯-均匀似然模型；实验结果表明所有评分函数（包括使用学习分布的方法）性能相同，MAGSAC++并不优于简单对手方法。

Conclusion: 对现有最先进方法进行全面重新评估，发现MAGSAC++的实际表现被高估，这为未来改进方法或应用于其他鲁棒拟合问题提供了重要参考。

Abstract: We revisit the problem of assigning a score (a quality of fit) to candidate geometric models -- one of the key components of RANSAC for robust geometric fitting. In a non-robust setting, the ``gold standard'' scoring function, known as the geometric error, follows from a probabilistic model with Gaussian noises. We extend it to spherical noises. In a robust setting, we consider a mixture with uniformly distributed outliers and show that a threshold-based parameterization leads to a unified view of likelihood-based and robust M-estimators and associated local optimization schemes.
  Next we analyze MAGSAC++ which stands out for two reasons. First, it achieves the best results according to existing benchmarks. Second, it makes quite different modeling assumptions and derivation steps. We discovered, however that the derivation does not correspond to sound principles and the resulting score function is in fact numerically equivalent to a simple Gaussian-uniform likelihood, a basic model within the proposed framework.
  Finally, we propose an experimental methodology for evaluating scoring functions: assuming either a large validation set, or a small random validation set in expectation. We find that all scoring functions, including using a learned inlier distribution, perform identically. In particular, MAGSAC++ score is found to be neither better performing than simple contenders nor less sensitive to the choice of the threshold hyperparameter.
  Our theoretical and experimental analysis thus comprehensively revisit the state-of-the-art, which is critical for any future research seeking to improve the methods or apply them to other robust fitting problems.

</details>


### [5] [HyGE-Occ: Hybrid View-Transformation with 3D Gaussian and Edge Priors for 3D Panoptic Occupancy Prediction](https://arxiv.org/abs/2512.19871)
*Jong Wook Kim,Wonseok Roh,Ha Dam Baek,Pilhyeon Lee,Jonghyun Choi,Sangpil Kim*

Main category: cs.CV

TL;DR: 这篇文章提出了HyGE-Occ框架，通过融合3D高斯和边缘先验的混合视角变换分支，提升3D全景占用预测的几何一致性和边界感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有的3D全景占用预测方法往往难以保持精确的几何结构并捕捉3D实例的精确空间范围，这对稳健的全景分离至关重要。

Method: HyGE-Occ采用混合视角变换分支，将基于连续高斯的深度表示与离散深度仓公式相融合，同时从BEV特征中提取边缘图作为辅助信息学习边缘线索。

Result: 在Occ3D-nuScenes数据集上的广泛实验表明，HyGE-Occ优于现有工作，展现出卓越的3D几何推理能力。

Conclusion: HyGE-Occ通过混合表示和边缘先验有效解决了3D全景占用预测中的几何一致性和边界感知问题，为密集三维场景重建提供了更精确的解决方案。

Abstract: 3D Panoptic Occupancy Prediction aims to reconstruct a dense volumetric scene map by predicting the semantic class and instance identity of every occupied region in 3D space. Achieving such fine-grained 3D understanding requires precise geometric reasoning and spatially consistent scene representation across complex environments. However, existing approaches often struggle to maintain precise geometry and capture the precise spatial range of 3D instances critical for robust panoptic separation. To overcome these limitations, we introduce HyGE-Occ, a novel framework that leverages a hybrid view-transformation branch with 3D Gaussian and edge priors to enhance both geometric consistency and boundary awareness in 3D panoptic occupancy prediction. HyGE-Occ employs a hybrid view-transformation branch that fuses a continuous Gaussian-based depth representation with a discretized depth-bin formulation, producing BEV features with improved geometric consistency and structural coherence. In parallel, we extract edge maps from BEV features and use them as auxiliary information to learn edge cues. In our extensive experiments on the Occ3D-nuScenes dataset, HyGE-Occ outperforms existing work, demonstrating superior 3D geometric reasoning.

</details>


### [6] [Widget2Code: From Visual Widgets to UI Code via Multimodal LLMs](https://arxiv.org/abs/2512.19918)
*Houston H. Zhang,Tao Zhang,Baoze Lin,Yuanqi Xue,Yincheng Zhu,Huan Liu,Li Gu,Linfeng Ye,Ziqiang Wang,Xinxin Zuo,Yang Wang,Yuanhao Yu,Zhixiang Chi*

Main category: cs.CV

TL;DR: 该论文提出了Widget-to-Code任务，针对传统UI2Code方法在处理紧凑、无上下文的微界面小部件时的局限性，构建了一个图像基准测试，并开发了WidgetFactory端到端框架，显著提升了视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 传统UI2Code方法主要针对网页和移动界面，而小部件作为紧凑、无上下文的微界面，缺乏可访问的标记数据，现有方法难以可靠生成视觉一致的代码。

Method: 开发了WidgetFactory框架，包含感知层面的原子组件组装、图标检索和可视化模块，以及系统层面的框架无关的WidgetDSL领域特定语言和编译器，支持多前端实现，并通过自适应渲染模块优化空间约束。

Result: 基准测试显示，通用多模态大语言模型优于专用UI2Code方法，但仍不可靠。WidgetFactory框架显著提升了视觉保真度，为未来研究建立了强基准和统一基础设施。

Conclusion: 论文成功解决了小部件代码生成的挑战，通过联合推进感知理解和结构化代码生成，建立了有效的基准和基础设施，为Widget2Code研究奠定了基础。

Abstract: User interface to code (UI2Code) aims to generate executable code that can faithfully reconstruct a given input UI. Prior work focuses largely on web pages and mobile screens, leaving app widgets underexplored. Unlike web or mobile UIs with rich hierarchical context, widgets are compact, context-free micro-interfaces that summarize key information through dense layouts and iconography under strict spatial constraints. Moreover, while (image, code) pairs are widely available for web or mobile UIs, widget designs are proprietary and lack accessible markup. We formalize this setting as the Widget-to-Code (Widget2Code) and introduce an image-only widget benchmark with fine-grained, multi-dimensional evaluation metrics. Benchmarking shows that although generalized multimodal large language models (MLLMs) outperform specialized UI2Code methods, they still produce unreliable and visually inconsistent code. To address these limitations, we develop a baseline that jointly advances perceptual understanding and structured code generation. At the perceptual level, we follow widget design principles to assemble atomic components into complete layouts, equipped with icon retrieval and reusable visualization modules. At the system level, we design an end-to-end infrastructure, WidgetFactory, which includes a framework-agnostic widget-tailored domain-specific language (WidgetDSL) and a compiler that translates it into multiple front-end implementations (e.g., React, HTML/CSS). An adaptive rendering module further refines spatial dimensions to satisfy compactness constraints. Together, these contributions substantially enhance visual fidelity, establishing a strong baseline and unified infrastructure for future Widget2Code research.

</details>


### [7] [DDAVS: Disentangled Audio Semantics and Delayed Bidirectional Alignment for Audio-Visual Segmentation](https://arxiv.org/abs/2512.20117)
*Jingqi Tian,Yiheng Du,Haoji Zhang,Yuji Wang,Isaac Ning Lee,Xulong Bai,Tianrui Zhu,Jingxuan Niu,Yansong Tang*

Main category: cs.CV

TL;DR: DDAVS方法通过解构音频语义和延迟双向对齐框架，有效解决了音频-视觉分割中多源纠缠和视听不对齐问题，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的音频-视觉分割方法存在多源纠缠和视听不对齐问题，导致对较响或较大物体的偏向，同时忽略较弱、较小或共现的声源。

Method: DDAVS采用可学习查询从音频原型记忆库中提取音频语义并锚定在结构化语义空间中，通过对比学习增强区分性和鲁棒性；引入延迟模态交互的双重交叉注意力机制来缓解视听不对齐。

Result: 在AVS-Objects和VPO基准测试上的广泛实验表明，DDAVS在单源、多源和多实例场景中 consistently 优于现有方法。

Conclusion: DDAVS框架在具有挑战性的真实世界音频-视觉分割条件下表现出有效的泛化能力。

Abstract: Audio-Visual Segmentation (AVS) aims to localize sound-producing objects at the pixel level by jointly leveraging auditory and visual information. However, existing methods often suffer from multi-source entanglement and audio-visual misalignment, which lead to biases toward louder or larger objects while overlooking weaker, smaller, or co-occurring sources. To address these challenges, we propose DDAVS, a Disentangled Audio Semantics and Delayed Bidirectional Alignment framework. To mitigate multi-source entanglement, DDAVS employs learnable queries to extract audio semantics and anchor them within a structured semantic space derived from an audio prototype memory bank. This is further optimized through contrastive learning to enhance discriminability and robustness. To alleviate audio-visual misalignment, DDAVS introduces dual cross-attention with delayed modality interaction, improving the robustness of multimodal alignment. Extensive experiments on the AVS-Objects and VPO benchmarks demonstrate that DDAVS consistently outperforms existing approaches, exhibiting strong performance across single-source, multi-source, and multi-instance scenarios. These results validate the effectiveness and generalization ability of our framework under challenging real-world audio-visual segmentation conditions. Project page: https://trilarflagz.github.io/DDAVS-page/

</details>


### [8] [Unified Brain Surface and Volume Registration](https://arxiv.org/abs/2512.19928)
*S. Mazdak Abulnaga,Andrew Hoopes,Malte Hoffmann,Robin Magnet,Maks Ovsjanikov,Lilla Zöllei,John Guttag,Bruce Fischl,Adrian Dalca*

Main category: cs.CV

TL;DR: NeurAlign是一种深度学习方法，实现了多部位脑MRI影像的统一配准，显著提升了精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统的脑MRI配准方法将体积和表面分割处理，导致不一致性。

Method: NeurAlign采用生成中间球面坐标空间，统一表示体积和表面，结合深度学习实现整体对齐。

Result: 实验表明，它优于其他方法：Dice得分提高达7分，速度提升多个数量级，且仅需MRI扫描。

Conclusion: NeurAlign凭借强大的准确性、高速推理和易于使用性，为联合皮质和皮层下配准树立了新的标准。

Abstract: Accurate registration of brain MRI scans is fundamental for cross-subject analysis in neuroscientific studies. This involves aligning both the cortical surface of the brain and the interior volume. Traditional methods treat volumetric and surface-based registration separately, which often leads to inconsistencies that limit downstream analyses. We propose a deep learning framework, NeurAlign, that registers $3$D brain MRI images by jointly aligning both cortical and subcortical regions through a unified volume-and-surface-based representation. Our approach leverages an intermediate spherical coordinate space to bridge anatomical surface topology with volumetric anatomy, enabling consistent and anatomically accurate alignment. By integrating spherical registration into the learning, our method ensures geometric coherence between volume and surface domains. In a series of experiments on both in-domain and out-of-domain datasets, our method consistently outperforms both classical and machine learning-based registration methods -- improving the Dice score by up to 7 points while maintaining regular deformation fields. Additionally, it is orders of magnitude faster than the standard method for this task, and is simpler to use because it requires no additional inputs beyond an MRI scan. With its superior accuracy, fast inference, and ease of use, NeurAlign sets a new standard for joint cortical and subcortical registration.

</details>


### [9] [Vehicle-centric Perception via Multimodal Structured Pre-training](https://arxiv.org/abs/2512.19934)
*Wentao Wu,Xiao Wang,Chenglong Li,Jin Tang,Bin Luo*

Main category: cs.CV

TL;DR: 该论文提出VehicleMAE-V2，一个针对车辆感知的预训练大模型，通过结合车辆相关的多模态结构化先验知识来改进掩码令牌重建过程，从而提升车辆感知表示的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在预训练阶段缺乏对车辆相关知识的有效学习，导致在建模通用车辆感知表示方面能力不足。为了解决这个问题，需要一种能够充分利用车辆结构化先验知识的预训练方法。

Method: 设计了三个模块：对称引导掩码模块（SMM）利用车辆对称性选择高质量掩码补丁；轮廓引导表示模块（CRM）通过最小化轮廓特征与重建特征的分布差异来保留整体结构信息；语义引导表示模块（SRM）通过对比学习和跨模态蒸馏来解决语义理解不足导致的特征混淆。

Result: 构建了包含约400万车辆图像和12,693条文本描述的大规模数据集Autobot4M，并在五个下游任务上的广泛实验证明了VehicleMAE-V2的优越性能。

Conclusion: VehicleMAE-V2通过有效利用车辆的多模态结构化先验知识，显著提升了车辆感知表示的泛化能力，为车辆中心的智能系统提供了更强大的基础模型。

Abstract: Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches lack effective learning of vehicle-related knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model's capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the probability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2.

</details>


### [10] [Block-Recurrent Dynamics in Vision Transformers](https://arxiv.org/abs/2512.19941)
*Mozes Jacobs,Thomas Fel,Richard Hakim,Alessandra Brondetta,Demba Ba,T. Andy Keller*

Main category: cs.CV

TL;DR: 该论文提出了块递归假设(BRH)，认为Transformer结构中的深度不是确有必要，可以重新通过少量循环块来重现大部分模型的性能。训练了一个Raptor模型来验证假设，DINOv2在2个block下达到96%的正确率。将深度视为动态系统，观察到了token动力学现象和低维吸引子的收敛行为。


<details>
  <summary>Details</summary>
Motivation: 虽然Vision Transformer(ViT)成为视觉主干网络，但对其计算动态性的机理解释仍然缺乏。尽管架构有动态结构的线索，但没有统一框架将Transformer深度视为良好描述的流。

Method: 引入了块递归假设(BRH)，通过训练块递归代理模型Raptor来测试假设。观察层间表示相似矩阵，分析动态可解释性，探索token动力学和收敛行为。

Result: 在小规模上证明随机深度和训练可促进循环结构，并能够准确拟合Raptor。实证证明BRH的存在，一个Raptor模型在仅2个block下恢复了DINOv2 ImageNet-1k线性探测96%的准确性。发现了token特定动态和低维吸引子收敛。

Conclusion: 研究发现沿着ViT深度出现了紧凑的递归程序，指向低复杂性规范解，使得这些模型可以通过系统动力学原理进行分析。

Abstract: As Vision Transformers (ViTs) become standard vision backbones, a mechanistic account of their computational phenomenology is essential. Despite architectural cues that hint at dynamical structure, there is no settled framework that interprets Transformer depth as a well-characterized flow. In this work, we introduce the Block-Recurrent Hypothesis (BRH), arguing that trained ViTs admit a block-recurrent depth structure such that the computation of the original $L$ blocks can be accurately rewritten using only $k \ll L$ distinct blocks applied recurrently. Across diverse ViTs, between-layer representational similarity matrices suggest few contiguous phases. To determine whether these phases reflect genuinely reusable computation, we train block-recurrent surrogates of pretrained ViTs: Recurrent Approximations to Phase-structured TransfORmers (Raptor). In small-scale, we demonstrate that stochastic depth and training promote recurrent structure and subsequently correlate with our ability to accurately fit Raptor. We then provide an empirical existence proof for BRH by training a Raptor model to recover $96\%$ of DINOv2 ImageNet-1k linear probe accuracy in only 2 blocks at equivalent computational cost. Finally, we leverage our hypothesis to develop a program of Dynamical Interpretability. We find i) directional convergence into class-dependent angular basins with self-correcting trajectories under small perturbations, ii) token-specific dynamics, where cls executes sharp late reorientations while patch tokens exhibit strong late-stage coherence toward their mean direction, and iii) a collapse to low rank updates in late depth, consistent with convergence to low-dimensional attractors. Altogether, we find a compact recurrent program emerges along ViT depth, pointing to a low-complexity normative solution that enables these models to be studied through principled dynamical systems analysis.

</details>


### [11] [SE360: Semantic Edit in 360$^\circ$ Panoramas via Hierarchical Data Construction](https://arxiv.org/abs/2512.19943)
*Haoyi Zhong,Fang-Lue Zhang,Andrew Chalmers,Taehyun Rhee*

Main category: cs.CV

TL;DR: SE360是一个用于360°全景图像多条件引导物体编辑的框架，通过自主数据生成管道和改进的扩散模型实现高质量编辑效果。


<details>
  <summary>Details</summary>
Motivation: 现有的指令式图像编辑方法在处理360°全景图像时效果不佳，在等距柱状投影和透视视图中会产生不合理的编辑结果。

Method: 提出SE360框架，包括自主粗到细数据生成管道（使用视觉语言模型和自适应投影调整）、两阶段数据精炼策略，以及基于Transformer的扩散模型进行多条件引导编辑。

Result: 实验证明该方法在视觉质量和语义准确性方面均优于现有方法，能够处理未标注的全景图像并保证几何一致性。

Conclusion: SE360框架有效解决了360°全景图像编辑的挑战，通过创新的数据生成和训练策略实现了高质量的物体编辑效果。

Abstract: While instruction-based image editing is emerging, extending it to 360$^\circ$ panoramas introduces additional challenges. Existing methods often produce implausible results in both equirectangular projections (ERP) and perspective views. To address these limitations, we propose SE360, a novel framework for multi-condition guided object editing in 360$^\circ$ panoramas. At its core is a novel coarse-to-fine autonomous data generation pipeline without manual intervention. This pipeline leverages a Vision-Language Model (VLM) and adaptive projection adjustment for hierarchical analysis, ensuring the holistic segmentation of objects and their physical context. The resulting data pairs are both semantically meaningful and geometrically consistent, even when sourced from unlabeled panoramas. Furthermore, we introduce a cost-effective, two-stage data refinement strategy to improve data realism and mitigate model overfitting to erase artifacts. Based on the constructed dataset, we train a Transformer-based diffusion model to allow flexible object editing guided by text, mask, or reference image in 360$^\circ$ panoramas. Our experiments demonstrate that our method outperforms existing methods in both visual quality and semantic accuracy.

</details>


### [12] [How Much 3D Do Video Foundation Models Encode?](https://arxiv.org/abs/2512.19949)
*Zixuan Huang,Xiang Li,Zhaoyang Lv,James M. Rehg*

Main category: cs.CV

TL;DR: 该论文提出了首个模型无关框架，通过浅层读取从视频基础模型的特征中估计多种3D属性，量化其3D理解能力，发现最先进的视频生成模型即使未经3D数据训练也展现了强大的3D场景理解能力。


<details>
  <summary>Details</summary>
Motivation: 研究大规模视频数据上预训练的视频基础模型是否能自然产生全局3D理解能力，填补现有研究在此领域的空白。

Method: 提出模型无关的框架，通过浅层读取机制从各种视频基础模型的特征中估计多个3D属性，系统性评估模型的3D感知能力。

Result: 发现先进视频生成模型对3D物体和场景有深刻理解，这种理解甚至超过专门为3D任务训练的大型专家模型。

Conclusion: 研究结果为构建可扩展的3D模型提供了宝贵观察，表明视频基础模型具有强大的隐含3D理解能力。

Abstract: Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs. Our study presents meaningful findings regarding the 3D awareness of VidFMs on multiple axes. In particular, we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes, despite not being trained on any 3D data. Such understanding can even surpass that of large expert models specifically trained for 3D tasks. Our findings, together with the 3D benchmarking of major VidFMs, provide valuable observations for building scalable 3D models.

</details>


### [13] [HistoWAS: A Pathomics Framework for Large-Scale Feature-Wide Association Studies of Tissue Topology and Patient Outcomes](https://arxiv.org/abs/2512.19954)
*Yuechen Yang,Junlin Guo,Yanfan Zhu,Jialin Yue,Junchao Zhu,Yu Wang,Shilin Zhao,Haichun Yang,Xingyi Guo,Jovan Tanevski,Laura Barisoni,Avi Z. Rosenberg,Yuankai Huo*

Main category: cs.CV

TL;DR: 提出了一种名为HistoWAS的计算框架，通过整合拓扑和空间特征，将组织空间结构与临床结果联系起来。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在量化组织微环境和空间相互作用方面的不足，缺乏有效工具来关联组织特征与临床参数。

Method: 结合了GIS点模式分析的30个拓扑和空间特征，以及基于PheWAS的关联研究引擎，进行大规模单变量回归分析。

Result: 在KPMP项目的385张WSI上验证了102个特征（包括72个传统特征和30个空间特征），代码和数据已开源。

Conclusion: HistoWAS通过空间特征增强组织分析，为生物标志物发现和组织特性研究提供了新工具。

Abstract: High-throughput "pathomic" analysis of Whole Slide Images (WSIs) offers new opportunities to study tissue characteristics and for biomarker discovery. However, the clinical relevance of the tissue characteristics at the micro- and macro-environment level is limited by the lack of tools that facilitate the measurement of the spatial interaction of individual structure characteristics and their association with clinical parameters. To address these challenges, we introduce HistoWAS (Histology-Wide Association Study), a computational framework designed to link tissue spatial organization to clinical outcomes. Specifically, HistoWAS implements (1) a feature space that augments conventional metrics with 30 topological and spatial features, adapted from Geographic Information Systems (GIS) point pattern analysis, to quantify tissue micro-architecture; and (2) an association study engine, inspired by Phenome-Wide Association Studies (PheWAS), that performs mass univariate regression for each feature with statistical correction. As a proof of concept, we applied HistoWAS to analyze a total of 102 features (72 conventional object-level features and our 30 spatial features) using 385 PAS-stained WSIs from 206 participants in the Kidney Precision Medicine Project (KPMP). The code and data have been released to https://github.com/hrlblab/histoWAS.

</details>


### [14] [WSD-MIL: Window Scale Decay Multiple Instance Learning for Whole Slide Image Classification](https://arxiv.org/abs/2512.19982)
*Le Feng,Li Xiao*

Main category: cs.CV

TL;DR: 论文提出了WSD-MIL方法，通过窗口尺度衰减注意力模块和多尺度局部关系捕捉机制，解决了Transformer在计算病理学中计算复杂度高和固定尺度注意力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的MIL方法主要关注特征提取和聚合策略，忽视了WSI中实例间的复杂语义关系。Transformer方法虽然能建模实例依赖，但二次计算复杂度限制了其在大规模WSI上的可扩展性，且固定尺度注意力机制难以适应不同WSI中肿瘤区域尺度的显著变化。

Method: WSD-MIL包含两个核心模块：1）窗口尺度衰减注意力模块，采用基于聚类的采样策略降低计算成本，并通过逐步衰减注意力窗口尺度来捕捉多尺度局部实例关系；2）基于压缩激励的区域门控模块，动态调整窗口权重以增强全局信息建模。

Result: 在CAMELYON16和TCGA-BRCA数据集上实现了最先进的性能，同时减少了62%的计算内存消耗。

Conclusion: WSD-MIL有效提升了计算病理学中多尺度肿瘤区域建模能力，在保持高性能的同时显著提高了计算效率，为解决WSI分析中的计算复杂度和尺度适应性问题提供了可行方案。

Abstract: In recent years, the integration of pre-trained foundational models with multiple instance learning (MIL) has improved diagnostic accuracy in computational pathology. However, existing MIL methods focus on optimizing feature extractors and aggregation strategies while overlooking the complex semantic relationships among instances within whole slide image (WSI). Although Transformer-based MIL approaches aiming to model instance dependencies, the quadratic computational complexity limits their scalability to large-scale WSIs. Moreover, due to the pronounced variations in tumor region scales across different WSIs, existing Transformer-based methods employing fixed-scale attention mechanisms face significant challenges in precisely capturing local instance correlations and fail to account for the distance-based decay effect of patch relevance. To address these challenges, we propose window scale decay MIL (WSD-MIL), designed to enhance the capacity to model tumor regions of varying scales while improving computational efficiency. WSD-MIL comprises: 1) a window scale decay based attention module, which employs a cluster-based sampling strategy to reduce computational costs while progressively decaying attention window-scale to capture local instance relationships at varying scales; and 2) a squeeze-and-excitation based region gate module, which dynamically adjusts window weights to enhance global information modeling. Experimental results demonstrate that WSD-MIL achieves state-of-the-art performance on the CAMELYON16 and TCGA-BRCA datasets while reducing 62% of the computational memory. The code will be publicly available.

</details>


### [15] [A Novel CNN Gradient Boosting Ensemble for Guava Disease Detection](https://arxiv.org/abs/2512.19989)
*Tamim Ahasan Rijon,Yeasin Arafath*

Main category: cs.CV

TL;DR: 基于CNN与传统机器学习方法相结合的集成模型，用于检测孟加拉国本地种植的番石榴疾病，实现高达99.99%的分类准确率


<details>
  <summary>Details</summary>
Motivation: 孟加拉国作为重要农业国，番石榴是其重要热带水果，但炭疽病和果蝇感染会降低其品质和产量，早期疾病检测专家系统可减少损失

Method: 使用来自孟加拉国Rajshahi和Pabna种植园的番石榴果实图像数据集，提出CNN机器学习与梯度提升机相结合的集成模型

Result: 实现了约99.99%的最高分类准确率，CNN-ML级联框架展现出强大的高精度疾病检测能力

Conclusion: 该框架适用于实时农业监测系统，能有效识别番石榴疾病

Abstract: As a significant agricultural country, Bangladesh utilizes its fertile land for guava cultivation and dedicated labor to boost its economic development. In a nation like Bangladesh, enhancing guava production and agricultural practices plays a crucial role in its economy. Anthracnose and fruit fly infection can lower the quality and productivity of guava, a crucial tropical fruit. Expert systems that detect diseases early can reduce losses and safeguard the harvest. Images of guava fruits classified into the Healthy, Fruit Flies, and Anthracnose classes are included in the Guava Fruit Disease Dataset 2024 (GFDD24), which comes from plantations in Rajshahi and Pabna, Bangladesh. This study aims to create models using CNN alongside traditional machine learning techniques that can effectively identify guava diseases in locally cultivated varieties in Bangladesh. In order to achieve the highest classification accuracy of approximately 99.99% for the guava dataset, we propose utilizing ensemble models that combine CNNML with Gradient Boosting Machine. In general, the CNN-ML cascade framework exhibits strong, high-accuracy guava disease detection that is appropriate for real-time agricultural monitoring systems.

</details>


### [16] [A Dual-Branch Local-Global Framework for Cross-Resolution Land Cover Mapping](https://arxiv.org/abs/2512.19990)
*Peng Gao,Ke Li,Di Wang,Yongshan Zhu,Yiming Zhang,Xuemei Luo,Yifeng Wang*

Main category: cs.CV

TL;DR: DDTM是一个双分支弱监督框架，用于解决跨分辨率土地覆盖映射中的严重分辨率不匹配问题，通过扩散模型细化局部语义和transformer确保全局一致性，在基准测试中达到66.52% mIoU的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的弱监督方法在将细粒度空间结构与粗标签对齐方面存在困难，导致噪声监督和映射精度下降，需要有效解决跨分辨率映射中的分辨率不匹配问题。

Method: 提出DDTM双分支框架：扩散分支在粗监督下逐步细化局部语义，transformer分支强制执行长距离上下文一致性；设计伪标签置信度评估模块来减轻噪声并选择性利用可靠监督信号。

Result: 在Chesapeake Bay基准测试中实现了66.52% mIoU，显著优于先前的弱监督方法，建立了新的最先进性能。

Conclusion: DDTM通过解耦局部语义细化和全局上下文推理，有效解决了跨分辨率土地覆盖映射的挑战，证明了双分支框架在弱监督学习中的有效性。

Abstract: Cross-resolution land cover mapping aims to produce high-resolution semantic predictions from coarse or low-resolution supervision, yet the severe resolution mismatch makes effective learning highly challenging. Existing weakly supervised approaches often struggle to align fine-grained spatial structures with coarse labels, leading to noisy supervision and degraded mapping accuracy. To tackle this problem, we propose DDTM, a dual-branch weakly supervised framework that explicitly decouples local semantic refinement from global contextual reasoning. Specifically, DDTM introduces a diffusion-based branch to progressively refine fine-scale local semantics under coarse supervision, while a transformer-based branch enforces long-range contextual consistency across large spatial extents. In addition, we design a pseudo-label confidence evaluation module to mitigate noise induced by cross-resolution inconsistencies and to selectively exploit reliable supervisory signals. Extensive experiments demonstrate that DDTM establishes a new state-of-the-art on the Chesapeake Bay benchmark, achieving 66.52\% mIoU and substantially outperforming prior weakly supervised methods. The code is available at https://github.com/gpgpgp123/DDTM.

</details>


### [17] [Few-Shot-Based Modular Image-to-Video Adapter for Diffusion Models](https://arxiv.org/abs/2512.20000)
*Zhenhao Li,Shaohan Yi,Zheng Liu,Leonartinus Gao,Minh Ngoc Le,Ambrose Ling,Zhuoran Wang,Md Amirul Islam,Zhixiang Chi,Yuanhao Yu*

Main category: cs.CV

TL;DR: 针对扩散模型在高维视频生成中数据稀缺导致记忆化倾向和泛化能力不足的问题，本文提出轻量级模块化图像转视频适配器MIVA，通过低数据量训练单个运动模式，实现精确运动控制并提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像动画领域面临两大挑战：视频信号高维度导致训练数据稀缺（引发模型记忆化而非遵循提示词），以及难以泛化到训练集未见过的新运动模式。现有方法对少量数据下的调优研究不足。

Method: 提出MIVA（模块化图像转视频适配器），作为预训练扩散模型的轻量子网络，每个模块仅学习单一运动模式，支持并行扩展。仅需约10个样本即可在消费级GPU上高效训练，推理时通过选择模块组合指定运动，无需提示词工程。

Result: 大量实验表明，MIVA在保持甚至超越大数据集训练模型生成质量的同时，实现了更精确的运动控制。

Conclusion: MIVA通过模块化设计有效解决了扩散模型在视频生成中的数据效率与泛化问题，为低资源条件下的可控视频生成提供了可行方案。

Abstract: Diffusion models (DMs) have recently achieved impressive photorealism in image and video generation. However, their application to image animation remains limited, even when trained on large-scale datasets. Two primary challenges contribute to this: the high dimensionality of video signals leads to a scarcity of training data, causing DMs to favor memorization over prompt compliance when generating motion; moreover, DMs struggle to generalize to novel motion patterns not present in the training set, and fine-tuning them to learn such patterns, especially using limited training data, is still under-explored. To address these limitations, we propose Modular Image-to-Video Adapter (MIVA), a lightweight sub-network attachable to a pre-trained DM, each designed to capture a single motion pattern and scalable via parallelization. MIVAs can be efficiently trained on approximately ten samples using a single consumer-grade GPU. At inference time, users can specify motion by selecting one or multiple MIVAs, eliminating the need for prompt engineering. Extensive experiments demonstrate that MIVA enables more precise motion control while maintaining, or even surpassing, the generation quality of models trained on significantly larger datasets.

</details>


### [18] [PaveSync: A Unified and Comprehensive Dataset for Pavement Distress Analysis and Classification](https://arxiv.org/abs/2512.20011)
*Blessing Agyei Kyem,Joshua Kofi Asamoah,Anthony Dontoh,Andrews Danyo,Eugene Denteh,Armstrong Aboah*

Main category: cs.CV

TL;DR: 该论文提出了首个全球代表性的路面缺陷检测基准数据集，整合了来自7个国家52,747张图像的多个公共数据源，包含135,277个边界框标注，涵盖13种缺陷类型，解决了现有数据集在标注风格和格式上的不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有路面缺陷检测数据集在标注风格、缺陷类型定义和格式上存在差异，限制了它们集成用于统一训练，导致模型难以在多样化的真实世界条件下泛化。

Method: 通过整合多个公开数据源，创建一个标准化的基准数据集，统一类别定义和标注格式。使用最先进的目标检测模型（如YOLOv8-YOLOv12、Faster R-CNN和DETR）进行基准测试。

Result: 基准测试显示，这些模型在多样化场景下实现了有竞争力的性能，包括对新环境的零样本迁移能力。

Conclusion: 该数据集为路面缺陷检测提供了首个全球代表性基准，实现了模型的公平比较，并促进了在真实世界条件下的泛化能力。

Abstract: Automated pavement defect detection often struggles to generalize across diverse real-world conditions due to the lack of standardized datasets. Existing datasets differ in annotation styles, distress type definitions, and formats, limiting their integration for unified training. To address this gap, we introduce a comprehensive benchmark dataset that consolidates multiple publicly available sources into a standardized collection of 52747 images from seven countries, with 135277 bounding box annotations covering 13 distinct distress types. The dataset captures broad real-world variation in image quality, resolution, viewing angles, and weather conditions, offering a unique resource for consistent training and evaluation. Its effectiveness was demonstrated through benchmarking with state-of-the-art object detection models including YOLOv8-YOLOv12, Faster R-CNN, and DETR, which achieved competitive performance across diverse scenarios. By standardizing class definitions and annotation formats, this dataset provides the first globally representative benchmark for pavement defect detection and enables fair comparison of models, including zero-shot transfer to new environments.

</details>


### [19] [SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images](https://arxiv.org/abs/2512.20013)
*Zepeng Xin,Kaiyu Li,Luodi Chen,Wanchen Li,Yuchen Xiao,Hui Qiao,Weizhan Zhang,Deyu Meng,Xiangyong Cao*

Main category: cs.CV

TL;DR: LaSeRS is the first large-scale dataset for complex language-guided segmentation in remote sensing, addressing hierarchical granularity, target multiplicity, reasoning requirements, and linguistic variability. The paper also introduces SegEarth-R2, an MLLM architecture with spatial attention supervision and flexible segmentation queries, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current models fail at complex geospatial scenarios like multi-target segmentation and implicit intent interpretation. Existing datasets oversimplify, leading to sensitivity-prone real-world models. There's a critical need for comprehensive benchmarks and models that handle complex language-to-pixel grounding in remote sensing.

Method: 1) Created LaSeRS dataset covering four critical dimensions of language-guided segmentation
2) Proposed SegEarth-R2 MLLM architecture with:
   - Spatial attention supervision for small object localization
   - Flexible segmentation query mechanism for single/multi-target scenarios

Result: SegEarth-R2 achieves outstanding performance on LaSeRS and other benchmarks, establishing a powerful baseline for next-generation geospatial segmentation. The model effectively handles complex segmentation tasks that previous methods failed at.

Conclusion: LaSeRS dataset addresses the critical gap in complex geospatial reasoning benchmarks, while SegEarth-R2 provides an effective solution for comprehensive language-guided segmentation in remote sensing, demonstrating significant improvements over existing approaches.

Abstract: Effectively grounding complex language to pixels in remote sensing (RS) images is a critical challenge for applications like disaster response and environmental monitoring. Current models can parse simple, single-target commands but fail when presented with complex geospatial scenarios, e.g., segmenting objects at various granularities, executing multi-target instructions, and interpreting implicit user intent. To drive progress against these failures, we present LaSeRS, the first large-scale dataset built for comprehensive training and evaluation across four critical dimensions of language-guided segmentation: hierarchical granularity, target multiplicity, reasoning requirements, and linguistic variability. By capturing these dimensions, LaSeRS moves beyond simple commands, providing a benchmark for complex geospatial reasoning. This addresses a critical gap: existing datasets oversimplify, leading to sensitivity-prone real-world models. We also propose SegEarth-R2, an MLLM architecture designed for comprehensive language-guided segmentation in RS, which directly confronts these challenges. The model's effectiveness stems from two key improvements: (1) a spatial attention supervision mechanism specifically handles the localization of small objects and their components, and (2) a flexible and efficient segmentation query mechanism that handles both single-target and multi-target scenarios. Experimental results demonstrate that our SegEarth-R2 achieves outstanding performance on LaSeRS and other benchmarks, establishing a powerful baseline for the next generation of geospatial segmentation. All data and code will be released at https://github.com/earth-insights/SegEarth-R2.

</details>


### [20] [A Contextual Analysis of Driver-Facing and Dual-View Video Inputs for Distraction Detection in Naturalistic Driving Environments](https://arxiv.org/abs/2512.20025)
*Anthony Dontoh,Stephanie Ivey,Armstrong Aboah*

Main category: cs.CV

TL;DR: 本研究验证了在驾驶分心检测中融合驾驶者视角和道路视角视频的效果，发现多视角输入的性能提升取决于模型架构设计，简单的双视角叠加可能因表示冲突导致性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有分心驾驶检测模型主要依赖驾驶者视角，忽视了环境上下文对驾驶行为的重要影响，因此研究是否结合道路视角能提升自然驾驶条件下的检测精度。

Method: 使用真实驾驶场景中的同步双摄像头数据，在三种时空动作识别架构（SlowFast-R50、X3D-M、SlowOnly-R50）上对比单视角和双视图叠加配置的性能。

Result: SlowOnly模型在双视角输入下准确率提升9.8%，但SlowFast模型因表示冲突导致准确率下降7.2%，表明架构需专门支持多视图融合。

Conclusion: 单纯增加视觉上下文不足以保证性能提升，未来多模态驾驶监控系统需要融合感知的架构设计。

Abstract: Despite increasing interest in computer vision-based distracted driving detection, most existing models rely exclusively on driver-facing views and overlook crucial environmental context that influences driving behavior. This study investigates whether incorporating road-facing views alongside driver-facing footage improves distraction detection accuracy in naturalistic driving conditions. Using synchronized dual-camera recordings from real-world driving, we benchmark three leading spatiotemporal action recognition architectures: SlowFast-R50, X3D-M, and SlowOnly-R50. Each model is evaluated under two input configurations: driver-only and stacked dual-view. Results show that while contextual inputs can improve detection in certain models, performance gains depend strongly on the underlying architecture. The single-pathway SlowOnly model achieved a 9.8 percent improvement with dual-view inputs, while the dual-pathway SlowFast model experienced a 7.2 percent drop in accuracy due to representational conflicts. These findings suggest that simply adding visual context is not sufficient and may lead to interference unless the architecture is specifically designed to support multi-view integration. This study presents one of the first systematic comparisons of single- and dual-view distraction detection models using naturalistic driving data and underscores the importance of fusion-aware design for future multimodal driver monitoring systems.

</details>


### [21] [MAPI-GNN: Multi-Activation Plane Interaction Graph Neural Network for Multimodal Medical Diagnosis](https://arxiv.org/abs/2512.20026)
*Ziwei Qin,Xuhui Song,Deqing Huang,Na Qin,Jun Li*

Main category: cs.CV

TL;DR: 提出MAPI-GNN模型，通过从语义解缠的特征子空间学习多面图配置，改进了基于单一静态图的医疗诊断图谱神经网络方法，在两个包含1300多个样本的任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图谱神经网络在医疗诊断中过度依赖单一静态图，无法建模患者特定的病理关系，限制了诊断效果。

Method: 使用多维度判别器发现潜在图感知模式，动态构建激活图堆栈，通过关系融合引擎聚合多面配置进行诊断。

Result: 在两个多样化任务（超过1300个患者样本）上的实验表明，MAPI-GNN显著优于最先进方法。

Conclusion: MAPI-GNN通过动态多面图建模有效提升了医疗诊断的准确性和鲁棒性，为多模态医疗诊断提供了新的解决方案。

Abstract: Graph neural networks are increasingly applied to multimodal medical diagnosis for their inherent relational modeling capabilities. However, their efficacy is often compromised by the prevailing reliance on a single, static graph built from indiscriminate features, hindering the ability to model patient-specific pathological relationships. To this end, the proposed Multi-Activation Plane Interaction Graph Neural Network (MAPI-GNN) reconstructs this single-graph paradigm by learning a multifaceted graph profile from semantically disentangled feature subspaces. The framework first uncovers latent graph-aware patterns via a multi-dimensional discriminator; these patterns then guide the dynamic construction of a stack of activation graphs; and this multifaceted profile is finally aggregated and contextualized by a relational fusion engine for a robust diagnosis. Extensive experiments on two diverse tasks, comprising over 1300 patient samples, demonstrate that MAPI-GNN significantly outperforms state-of-the-art methods.

</details>


### [22] [$\text{H}^2$em: Learning Hierarchical Hyperbolic Embeddings for Compositional Zero-Shot Learning](https://arxiv.org/abs/2512.20029)
*Lin Li,Jiahui Li,Jiaming Lei,Jun Xiao,Feifei Shao,Long Chen*

Main category: cs.CV

TL;DR: H2em是一个为组合零样本学习设计的框架，通过双曲线嵌入来建模层次结构，解决现有方法在真实大规模场景下泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有组合零样本学习方法大多忽略丰富的层次结构（如原始概念的语义层次和概念与组合之间的层次关系），且基于欧几里得空间的模型无法有效处理大规模层次结构，导致泛化能力受限。

Method: 提出H2em框架，利用双曲线几何的特性嵌入层次结构；设计双层次蕴含损失和判别对齐损失来结构化嵌入空间；开发双曲线跨模态注意力机制实现实例感知的跨模态信息融合。

Result: 在三个基准测试上的广泛实验表明，H2em在封闭世界和开放世界场景下均达到了新的最先进水平。

Conclusion: H2em通过双曲线几何有效建模组合零样本学习中的层次结构，解决了现有方法在大规模层次结构下的泛化瓶颈，为现实世界的应用提供了可行方案。

Abstract: Compositional zero-shot learning (CZSL) aims to recognize unseen state-object compositions by generalizing from a training set of their primitives (state and object). Current methods often overlook the rich hierarchical structures, such as the semantic hierarchy of primitives (e.g., apple fruit) and the conceptual hierarchy between primitives and compositions (e.g, sliced apple apple). A few recent efforts have shown effectiveness in modeling these hierarchies through loss regularization within Euclidean space. In this paper, we argue that they fail to scale to the large-scale taxonomies required for real-world CZSL: the space's polynomial volume growth in flat geometry cannot match the exponential structure, impairing generalization capacity. To this end, we propose H2em, a new framework that learns Hierarchical Hyperbolic EMbeddings for CZSL. H2em leverages the unique properties of hyperbolic geometry, a space naturally suited for embedding tree-like structures with low distortion. However, a naive hyperbolic mapping may suffer from hierarchical collapse and poor fine-grained discrimination. We further design two learning objectives to structure this space: a Dual-Hierarchical Entailment Loss that uses hyperbolic entailment cones to enforce the predefined hierarchies, and a Discriminative Alignment Loss with hard negative mining to establish a large geodesic distance between semantically similar compositions. Furthermore, we devise Hyperbolic Cross-Modal Attention to realize instance-aware cross-modal infusion within hyperbolic geometry. Extensive ablations on three benchmarks demonstrate that H2em establishes a new state-of-the-art in both closed-world and open-world scenarios. Our codes will be released.

</details>


### [23] [VALLR-Pin: Dual-Decoding Visual Speech Recognition for Mandarin with Pinyin-Guided LLM Refinement](https://arxiv.org/abs/2512.20032)
*Chang Sun,Dongliang Xie,Bo Qin,Hong Yang*

Main category: cs.CV

TL;DR: VALLR-Pin是一个两阶段框架，将VALLR架构从英语扩展到普通话。它使用共享的视频编码器和双重解码器联合预测汉字序列及其拼音，通过多任务学习和LLM解决同音字问题来提升普通话唇读性能。


<details>
  <summary>Details</summary>
Motivation: 普通话视觉语音识别面临音素高度模糊和同音词普遍存在的挑战，现有方法难以处理这些歧义问题。

Method: 采用两阶段框架：1）共享视频编码器+双重解码器联合预测字符和拼音；2）使用LLM结合拼音输出来解析歧义和优化转录；3）通过合成噪声样本微调LLM以识别特定错误模式。

Result: 该方法通过视觉特征与语音和语言上下文的协同作用，有效提高了普通话唇读的准确性。

Conclusion: VALLR-Pin成功地将视觉特征与语音和语言上下文相结合，为解决普通话唇读中的歧义问题提供了一种有效方案。

Abstract: Visual Speech Recognition aims to transcribe spoken words from silent lip-motion videos. This task is particularly challenging for Mandarin, as visemes are highly ambiguous and homophones are prevalent. We propose VALLR-Pin, a novel two-stage framework that extends the recent VALLR architecture from English to Mandarin. First, a shared video encoder feeds into dual decoders, which jointly predict both Chinese character sequences and their standard Pinyin romanization. The multi-task learning of character and phonetic outputs fosters robust visual-semantic representations. During inference, the text decoder generates multiple candidate transcripts. We construct a prompt by concatenating the Pinyin output with these candidate Chinese sequences and feed it to a large language model to resolve ambiguities and refine the transcription. This provides the LLM with explicit phonetic context to correct homophone-induced errors. Finally, we fine-tune the LLM on synthetic noisy examples: we generate imperfect Pinyin-text pairs from intermediate VALLR-Pin checkpoints using the training data, creating instruction-response pairs for error correction. This endows the LLM with awareness of our model's specific error patterns. In summary, VALLR-Pin synergizes visual features with phonetic and linguistic context to improve Mandarin lip-reading performance.

</details>


### [24] [FlashLips: 100-FPS Mask-Free Latent Lip-Sync using Reconstruction Instead of Diffusion or GANs](https://arxiv.org/abs/2512.20033)
*Andreas Zinonos,Michał Stypułkowski,Antoni Bigata,Stavros Petridis,Maja Pantic,Nikita Drobyshev*

Main category: cs.CV

TL;DR: FlashLips是一个两阶段、无掩码的唇形同步系统，通过解耦嘴唇控制和渲染，在单个GPU上实现超过100FPS的实时性能，同时匹配最先进模型的视觉质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有唇形同步系统在保持视觉质量的同时实现实时性能的挑战，通过简单稳定的流水线结合确定性重建和鲁棒的音频控制。

Method: 第一阶段使用紧凑的单步潜在空间编辑器，通过参考身份、掩码目标帧和低维嘴唇姿态向量进行图像重建；第二阶段使用基于流匹配的音频到姿态变换器预测嘴唇姿态向量。通过自监督学习消除显式掩码需求。

Result: 系统在单个GPU上达到超过100FPS的实时性能，视觉质量与大型最先进模型相当，实现了高感知质量和超实时速度。

Conclusion: FlashLips展示了一种简单有效的唇形同步方法，通过两阶段设计成功平衡了视觉质量和实时性能，为实时视频应用提供了实用解决方案。

Abstract: We present FlashLips, a two-stage, mask-free lip-sync system that decouples lips control from rendering and achieves real-time performance running at over 100 FPS on a single GPU, while matching the visual quality of larger state-of-the-art models. Stage 1 is a compact, one-step latent-space editor that reconstructs an image using a reference identity, a masked target frame, and a low-dimensional lips-pose vector, trained purely with reconstruction losses - no GANs or diffusion. To remove explicit masks at inference, we use self-supervision: we generate mouth-altered variants of the target image, that serve as pseudo ground truth for fine-tuning, teaching the network to localize edits to the lips while preserving the rest. Stage 2 is an audio-to-pose transformer trained with a flow-matching objective to predict lips-poses vectors from speech. Together, these stages form a simple and stable pipeline that combines deterministic reconstruction with robust audio control, delivering high perceptual quality and faster-than-real-time speed.

</details>


### [25] [Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva](https://arxiv.org/abs/2512.20042)
*Nguyen Lam Phu Quy,Pham Phu Hoa,Tran Chi Nguyen,Dao Sy Duy Minh,Nguyen Hoang Minh Ngoc,Huynh Trung Kiet*

Main category: cs.CV

TL;DR: 论文提出了一个多模态流程，通过整合外部文本知识来增强图像描述，生成更具上下文深度的图像标题。


<details>
  <summary>Details</summary>
Motivation: 现实世界图像描述常缺乏上下文深度，忽略了事件背景、时间线索、结果等非视觉可辨的关键细节，这限制了图像理解在新闻、教育等领域的应用效果。

Method: 使用BEIT-3和SigLIP检索语义相似图像，通过ORB和SIFT进行几何对齐重排，从相关文章中提取上下文信息，然后通过微调的Qwen3模型结合Instruct BLIP生成的基础描述，生成事件丰富的上下文感知描述。

Result: 在OpenEvents v1数据集上的评估显示，该方法比传统方法能生成显著更丰富的图像描述。

Conclusion: 该方法在需要深度视觉文本理解的实际应用中具有强大潜力。

Abstract: Real-world image captions often lack contextual depth, omitting crucial details such as event background, temporal cues, outcomes, and named entities that are not visually discernible. This gap limits the effectiveness of image understanding in domains like journalism, education, and digital archives, where richer, more informative descriptions are essential. To address this, we propose a multimodal pipeline that augments visual input with external textual knowledge. Our system retrieves semantically similar images using BEIT-3 (Flickr30k-384 and COCO-384) and SigLIP So-384, reranks them using ORB and SIFT for geometric alignment, and extracts contextual information from related articles via semantic search. A fine-tuned Qwen3 model with QLoRA then integrates this context with base captions generated by Instruct BLIP (Vicuna-7B) to produce event-enriched, context-aware descriptions. Evaluated on the OpenEvents v1 dataset, our approach generates significantly more informative captions compared to traditional methods, showing strong potential for real-world applications requiring deeper visual-textual understanding

</details>


### [26] [Progressive Learned Image Compression for Machine Perception](https://arxiv.org/abs/2512.20070)
*Jungwoo Kim,Jun-Hyuk Kim,Jong-Seok Lee*

Main category: cs.CV

TL;DR: 该论文提出了PICM-Net，一种基于三态平面编码的渐进式学习图像压缩方法，专门面向机器感知任务，支持细粒度可扩展性，并通过自适应解码控制器动态调整解码质量。


<details>
  <summary>Details</summary>
Motivation: 现有的学习图像编码器主要面向人类感知，而面向机器感知的渐进式图像压缩（支持单一码流多质量解码）尚未得到充分探索。

Method: 基于三态平面编码构建PICM-Net，分析人机感知的率失真优先级差异，设计自适应解码控制器动态推断所需解码级别以维持下游任务置信度。

Result: 实验表明该方法能实现高效自适应的渐进传输，同时在下游分类任务中保持高性能。

Conclusion: 为机器感知的渐进图像压缩建立了新范式，平衡了压缩效率与机器任务性能。

Abstract: Recent advances in learned image codecs have been extended from human perception toward machine perception. However, progressive image compression with fine granular scalability (FGS)-which enables decoding a single bitstream at multiple quality levels-remains unexplored for machine-oriented codecs. In this work, we propose a novel progressive learned image compression codec for machine perception, PICM-Net, based on trit-plane coding. By analyzing the difference between human- and machine-oriented rate-distortion priorities, we systematically examine the latent prioritization strategies in terms of machine-oriented codecs. To further enhance real-world adaptability, we design an adaptive decoding controller, which dynamically determines the necessary decoding level during inference time to maintain the desired confidence of downstream machine prediction. Extensive experiments demonstrate that our approach enables efficient and adaptive progressive transmission while maintaining high performance in the downstream classification task, establishing a new paradigm for machine-aware progressive image compression.

</details>


### [27] [Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark](https://arxiv.org/abs/2512.20174)
*Hao Guo,Xugong Qin,Jun Jie Ou Yang,Peng Zhang,Gangyan Zeng,Yubo Li,Hailun Lin*

Main category: cs.CV

TL;DR: 本文提出了一个基于自然语言的文档图像检索（NL-DIR）新基准，包含41K真实文档图像和细粒度语义查询，通过两阶段检索方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像查询的DIR方法在现实场景中难以处理具有细粒度语义的文本查询，需要更有效的自然语言查询方法。

Method: 构建NL-DIR数据集，使用大语言模型生成高质量查询并人工验证；评估对比学习视觉语言模型和OCR-free VDU模型的零样本和微调性能；提出两阶段检索方法。

Result: 建立了包含41K文档图像和精细语义查询的基准数据集，验证了不同模型的检索效果，两阶段方法在效率和性能上均有提升。

Conclusion: NL-DIR基准为视觉文档理解社区提供了新的研究机会，数据集和代码已公开，有望推动DIR技术的发展。

Abstract: Document image retrieval (DIR) aims to retrieve document images from a gallery according to a given query. Existing DIR methods are primarily based on image queries that retrieve documents within the same coarse semantic category, e.g., newspapers or receipts. However, these methods struggle to effectively retrieve document images in real-world scenarios where textual queries with fine-grained semantics are usually provided. To bridge this gap, we introduce a new Natural Language-based Document Image Retrieval (NL-DIR) benchmark with corresponding evaluation metrics. In this work, natural language descriptions serve as semantically rich queries for the DIR task. The NL-DIR dataset contains 41K authentic document images, each paired with five high-quality, fine-grained semantic queries generated and evaluated through large language models in conjunction with manual verification. We perform zero-shot and fine-tuning evaluations of existing mainstream contrastive vision-language models and OCR-free visual document understanding (VDU) models. A two-stage retrieval method is further investigated for performance improvement while achieving both time and space efficiency. We hope the proposed NL-DIR benchmark can bring new opportunities and facilitate research for the VDU community. Datasets and codes will be publicly available at huggingface.co/datasets/nianbing/NL-DIR.

</details>


### [28] [Item Region-based Style Classification Network (IRSN): A Fashion Style Classifier Based on Domain Knowledge of Fashion Experts](https://arxiv.org/abs/2512.20088)
*Jinyoung Choi,Youngchae Kwon,Injung Kim*

Main category: cs.CV

TL;DR: 基于物品区域的时尚风格分类网络（IRSN）通过分析物品特定特征及其组合，结合全局特征来提升时尚风格分类的准确性。采用双骨干架构和门控特征融合技术，在多个数据集上显著提高了分类性能。


<details>
  <summary>Details</summary>
Motivation: 时尚风格分类的挑战在于同一风格内视觉差异大且不同风格间视觉相似度高。风格不仅由全局外观表达，还受单个物品属性及其组合方式影响，因此需要同时考虑全局和局部特征。

Method: 提出IRSN网络，使用物品区域池化（IRP）提取各物品区域特征，通过门控特征融合（GFF）分析并组合这些特征。采用双骨干架构，结合领域特定特征提取器和在大规模图像-文本数据集上预训练的通用特征提取器。

Result: 在FashionStyle14和ShowniqV3数据集上，IRSN应用于EfficientNet、ConvNeXt和Swin Transformer等六种骨干网络，平均分类准确率分别提升6.9%和7.6%，最大提升达14.5%和15.1%。可视化分析显示IRSN能更好区分相似风格类别。

Conclusion: IRSN通过有效整合物品级别特征和全局特征，显著提高了时尚风格分类的准确性。双骨干架构和门控特征融合技术能更好捕捉风格间的细微差异，为时尚分析提供了有力工具。

Abstract: Fashion style classification is a challenging task because of the large visual variation within the same style and the existence of visually similar styles.
  Styles are expressed not only by the global appearance, but also by the attributes of individual items and their combinations.
  In this study, we propose an item region-based fashion style classification network (IRSN) to effectively classify fashion styles by analyzing item-specific features and their combinations in addition to global features.
  IRSN extracts features of each item region using item region pooling (IRP), analyzes them separately, and combines them using gated feature fusion (GFF).
  In addition, we improve the feature extractor by applying a dual-backbone architecture that combines a domain-specific feature extractor and a general feature extractor pre-trained with a large-scale image-text dataset.
  In experiments, applying IRSN to six widely-used backbones, including EfficientNet, ConvNeXt, and Swin Transformer, improved style classification accuracy by an average of 6.9% and a maximum of 14.5% on the FashionStyle14 dataset and by an average of 7.6% and a maximum of 15.1% on the ShowniqV3 dataset. Visualization analysis also supports that the IRSN models are better than the baseline models at capturing differences between similar style classes.

</details>


### [29] [Effect of Activation Function and Model Optimizer on the Performance of Human Activity Recognition System Using Various Deep Learning Models](https://arxiv.org/abs/2512.20104)
*Subrata Kumer Paula,Dewan Nafiul Islam Noora,Rakhi Rani Paula,Md. Ekramul Hamidb,Fahmid Al Faridc,Hezerul Abdul Karimd,Md. Maruf Al Hossain Princee,Abu Saleh Musa Miahb*

Main category: cs.CV

TL;DR: 该研究分析激活函数和优化器组合对人机活动识别的影响，发现ConvLSTM结合Adam或RMSprop在医疗相关活动识别中表现最佳，准确率可达99%


<details>
  <summary>Details</summary>
Motivation: 现有HAR研究多关注架构设计，而激活函数与优化器的组合影响尚未充分分析，特别是对医疗环境下的实时决策和自动化应用至关重要

Method: 使用三种激活函数(ReLU、Sigmoid、Tanh)与四种优化器(SGD、Adam、RMSprop、Adagrad)组合，在BiLSTM和ConvLSTM两种循环架构上进行实验，数据集来自HMDB51和UCF101的六个医疗相关活动类别

Result: ConvLSTM在所有数据集上表现优于BiLSTM，结合Adam或RMSopt时准确率达99%；BiLSTM在UCF101上可达98%但在HMDB51上降至60%，显示跨数据集鲁棒性较差

Conclusion: 研究为优化HAR系统提供实用指导，特别适合医疗环境中需要快速精确活动检测的实际应用，推荐采用ConvLSTM与Adam/RMSprop组合

Abstract: Human Activity Recognition (HAR) plays a vital role in healthcare, surveillance, and innovative environments, where reliable action recognition supports timely decision-making and automation. Although deep learning-based HAR systems are widely adopted, the impact of Activation Functions (AFs) and Model Optimizers (MOs) on performance has not been sufficiently analyzed, particularly regarding how their combinations influence model behavior in practical scenarios. Most existing studies focus on architecture design, while the interaction between AF and MO choices remains relatively unexplored. In this work, we investigate the effect of three commonly used activation functions (ReLU, Sigmoid, and Tanh) combined with four optimization algorithms (SGD, Adam, RMSprop, and Adagrad) using two recurrent deep learning architectures, namely BiLSTM and ConvLSTM. Experiments are conducted on six medically relevant activity classes selected from the HMDB51 and UCF101 datasets, considering their suitability for healthcare-oriented HAR applications. Our experimental results show that ConvLSTM consistently outperforms BiLSTM across both datasets. ConvLSTM, combined with Adam or RMSprop, achieves an accuracy of up to 99.00%, demonstrating strong spatio-temporal learning capabilities and stable performance. While BiLSTM performs reasonably well on UCF101, with accuracy approaching 98.00%, its performance drops to approximately 60.00% on HMDB51, indicating limited robustness across datasets and weaker sensitivity to AF and MO variations. This study provides practical insights for optimizing HAR systems, particularly for real-world healthcare environments where fast and precise activity detection is critical.

</details>


### [30] [LiDARDraft: Generating LiDAR Point Cloud from Versatile Inputs](https://arxiv.org/abs/2512.20105)
*Haiyun Wei,Fan Lu,Yunwei Zhu,Zehan Zheng,Weiyi Xue,Lin Shao,Xudong Zhang,Ya Wu,Rong Fu,Guang Chen*

Main category: cs.CV

TL;DR: 生成真实多样的LiDAR点云对自动驾驶仿真至关重要。为解决现有方法在复杂LiDAR点云分布与简单控制信号之间不平衡的问题，本文提出LiDARDraft方法，通过3D布局桥接多样化条件信号与LiDAR点云生成。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR点云生成方法难以在保证高质量的同时实现多样化可控性，主要受限于复杂点云分布与简单控制信号之间的不平衡问题。

Method: 提出LiDARDraft框架：1）将文本、图像和点云统一表示为3D布局；2）转换为语义和深度控制信号；3）使用基于rangemap的ControlNet指导LiDAR点云生成。

Result: 该方法在可控LiDAR点云生成方面表现优异，支持从任意文本描述、图像和草图创建自动驾驶环境的

Conclusion: LiDARDraft通过3D布局和像素级对齐方法，有效解决了LiDAR点云生成中的控制与质量平衡问题，实现了从多样化输入创建自动驾驶仿真的能力。

Abstract: Generating realistic and diverse LiDAR point clouds is crucial for autonomous driving simulation. Although previous methods achieve LiDAR point cloud generation from user inputs, they struggle to attain high-quality results while enabling versatile controllability, due to the imbalance between the complex distribution of LiDAR point clouds and the simple control signals. To address the limitation, we propose LiDARDraft, which utilizes the 3D layout to build a bridge between versatile conditional signals and LiDAR point clouds. The 3D layout can be trivially generated from various user inputs such as textual descriptions and images. Specifically, we represent text, images, and point clouds as unified 3D layouts, which are further transformed into semantic and depth control signals. Then, we employ a rangemap-based ControlNet to guide LiDAR point cloud generation. This pixel-level alignment approach demonstrates excellent performance in controllable LiDAR point clouds generation, enabling "simulation from scratch", allowing self-driving environments to be created from arbitrary textual descriptions, images and sketches.

</details>


### [31] [UMAMI: Unifying Masked Autoregressive Models and Deterministic Rendering for View Synthesis](https://arxiv.org/abs/2512.20107)
*Thanh-Tung Le,Tuan Pham,Tung Nguyen,Deying Kong,Xiaohui Xie,Stephan Mandt*

Main category: cs.CV

TL;DR: 本文提出了一种混合框架，将确定性网络和随机扩散方法的优势结合用于新视角合成。通过双向Transformer编码多视图标记和Plucker射线嵌入，使用回归头和扩散头分别处理几何约束明确区域和遮挡区域，实现了高质量的实时渲染。


<details>
  <summary>Details</summary>
Motivation: 现有确定性网络在可见区域渲染快但对不可见区域模糊，而随机扩散方法能生成合理内容但计算成本高。需要统一两种方法的优势来处理新视角合成问题。

Method: 使用双向Transformer编码多视图图像标记和Plucker射线嵌入，构建共享潜在表示。通过回归头处理几何明确区域，扩散头完成遮挡区域，端到端联合训练光度和扩散损失。

Result: 实验表明该方法达到最先进的图像质量，同时比完全生成式基准方法减少了一个数量级的渲染时间。

Conclusion: 提出的混合框架成功结合了确定性渲染的速度优势和扩散模型的细节生成能力，实现了高质量且高效的新视角合成，无需手工设计的3D归纳偏置。

Abstract: Novel view synthesis (NVS) seeks to render photorealistic, 3D-consistent images of a scene from unseen camera poses given only a sparse set of posed views. Existing deterministic networks render observed regions quickly but blur unobserved areas, whereas stochastic diffusion-based methods hallucinate plausible content yet incur heavy training- and inference-time costs. In this paper, we propose a hybrid framework that unifies the strengths of both paradigms. A bidirectional transformer encodes multi-view image tokens and Plucker-ray embeddings, producing a shared latent representation. Two lightweight heads then act on this representation: (i) a feed-forward regression head that renders pixels where geometry is well constrained, and (ii) a masked autoregressive diffusion head that completes occluded or unseen regions. The entire model is trained end-to-end with joint photometric and diffusion losses, without handcrafted 3D inductive biases, enabling scalability across diverse scenes. Experiments demonstrate that our method attains state-of-the-art image quality while reducing rendering time by an order of magnitude compared with fully generative baselines.

</details>


### [32] [Multi Modal Attention Networks with Uncertainty Quantification for Automated Concrete Bridge Deck Delamination Detection](https://arxiv.org/abs/2512.20113)
*Alireza Moayedikia,Sattar Dorafshan*

Main category: cs.CV

TL;DR: 提出多模态注意力网络，融合探地雷达时间模式和热成像空间特征检测桥梁板脱层；引入时间注意力、空间注意力和跨模态融合，通过不确定性量化提高安全决策；在平衡至中度不平衡数据上大幅优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉评估方法和单模态检测技术存在互补性限制：探地雷达对水分和浅表缺陷敏感，热成像依赖天气且深度有限，亟需融合多模态数据的自动化检测方案。

Method: 设计多模态注意力网络架构：雷达信号使用时间注意力处理时间模式，热成像使用空间注意力提取空间特征，通过可学习嵌入进行跨模态融合；集成蒙特卡洛dropout和方差估计进行不确定性量化，分解为认知不确定性和偶然不确定性。

Result: 在五个桥梁数据集上实验表明：在平衡至中度不平衡数据中，该方法在准确率和AUC上显著优于单模态和简单拼接融合基线；跨模态注意力提供关键性能提升，多头机制改善校准度；不确定性量化降低校准误差，支持选择性预测。

Conclusion: 基于注意力的架构在典型场景下表现优异，但极端类别不平衡时易出现多数类崩塌；系统具备部署效率，支持实时检测并明确能力边界，为工程应用提供实践指导。

Abstract: Deteriorating civil infrastructure requires automated inspection techniques overcoming limitations of visual assessment. While Ground Penetrating Radar and Infrared Thermography enable subsurface defect detection, single modal approaches face complementary constraints radar struggles with moisture and shallow defects, while thermography exhibits weather dependency and limited depth. This paper presents a multi modal attention network fusing radar temporal patterns with thermal spatial signatures for bridge deck delamination detection. Our architecture introduces temporal attention for radar processing, spatial attention for thermal features, and cross modal fusion with learnable embeddings discovering complementary defect patterns invisible to individual sensors. We incorporate uncertainty quantification through Monte Carlo dropout and learned variance estimation, decomposing uncertainty into epistemic and aleatoric components for safety critical decisions. Experiments on five bridge datasets reveal that on balanced to moderately imbalanced data, our approach substantially outperforms baselines in accuracy and AUC representing meaningful improvements over single modal and concatenation based fusion. Ablation studies demonstrate cross modal attention provides critical gains beyond within modality attention, while multi head mechanisms achieve improved calibration. Uncertainty quantification reduces calibration error, enabling selective prediction by rejecting uncertain cases. However, under extreme class imbalance, attention mechanisms show vulnerability to majority class collapse. These findings provide actionable guidance: attention based architecture performs well across typical scenarios, while extreme imbalance requires specialized techniques. Our system maintains deployment efficiency, enabling real time inspection with characterized capabilities and limitations.

</details>


### [33] [HEART-VIT: Hessian-Guided Efficient Dynamic Attention and Token Pruning in Vision Transformer](https://arxiv.org/abs/2512.20120)
*Mohammad Helal Uddin,Liam Seymour,Sabur Baidya*

Main category: cs.CV

TL;DR: HEART-ViT 是一个基于 Hessian 的、统一的动态注意力与 token 剪枝框架，通过二阶信息实现视觉 Transformer 模型的高效优化。


<details>
  <summary>Details</summary>
Motivation: 当前视觉 Transformer 尽管精度高，但其二次注意力开销和冗余计算严重制约了其在资源受限平台上的部署需求，现有剪枝方法多依赖启发式或一阶信号，精度牺牲大且难以泛化。

Method: 框架结合 Hessian 向量积估算 token 和注意头的曲率加权敏感度，通过损失预算实现剪枝决策的科学化，统一 token 剪枝（主导计算节省）与头剪枝（细粒度冗余去除）。

Result: 在多个数据集和模型上，HEART-ViT 能减少高达 49.4% 的浮点运算，降低 36% 延迟并提升 46% 吞吐量，甚至在微调后基准精度持平或更高。

Conclusion: 该框架首次打通理论与实践的桥梁，提供了一种精度保持且边缘高效的统一曲率驱动剪枝方案，在真实设备上验证了推理速度与能效提升。

Abstract: Vision Transformers (ViTs) deliver state-of-the-art accuracy but their quadratic attention cost and redundant computations severely hinder deployment on latency and resource-constrained platforms. Existing pruning approaches treat either tokens or heads in isolation, relying on heuristics or first-order signals, which often sacrifice accuracy or fail to generalize across inputs. We introduce HEART-ViT, a Hessian-guided efficient dynamic attention and token pruning framework for vision transformers, which to the best of our knowledge is the first unified, second-order, input-adaptive framework for ViT optimization. HEART-ViT estimates curvature-weighted sensitivities of both tokens and attention heads using efficient Hessian-vector products, enabling principled pruning decisions under explicit loss budgets.This dual-view sensitivity reveals an important structural insight: token pruning dominates computational savings, while head pruning provides fine-grained redundancy removal, and their combination achieves a superior trade-off. On ImageNet-100 and ImageNet-1K with ViT-B/16 and DeiT-B/16, HEART-ViT achieves up to 49.4 percent FLOPs reduction, 36 percent lower latency, and 46 percent higher throughput, while consistently matching or even surpassing baseline accuracy after fine-tuning, for example 4.7 percent recovery at 40 percent token pruning. Beyond theoretical benchmarks, we deploy HEART-ViT on different edge devices such as AGX Orin, demonstrating that our reductions in FLOPs and latency translate directly into real-world gains in inference speed and energy efficiency. HEART-ViT bridges the gap between theory and practice, delivering the first unified, curvature-driven pruning framework that is both accuracy-preserving and edge-efficient.

</details>


### [34] [milliMamba: Specular-Aware Human Pose Estimation via Dual mmWave Radar with Multi-Frame Mamba Fusion](https://arxiv.org/abs/2512.20128)
*Niraj Prakash Kini,Shiau-Rung Tsai,Guan-Hsun Lin,Wen-Hsiao Peng,Ching-Wen Ma,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: 本文提出milliMamba框架，用于雷达基础的2D人体姿态估计，通过联合建模时空依赖性来应对雷达信号稀疏性问题。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达在隐私保护和光照不变性方面优于RGB传感器，但由于镜面反射导致信号稀疏，提取稳健特征极具挑战性。

Method: 采用Cross-View Fusion Mamba编码器高效提取长序列时空特征（线性复杂度），配合Spatio-Temporal-Cross Attention解码器预测多帧关节坐标，并加入速度损失增强运动平滑性。

Result: 在TransHuPR和HuPR数据集上分别超越基线11.0 AP和14.6 AP，同时保持合理复杂度。

Conclusion: milliMamba通过时空建模有效利用上下文信息推断缺失关节，显著提升雷达姿态估计性能。

Abstract: Millimeter-wave radar offers a privacy-preserving and lighting-invariant alternative to RGB sensors for Human Pose Estimation (HPE) task. However, the radar signals are often sparse due to specular reflection, making the extraction of robust features from radar signals highly challenging. To address this, we present milliMamba, a radar-based 2D human pose estimation framework that jointly models spatio-temporal dependencies across both the feature extraction and decoding stages. Specifically, given the high dimensionality of radar inputs, we adopt a Cross-View Fusion Mamba encoder to efficiently extract spatio-temporal features from longer sequences with linear complexity. A Spatio-Temporal-Cross Attention decoder then predicts joint coordinates across multiple frames. Together, this spatio-temporal modeling pipeline enables the model to leverage contextual cues from neighboring frames and joints to infer missing joints caused by specular reflections. To reinforce motion smoothness, we incorporate a velocity loss alongside the standard keypoint loss during training. Experiments on the TransHuPR and HuPR datasets demonstrate that our method achieves significant performance improvements, exceeding the baselines by 11.0 AP and 14.6 AP, respectively, while maintaining reasonable complexity. Code: https://github.com/NYCU-MAPL/milliMamba

</details>


### [35] [Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)](https://arxiv.org/abs/2512.20148)
*Robert van de Ven,Trim Bresilla,Bram Nelissen,Ard Nieuwenhuizen,Eldert J. van Henten,Gert Kootstra*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D高斯泼溅重建果园场景的新方法，通过简化标注流程，将手动标注需求从28191个减少到105个（减少99.6%），有效解决了苹果姿态估计中标注困难的问题。


<details>
  <summary>Details</summary>
Motivation: 果园自动化任务面临环境变异和遮挡的挑战，特别是苹果姿态估计中关键点（如花萼）常被遮挡。现有方法虽不再依赖关键点但仍需其进行标注，而遮挡导致标注冲突和缺失，因此需要简化标注流程并扩大数据集。

Method: 提出包含3D高斯泼溅重建果园场景、简化标注、自动投影标注到图像、姿态估计方法训练与评估的新流程。利用3D重建技术将手动标注需求大幅降低。

Result: 实验结果显示，使用≤95%遮挡苹果的训练标签效果最佳，在原始图像上F1得分为0.927，渲染图像上为0.970。训练集大小对模型性能影响较小，遮挡最少的水果位置估计最佳。

Conclusion: 该方法显著减少了标注工作量，但测试的姿态估计方法无法正确学习苹果的方向估计，遮挡增加会降低位置估计精度。

Abstract: Automating tasks in orchards is challenging because of the large amount of variation in the environment and occlusions. One of the challenges is apple pose estimation, where key points, such as the calyx, are often occluded. Recently developed pose estimation methods no longer rely on these key points, but still require them for annotations, making annotating challenging and time-consuming. Due to the abovementioned occlusions, there can be conflicting and missing annotations of the same fruit between different images. Novel 3D reconstruction methods can be used to simplify annotating and enlarge datasets. We propose a novel pipeline consisting of 3D Gaussian Splatting to reconstruct an orchard scene, simplified annotations, automated projection of the annotations to images, and the training and evaluation of a pose estimation method. Using our pipeline, 105 manual annotations were required to obtain 28,191 training labels, a reduction of 99.6%. Experimental results indicated that training with labels of fruits that are $\leq95\%$ occluded resulted in the best performance, with a neutral F1 score of 0.927 on the original images and 0.970 on the rendered images. Adjusting the size of the training dataset had small effects on the model performance in terms of F1 score and pose estimation accuracy. It was found that the least occluded fruits had the best position estimation, which worsened as the fruits became more occluded. It was also found that the tested pose estimation method was unable to correctly learn the orientation estimation of apples.

</details>


### [36] [CoDi -- an exemplar-conditioned diffusion model for low-shot counting](https://arxiv.org/abs/2512.20153)
*Grega Šuštar,Jer Pelhan,Alan Lukežič,Matej Kristan*

Main category: cs.CV

TL;DR: 这篇论文提出了CoDi，这是一种基于扩散生成模型且无需查询的先验方法，旨在以更可靠的方式解决小物体密集区域的计数问题。


<details>
  <summary>Details</summary>
Motivation: 现有密度计数方法对物体定位能力较差，而基于点检测的计数方法在物体数量非常大的图像中表现不佳，且需要临时技术（如重采样和分块处理）。

Method: 提出了CoDi，一种基于潜在扩散的少样本计数方法，生成高质量密度图，通过非极大值抑制确定物体位置。核心贡献是新的基于示例的条件模块，提取并调整物体原型以适配去噪网络的中间层。

Result: 在FSC基准测试中，CoDi在few-shot、one-shot和无参考场景下，分别比现有最优方法在MAE上提高了15%、13%和10%；在MCAC基准测试上，比最优方法提高了44% MAE。

Conclusion: CoDi在少样本目标计数任务中表现出色，尤其是在密集小物体场景下，大幅提升了计数精度和定位能力，为相关领域设立了新的技术标杆。

Abstract: Low-shot object counting addresses estimating the number of previously unobserved objects in an image using only few or no annotated test-time exemplars. A considerable challenge for modern low-shot counters are dense regions with small objects. While total counts in such situations are typically well addressed by density-based counters, their usefulness is limited by poor localization capabilities. This is better addressed by point-detection-based counters, which are based on query-based detectors. However, due to limited number of pre-trained queries, they underperform on images with very large numbers of objects, and resort to ad-hoc techniques like upsampling and tiling. We propose CoDi, the first latent diffusion-based low-shot counter that produces high-quality density maps on which object locations can be determined by non-maxima suppression. Our core contribution is the new exemplar-based conditioning module that extracts and adjusts the object prototypes to the intermediate layers of the denoising network, leading to accurate object location estimation. On FSC benchmark, CoDi outperforms state-of-the-art by 15% MAE, 13% MAE and 10% MAE in the few-shot, one-shot, and reference-less scenarios, respectively, and sets a new state-of-the-art on MCAC benchmark by outperforming the top method by 44% MAE. The code is available at https://github.com/gsustar/CoDi.

</details>


### [37] [AMoE: Agglomerative Mixture-of-Experts Vision Foundation Model](https://arxiv.org/abs/2512.20157)
*Sofian Chaybouti,Sanath Narayan,Yasser Dahou,Phúc H. Lê Khac,Ankit Singh,Ngoc Dung Huynh,Wamiq Reyaz Para,Hilde Kuehne,Hakim Hacid*

Main category: cs.CV

TL;DR: 本文提出了AMoE方法，通过多教师蒸馏从SigLIP2和DINOv3同时训练视觉基础模型，并引入不对称关系知识蒸馏、令牌平衡批处理和分层聚类采样等关键技术，显著降低了计算成本并提高了数据效率。


<details>
  <summary>Details</summary>
Motivation: 当前通过多教师蒸馏训练的视觉基础模型虽然前景广阔，但其学习动态和数据效率尚未得到充分研究。本文旨在系统研究多教师蒸馏方法，并探索在较低计算成本下训练的有效因素。

Method: 提出AMoE方法：1）使用不对称关系知识蒸馏损失保持各教师模型的几何特性；2）采用令牌平衡批处理技术稳定不同分辨率下的表示学习；3）应用分层聚类和采样提高样本效率。基于这些发现构建了OpenLVD200M图像语料库。

Result: 研究表明，所提出的方法能够有效实现多教师知识蒸馏，在保持性能的同时显著降低计算需求。OpenLVD200M语料库在多教师蒸馏中表现出卓越的效率。

Conclusion: 通过多教师蒸馏方法结合关键技术优化，可以在较低计算成本下训练高效的视觉基础模型。该方法为统一视觉表示学习提供了可行路径，并发布了OpenLVD200M数据集和蒸馏模型。

Abstract: Vision foundation models trained via multi-teacher distillation offer a promising path toward unified visual representations, yet the learning dynamics and data efficiency of such approaches remain underexplored. In this paper, we systematically study multi-teacher distillation for vision foundation models and identify key factors that enable training at lower computational cost. We introduce Agglomerative Mixture-of-Experts Vision Foundation Models (AMoE), which distill knowledge from SigLIP2 and DINOv3 simultaneously into a Mixture-of-Experts student. We show that (1) our Asymmetric Relation-Knowledge Distillation loss preserves the geometric properties of each teacher while enabling effective knowledge transfer, (2) token-balanced batching that packs varying-resolution images into sequences with uniform token budgets stabilizes representation learning across resolutions without sacrificing performance, and (3) hierarchical clustering and sampling of training data--typically reserved for self-supervised learning--substantially improves sample efficiency over random sampling for multi-teacher distillation. By combining these findings, we curate OpenLVD200M, a 200M-image corpus that demonstrates superior efficiency for multi-teacher distillation. Instantiated in a Mixture-of-Experts. We release OpenLVD200M and distilled models.

</details>


### [38] [Generative Latent Coding for Ultra-Low Bitrate Image Compression](https://arxiv.org/abs/2512.20194)
*Zhaoyang Jia,Jiahao Li,Bin Li,Houqiang Li,Yan Lu*

Main category: cs.CV

TL;DR: 提出了基于生成式潜在空间的新型图像压缩方法GLC，通过在VQ-VAE的潜在空间进行变换编码，解决了像素空间压缩在低码率下难以兼顾真实感和保真度的问题，实现了优于传统方法的高质量压缩效果。


<details>
  <summary>Details</summary>
Motivation: 传统图像压缩方法在像素空间进行变换编码，在低码率下难以同时实现高真实感和高保真度，因为像素空间的失真不一定符合人类感知。需要探索更适合人类感知的压缩空间。

Method: 提出生成式潜在编码(GLC)架构：1) 在VQ-VAE的生成式潜在空间而非像素空间进行变换编码；2) 引入分类超模块降低超信息比特成本；3) 基于代码预测的监督增强语义一致性。

Result: 在自然图像上实现少于0.04bpp，人脸图像上少于0.01bpp的高视觉质量压缩。在CLIC2020测试集上，以45%更少的比特数达到与MS-ILLM相同的FID指标。支持图像修复和风格迁移等应用。

Conclusion: 生成式潜在空间具有更好的稀疏性、语义丰富性和人类感知对齐特性，是高质量图像压缩的理想选择。GLC框架为低码率高保真压缩提供了有效解决方案，并具有良好的扩展应用潜力。

Abstract: Most existing image compression approaches perform transform coding in the pixel space to reduce its spatial redundancy. However, they encounter difficulties in achieving both high-realism and high-fidelity at low bitrate, as the pixel-space distortion may not align with human perception. To address this issue, we introduce a Generative Latent Coding (GLC) architecture, which performs transform coding in the latent space of a generative vector-quantized variational auto-encoder (VQ-VAE), instead of in the pixel space. The generative latent space is characterized by greater sparsity, richer semantic and better alignment with human perception, rendering it advantageous for achieving high-realism and high-fidelity compression. Additionally, we introduce a categorical hyper module to reduce the bit cost of hyper-information, and a code-prediction-based supervision to enhance the semantic consistency. Experiments demonstrate that our GLC maintains high visual quality with less than 0.04 bpp on natural images and less than 0.01 bpp on facial images. On the CLIC2020 test set, we achieve the same FID as MS-ILLM with 45% fewer bits. Furthermore, the powerful generative latent space enables various applications built on our GLC pipeline, such as image restoration and style transfer. The code is available at https://github.com/jzyustc/GLC.

</details>


### [39] [JDPNet: A Network Based on Joint Degradation Processing for Underwater Image Enhancement](https://arxiv.org/abs/2512.20213)
*Tao Ye,Hongbin Ren,Chongbing Zhang,Haoran Chen,Xiaosong Li*

Main category: cs.CV

TL;DR: 该论文提出JDPNet网络，通过挖掘耦合退化的潜在信息，解决水下图像中多种非线性耦合退化问题。


<details>
  <summary>Details</summary>
Motivation: 水下图像由于环境复杂性和水的介质特性存在多种耦合退化，现有方法难以有效处理非线性相互作用，需要从底层视角统一处理耦合退化。

Method: 提出联合退化处理网络JDPNet，包含联合特征挖掘模块和概率引导分布策略，以及AquaBalanceLoss损失函数来平衡颜色、清晰度和对比度。

Result: 在六个公开数据集和两个新构建数据集上的实验表明，JDPNet在性能、参数规模和计算成本之间实现了更好的平衡，达到最先进性能。

Conclusion: JDPNet通过统一的框架有效挖掘和处理耦合退化信息，为水下图像增强提供了高效的解决方案。

Abstract: Given the complexity of underwater environments and the variability of water as a medium, underwater images are inevitably subject to various types of degradation. The degradations present nonlinear coupling rather than simple superposition, which renders the effective processing of such coupled degradations particularly challenging. Most existing methods focus on designing specific branches, modules, or strategies for specific degradations, with little attention paid to the potential information embedded in their coupling. Consequently, they struggle to effectively capture and process the nonlinear interactions of multiple degradations from a bottom-up perspective. To address this issue, we propose JDPNet, a joint degradation processing network, that mines and unifies the potential information inherent in coupled degradations within a unified framework. Specifically, we introduce a joint feature-mining module, along with a probabilistic bootstrap distribution strategy, to facilitate effective mining and unified adjustment of coupled degradation features. Furthermore, to balance color, clarity, and contrast, we design a novel AquaBalanceLoss to guide the network in learning from multiple coupled degradation losses. Experiments on six publicly available underwater datasets, as well as two new datasets constructed in this study, show that JDPNet exhibits state-of-the-art performance while offering a better tradeoff between performance, parameter size, and computational cost.

</details>


### [40] [LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation](https://arxiv.org/abs/2512.20217)
*Xiangxuan Ren,Zhongdao Wang,Pin Tang,Guoqing Wang,Jilai Zheng,Chao Ma*

Main category: cs.CV

TL;DR: LiteFusion提出了一个新的多模态3D检测器，用LiDAR数据作为几何信息的补充源来增强基于相机的检测，无需3D骨干网络，提高了部署友好性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决当前多模态3D检测器对LiDAR传感器的强依赖性和在非GPU硬件上的部署困难。

Method: LiteFusion将LiDAR点云的互补特征在四元数空间中集成到图像特征中，保留正交约束，生成紧凑的跨模态嵌入。

Result: 在nuScenes数据集上，LiteFusion将基线视觉检测器的mAP和NDS分别提升20.4%和19.7%，参数仅增加1.1%，且在无LiDAR输入时仍保持强健性能。

Conclusion: LiteFusion通过简化融合架构，显著提升了3D检测的准确性、鲁棒性和部署灵活性。

Abstract: 3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.

</details>


### [41] [IndicDLP: A Foundational Dataset for Multi-Lingual and Multi-Domain Document Layout Parsing](https://arxiv.org/abs/2512.20236)
*Oikantik Nath,Sahithi Kukkala,Mitesh Khapra,Ravi Kiran Sarvadevabhatla*

Main category: cs.CV

TL;DR: IndicDLP是一个大规模多语言文档布局数据集，涵盖11种印度语言和英语，解决了现有数据集中粒度标注不足和多样性缺乏的问题，显著提升了文档布局分析性能。


<details>
  <summary>Details</summary>
Motivation: 现有文档布局数据集如PubLayNet和DocBank缺乏细粒度标注和多语言多样性，而人标注数据集如M6Doc和D4LA规模太小。印地语文献多样性丰富但在现有数据集中代表性不足，阻碍了文档理解技术的发展。

Method: 引入了IndicDLP数据集，涵盖11种代表性印度语言和英语，覆盖12个常见文档领域。同时从DocLayNet和M6Doc中整理出UED-mini数据集用于预训练。实验通过在IndicDLP上微调现有的英语模型来验证其有效性。

Result: 实验表明，在IndicDLP上微调现有英语模型能显著提升性能。此外，在IndicDLP上训练的模型能够很好地泛化到非印度语言布局，证明了其作为文档数字化宝贵资源的有效性。

Conclusion: 这项工作在规模、多样性和标注粒度方面弥补了现有数据集的不足，推动了包容性和高效的文档理解技术发展，为多语言文档布局分析提供了重要基础。

Abstract: Document layout analysis is essential for downstream tasks such as information retrieval, extraction, OCR, and digitization. However, existing large-scale datasets like PubLayNet and DocBank lack fine-grained region labels and multilingual diversity, making them insufficient for representing complex document layouts. In contrast, human-annotated datasets such as M6Doc and D4LA offer richer labels and greater domain diversity, but are too small to train robust models and lack adequate multilingual coverage. This gap is especially pronounced for Indic documents, which encompass diverse scripts yet remain underrepresented in current datasets, further limiting progress in this space. To address these shortcomings, we introduce IndicDLP, a large-scale foundational document layout dataset spanning 11 representative Indic languages alongside English and 12 common document domains. Additionally, we curate UED-mini, a dataset derived from DocLayNet and M6Doc, to enhance pretraining and provide a solid foundation for Indic layout models. Our experiments demonstrate that fine-tuning existing English models on IndicDLP significantly boosts performance, validating its effectiveness. Moreover, models trained on IndicDLP generalize well beyond Indic layouts, making it a valuable resource for document digitization. This work bridges gaps in scale, diversity, and annotation granularity, driving inclusive and efficient document understanding.

</details>


### [42] [Degradation-Aware Metric Prompting for Hyperspectral Image Restoration](https://arxiv.org/abs/2512.20251)
*Binfeng Wang,Di Wang,Haonan Guo,Ying Fu,Jing Zhang*

Main category: cs.CV

TL;DR: DAMP框架通过设计空间-光谱退化指标量化多维退化作为提示，结合自适应模块和专家混合架构，实现了无需预定义退化先验的统一高光谱图像恢复。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖难以获取的显式退化先验（如退化标签），无法适应真实场景中复杂混合的退化情况。

Method: 提出退化感知度量提示框架：1）设计空间-光谱退化指标作为连续退化提示；2）引入空间-光谱自适应模块动态调节特征提取；3）通过专家混合架构实现自适应恢复。

Result: 在自然和遥感高光谱数据集上的实验表明，DAMP达到最先进性能，并展现出优异的泛化能力。

Conclusion: DAMP框架通过度量提示和自适应模块，有效解决了混合退化场景下的统一高光谱图像恢复问题，具有强泛化性。

Abstract: Unified hyperspectral image (HSI) restoration aims to recover various degraded HSIs using a single model, offering great practical value. However, existing methods often depend on explicit degradation priors (e.g., degradation labels) as prompts to guide restoration, which are difficult to obtain due to complex and mixed degradations in real-world scenarios. To address this challenge, we propose a Degradation-Aware Metric Prompting (DAMP) framework. Instead of relying on predefined degradation priors, we design spatial-spectral degradation metrics to continuously quantify multi-dimensional degradations, serving as Degradation Prompts (DP). These DP enable the model to capture cross-task similarities in degradation distributions and enhance shared feature learning. Furthermore, we introduce a Spatial-Spectral Adaptive Module (SSAM) that dynamically modulates spatial and spectral feature extraction through learnable parameters. By integrating SSAM as experts within a Mixture-of-Experts architecture, and using DP as the gating router, the framework enables adaptive, efficient, and robust restoration under diverse, mixed, or unseen degradations. Extensive experiments on natural and remote sensing HSI datasets show that DAMP achieves state-of-the-art performance and demonstrates exceptional generalization capability. Code is publicly available at https://github.com/MiliLab/DAMP.

</details>


### [43] [BiCoR-Seg: Bidirectional Co-Refinement Framework for High-Resolution Remote Sensing Image Segmentation](https://arxiv.org/abs/2512.20255)
*Jinghao Shi,Jianing Song*

Main category: cs.CV

TL;DR: 提出了BiCoR-Seg框架，通过热图驱动的双向信息协同模块HBIS和层次监督策略解决高分辨率遥感图像语义分割中的类间相似性和类内变异性问题


<details>
  <summary>Details</summary>
Motivation: 高分辨率遥感图像语义分割面临高类间相似性和大类内变异性的挑战，现有方法难以将抽象且有区分性的语义知识有效注入像素级特征学习，导致复杂场景中边界模糊和类别混淆

Method: 设计了热图驱动的双向信息协同模块HBIS，通过生成类别级热图建立特征图和类别嵌入之间的双向信息流；基于HBIS引入层次监督策略；提出跨层类别嵌入Fisher判别损失来提高嵌入表示的区分能力

Result: 在LoveDA、Vaihingen和Potsdam数据集上的大量实验表明，BiCoR-Seg实现了出色的分割性能，同时提供了更强的可解释性

Conclusion: BiCoR-Seg框架有效解决了高分辨率遥感图像语义分割的关键挑战，在提升分割性能的同时增强了模型的可解释性

Abstract: High-resolution remote sensing image semantic segmentation (HRSS) is a fundamental yet critical task in the field of Earth observation. However, it has long faced the challenges of high inter-class similarity and large intra-class variability. Existing approaches often struggle to effectively inject abstract yet strongly discriminative semantic knowledge into pixel-level feature learning, leading to blurred boundaries and class confusion in complex scenes. To address these challenges, we propose Bidirectional Co-Refinement Framework for HRSS (BiCoR-Seg). Specifically, we design a Heatmap-driven Bidirectional Information Synergy Module (HBIS), which establishes a bidirectional information flow between feature maps and class embeddings by generating class-level heatmaps. Based on HBIS, we further introduce a hierarchical supervision strategy, where the interpretable heatmaps generated by each HBIS module are directly utilized as low-resolution segmentation predictions for supervision, thereby enhancing the discriminative capacity of shallow features. In addition, to further improve the discriminability of the embedding representations, we propose a cross-layer class embedding Fisher Discriminative Loss to enforce intra-class compactness and enlarge inter-class separability. Extensive experiments on the LoveDA, Vaihingen, and Potsdam datasets demonstrate that BiCoR-Seg achieves outstanding segmentation performance while offering stronger interpretability. The released code is available at https://github.com/ShiJinghao566/BiCoR-Seg.

</details>


### [44] [LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation](https://arxiv.org/abs/2512.20257)
*Daniele Cardullo,Simone Teglia,Irene Amerini*

Main category: cs.CV

TL;DR: LADLE-MM是一种在有限标注设置下工作的多模态虚假信息检测器，它通过模型初始化组合和减少可训练参数，在资源受限时仍能保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 随着多媒体生成工具的普及，多模态虚假信息威胁加剧。现有检测方法通常计算密集或需要大量标注数据，难以在资源受限环境下应用。

Method: LADLE-MM包含两个单模态分支和一个多模态分支，利用BLIP提取的多模态嵌入作为固定参考空间增强表示，显著减少可训练参数。

Result: 在DGM4基准测试中，使用60.3%更少的可训练参数仍达到竞争性性能；在VERITE数据集上超越使用复杂LVLM架构的最先进方法。

Conclusion: LADLE-MM展示了在有限标注和训练资源下的有效检测能力，具有强大的泛化能力和对单模态偏见的鲁棒性。

Abstract: With the rise of easily accessible tools for generating and manipulating multimedia content, realistic synthetic alterations to digital media have become a widespread threat, often involving manipulations across multiple modalities simultaneously. Recently, such techniques have been increasingly employed to distort narratives of important events and to spread misinformation on social media, prompting the development of misinformation detectors. In the context of misinformation conveyed through image-text pairs, several detection methods have been proposed. However, these approaches typically rely on computationally intensive architectures or require large amounts of annotated data. In this work we introduce LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation, a model-soup initialized multimodal misinformation detector designed to operate under a limited annotation setup and constrained training resources. LADLE-MM is composed of two unimodal branches and a third multimodal one that enhances image and text representations with additional multimodal embeddings extracted from BLIP, serving as fixed reference space. Despite using 60.3% fewer trainable parameters than previous state-of-the-art models, LADLE-MM achieves competitive performance on both binary and multi-label classification tasks on the DGM4 benchmark, outperforming existing methods when trained without grounding annotations. Moreover, when evaluated on the VERITE dataset, LADLE-MM outperforms current state-of-the-art approaches that utilize more complex architectures involving Large Vision-Language-Models, demonstrating the effective generalization ability in an open-set setting and strong robustness to unimodal bias.

</details>


### [45] [${D}^{3}${ETOR}: ${D}$ebate-Enhanced Pseudo Labeling and Frequency-Aware Progressive ${D}$ebiasing for Weakly-Supervised Camouflaged Object ${D}$etection with Scribble Annotations](https://arxiv.org/abs/2512.20260)
*Jiawei Ge,Jiuxin Cao,Xinyi Li,Xuelin Zhu,Chang Liu,Bo Liu,Chen Feng,Ioannis Patras*

Main category: cs.CV

TL;DR: D³ETOR是一个两阶段的弱监督伪装目标检测框架，通过辩论增强的伪标签和频率感知的渐进去偏技术，解决了现有方法在伪标签质量和注释偏差方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督伪装目标检测方法存在两个主要问题：一是通用分割模型生成的伪标签不可靠，缺乏任务特定的语义理解；二是忽略了涂鸦注释的固有偏差，无法捕获伪装目标的全局结构。

Method: 提出两阶段框架：第一阶段采用自适应熵驱动点采样和多智能体辩论机制增强SAM模型；第二阶段设计FADeNet网络，融合多级频率感知特征并动态调整监督权重来缓解注释偏差。

Result: D³ETOR在多个基准测试上达到了最先进的性能，显著缩小了弱监督与全监督伪装目标检测之间的差距。

Conclusion: 通过联合利用伪标签和涂鸦语义的监督信号，该方法有效解决了弱监督伪装目标检测的关键挑战。

Abstract: Weakly-Supervised Camouflaged Object Detection (WSCOD) aims to locate and segment objects that are visually concealed within their surrounding scenes, relying solely on sparse supervision such as scribble annotations. Despite recent progress, existing WSCOD methods still lag far behind fully supervised ones due to two major limitations: (1) the pseudo masks generated by general-purpose segmentation models (e.g., SAM) and filtered via rules are often unreliable, as these models lack the task-specific semantic understanding required for effective pseudo labeling in COD; and (2) the neglect of inherent annotation bias in scribbles, which hinders the model from capturing the global structure of camouflaged objects. To overcome these challenges, we propose ${D}^{3}$ETOR, a two-stage WSCOD framework consisting of Debate-Enhanced Pseudo Labeling and Frequency-Aware Progressive Debiasing. In the first stage, we introduce an adaptive entropy-driven point sampling method and a multi-agent debate mechanism to enhance the capability of SAM for COD, improving the interpretability and precision of pseudo masks. In the second stage, we design FADeNet, which progressively fuses multi-level frequency-aware features to balance global semantic understanding with local detail modeling, while dynamically reweighting supervision strength across regions to alleviate scribble bias. By jointly exploiting the supervision signals from both the pseudo masks and scribble semantics, ${D}^{3}$ETOR significantly narrows the gap between weakly and fully supervised COD, achieving state-of-the-art performance on multiple benchmarks.

</details>


### [46] [UbiQVision: Quantifying Uncertainty in XAI for Image Recognition](https://arxiv.org/abs/2512.20288)
*Akshat Dubey,Aleksandar Anžel,Bahar İlgen,Georges Hattab*

Main category: cs.CV

TL;DR: 该论文提出了一种量化SHAP解释不确定性的新框架，针对医学影像应用中SHAP方法存在的不稳定性问题，通过狄利克雷后验采样和证据理论来处理认知和随机不确定性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学影像领域的复杂架构虽然提升了性能，但牺牲了可解释性。SHAP作为主流解释方法，在存在不确定性时解释结果不稳定不可靠，这限制了其在关键医学应用中的可信度。

Method: 采用狄利克雷后验采样和登普斯特-谢弗理论，结合信念图、似然图和融合图方法，配合统计定量分析来量化SHAP解释中的不确定性。

Result: 在三个具有不同类别分布、图像质量和模态类型的医学影像数据集上进行了评估，涵盖了病理学、眼科和放射学领域，成功量化了由图像分辨率和模态特性差异引入的认知不确定性。

Conclusion: 提出的框架能够有效量化SHAP解释中的不确定性，为医学影像领域的可解释AI提供了更可靠的工具，特别是在处理复杂多变的医学数据时具有重要意义。

Abstract: Recent advances in deep learning have led to its widespread adoption across diverse domains, including medical imaging. This progress is driven by increasingly sophisticated model architectures, such as ResNets, Vision Transformers, and Hybrid Convolutional Neural Networks, that offer enhanced performance at the cost of greater complexity. This complexity often compromises model explainability and interpretability. SHAP has emerged as a prominent method for providing interpretable visualizations that aid domain experts in understanding model predictions. However, SHAP explanations can be unstable and unreliable in the presence of epistemic and aleatoric uncertainty. In this study, we address this challenge by using Dirichlet posterior sampling and Dempster-Shafer theory to quantify the uncertainty that arises from these unstable explanations in medical imaging applications. The framework uses a belief, plausible, and fusion map approach alongside statistical quantitative analysis to produce quantification of uncertainty in SHAP. Furthermore, we evaluated our framework on three medical imaging datasets with varying class distributions, image qualities, and modality types which introduces noise due to varying image resolutions and modality-specific aspect covering the examples from pathology, ophthalmology, and radiology, introducing significant epistemic uncertainty.

</details>


### [47] [TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation](https://arxiv.org/abs/2512.20296)
*Ji-Hoon Kim,Junseok Ahn,Doyeop Kwak,Joon Son Chung,Shinji Watanabe*

Main category: cs.CV

TL;DR: TAVID框架从文本和参考图像联合合成交互式视频和会话语音，通过双向跨模态映射器同步生成面部动作和语音，解决了传统方法中音视频分离的问题。


<details>
  <summary>Details</summary>
Motivation: 构建类人对话系统需要同时处理面部表情和语音的紧密耦合交互，但现有研究通常孤立处理说话头或倾听头生成，忽略了多模态交互的本质。

Method: TAVID通过运动映射器和说话者映射器两个跨模态映射器，实现音频和视觉模态间双向信息交换，整合面部和语音生成流程。

Result: 在说话头真实感、倾听头响应性、二元交互流畅性和语音质量四个维度上的广泛实验验证了方法的有效性。

Conclusion: TAVID提供了一种统一框架，能够同步生成高质量的面部动作和会话语音，显著提升了多模态对话系统的自然度和交互性。

Abstract: The objective of this paper is to jointly synthesize interactive videos and conversational speech from text and reference images. With the ultimate goal of building human-like conversational systems, recent studies have explored talking or listening head generation as well as conversational speech generation. However, these works are typically studied in isolation, overlooking the multimodal nature of human conversation, which involves tightly coupled audio-visual interactions. In this paper, we introduce TAVID, a unified framework that generates both interactive faces and conversational speech in a synchronized manner. TAVID integrates face and speech generation pipelines through two cross-modal mappers (i.e., a motion mapper and a speaker mapper), which enable bidirectional exchange of complementary information between the audio and visual modalities. We evaluate our system across four dimensions: talking face realism, listening head responsiveness, dyadic interaction fluency, and speech quality. Extensive experiments demonstrate the effectiveness of our approach across all these aspects.

</details>


### [48] [The devil is in the details: Enhancing Video Virtual Try-On via Keyframe-Driven Details Injection](https://arxiv.org/abs/2512.20340)
*Qingdong He,Xueqin Chen,Yanjie Pan,Peng Tang,Pengcheng Xu,Zhenye Gan,Chengjie Wang,Xiaobin Hu,Jiangning Zhang,Yabiao Wang*

Main category: cs.CV

TL;DR: KeyTailor是一个基于关键帧驱动的视频虚拟试穿框架，通过关键帧采样和细节注入策略解决了现有方法在服装动态细节和背景一致性方面的不足，同时避免了额外的计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散变换器的视频虚拟试穿方法难以捕捉细粒度服装动态并保持视频帧间的背景完整性，同时存在计算成本高和数据质量受限的问题。

Method: 采用指令引导的关键帧采样策略筛选信息丰富的帧，通过服装细节增强模块和协作背景优化模块提取服装动态并优化背景一致性，然后将这些细节注入标准扩散变换器块中。

Result: 实验表明KeyTailor在动态和静态场景下在服装保真度和背景完整性方面均优于最先进的基线方法。

Conclusion: KeyTailor框架通过关键帧驱动的细节注入策略有效提升了视频虚拟试穿的质量和效率，同时提出的ViT-HD数据集为模型训练提供了高质量数据支持。

Abstract: Although diffusion transformer (DiT)-based video virtual try-on (VVT) has made significant progress in synthesizing realistic videos, existing methods still struggle to capture fine-grained garment dynamics and preserve background integrity across video frames. They also incur high computational costs due to additional interaction modules introduced into DiTs, while the limited scale and quality of existing public datasets also restrict model generalization and effective training. To address these challenges, we propose a novel framework, KeyTailor, along with a large-scale, high-definition dataset, ViT-HD. The core idea of KeyTailor is a keyframe-driven details injection strategy, motivated by the fact that keyframes inherently contain both foreground dynamics and background consistency. Specifically, KeyTailor adopts an instruction-guided keyframe sampling strategy to filter informative frames from the input video. Subsequently,two tailored keyframe-driven modules, the garment details enhancement module and the collaborative background optimization module, are employed to distill garment dynamics into garment-related latents and to optimize the integrity of background latents, both guided by keyframes.These enriched details are then injected into standard DiT blocks together with pose, mask, and noise latents, enabling efficient and realistic try-on video synthesis. This design ensures consistency without explicitly modifying the DiT architecture, while simultaneously avoiding additional complexity. In addition, our dataset ViT-HD comprises 15, 070 high-quality video samples at a resolution of 810*1080, covering diverse garments. Extensive experiments demonstrate that KeyTailor outperforms state-of-the-art baselines in terms of garment fidelity and background integrity across both dynamic and static scenarios.

</details>


### [49] [CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation](https://arxiv.org/abs/2512.20362)
*V. Kovalev,A. Kuvshinov,A. Buzovkin,D. Pokidov,D. Timonin*

Main category: cs.CV

TL;DR: CRAFT框架通过结构化推理和约束驱动的方法，在无需重新训练的情况下提升多模态图像生成的准确性和可靠性


<details>
  <summary>Details</summary>
Motivation: 现有推理方法依赖模糊的整体评价或无约束的提示词重写，缺乏可解释性、可控性和可靠停止机制。受大语言模型结构化思维启发，希望将验证、针对性修正和早停机制引入多模态生成

Method: 将提示分解为依赖结构化的视觉问题，使用视觉语言模型验证生成图像，通过LLM智能体仅在约束失败时进行针对性提示词编辑，迭代执行直到满足所有约束条件

Result: 在多个模型家族和挑战性基准测试中，CRAFT持续提升组合准确性、文本渲染和偏好评估，特别是对轻量级生成器效果显著，推理时间开销可忽略

Conclusion: 显式结构化、约束驱动的推理时推理是提高多模态生成模型可靠性的关键要素，可使小型廉价模型达到更昂贵系统的质量水平

Abstract: Recent work has shown that inference-time reasoning and reflection can improve text-to-image generation without retraining. However, existing approaches often rely on implicit, holistic critiques or unconstrained prompt rewrites, making their behavior difficult to interpret, control, or stop reliably. In contrast, large language models have benefited from explicit, structured forms of **thinking** based on verification, targeted correction, and early stopping.
  We introduce CRAFT (Continuous Reasoning and Agentic Feedback Tuning), a training-free, model-agnostic framework that brings this structured reasoning paradigm to multimodal image generation. CRAFT decomposes a prompt into dependency-structured visual questions, veries generated images using a vision-language model, and applies targeted prompt edits through an LLM agent only where constraints fail. The process iterates with an explicit stopping criterion once all constraints are satised, yielding an interpretable and controllable inference-time renement loop.
  Across multiple model families and challenging benchmarks, CRAFT consistently improves compositional accuracy, text rendering, and preference-based evaluations, with particularly strong gains for lightweight generators. Importantly, these improvements incur only a negligible inference-time overhead, allowing smaller or cheaper models to approach the quality of substantially more expensive systems. Our results suggest that explicitly structured, constraint-driven inference-time reasoning is a key ingredient for improving the reliability of multimodal generative models.

</details>


### [50] [Linking Faces and Voices Across Languages: Insights from the FAME 2026 Challenge](https://arxiv.org/abs/2512.20376)
*Marta Moscati,Ahmed Abdullah,Muhammad Saad Saeed,Shah Nawaz,Rohan Kumar Das,Muhammad Zaigham Zaheer,Junaid Mir,Muhammad Haroon Yousaf,Khalid Mahmood Malik,Markus Schedl*

Main category: cs.CV

TL;DR: FAME 2026挑战赛旨在开发在多语言环境中有效的人脸-声音关联方法


<details>
  <summary>Details</summary>
Motivation: 全球半数以上人口是双语者，且人们经常在多语言场景下交流，需要能够在测试语言与训练语言不同时仍有效的人脸-声音关联技术

Method: 挑战赛聚焦于开发当测试时语言与训练语言不同时仍能有效进行人脸-声音关联的方法

Result: 这是对挑战赛的简要总结报告

Conclusion: FAME 2026挑战赛针对多语言环境下的人脸-声音关联问题提出了重要研究方向

Abstract: Over half of the world's population is bilingual and people often communicate under multilingual scenarios. The Face-Voice Association in Multilingual Environments (FAME) 2026 Challenge, held at ICASSP 2026, focuses on developing methods for face-voice association that are effective when the language at test-time is different than the training one. This report provides a brief summary of the challenge.

</details>


### [51] [SmartSplat: Feature-Smart Gaussians for Scalable Compression of Ultra-High-Resolution Images](https://arxiv.org/abs/2512.20377)
*Linfei Li,Lin Zhang,Zhong Wang,Ying Shen*

Main category: cs.CV

TL;DR: SmartSplat是一种基于高斯泼溅的图像压缩框架，通过梯度-颜色引导变分采样和自适应高斯颜色采样，实现超高分辨率图像的高效压缩和高质量重建。


<details>
  <summary>Details</summary>
Motivation: 生成式AI快速发展导致超高分辨率视觉内容爆炸式增长，传统压缩方法在压缩比和重建质量之间难以平衡，特别是在超高分辨率场景下。2D高斯图像模型虽然提高了表示效率，但现有方法在高分辨率场景下仍有不足。

Method: 提出SmartSplat框架，包含梯度-颜色引导变分采样策略、基于排他性的均匀采样方案和尺度自适应高斯颜色采样方法。通过联合优化空间布局、尺度和颜色初始化，用有限数量的高斯有效捕获局部结构和全局纹理。

Result: 在DIV8K和新建的16K数据集上的实验表明，SmartSplat在相同压缩比下优于现有最先进方法，并能突破其压缩极限，显示出强大的可扩展性和实际应用价值。

Conclusion: SmartSplat成功解决了超高分辨率图像压缩中压缩比与重建质量的平衡问题，为生成式AI内容的实时解码和高效压缩提供了有效解决方案。

Abstract: Recent advances in generative AI have accelerated the production of ultra-high-resolution visual content, posing significant challenges for efficient compression and real-time decoding on end-user devices. Inspired by 3D Gaussian Splatting, recent 2D Gaussian image models improve representation efficiency, yet existing methods struggle to balance compression ratio and reconstruction fidelity in ultra-high-resolution scenarios. To address this issue, we propose SmartSplat, a highly adaptive and feature-aware GS-based image compression framework that supports arbitrary image resolutions and compression ratios. SmartSplat leverages image-aware features such as gradients and color variances, introducing a Gradient-Color Guided Variational Sampling strategy together with an Exclusion-based Uniform Sampling scheme to improve the non-overlapping coverage of Gaussian primitives in pixel space. In addition, we propose a Scale-Adaptive Gaussian Color Sampling method to enhance color initialization across scales. Through joint optimization of spatial layout, scale, and color initialization, SmartSplat efficiently captures both local structures and global textures using a limited number of Gaussians, achieving high reconstruction quality under strong compression. Extensive experiments on DIV8K and a newly constructed 16K dataset demonstrate that SmartSplat consistently outperforms state-of-the-art methods at comparable compression ratios and exceeds their compression limits, showing strong scalability and practical applicability. The code is publicly available at https://github.com/lif314/SmartSplat.

</details>


### [52] [DETACH : Decomposed Spatio-Temporal Alignment for Exocentric Video and Ambient Sensors with Staged Learning](https://arxiv.org/abs/2512.20409)
*Junho Yoon,Jaemo Jung,Hyunju Kim,Dongman Lee*

Main category: cs.CV

TL;DR: 这篇论文提出了DETACH框架，通过分解的时空处理解决外中心视频与传感器对齐问题，改进了传统全局对齐方法的局限性，在动作识别任务中取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统基于穿戴传感器的自我中心视频对齐存在用户不适、隐私问题和扩展性限制，需要探索非侵入性、可扩展的外中心视频与传感器对齐方案，但现有全局对齐方法在该场景下存在无法捕捉局部细节和依赖时间模式导致错误对齐的问题。

Method: 提出DETACH分解时空框架：通过显式分解保留局部细节；利用在线聚类发现传感器空间特征进行语义标注；采用两阶段对齐方法，先通过相互监督建立空间对应关系，再通过时空加权对比损失进行时间对齐。

Result: 在Opportunity++和HWU-USP数据集上的下游任务实验表明，相比适应的自我中心-穿戴基线方法有显著改进。

Conclusion: DETACH框架有效解决了外中心视频与传感器对齐的两个关键问题，为非侵入式动作识别提供了可行的解决方案。

Abstract: Aligning egocentric video with wearable sensors have shown promise for human action recognition, but face practical limitations in user discomfort, privacy concerns, and scalability. We explore exocentric video with ambient sensors as a non-intrusive, scalable alternative. While prior egocentric-wearable works predominantly adopt Global Alignment by encoding entire sequences into unified representations, this approach fails in exocentric-ambient settings due to two problems: (P1) inability to capture local details such as subtle motions, and (P2) over-reliance on modality-invariant temporal patterns, causing misalignment between actions sharing similar temporal patterns with different spatio-semantic contexts. To resolve these problems, we propose DETACH, a decomposed spatio-temporal framework. This explicit decomposition preserves local details, while our novel sensor-spatial features discovered via online clustering provide semantic grounding for context-aware alignment. To align the decomposed features, our two-stage approach establishes spatial correspondence through mutual supervision, then performs temporal alignment via a spatial-temporal weighted contrastive loss that adaptively handles easy negatives, hard negatives, and false negatives. Comprehensive experiments with downstream tasks on Opportunity++ and HWU-USP datasets demonstrate substantial improvements over adapted egocentric-wearable baselines.

</details>


### [53] [Chain-of-Anomaly Thoughts with Large Vision-Language Models](https://arxiv.org/abs/2512.20417)
*Pedro Domingos,João Pereira,Vasco Lopes,João Neves,David Semedo*

Main category: cs.CV

TL;DR: <tldr>


<details>
  <summary>Details</summary>
Motivation: <motivation>

Method: <method>

Result: <result>

Conclusion: <conclusion>

Abstract: Automated video surveillance with Large Vision-Language Models is limited by their inherent bias towards normality, often failing to detect crimes. While Chain-of-Thought reasoning strategies show significant potential for improving performance in language tasks, the lack of inductive anomaly biases in their reasoning further steers the models towards normal interpretations. To address this, we propose Chain-of-Anomaly-Thoughts (CoAT), a multi-agent reasoning framework that introduces inductive criminal bias in the reasoning process through a final, anomaly-focused classification layer. Our method significantly improves Anomaly Detection, boosting F1-score by 11.8 p.p. on challenging low-resolution footage and Anomaly Classification by 3.78 p.p. in high-resolution videos.

</details>


### [54] [Skin Lesion Classification Using a Soft Voting Ensemble of Convolutional Neural Networks](https://arxiv.org/abs/2512.20431)
*Abdullah Al Shafi,Abdul Muntakim,Pintu Chandra Shill,Rowzatul Zannat,Abdullah Al-Amin*

Main category: cs.CV

TL;DR: 提出了一种使用CNN软投票集成的方法进行早期皮肤癌分类，通过平衡数据集、图像增强和分割预处理，在三个基准数据集上取得了96.32%、90.86%和93.92%的高精度。


<details>
  <summary>Details</summary>
Motivation: 早期检测皮肤癌能显著提高生存率，人工智能技术特别是CNN能提升诊断准确性。本文旨在开发一种平衡准确性和速度的实用皮肤癌分类方法。

Method: 采用软投票集成三个CNN模型（MobileNetV2、VGG19、InceptionV3），先对三个数据集进行重新平衡、图像增强和过滤，使用混合双编码器进行病灶分割，然后进行分类。

Result: 在HAM10000、ISIC 2016和ISIC 2019三个数据集上的病灶识别准确率分别达到96.32%、90.86%和93.92%，性能评估指标表现出色。

Conclusion: 提出的软投票集成CNN方法在皮肤癌分类中表现出高准确性和实用性，通过准确的病灶分割和模型集成平衡了诊断精度和实时部署需求。

Abstract: Skin cancer can be identified by dermoscopic examination and ocular inspection, but early detection significantly increases survival chances. Artificial intelligence (AI), using annotated skin images and Convolutional Neural Networks (CNNs), improves diagnostic accuracy. This paper presents an early skin cancer classification method using a soft voting ensemble of CNNs. In this investigation, three benchmark datasets, namely HAM10000, ISIC 2016, and ISIC 2019, were used. The process involved rebalancing, image augmentation, and filtering techniques, followed by a hybrid dual encoder for segmentation via transfer learning. Accurate segmentation focused classification models on clinically significant features, reducing background artifacts and improving accuracy. Classification was performed through an ensemble of MobileNetV2, VGG19, and InceptionV3, balancing accuracy and speed for real-world deployment. The method achieved lesion recognition accuracies of 96.32\%, 90.86\%, and 93.92\% for the three datasets. The system performance was evaluated using established skin lesion detection metrics, yielding impressive results.

</details>


### [55] [High Dimensional Data Decomposition for Anomaly Detection of Textured Images](https://arxiv.org/abs/2512.20432)
*Ji Song,Xing Wang,Jianguo Wu,Xiaowei Yue*

Main category: cs.CV

TL;DR: 本文提出了纹理基底集成平滑分解(TBSD)方法，用于纹理图像中的高效异常检测。该方法通过提取准周期性纹理模式并用其作为先验知识，有效减少误识别并提升异常检测精度。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测方法在处理纹理缺陷图像时存在误识别、鲁棒性差、对大型结构化数据集依赖过强等问题。针对具有平滑背景和稀疏异常的纹理图像，需要更有效的检测方法。

Method: TBSD方法包含两个主要过程：首先学习纹理基底函数以提取准周期性纹理模式，然后利用纹理基底作为先验知识进行异常检测。该方法研究了准周期性的数学公式及其理论特性。

Result: 在仿真和真实数据集上的实验表明，TBSD方法相比基准方法具有更少的误识别、更小的训练数据集需求以及更优异的异常检测性能。

Conclusion: TBSD方法能够有效处理纹理图像的异常检测问题，通过纹理基底集成平滑分解技术显著提升了检测精度和鲁棒性，同时降低了对大规模数据集的依赖。

Abstract: In the realm of diverse high-dimensional data, images play a significant role across various processes of manufacturing systems where efficient image anomaly detection has emerged as a core technology of utmost importance. However, when applied to textured defect images, conventional anomaly detection methods have limitations including non-negligible misidentification, low robustness, and excessive reliance on large-scale and structured datasets. This paper proposes a texture basis integrated smooth decomposition (TBSD) approach, which is targeted at efficient anomaly detection in textured images with smooth backgrounds and sparse anomalies. Mathematical formulation of quasi-periodicity and its theoretical properties are investigated for image texture estimation. TBSD method consists of two principal processes: the first process learns the texture basis functions to effectively extract quasi-periodic texture patterns; the subsequent anomaly detection process utilizes that texture basis as prior knowledge to prevent texture misidentification and capture potential anomalies with high accuracy.The proposed method surpasses benchmarks with less misidentification, smaller training dataset requirement, and superior anomaly detection performance on both simulation and real-world datasets.

</details>


### [56] [Beyond Motion Pattern: An Empirical Study of Physical Forces for Human Motion Understanding](https://arxiv.org/abs/2512.20451)
*Anh Dao,Manh Tran,Yufei Zhang,Xiaoming Liu,Zijun Cui*

Main category: cs.CV

TL;DR: 该论文通过在运动理解任务中引入物理推断的力特征，系统评估了生物力学力信息在步态识别、动作识别和视频描述等任务中的提升效果，证明了力特征在动态、遮挡或外观变化条件下对视觉和运动学特征的有效补充。


<details>
  <summary>Details</summary>
Motivation: 现有运动理解方法大多忽视了生物力学中的关节驱动力等物理线索，无法充分利用运动过程中的物理特性。论文旨在探究物理推断的力是否以及何时能够增强运动理解能力。

Method: 通过将力特征整合到现有的运动理解流程中，系统评估了力信息在步态识别(CASIA-B、Gait3D)、动作识别(Penn Action)和精细视频描述(Qwen2.5-VL)三个主要任务上对基线模型的性能提升。

Result: 在8个基准测试中，引入力特征均带来了性能提升：CASIA-B步态识别准确率提高0.87%，穿外套条件下提升2.7%，侧视条件下提升3.0%；Gait3D提升1.3%；Penn Action动作识别提升2.00%，高耗能动作如拳击/拍打提升6.96%；视频描述的ROUGE-L得分从0.310提升至0.339。

Conclusion: 物理推断的力特征能够在动态、遮挡或外观变化条件下显著补充视觉和运动学特征，提升运动理解任务的性能表现，证明了力线索在运动分析中的重要性。

Abstract: Human motion understanding has advanced rapidly through vision-based progress in recognition, tracking, and captioning. However, most existing methods overlook physical cues such as joint actuation forces that are fundamental in biomechanics. This gap motivates our study: if and when do physically inferred forces enhance motion understanding? By incorporating forces into established motion understanding pipelines, we systematically evaluate their impact across baseline models on 3 major tasks: gait recognition, action recognition, and fine-grained video captioning. Across 8 benchmarks, incorporating forces yields consistent performance gains; for example, on CASIA-B, Rank-1 gait recognition accuracy improved from 89.52% to 90.39% (+0.87), with larger gain observed under challenging conditions: +2.7% when wearing a coat and +3.0% at the side view. On Gait3D, performance also increases from 46.0% to 47.3% (+1.3). In action recognition, CTR-GCN achieved +2.00% on Penn Action, while high-exertion classes like punching/slapping improved by +6.96%. Even in video captioning, Qwen2.5-VL's ROUGE-L score rose from 0.310 to 0.339 (+0.029), indicating that physics-inferred forces enhance temporal grounding and semantic richness. These results demonstrate that force cues can substantially complement visual and kinematic features under dynamic, occluded, or appearance-varying conditions.

</details>


### [57] [UTDesign: A Unified Framework for Stylized Text Editing and Generation in Graphic Design Images](https://arxiv.org/abs/2512.20479)
*Yiming Zhao,Yuanpeng Gao,Yuxuan Luo,Jiwei Duan,Shisong Lin,Longfei Xiong,Zhouhui Lian*

Main category: cs.CV

TL;DR: UTDesign是一个统一的框架，用于设计图像中的高精度风格化文本编辑和条件文本生成，支持英文和中文脚本，通过DiT-based文本风格转换模型和条件编码器实现状态一致性和文本准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在视觉内容生成方面表现出色，但其在小规模排版和非拉丁字符（如中文）上的文本渲染性能仍然有限。

Method: 提出UTDesign框架，包括从零开始在合成数据集上训练的DiT-based文本风格转换模型，生成透明RGBA文本前景；扩展为条件文本生成框架，通过多模态条件编码器在背景图像、提示和布局规范下合成文本；集成预训练文本到图像模型和MLLM-based布局规划器构建全自动文本到设计管道。

Result: 大量实验表明，UTDesign在开源方法中实现了风格一致性和文本准确性的最新性能，并在与专有商业方法比较中展现出独特优势。

Conclusion: UTDesign通过其统一的框架和创新的模型设计，有效提升了设计图像中文本编辑和生成的精度和一致性，特别是在支持中文等非拉丁字符方面表现突出。

Abstract: AI-assisted graphic design has emerged as a powerful tool for automating the creation and editing of design elements such as posters, banners, and advertisements. While diffusion-based text-to-image models have demonstrated strong capabilities in visual content generation, their text rendering performance, particularly for small-scale typography and non-Latin scripts, remains limited. In this paper, we propose UTDesign, a unified framework for high-precision stylized text editing and conditional text generation in design images, supporting both English and Chinese scripts. Our framework introduces a novel DiT-based text style transfer model trained from scratch on a synthetic dataset, capable of generating transparent RGBA text foregrounds that preserve the style of reference glyphs. We further extend this model into a conditional text generation framework by training a multi-modal condition encoder on a curated dataset with detailed text annotations, enabling accurate, style-consistent text synthesis conditioned on background images, prompts, and layout specifications. Finally, we integrate our approach into a fully automated text-to-design (T2D) pipeline by incorporating pre-trained text-to-image (T2I) models and an MLLM-based layout planner. Extensive experiments demonstrate that UTDesign achieves state-of-the-art performance among open-source methods in terms of stylistic consistency and text accuracy, and also exhibits unique advantages compared to proprietary commercial approaches. Code and data for this paper are available at https://github.com/ZYM-PKU/UTDesign.

</details>


### [58] [Multi-temporal Adaptive Red-Green-Blue and Long-Wave Infrared Fusion for You Only Look Once-Based Landmine Detection from Unmanned Aerial Systems](https://arxiv.org/abs/2512.20487)
*James E. Gallagher,Edward J. Oughton,Jana Kosecka*

Main category: cs.CV

TL;DR: 这篇论文评估了使用自适应RGB和长波红外融合的无人机系统探测地表地雷，YOLOv11在特定热融合参数下达到最佳性能（86.8% mAP），但存在准确性与效率的权衡。


<details>
  <summary>Details</summary>
Motivation: 地雷是人道主义持久威胁，全球有1.1亿枚活跃地雷，每年造成2.6万人伤亡，需要高效探测技术。

Method: 使用YOLO架构（v8、v10、v11）在114张测试图像上进行评估，比较RGB和LWIR融合，分析不同热融合参数和训练数据集的影响。

Result: YOLOv11在10-30%热融合和5-10米高度下达到最佳性能（86.8% mAP），反坦克地雷探测准确率（61.9%）远高于反人员地雷（19.2%）。

Conclusion: 多时相训练数据集优于季节特定方法，YOLOv11在准确性和效率间提供了最佳平衡，未来需研究不同埋藏深度和土壤类型的热对比效应。

Abstract: Landmines remain a persistent humanitarian threat, with 110 million actively deployed mines across 60 countries, claiming 26,000 casualties annually. This research evaluates adaptive Red-Green-Blue (RGB) and Long-Wave Infrared (LWIR) fusion for Unmanned Aerial Systems (UAS)-based detection of surface-laid landmines, leveraging the thermal contrast between the ordnance and the surrounding soil to enhance feature extraction. Using You Only Look Once (YOLO) architectures (v8, v10, v11) across 114 test images, generating 35,640 model-condition evaluations, YOLOv11 achieved optimal performance (86.8% mAP), with 10 to 30% thermal fusion at 5 to 10m altitude identified as the optimal detection parameters. A complementary architectural comparison revealed that while RF-DETR achieved the highest accuracy (69.2% mAP), followed by Faster R-CNN (67.6%), YOLOv11 (64.2%), and RetinaNet (50.2%), YOLOv11 trained 17.7 times faster than the transformer-based RF-DETR (41 minutes versus 12 hours), presenting a critical accuracy-efficiency tradeoff for operational deployment. Aggregated multi-temporal training datasets outperformed season-specific approaches by 1.8 to 9.6%, suggesting that models benefit from exposure to diverse thermal conditions. Anti-Tank (AT) mines achieved 61.9% detection accuracy, compared with 19.2% for Anti-Personnel (AP) mines, reflecting both the size differential and thermal-mass differences between these ordnance classes. As this research examined surface-laid mines where thermal contrast is maximized, future research should quantify thermal contrast effects for mines buried at varying depths across heterogeneous soil types.

</details>


### [59] [Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition](https://arxiv.org/abs/2512.20501)
*Gorjan Radevski*

Main category: cs.CV

TL;DR: 该论文系统研究多模态对齐、翻译、融合和迁移方法，通过五个章节分别解决了空间语言理解、医学文本导航、知识图谱构建、组合动作识别和自中心动作识别等挑战。


<details>
  <summary>Details</summary>
Motivation: 提升机器学习对复杂多模态输入的理解能力，包括空间关系、医学文本、结构化知识、视频动作等不同领域的多模态数据处理需求。

Method: 分别提出：Spatial-Reasoning Bert（空间语言到视觉布局）、基于空间共现的医学文本到3D图谱映射、文本到知识图谱实体链接、视频帧与物体检测的多模态融合、多模态知识蒸馏到RGB模型。

Result: 实现了自动场景生成、医学文本可导航性提升、知识图谱提取准确性改善、动作识别鲁棒性增强，以及计算效率优化的多模态能力迁移。

Conclusion: 这些方法显著提升了计算系统处理复杂多模态输入的能力，为跨领域应用提供了有效的技术基础。

Abstract: This manuscript explores multimodal alignment, translation, fusion, and transference to enhance machine understanding of complex inputs. We organize the work into five chapters, each addressing unique challenges in multimodal machine learning.
  Chapter 3 introduces Spatial-Reasoning Bert for translating text-based spatial relations into 2D arrangements between clip-arts. This enables effective decoding of spatial language into visual representations, paving the way for automated scene generation aligned with human spatial understanding.
  Chapter 4 presents a method for translating medical texts into specific 3D locations within an anatomical atlas. We introduce a loss function leveraging spatial co-occurrences of medical terms to create interpretable mappings, significantly enhancing medical text navigability.
  Chapter 5 tackles translating structured text into canonical facts within knowledge graphs. We develop a benchmark for linking natural language to entities and predicates, addressing ambiguities in text extraction to provide clearer, actionable insights.
  Chapter 6 explores multimodal fusion methods for compositional action recognition. We propose a method fusing video frames and object detection representations, improving recognition robustness and accuracy.
  Chapter 7 investigates multimodal knowledge transference for egocentric action recognition. We demonstrate how multimodal knowledge distillation enables RGB-only models to mimic multimodal fusion-based capabilities, reducing computational requirements while maintaining performance.
  These contributions advance methodologies for spatial language understanding, medical text interpretation, knowledge graph enrichment, and action recognition, enhancing computational systems' ability to process complex, multimodal inputs across diverse applications.

</details>


### [60] [SirenPose: Dynamic Scene Reconstruction via Geometric Supervision](https://arxiv.org/abs/2512.20531)
*Kaitong Cai,Jensen Zhang,Jing Yang,Keze Wang*

Main category: cs.CV

TL;DR: SirenPose结合周期激活正弦网络与关键点几何监督，通过物理约束和图表征网络，提升单目视频动态3D重建的时空一致性和运动保真度，在多项基准测试中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在快速运动、多目标交互、遮挡等复杂场景下难以保持运动保真度和时空一致性，需要新的几何感知损失函数来解决这些挑战。

Method: 融合正弦表示网络的周期激活特性与关键点几何监督，引入物理约束增强时空一致性，使用图神经网络建模关键点关系，扩展UniKPT数据集至60万标注实例。

Result: 在Sintel、Bonn和DAVIS基准测试中全面超越现有方法：DAVIS上FVD降低17.8%、FID降低28.7%、LPIPS提升6.0%，姿态估计误差低于Monst3R，运动平滑度和几何精度显著改善。

Conclusion: SirenPose通过几何感知损失设计和物理约束，有效处理快速运动、复杂动态和遮挡场景，实现了高精度、时空一致的3D动态重建。

Abstract: We introduce SirenPose, a geometry-aware loss formulation that integrates the periodic activation properties of sinusoidal representation networks with keypoint-based geometric supervision, enabling accurate and temporally consistent reconstruction of dynamic 3D scenes from monocular videos. Existing approaches often struggle with motion fidelity and spatiotemporal coherence in challenging settings involving fast motion, multi-object interaction, occlusion, and rapid scene changes. SirenPose incorporates physics inspired constraints to enforce coherent keypoint predictions across both spatial and temporal dimensions, while leveraging high frequency signal modeling to capture fine grained geometric details. We further expand the UniKPT dataset to 600,000 annotated instances and integrate graph neural networks to model keypoint relationships and structural correlations. Extensive experiments on benchmarks including Sintel, Bonn, and DAVIS demonstrate that SirenPose consistently outperforms state-of-the-art methods. On DAVIS, SirenPose achieves a 17.8 percent reduction in FVD, a 28.7 percent reduction in FID, and a 6.0 percent improvement in LPIPS compared to MoSCA. It also improves temporal consistency, geometric accuracy, user score, and motion smoothness. In pose estimation, SirenPose outperforms Monst3R with lower absolute trajectory error as well as reduced translational and rotational relative pose error, highlighting its effectiveness in handling rapid motion, complex dynamics, and physically plausible reconstruction.

</details>


### [61] [AlignPose: Generalizable 6D Pose Estimation via Multi-view Feature-metric Alignment](https://arxiv.org/abs/2512.20538)
*Anna Šárová Mikeštíková,Médéric Fourmy,Martin Cífka,Josef Sivic,Vladimir Petrik*

Main category: cs.CV

TL;DR: AlignPose方法通过多视角RGB图像解决单视角6D物体姿态估计的深度模糊、杂乱和遮挡问题，无需特定物体训练或对称标注，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 单视角RGB方法受限于深度模糊、杂乱和遮挡问题，多视角方法虽能解决但现有方法依赖精确的单视角估计或缺乏泛化能力。

Method: AlignPose通过多视角特征度量细化技术，优化单一一致的世界坐标系物体姿态，最小化渲染特征与观察特征在所有视角中的差异。

Result: 在YCB-V、T-LESS、ITODD-MV、HouseCat6D四个数据集上，AlignPose优于其他已发表方法，尤其在工业数据集上表现突出。

Conclusion: 多视角方法能有效解决单视角姿态估计的局限，AlignPose在实践可用的多视角场景中具有显著优势。

Abstract: Single-view RGB model-based object pose estimation methods achieve strong generalization but are fundamentally limited by depth ambiguity, clutter, and occlusions. Multi-view pose estimation methods have the potential to solve these issues, but existing works rely on precise single-view pose estimates or lack generalization to unseen objects. We address these challenges via the following three contributions. First, we introduce AlignPose, a 6D object pose estimation method that aggregates information from multiple extrinsically calibrated RGB views and does not require any object-specific training or symmetry annotation. Second, the key component of this approach is a new multi-view feature-metric refinement specifically designed for object pose. It optimizes a single, consistent world-frame object pose minimizing the feature discrepancy between on-the-fly rendered object features and observed image features across all views simultaneously. Third, we report extensive experiments on four datasets (YCB-V, T-LESS, ITODD-MV, HouseCat6D) using the BOP benchmark evaluation and show that AlignPose outperforms other published methods, especially on challenging industrial datasets where multiple views are readily available in practice.

</details>


### [62] [Multi-Grained Text-Guided Image Fusion for Multi-Exposure and Multi-Focus Scenarios](https://arxiv.org/abs/2512.20556)
*Mingwei Tang,Jiahao Nie,Guang Yang,Ziqing Cui,Jie Li*

Main category: cs.CV

TL;DR: MTIF是一种基于多粒度文本引导的图像融合新方法，通过分层跨模态调制和密集语义增强，显著提升了多曝光和多焦点图像融合质量。


<details>
  <summary>Details</summary>
Motivation: 传统图像融合方法在处理动态范围和焦距差异时存在挑战，现有文本引导方法因使用粗粒度描述而难以理解细节并实现精确跨模态对齐，需要更精细的文本指导机制。

Method: 提出三阶段方法：1）引入细度、结构和语义多粒度文本描述；2）分层跨模态调制模块实现文本引导；3）显著性驱动增强模块扩充训练数据，强化跨模态对齐。

Result: 在多曝光和多焦点图像融合任务上的大量实验表明，MTIF方法持续优于现有方法。

Conclusion: MTIF通过多粒度文本描述和分层跨模态调制，有效解决了图像融合中的细节对齐问题，为文本引导的图像融合提供了新范式。

Abstract: Image fusion aims to synthesize a single high-quality image from a pair of inputs captured under challenging conditions, such as differing exposure levels or focal depths. A core challenge lies in effectively handling disparities in dynamic range and focus depth between the inputs. With the advent of vision-language models, recent methods incorporate textual descriptions as auxiliary guidance to enhance fusion quality. However, simply incorporating coarse-grained descriptions hampers the understanding of fine-grained details and poses challenges for precise cross-modal alignment. To address these limitations, we propose Multi-grained Text-guided Image Fusion (MTIF), a novel fusion paradigm with three key designs. First, it introduces multi-grained textual descriptions that separately capture fine details, structural cues, and semantic content, guiding image fusion through a hierarchical cross-modal modulation module. Second, it involves supervision signals at each granularity to facilitate alignment between visual and textual features and enhance the utility of auxiliary text. Third, it adopts a saliency-driven enrichment module to augment training data with dense semantic content, further strengthening the cross-modal modulation and alignment. Extensive experiments show that MTIF consistently outperforms previous methods on both multi-exposure and multi-focus image fusion tasks.

</details>


### [63] [Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models](https://arxiv.org/abs/2512.20557)
*Shengchao Zhou,Yuxin Chen,Yuying Ge,Wei Huang,Jiehong Lin,Ying Shan,Xiaojuan Qi*

Main category: cs.CV

TL;DR: DSR Suite包含DSR-Train和DSR-Bench数据集以及Geometry Selection Module（GSM）模块，用于增强视觉语言模型的动态空间推理能力，通过利用4D几何先验知识解决现有模型在此任务上的不足。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在动态空间推理（如3D空间中物体几何和关系随时间演变）方面表现较弱，主要由于缺乏可扩展的4D感知训练资源。

Method: 提出了自动化流水线从野外视频生成多选问答对用于DSR；开发了Geometry Selection Module（GSM）将几何先验知识整合到VLMs中，通过提取问题相关几何知识生成紧凑的几何标记。

Result: 实验表明，将DSR-Train和GSM集成到Qwen2.5-VL-7B模型能显著提升其动态空间推理能力，同时保持通用视频理解基准的准确率。

Conclusion: DSR Suite通过数据集、基准测试和模型三个方面的创新，有效提升了VLMs在动态空间推理任务上的性能，为解决这一挑战提供了全面方案。

Abstract: Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.

</details>


### [64] [FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models](https://arxiv.org/abs/2512.20561)
*Kaitong Cai,Jusheng Zhang,Jing Yang,Yijia Fan,Pengtao Xie,Jian Wang,Keze Wang*

Main category: cs.CV

TL;DR: FlashVLM是一个基于文本引导的可视化tokens选择框架，能够在保持准确性的同时大幅压缩77.8%的可视化tokens，实现超无损压缩。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型处理图像或视频帧时需要处理大量可视化tokens，导致二次注意力成本和严重冗余。现有的token减少方法要么忽略文本查询，要么依赖不稳定的注意力图，在激进剪枝时语义对齐性能会下降。

Method: FlashVLM通过计算图像tokens和标准化文本嵌入在语言模型空间中的显式跨模态相似性，并结合内在视觉显著性，使用对数域加权和温度控制锐化。同时通过多样性保留分区保留最小但有代表性的背景tokens集来维持全局上下文。

Result: 在相同token预算下，FlashVLM在LLaVA 1.5上剪枝高达77.8%的可视化tokens，甚至在94.4%压缩率下保持92.8%的准确率，略微超过未剪枝的基线性能。在14个图像和视频基准测试中展现了最先进的效率-性能平衡。

Conclusion: FlashVLM提供了一个有效的文本引导可视化tokens选择框架，能够在大幅压缩计算成本的同时保持甚至略微提升模型性能，在主流VLM中展现出强大的鲁棒性和泛化能力。

Abstract: Large vision-language models (VLMs) typically process hundreds or thousands of visual tokens per image or video frame, incurring quadratic attention cost and substantial redundancy. Existing token reduction methods often ignore the textual query or rely on deep attention maps, whose instability under aggressive pruning leads to degraded semantic alignment.
  We propose FlashVLM, a text guided visual token selection framework that dynamically adapts visual inputs to the query. Instead of relying on noisy attention weights, FlashVLM computes an explicit cross modal similarity between projected image tokens and normalized text embeddings in the language model space. This extrinsic relevance is fused with intrinsic visual saliency using log domain weighting and temperature controlled sharpening. In addition, a diversity preserving partition retains a minimal yet representative set of background tokens to maintain global context.
  Under identical token budgets and evaluation protocols, FlashVLM achieves beyond lossless compression, slightly surpassing the unpruned baseline while pruning up to 77.8 percent of visual tokens on LLaVA 1.5, and maintaining 92.8 percent accuracy even under 94.4 percent compression. Extensive experiments on 14 image and video benchmarks demonstrate that FlashVLM delivers state of the art efficiency performance trade offs while maintaining strong robustness and generalization across mainstream VLMs.

</details>


### [65] [LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving](https://arxiv.org/abs/2512.20563)
*Long Nguyen,Micha Fauth,Bernhard Jaeger,Daniel Dauner,Maximilian Igl,Andreas Geiger,Kashyap Chitta*

Main category: cs.CV

TL;DR: 该论文针对仿真环境中模仿学习策略性能不佳的问题，研究了专家演示与学生观察之间的错配如何限制模仿学习效果，并提出改进方法，在CARLA基准测试中达到新SOTA。


<details>
  <summary>Details</summary>
Motivation: 仿真器能生成无限驾驶数据，但模仿学习策略在仿真中仍难以实现鲁棒的闭环性能。作者希望通过解决专家演示（高可见性、低不确定性）与学生观察（传感器有限、不确定性高）之间的不对称性来提升性能。

Method: 通过缩小专家与学生之间差距的改进方法，包括TransFuser v6学生策略，并集成感知监督到共享的模拟到真实管道中。

Result: 在CARLA闭环基准测试中达到新SOTA：Bench2Drive上95 DS，Longest6~v2和Town13上性能提升两倍以上。在NAVSIM和Waymo端到端驾驶基准测试中也获得一致增益。

Conclusion: 通过仔细修改缩小专家与学生之间的差距，提出的TransFuser v6策略在各种驾驶基准测试中实现了最先进的性能，证明了方法有效性。

Abstract: Simulators can generate virtually unlimited driving data, yet imitation learning policies in simulation still struggle to achieve robust closed-loop performance. Motivated by this gap, we empirically study how misalignment between privileged expert demonstrations and sensor-based student observations can limit the effectiveness of imitation learning. More precisely, experts have significantly higher visibility (e.g., ignoring occlusions) and far lower uncertainty (e.g., knowing other vehicles' actions), making them difficult to imitate reliably. Furthermore, navigational intent (i.e., the route to follow) is under-specified in student models at test time via only a single target point. We demonstrate that these asymmetries can measurably limit driving performance in CARLA and offer practical interventions to address them. After careful modifications to narrow the gaps between expert and student, our TransFuser v6 (TFv6) student policy achieves a new state of the art on all major publicly available CARLA closed-loop benchmarks, reaching 95 DS on Bench2Drive and more than doubling prior performances on Longest6~v2 and Town13. Additionally, by integrating perception supervision from our dataset into a shared sim-to-real pipeline, we show consistent gains on the NAVSIM and Waymo Vision-Based End-to-End driving benchmarks. Our code, data, and models are publicly available at https://github.com/autonomousvision/lead.

</details>


### [66] [Repurposing Video Diffusion Transformers for Robust Point Tracking](https://arxiv.org/abs/2512.20606)
*Soowon Son,Honggyu An,Chaehyun Kim,Hyunah Ko,Jisu Nam,Dahyun Chung,Siyoon Jin,Jung Yi,Jaewon Min,Junhwa Hur,Seungryong Kim*

Main category: cs.CV

TL;DR: DiTracker利用视频Diffusion Transformers的时空注意力机制，通过查询-键注意力匹配、轻量级LoRA微调和ResNet主干网融合，在点跟踪任务上取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖独立处理帧的浅层卷积网络，缺乏时间连贯性且在挑战性条件下匹配不可靠。视频扩散变换器（DiTs）在大型真实视频上预训练，具有时空注意力机制，天然具备点跟踪能力

Method: DiTracker通过：(1)查询-键注意力匹配，(2)轻量级LoRA微调，(3)与ResNet主干网的成本融合，来适配视频DiTs

Result: 尽管使用8倍小批量训练，DiTracker在ITTO基准上达到SOTA性能，在TAP-Vid基准上与SOTA模型持平或超越

Conclusion: 视频DiT特征被验证为点跟踪的有效且高效基础框架

Abstract: Point tracking aims to localize corresponding points across video frames, serving as a fundamental task for 4D reconstruction, robotics, and video editing. Existing methods commonly rely on shallow convolutional backbones such as ResNet that process frames independently, lacking temporal coherence and producing unreliable matching costs under challenging conditions. Through systematic analysis, we find that video Diffusion Transformers (DiTs), pre-trained on large-scale real-world videos with spatio-temporal attention, inherently exhibit strong point tracking capability and robustly handle dynamic motions and frequent occlusions. We propose DiTracker, which adapts video DiTs through: (1) query-key attention matching, (2) lightweight LoRA tuning, and (3) cost fusion with a ResNet backbone. Despite training with 8 times smaller batch size, DiTracker achieves state-of-the-art performance on challenging ITTO benchmark and matches or outperforms state-of-the-art models on TAP-Vid benchmarks. Our work validates video DiT features as an effective and efficient foundation for point tracking.

</details>


### [67] [FedPOD: the deployable units of training for federated learning](https://arxiv.org/abs/2512.20610)
*Daewoon Kim,Si Young Yie,Jae Sung Lee*

Main category: cs.CV

TL;DR: FedPOD是为联邦学习提出的优化算法，通过包括异常值参与者、消除对前几轮信息的依赖以及应用验证损失计算方法，改进了FedPIDAvg，在效率和性能上表现相当，且兼容Kubernetes自动扩展。


<details>
  <summary>Details</summary>
Motivation: FedPIDAvg在性能提升和通信成本降低方面有效，但排除异常值参与者限制了数据利用，且PID控制器需要在整个过程中维持相同参与者。FedPOD旨在解决这些问题。

Method: FedPOD通过包括被排除的异常值参与者、消除对前几轮信息的依赖、应用每轮验证损失计算方法，并借鉴Kubernetes的POD概念实现灵活扩展。

Result: FedPOD在Dice分数（WT 0.78、ET 0.71、TC 0.72）和收敛分数（平均0.74）上与FedPIDAvg相当，同时在效率和灵活性上有所提升。

Conclusion: FedPOD展示了在联邦学习中通过改进效率、灵活性和性能指标的潜力，为优化学习过程和系统集成提供了新方向。

Abstract: This paper proposes FedPOD (Proportionally Orchestrated Derivative) for optimizing learning efficiency and communication cost in federated learning among multiple clients. Inspired by FedPIDAvg, we define a round-wise task for FedPOD to enhance training efficiency. FedPIDAvg achieved performance improvement by incorporating the training loss reduction for prediction entropy as weights using differential terms. Furthermore, by modeling data distribution with a Poisson distribution and using a PID controller, it reduced communication costs even in skewed data distribution. However, excluding participants classified as outliers based on the Poisson distribution can limit data utilization. Additionally, PID controller requires the same participants to be maintained throughout the federated learning process as it uses previous rounds' learning information in the current round. In our approach, FedPOD addresses these issues by including participants excluded as outliers, eliminating dependency on previous rounds' learning information, and applying a method for calculating validation loss at each round. In this challenge, FedPOD presents comparable performance to FedPIDAvg in metrics of Dice score, 0.78, 0.71 and 0.72 for WT, ET and TC in average, and projected convergence score, 0.74 in average. Furthermore, the concept of FedPOD draws inspiration from Kubernetes' smallest computing unit, POD, designed to be compatible with Kubernetes auto-scaling. Extending round-wise tasks of FedPOD to POD units allows flexible design by applying scale-out similar to Kubernetes' auto-scaling. This work demonstrated the potentials of FedPOD to enhance federated learning by improving efficiency, flexibility, and performance in metrics.

</details>


### [68] [Active Intelligence in Video Avatars via Closed-loop World Modeling](https://arxiv.org/abs/2512.20615)
*Xuanhua He,Tianyu Yang,Ke Cao,Ruiqi Wu,Cheng Meng,Yong Zhang,Zhuoliang Kang,Xiaoming Wei,Qifeng Chen*

Main category: cs.CV

TL;DR: 这篇论文提出了L-IVA任务和基准以及ORCA框架，旨在解决当前视频虚拟人生成方法缺乏真正自主性的问题，通过内部世界模型和闭环OTAR循环实现视频虚拟人的主动智能。


<details>
  <summary>Details</summary>
Motivation: 当前视频虚拟人生成方法虽然在身份保持和动作对齐方面表现出色，但缺乏真正的自主性，无法通过自适应环境交互自主追求长期目标。

Method: 提出ORCA框架，包含两个关键创新：(1)闭环OTAR循环(观察-思考-行动-反思)，在生成不确定性下保持稳定的状态跟踪；(2)分层双系统架构，系统2进行战略推理，系统1将抽象计划转换为精确的动作描述。

Result: 大量实验表明，ORCA在任务成功率和行为一致性方面显著优于开环和非反思基准方法。

Conclusion: ORCA验证了基于内部世界模型的设计能够促进视频虚拟人智能从被动动画向主动目标导向行为的进步。

Abstract: Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agency, they cannot autonomously pursue long-term goals through adaptive environmental interaction. We address this by introducing L-IVA (Long-horizon Interactive Visual Avatar), a task and benchmark for evaluating goal-directed planning in stochastic generative environments, and ORCA (Online Reasoning and Cognitive Architecture), the first framework enabling active intelligence in video avatars. ORCA embodies Internal World Model (IWM) capabilities through two key innovations: (1) a closed-loop OTAR cycle (Observe-Think-Act-Reflect) that maintains robust state tracking under generative uncertainty by continuously verifying predicted outcomes against actual generations, and (2) a hierarchical dual-system architecture where System 2 performs strategic reasoning with state prediction while System 1 translates abstract plans into precise, model-specific action captions. By formulating avatar control as a POMDP and implementing continuous belief updating with outcome verification, ORCA enables autonomous multi-step task completion in open-domain scenarios. Extensive experiments demonstrate that ORCA significantly outperforms open-loop and non-reflective baselines in task success rate and behavioral coherence, validating our IWM-inspired design for advancing video avatar intelligence from passive animation to active, goal-oriented behavior.

</details>


### [69] [SpatialTree: How Spatial Abilities Branch Out in MLLMs](https://arxiv.org/abs/2512.20617)
*Yuxi Xiao,Longfei Li,Shen Yan,Xinhang Liu,Sida Peng,Yunchao Wei,Xiaowei Zhou,Bingyi Kang*

Main category: cs.CV

TL;DR: 该论文提出了SpatialTree框架，将空间能力分为四个层次，并创建了层级化基准测试，揭示了不同层次能力的相关性和迁移特点，提出了auto-think策略来改善多模态大模型的空间能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究对多模态大模型中空间能力的层次结构理解不足，大多数研究只关注狭窄的任务集合。作者希望基于认知科学构建一个层次化的框架来系统化理解和评估MLLMs的空间能力。

Method: 1) 提出SpatialTree框架，将空间能力分为四级：低层感知(L1)、心理映射(L2)、模拟(L3)和代理能力(L4) 2) 构建基于能力的层级基准测试 3) 评估主流MLLMs在27个子能力上的表现 4) 探索监督微调和强化学习的优化策略

Result: 评估结果显示清晰的结构：L1技能基本正交，而高层技能高度相关；发现负向迁移存在于L1内部，但从低层到高层的跨级迁移效果显著；提出auto-think策略能抑制不必要的

Conclusion: SpatialTree为理解和系统化扩展MLLMs的空间能力提供了概念验证框架，揭示了空间能力的层次化特征和优化策略的有效性。

Abstract: Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive "thinking" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.

</details>


### [70] [SemanticGen: Video Generation in Semantic Space](https://arxiv.org/abs/2512.20619)
*Jianhong Bai,Xiaoshi Wu,Xintao Wang,Fu Xiao,Yuanxing Zhang,Qinghe Wang,Xiaoyu Shi,Menghan Xia,Zuozhu Liu,Haoji Hu,Pengfei Wan,Kun Gai*

Main category: cs.CV

TL;DR: SemanticGen提出了一种在语义空间而非VAE潜在空间生成视频的新方法，通过两阶段生成过程解决现有方法收敛慢和计算成本高的问题


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在VAE潜在空间中学习潜在分布，该方法收敛慢且生成长视频时计算成本高昂，需要更高效的方法

Method: 采用两阶段生成：第一阶段用扩散模型生成紧凑的语义视频特征（定义视频全局布局），第二阶段基于语义特征生成VAE潜在变量来产生最终输出

Result: 在语义空间生成比VAE潜在空间收敛更快，在长视频生成中表现出计算效率且有效，实验证明能生成高质量视频并优于现有方法

Conclusion: SemanticGen通过在语义空间进行全局规划再添加高频细节的方法，有效解决了视频生成中收敛慢和计算成本高的问题

Abstract: State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [71] [HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data](https://arxiv.org/abs/2512.19864)
*Shashi Kant Gupta,Arijeet Pramanik,Jerrin John Thomas,Regina Schwind,Lauren Wiener,Avi Raju,Jeremy Kornbluth,Yanshan Wang,Zhaohui Su,Hrituraj Singh*

Main category: cs.CL

TL;DR: 该研究提出了一个基于LLM的智能体框架，用于从电子健康记录中结构化提取肿瘤学数据，解决了现有方法的局限性，并在大规模真实数据集上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录中的非结构化笔记包含丰富的临床信息，但现有自动化方法无法有效处理复杂的肿瘤学数据提取，特别是在处理大量包含矛盾信息的临床文档时存在挑战。

Method: 使用大型语言模型作为推理智能体，具备上下文敏感检索和迭代合成能力，将复杂的肿瘤学数据提取任务分解为模块化、自适应的子任务。

Result: 在包含40万份非结构化临床笔记和2250名癌症患者的大规模数据集上，平均F1-score达到0.93，103个肿瘤学特异性临床变量中有100个超过0.85，关键变量（如生物标志物和药物）超过0.95。

Conclusion: 这是首个基于LLM智能体的大规模端到端肿瘤学数据提取应用，显著降低了标注成本，集成到数据整理工作流中实现了0.94的直接人工批准率。

Abstract: Unstructured notes within the electronic health record (EHR) contain rich clinical information vital for cancer treatment decision making and research, yet reliably extracting structured oncology data remains challenging due to extensive variability, specialized terminology, and inconsistent document formats. Manual abstraction, although accurate, is prohibitively costly and unscalable. Existing automated approaches typically address narrow scenarios - either using synthetic datasets, restricting focus to document-level extraction, or isolating specific clinical variables (e.g., staging, biomarkers, histology) - and do not adequately handle patient-level synthesis across the large number of clinical documents containing contradictory information. In this study, we propose an agentic framework that systematically decomposes complex oncology data extraction into modular, adaptive tasks. Specifically, we use large language models (LLMs) as reasoning agents, equipped with context-sensitive retrieval and iterative synthesis capabilities, to exhaustively and comprehensively extract structured clinical variables from real-world oncology notes. Evaluated on a large-scale dataset of over 400,000 unstructured clinical notes and scanned PDF reports spanning 2,250 cancer patients, our method achieves an average F1-score of 0.93, with 100 out of 103 oncology-specific clinical variables exceeding 0.85, and critical variables (e.g., biomarkers and medications) surpassing 0.95. Moreover, integration of the agentic system into a data curation workflow resulted in 0.94 direct manual approval rate, significantly reducing annotation costs. To our knowledge, this constitutes the first exhaustive, end-to-end application of LLM-based agents for structured oncology data extraction at scale

</details>


### [72] [How well do Large Language Models Recognize Instructional Moves? Establishing Baselines for Foundation Models in Educational Discourse](https://arxiv.org/abs/2512.19903)
*Kirk Vanacore,Rene F. Kizilcec*

Main category: cs.CL

TL;DR: 该研究评估了大型语言模型在零样本、单样本和少样本提示下对真实课堂记录中教学行为的分类能力。研究发现少样本提示显著提升性能，但模型的可靠性仍有限。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型在教育技术中被广泛采用，了解其未经定制时的基础能力对于设定预期和基准测试至关重要。

Method: 比较了六种语言模型在课堂记录教学行为分类任务上的表现，评估了零样本、单样本和少样本三种典型提示方法。

Result: 少样本提示显著提升了最先进模型的性能（Cohen's Kappa = 0.58），但性能因教学行为类型而异，且高召回率往往伴随着误报增加。

Conclusion: 基础模型展现出有意义但有限的教学话语理解能力，提示设计有助于发掘潜力但无法消除根本的可靠性限制。

Abstract: Large language models (LLMs) are increasingly adopted in educational technologies for a variety of tasks, from generating instructional materials and assisting with assessment design to tutoring. While prior work has investigated how models can be adapted or optimized for specific tasks, far less is known about how well LLMs perform at interpreting authentic educational scenarios without significant customization. As LLM-based systems become widely adopted by learners and educators in everyday academic contexts, understanding their out-of-the-box capabilities is increasingly important for setting expectations and benchmarking. We compared six LLMs to estimate their baseline performance on a simple but important task: classifying instructional moves in authentic classroom transcripts. We evaluated typical prompting methods: zero-shot, one-shot, and few-shot prompting. We found that while zero-shot performance was moderate, providing comprehensive examples (few-shot prompting) significantly improved performance for state-of-the-art models, with the strongest configuration reaching Cohen's Kappa = 0.58 against expert-coded annotations. At the same time, improvements were neither uniform nor complete: performance varied considerably by instructional move, and higher recall frequently came at the cost of increased false positives. Overall, these findings indicate that foundation models demonstrate meaningful yet limited capacity to interpret instructional discourse, with prompt design helping to surface capability but not eliminating fundamental reliability constraints.

</details>


### [73] [Counterfactual LLM-based Framework for Measuring Rhetorical Style](https://arxiv.org/abs/2512.19908)
*Jingyi Qiu,Hong Chen,Zongyi Li*

Main category: cs.CL

TL;DR: 基于LLM的反事实框架量化机器学习论文的修辞风格，发现2017-2025年间ICLR论文的修辞强度显著上升，且这种上升主要由LLM写作辅助驱动。愿景式修辞能显著预测下游关注度。


<details>
  <summary>Details</summary>
Motivation: AI领域的快速发展引发了对机器学习论文中'炒作'现象的担忧，但现有方法难以将修辞风格与实质内容分离进行量化分析。

Method: 提出基于LLM的反事实框架：使用多个LLM修辞角色基于相同实质内容生成反事实文本，通过LLM法官进行成对比较，并用Bradley-Terry模型汇总结果。

Result: 对8,485篇ICLR论文的分析发现：愿景式修辞能显著预测引用和媒体关注；2023年后修辞强度急剧上升，这主要由LLM写作辅助驱动；框架对角色选择具有鲁棒性，且LLM判断与人工标注高度相关。

Conclusion: LLMs可作为测量和改进科学评估的工具，有效量化论文修辞风格，为科学交流的质量评估提供了新方法。

Abstract: The rise of AI has fueled growing concerns about ``hype'' in machine learning papers, yet a reliable way to quantify rhetorical style independently of substantive content has remained elusive. Because bold language can stem from either strong empirical results or mere rhetorical style, it is often difficult to distinguish between the two. To disentangle rhetorical style from substantive content, we introduce a counterfactual, LLM-based framework: multiple LLM rhetorical personas generate counterfactual writings from the same substantive content, an LLM judge compares them through pairwise evaluations, and the outcomes are aggregated using a Bradley--Terry model. Applying this method to 8,485 ICLR submissions sampled from 2017 to 2025, we generate more than 250,000 counterfactual writings and provide a large-scale quantification of rhetorical style in ML papers. We find that visionary framing significantly predicts downstream attention, including citations and media attention, even after controlling for peer-review evaluations. We also observe a sharp rise in rhetorical strength after 2023, and provide empirical evidence showing that this increase is largely driven by the adoption of LLM-based writing assistance. The reliability of our framework is validated by its robustness to the choice of personas and the high correlation between LLM judgments and human annotations. Our work demonstrates that LLMs can serve as instruments to measure and improve scientific evaluation.

</details>


### [74] [PRISM: A Personality-Driven Multi-Agent Framework for Social Media Simulation](https://arxiv.org/abs/2512.19933)
*Zhixiang Lu,Xueyuan Deng,Yiran Liu,Yulong Li,Qiang Yan,Imran Razzak,Jionglong Su*

Main category: cs.CL

TL;DR: PRISM模型：一种结合随机微分方程和性格条件部分观察马尔可夫决策过程的混合框架，通过MBTI性格类型打破传统观点动力学模型的同质性假设，更精准模拟在线社媒中的极化现象。


<details>
  <summary>Details</summary>
Motivation: 传统基于代理的观点动力学模型因同质性假设无法捕捉在线极化背后的心理异质性，阻碍了对意识形态分裂放大机制的理解。

Method: 提出PRISM混合框架：用随机微分方程(SDE)模拟连续情感演化，用性格条件部分观察马尔可夫决策过程(PC-POMDP)处理离散决策，基于MBTI类型为MLLM智能体分配差异化认知策略。

Result: PRISM在性格一致性上显著优于传统同质化和大五人格基准模型，能有效复现理性压制和情感共鸣等涌现现象。

Conclusion: 该框架为分析复杂社交媒体生态系统提供了强大工具，突破了传统模型在心理异质性建模方面的局限。

Abstract: Traditional agent-based models (ABMs) of opinion dynamics often fail to capture the psychological heterogeneity driving online polarization due to simplistic homogeneity assumptions. This limitation obscures the critical interplay between individual cognitive biases and information propagation, thereby hindering a mechanistic understanding of how ideological divides are amplified. To address this challenge, we introduce the Personality-Refracted Intelligent Simulation Model (PRISM), a hybrid framework coupling stochastic differential equations (SDE) for continuous emotional evolution with a personality-conditional partially observable Markov decision process (PC-POMDP) for discrete decision-making. In contrast to continuous trait approaches, PRISM assigns distinct Myers-Briggs Type Indicator (MBTI) based cognitive policies to multimodal large language model (MLLM) agents, initialized via data-driven priors from large-scale social media datasets. PRISM achieves superior personality consistency aligned with human ground truth, significantly outperforming standard homogeneous and Big Five benchmarks. This framework effectively replicates emergent phenomena such as rational suppression and affective resonance, offering a robust tool for analyzing complex social media ecosystems.

</details>


### [75] [Bias Beneath the Tone: Empirical Characterisation of Tone Bias in LLM-Driven UX Systems](https://arxiv.org/abs/2512.19950)
*Heet Bodara,Md Masum Mushfiq,Isma Farah Siddiqui*

Main category: cs.CL

TL;DR: 本研究探讨了大语言模型的语调偏见问题，发现即使在中性提示下生成的对话也存在明显的语调倾斜（特别是积极和礼貌倾向），这种偏见具有系统性、可测量性，需要通过可控对话合成和机器学习分类器来检测和应对。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在对话系统中广泛应用，但其回答可能存在隐藏的语调偏见（如过度礼貌、乐观或谨慎），这种偏见会影响用户对信任、同理心和公平性的感知，需要系统性地检测和衡量。

Method: 整合可控的大语言模型对话合成与语调分类模型：1）创建两个合成对话数据集（中性提示集和显式引导集）；2）使用预训练DistilBERT模型进行弱监督标记；3）训练多个分类器并构建集成模型来检测语调模式。

Result: 集成模型达到宏观F1分数0.92；中性数据集显示出持续的语调倾斜，表明偏见来源于模型的基础对话风格；语调偏见具有系统性和可测量性。

Conclusion: 语调偏见是大语言模型的一个隐藏行为特征，对设计公平可信的对话AI具有重要意义；通过可控合成和分类器可以有效地检测和量化这种偏见。

Abstract: Large Language Models are increasingly used in conversational systems such as digital personal assistants, shaping how people interact with technology through language. While their responses often sound fluent and natural, they can also carry subtle tone biases such as sounding overly polite, cheerful, or cautious even when neutrality is expected. These tendencies can influence how users perceive trust, empathy, and fairness in dialogue. In this study, we explore tone bias as a hidden behavioral trait of large language models. The novelty of this research lies in the integration of controllable large language model based dialogue synthesis with tone classification models, enabling robust and ethical emotion recognition in personal assistant interactions. We created two synthetic dialogue datasets, one generated from neutral prompts and another explicitly guided to produce positive or negative tones. Surprisingly, even the neutral set showed consistent tonal skew, suggesting that bias may stem from the model's underlying conversational style. Using weak supervision through a pretrained DistilBERT model, we labeled tones and trained several classifiers to detect these patterns. Ensemble models achieved macro F1 scores up to 0.92, showing that tone bias is systematic, measurable, and relevant to designing fair and trustworthy conversational AI.

</details>


### [76] [Schoenfeld's Anatomy of Mathematical Reasoning by Language Models](https://arxiv.org/abs/2512.19995)
*Ming Li,Chenrui Fan,Yize Cheng,Soheil Feizi,Tianyi Zhou*

Main category: cs.CL

TL;DR: ThinkARM框架将大语言模型中的推理轨迹抽象为功能性推理步骤（如分析、探索、实施、验证等），揭示了推理模型和非推理模型在思维动态和结构上的差异，并表明探索是关键分支步骤，与正确性相关。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然展示了推理轨迹，但其背后的认知结构和步骤难以识别和分析。研究旨在通过引入中间尺度的镜头，使推理步骤明确化，从而系统分析推理的结构、稳定性和变化。

Method: 采用Schoenfeld的Episode Theory作为归纳性中间尺度镜头，提出ThinkARM框架，将推理轨迹抽象为功能性推理步骤，并应用于数学问题解决中的多样模型分析。

Result: ThinkARM揭示了推理模型和非推理模型在思维动态和结构上的可复现差异，探索作为关键分支步骤与正确性相关，效率导向方法选择性地抑制评估反馈步骤。

Conclusion: 片段级表征使推理步骤明确，能够系统分析现代语言模型中推理的结构、稳定性和变化，为理解模型推理提供新视角。

Abstract: Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.

</details>


### [77] [Memory-T1: Reinforcement Learning for Temporal Reasoning in Multi-session Agents](https://arxiv.org/abs/2512.20092)
*Yiming Du,Baojun Wang,Yifan Xiang,Zhaowei Wang,Wenyu Huang,Boyang Xue,Bin Liang,Xingshan Zeng,Fei Mi,Haoli Bai,Lifeng Shang,Jeff Z. Pan,Yuxin Jiang,Kam-Fai Wong*

Main category: cs.CL

TL;DR: Memory-T1是一个基于强化学习的时间感知记忆选择框架，通过粗到细的策略从长对话历史中筛选时间相关信息，显著提升了长时间对话推理的性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型在处理长对话历史时难以准确识别时间相关信息，对话历史长度增加和噪音积累会严重损害推理性能。

Method: 使用强化学习训练时间感知记忆选择策略：先通过时间和相关性过滤器对对话历史进行粗筛选，然后由RL智能体精确选择证据会话。RL训练采用多级奖励函数优化答案准确性、证据基础和时序一致性。

Result: 在Time-Dialog基准测试中，Memory-T1将7B模型提升至67.0%的总体得分，创下了开源模型的新SOTA，比14B基线模型高出10.2%。在128k tokens下仍保持鲁棒性。

Conclusion: Memory-T1有效解决了长对话时序推理的挑战，时序一致性和证据基础奖励共同贡献了15.0%的性能提升，证明了框架对长对话历史中噪音的有效处理能力。

Abstract: Temporal reasoning over long, multi-session dialogues is a critical capability for conversational agents. However, existing works and our pilot study have shown that as dialogue histories grow in length and accumulate noise, current long-context models struggle to accurately identify temporally pertinent information, significantly impairing reasoning performance. To address this, we introduce Memory-T1, a framework that learns a time-aware memory selection policy using reinforcement learning (RL). It employs a coarse-to-fine strategy, first pruning the dialogue history into a candidate set using temporal and relevance filters, followed by an RL agent that selects the precise evidence sessions. The RL training is guided by a multi-level reward function optimizing (i) answer accuracy, (ii) evidence grounding, and (iii) temporal consistency. In particular, the temporal consistency reward provides a dense signal by evaluating alignment with the query time scope at both the session-level (chronological proximity) and the utterance-level (chronological fidelity), enabling the agent to resolve subtle chronological ambiguities. On the Time-Dialog benchmark, Memory-T1 boosts a 7B model to an overall score of 67.0\%, establishing a new state-of-the-art performance for open-source models and outperforming a 14B baseline by 10.2\%. Ablation studies show temporal consistency and evidence grounding rewards jointly contribute to a 15.0\% performance gain. Moreover, Memory-T1 maintains robustness up to 128k tokens, where baseline models collapse, proving effectiveness against noise in extensive dialogue histories. The code and datasets are publicly available at https://github.com/Elvin-Yiming-Du/Memory-T1/

</details>


### [78] [A Novel Graph-Sequence Learning Model for Inductive Text Classification](https://arxiv.org/abs/2512.20097)
*Zuo Wang,Ye Yuan*

Main category: cs.CL

TL;DR: 提出的TextGSL模型通过构建单文本级图并建立多类型边关系，结合自适应多边消息传递和Transformer层，解决了现有GNN方法在文本分类中结构信息利用不足和序列信息缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的文本分类方法存在两个主要局限：一是未能充分考虑词对间的多样化结构信息（如共现、语法、语义），二是在图结构学习模块中忽略了序列信息，无法处理新词和新关系的文本分类任务。

Method: 1. 为每个文本构建单文本级图，基于词对间的多样化关系建立不同类型的边；2. 设计自适应多边消息传递机制来聚合词对间的多样化结构信息；3. 引入Transformer层捕获文本数据的序列信息，从而学习更具判别性的文本表示。

Result: 在多个基准数据集上的实验结果表明，TextGSL在准确率方面优于多个强基线模型。

Conclusion: TextGSL通过结合图结构和序列学习的优势，有效提升了文本分类的性能，特别是在处理多样化结构信息和序列依赖关系方面表现出色。

Abstract: Text classification plays an important role in various downstream text-related tasks, such as sentiment analysis, fake news detection, and public opinion analysis. Recently, text classification based on Graph Neural Networks (GNNs) has made significant progress due to their strong capabilities of structural relationship learning. However, these approaches still face two major limitations. First, these approaches fail to fully consider the diverse structural information across word pairs, e.g., co-occurrence, syntax, and semantics. Furthermore, they neglect sequence information in the text graph structure information learning module and can not classify texts with new words and relations. In this paper, we propose a Novel Graph-Sequence Learning Model for Inductive Text Classification (TextGSL) to address the previously mentioned issues. More specifically, we construct a single text-level graph for all words in each text and establish different edge types based on the diverse relationships between word pairs. Building upon this, we design an adaptive multi-edge message-passing paradigm to aggregate diverse structural information between word pairs. Additionally, sequential information among text data can be captured by the proposed TextGSL through the incorporation of Transformer layers. Therefore, TextGSL can learn more discriminative text representations. TextGSL has been comprehensively compared with several strong baselines. The experimental results on diverse benchmarking datasets demonstrate that TextGSL outperforms these baselines in terms of accuracy.

</details>


### [79] [ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language](https://arxiv.org/abs/2512.20111)
*Aly Lidayan,Jakob Bjorner,Satvik Golechha,Kartik Goyal,Alane Suhr*

Main category: cs.CL

TL;DR: ABBEL框架使用语言表达的信念瓶颈来压缩多步决策任务的历史上下文，通过RL训练提升性能，在保持近恒定内存使用的同时，性能甚至优于完整上下文设置。


<details>
  <summary>Details</summary>
Motivation: 随着序列决策任务长度增加，完整交互历史的计算成本变得不可行，需要一种方法来保持简洁的上下文。

Method: 提出ABBEL框架，用信念状态（任务相关未知信息的自然语言摘要）替代长历史，每一步先更新信念再基于信念选择动作，并通过RL训练改进信念生成质量。

Result: 在六个多步环境中评估，ABBEL实现可解释信念生成和近恒定内存使用，但单纯信念瓶颈易受错误传播影响；RL训练后性能超过完整上下文设置，内存使用更少。

Conclusion: ABBEL框架结合RL训练能有效解决长序列决策的内存问题，在压缩上下文的同时提升性能，为LLM智能体提供可扩展的解决方案。

Abstract: As the length of sequential decision-making tasks increases, it becomes computationally impractical to keep full interaction histories in context. We introduce a general framework for LLM agents to maintain concise contexts through multi-step interaction: Acting through Belief Bottlenecks Expressed in Language (ABBEL), and methods to further improve ABBEL agents with RL post-training. ABBEL replaces long multi-step interaction history by a belief state, i.e., a natural language summary of what has been discovered about task-relevant unknowns. Under ABBEL, at each step the agent first updates a prior belief with the most recent observation from the environment to form a posterior belief, then uses only the posterior to select an action. We systematically evaluate frontier models under ABBEL across six diverse multi-step environments, finding that ABBEL supports generating interpretable beliefs while maintaining near-constant memory use over interaction steps. However, bottleneck approaches are generally prone to error propagation, which we observe causing inferior performance when compared to the full context setting due to errors in belief updating. Therefore, we train LLMs to generate and act on beliefs within the ABBEL framework via reinforcement learning (RL). We experiment with belief grading, to reward higher quality beliefs, as well as belief length penalties to reward more compressed beliefs. Our experiments demonstrate the ability of RL to improve ABBEL's performance beyond the full context setting, while using less memory than contemporaneous approaches.

</details>


### [80] [M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2512.20136)
*Hyeongcheol Park,Jiyoung Seo,Jaewon Mun,Hogun Park,Wonmin Byeon,Sung June Kim,Hyeonsoo Im,JeungSub Lee,Sangpil Kim*

Main category: cs.CL

TL;DR: M³KG-RAG提出了多跳多模态知识图谱增强的检索增强生成方法，通过构建上下文丰富的多模态实体三元组和精确的知识检索修剪机制，显著提升多模态大语言模型在音频-视觉领域的推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决多模态RAG在音频-视觉领域的两个关键挑战：现有知识图谱的模态覆盖不足和多跳连接有限，以及基于相似度的检索无法过滤无关或冗余知识的问题。

Method: 1）设计轻量级多代理管道构建多跳MMKG（M³KG），包含上下文丰富的多模态实体三元组；2）提出GRASP框架，确保精确的实体接地、评估答案支持相关性并修剪冗余上下文。

Result: 在多样化多模态基准测试中，M³KG-RAG显著优于现有方法，提升了多模态大语言模型的多模态推理和接地能力。

Conclusion: M³KG-RAG通过多跳知识图谱结构和选择性检索机制，有效解决了多模态RAG的关键限制，为音频-视觉领域的知识增强提供了有效解决方案。

Abstract: Retrieval-Augmented Generation (RAG) has recently been extended to multimodal settings, connecting multimodal large language models (MLLMs) with vast corpora of external knowledge such as multimodal knowledge graphs (MMKGs). Despite their recent success, multimodal RAG in the audio-visual domain remains challenging due to 1) limited modality coverage and multi-hop connectivity of existing MMKGs, and 2) retrieval based solely on similarity in a shared multimodal embedding space, which fails to filter out off-topic or redundant knowledge. To address these limitations, we propose M$^3$KG-RAG, a Multi-hop Multimodal Knowledge Graph-enhanced RAG that retrieves query-aligned audio-visual knowledge from MMKGs, improving reasoning depth and answer faithfulness in MLLMs. Specifically, we devise a lightweight multi-agent pipeline to construct multi-hop MMKG (M$^3$KG), which contains context-enriched triplets of multimodal entities, enabling modality-wise retrieval based on input queries. Furthermore, we introduce GRASP (Grounded Retrieval And Selective Pruning), which ensures precise entity grounding to the query, evaluates answer-supporting relevance, and prunes redundant context to retain only knowledge essential for response generation. Extensive experiments across diverse multimodal benchmarks demonstrate that M$^3$KG-RAG significantly enhances MLLMs' multimodal reasoning and grounding over existing approaches.

</details>


### [81] [Fun-Audio-Chat Technical Report](https://arxiv.org/abs/2512.20156)
*Qian Chen,Luyao Cheng,Chong Deng,Xiangang Li,Jiaqing Liu,Chao-Hong Tan,Wen Wang,Junhao Xu,Jieping Ye,Qinglin Zhang,Qiquan Zhang,Jingren Zhou*

Main category: cs.CL

TL;DR: Fun-Audio-Chat是一个大型音频语言模型，通过双分辨率语音表示和核心鸡尾酒训练解决了语音-文本联合模型中分辨率不匹配和灾难性遗忘的问题，实现了高效高质量的音频处理。


<details>
  <summary>Details</summary>
Motivation: 现有语音-文本联合模型面临时间分辨率不匹配（语音25Hz vs 文本3Hz）、高计算成本和文本LLM知识灾难性遗忘的问题，需要新的解决方案。

Method: 1）双分辨率语音表示：共享LLM处理5Hz高效音频，语音精炼头生成25Hz高质量令牌；2）核心鸡尾酒训练：两阶段微调缓解遗忘；3）多任务DPO训练增强鲁棒性。

Result: 8B和MoE 30B-A3B模型在语音转文本、语音转语音任务上表现优秀，在音频理解、语音功能调用等任务上达到竞争性甚至优越性能。

Conclusion: Fun-Audio-Chat通过创新的训练方法有效平衡了效率与质量，避免了大规模音频-文本预训练的需求，在保持文本LLM知识的同时获得了强大的音频处理能力。

Abstract: Recent advancements in joint speech-text models show great potential for seamless voice interactions. However, existing models face critical challenges: temporal resolution mismatch between speech tokens (25Hz) and text tokens (~3Hz) dilutes semantic information, incurs high computational costs, and causes catastrophic forgetting of text LLM knowledge. We introduce Fun-Audio-Chat, a Large Audio Language Model addressing these limitations via two innovations from our previous work DrVoice. First, Dual-Resolution Speech Representations (DRSR): the Shared LLM processes audio at efficient 5Hz (via token grouping), while the Speech Refined Head generates high-quality tokens at 25Hz, balancing efficiency (~50% GPU reduction) and quality. Second, Core-Cocktail Training, a two-stage fine-tuning with intermediate merging that mitigates catastrophic forgetting. We then apply Multi-Task DPO Training to enhance robustness, audio understanding, instruction-following and voice empathy. This multi-stage post-training enables Fun-Audio-Chat to retain text LLM knowledge while gaining powerful audio understanding, reasoning, and generation. Unlike recent LALMs requiring large-scale audio-text pre-training, Fun-Audio-Chat leverages pre-trained models and extensive post-training. Fun-Audio-Chat 8B and MoE 30B-A3B achieve competitive performance on Speech-to-Text and Speech-to-Speech tasks, ranking top among similar-scale models on Spoken QA benchmarks. They also achieve competitive to superior performance on Audio Understanding, Speech Function Calling, Instruction-Following and Voice Empathy. We develop Fun-Audio-Chat-Duplex, a full-duplex variant with strong performance on Spoken QA and full-duplex interactions. We open-source Fun-Audio-Chat-8B with training and inference code, and provide an interactive demo.

</details>


### [82] [Multi-hop Reasoning via Early Knowledge Alignment](https://arxiv.org/abs/2512.20144)
*Yuxin Wang,Shicheng Fang,Bo Wang,Qi Luo,Xuanjing Huang,Yining Zheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: 介绍了Early Knowledge Alignment (EKA)模块，通过在迭代RAG系统中先对齐检索集与LLMs，再规划检索，显著提升多跳问答的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有迭代RAG系统在规划问题分解时未利用检索语料库信息，导致检索效率低下和级联错误。EKA旨在通过早期知识对齐强化推理基础。

Method: 提出EKA模块，在规划步骤前将检索到的上下文相关知识对齐到LLMs，减少推理过程中的不必要探索，聚焦相关子集。

Result: 在六个标准RAG数据集上的实验表明，EKA显著提高检索精度，减少级联错误，提升性能和效率。熵分析显示其有效减少探索开销。

Conclusion: EKA作为一种无需训练的推理策略，可扩展至大模型，在迭代RAG系统中推进了最先进水平，阐明了结构化推理与高效探索的关键相互作用。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for Large Language Models (LLMs) to address knowledge-intensive queries requiring domain-specific or up-to-date information. To handle complex multi-hop questions that are challenging for single-step retrieval, iterative RAG approaches incorporating reinforcement learning have been proposed. However, existing iterative RAG systems typically plan to decompose questions without leveraging information about the available retrieval corpus, leading to inefficient retrieval and reasoning chains that cascade into suboptimal performance. In this paper, we introduce Early Knowledge Alignment (EKA), a simple but effective module that aligns LLMs with retrieval set before planning in iterative RAG systems with contextually relevant retrieved knowledge. Extensive experiments on six standard RAG datasets demonstrate that by establishing a stronger reasoning foundation, EKA significantly improves retrieval precision, reduces cascading errors, and enhances both performance and efficiency. Our analysis from an entropy perspective demonstrate that incorporating early knowledge reduces unnecessary exploration during the reasoning process, enabling the model to focus more effectively on relevant information subsets. Moreover, EKA proves effective as a versatile, training-free inference strategy that scales seamlessly to large models. Generalization tests across diverse datasets and retrieval corpora confirm the robustness of our approach. Overall, EKA advances the state-of-the-art in iterative RAG systems while illuminating the critical interplay between structured reasoning and efficient exploration in reinforcement learning-augmented frameworks. The code is released at \href{https://github.com/yxzwang/EarlyKnowledgeAlignment}{Github}.

</details>


### [83] [SpidR: Learning Fast and Stable Linguistic Units for Spoken Language Models Without Supervision](https://arxiv.org/abs/2512.20308)
*Maxime Poli,Mahi Luthra,Youssef Benchekroun,Yosuke Higuchi,Martin Gleize,Jiayi Shen,Robin Algayres,Yu-An Chung,Mido Assran,Juan Pino,Emmanuel Dupoux*

Main category: cs.CL

TL;DR: SpidR是一种自监督语音表示模型，通过结合掩码预测、自蒸馏和在线聚类方法，直接从语音波形中学习高质量的音位信息表示，显著提升文本无关语音语言建模性能，并大幅减少预训练时间。


<details>
  <summary>Details</summary>
Motivation: 随着语言建模和语音表示学习的并行发展，研究者希望绕过文本中介直接学习语音语言，这需要直接从语音中提取语义表示。

Method: 模型使用掩码预测目标结合自蒸馏和在线聚类进行训练，让学生模型中间层预测从教师模型中间层派生的分配结果，这种学习目标稳定了在线聚类过程。

Result: SpidR在语言建模基准测试中优于wav2vec 2.0、HuBERT等模型，显著减少了预训练时间（从一周缩短到一天），验证了语音单元质量与语言建模性能的相关性。

Conclusion: SpidR为文本无关语音语言建模提供了高效解决方案，通过稳定的聚类方法和快速预训练实现了优越性能，促进了直接语音语言学习的发展。

Abstract: The parallel advances in language modeling and speech representation learning have raised the prospect of learning language directly from speech without textual intermediates. This requires extracting semantic representations directly from speech. Our contributions are threefold. First, we introduce SpidR, a self-supervised speech representation model that efficiently learns representations with highly accessible phonetic information, which makes it particularly suited for textless spoken language modeling. It is trained on raw waveforms using a masked prediction objective combined with self-distillation and online clustering. The intermediate layers of the student model learn to predict assignments derived from the teacher's intermediate layers. This learning objective stabilizes the online clustering procedure compared to previous approaches, resulting in higher quality codebooks. SpidR outperforms wav2vec 2.0, HuBERT, WavLM, and DinoSR on downstream language modeling benchmarks (sWUGGY, sBLIMP, tSC). Second, we systematically evaluate across models and layers the correlation between speech unit quality (ABX, PNMI) and language modeling performance, validating these metrics as reliable proxies. Finally, SpidR significantly reduces pretraining time compared to HuBERT, requiring only one day of pretraining on 16 GPUs, instead of a week. This speedup is enabled by the pretraining method and an efficient codebase, which allows faster iteration and easier experimentation. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr.

</details>


### [84] [Retrieval-augmented Prompt Learning for Pre-trained Foundation Models](https://arxiv.org/abs/2512.20145)
*Xiang Chen,Yixin Ou,Quan Feng,Lei Li,Piji Li,Haibo Ye,Sheng-Jun Huang,Shuofei Qiao,Shumin Deng,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 提出了RetroPrompt方法，通过检索机制从知识库中获取上下文信息，平衡记忆与泛化，在少样本和零样本场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统的提示学习方法在完全监督训练过程中难以充分利用非典型实例并避免对浅层模式的过拟合，限制了记忆与泛化的稳定性。

Method: RetroPrompt在输入、训练和推理阶段引入检索机制，从训练数据生成的公开知识库中动态获取相关上下文信息，减少对死记硬背的依赖。

Result: 在自然语言处理和计算机视觉任务的多数据集上，RetroPrompt在零样本和少样本场景中均表现出优越性能。

Conclusion: RetroPrompt通过解耦知识与单纯记忆，有效提升了模型的泛化能力，减少了对机械记忆的依赖。

Abstract: The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the ``pre-train, prompt, and predict'' paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RetroPrompt, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RetroPrompt leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RetroPrompt, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RetroPrompt effectively reduces the reliance on rote memorization, leading to enhanced generalization.

</details>


### [85] [AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications](https://arxiv.org/abs/2512.20164)
*Honglin Mu,Jinghao Liu,Kaiyang Wan,Rui Xing,Xiuying Chen,Timothy Baldwin,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文揭示了LLM在简历筛选等任务中容易被隐藏的对抗性指令操纵的漏洞，提出了FIDS防御方法，相比提示工程防御效果更好且假拒率更低。


<details>
  <summary>Details</summary>
Motivation: LLM虽然在代码审查等内容理解任务上表现出色，但在简历筛选、同行评审等应用领域缺乏针对对抗性指令攻击的防御机制，存在安全隐患。

Method: 构建基准测试评估简历筛选中的漏洞，提出FIDS（通过分离的外来指令检测）方法，使用LoRA适配器进行训练时防御，并与提示工程防御进行对比。

Result: 攻击成功率超过80%；提示防御减少10.1%攻击但假拒率增加12.5%；FIDS减少15.4%攻击且假拒率仅增10.4%；组合防御可减少26.3%攻击。

Conclusion: 训练时防御（如FIDS）在安全性和效用保持方面均优于推理时缓解措施，组合方法能显著提升防御效果。

Abstract: Large Language Models (LLMs) excel at text comprehension and generation, making them ideal for automated tasks like code review and content moderation. However, our research identifies a vulnerability: LLMs can be manipulated by "adversarial instructions" hidden in input data, such as resumes or code, causing them to deviate from their intended task. Notably, while defenses may exist for mature domains such as code review, they are often absent in other common applications such as resume screening and peer review. This paper introduces a benchmark to assess this vulnerability in resume screening, revealing attack success rates exceeding 80% for certain attack types. We evaluate two defense mechanisms: prompt-based defenses achieve 10.1% attack reduction with 12.5% false rejection increase, while our proposed FIDS (Foreign Instruction Detection through Separation) using LoRA adaptation achieves 15.4% attack reduction with 10.4% false rejection increase. The combined approach provides 26.3% attack reduction, demonstrating that training-time defenses outperform inference-time mitigations in both security and utility preservation.

</details>


### [86] [FaithLens: Detecting and Explaining Faithfulness Hallucination](https://arxiv.org/abs/2512.20182)
*Shuzheng Si,Qingyi Wang,Haozhe Zhao,Yuzhuo Bai,Guanqiao Chen,Kangyang Luo,Gang Chen,Fanchao Qi,Minjia Zhang,Baobao Chang,Maosong Sun*

Main category: cs.CL

TL;DR: FaithLens是一个检测大语言模型输出中真实性幻觉的模型，通过合成训练数据、数据过滤和强化学习，在12个任务上性能超过GPT-4等先进模型，并能提供高质量的解释。


<details>
  <summary>Details</summary>
Motivation: 在检索增强生成和文本摘要等实际应用中，检测大语言模型输出中的真实性幻觉至关重要，需要提高模型的可信度。

Method: 1）使用先进大语言模型合成带解释的训练数据，并通过数据过滤确保标签正确性、解释质量和数据多样性；2）在精心整理的数据上微调模型作为冷启动；3）使用基于规则的强化学习进一步优化，奖励预测正确性和解释质量。

Result: 在12个多样化任务上的测试显示，8B参数的FaithLens模型性能超过GPT-4等先进模型，并能产生高质量的解释。

Conclusion: FaithLens在可信度、效率和有效性之间实现了独特平衡，为检测大语言模型输出的真实性幻觉提供了一种有效解决方案。

Abstract: Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.

</details>


### [87] [Corpus of Cross-lingual Dialogues with Minutes and Detection of Misunderstandings](https://arxiv.org/abs/2512.20204)
*Marko Čechovič,Natália Komorníková,Dominik Macháček,Ondřej Bojar*

Main category: cs.CL

TL;DR: 本文构建了一个包含12种语言、5小时语音录音的跨语言对话语料库，用于评估同时语音翻译系统，并探索了在跨语言会议中自动检测误解的方法，其中Gemini模型在识别误解文本段方面达到77%的召回率和47%的准确率。


<details>
  <summary>Details</summary>
Motivation: 为了评估没有共同语言的个体在自动同步语音翻译辅助下进行会议的系统性能，需要构建一个多用途且现实的评估语料库。

Method: 创建了一个包含5小时语音录音、12种原始语言的ASR和黄金转录文本、自动和人工校正的英语翻译的跨语言对话语料库；同时提出自动检测误解的方法，通过人工标注误解并测试大型语言模型（如Gemini）的自动检测能力。

Result: 构建的语料库包含丰富的多语言数据，适用于跨语言摘要研究；Gemini模型在自动检测误解方面表现良好，召回率达到77%，准确率为47%。

Conclusion: 该语料库为跨语言会议系统的评估提供了重要资源，同时证明了当前大型语言模型在检测跨语言误解方面具有潜力，尽管准确率有待提升。

Abstract: Speech processing and translation technology have the potential to facilitate meetings of individuals who do not share any common language. To evaluate automatic systems for such a task, a versatile and realistic evaluation corpus is needed. Therefore, we create and present a corpus of cross-lingual dialogues between individuals without a common language who were facilitated by automatic simultaneous speech translation. The corpus consists of 5 hours of speech recordings with ASR and gold transcripts in 12 original languages and automatic and corrected translations into English. For the purposes of research into cross-lingual summarization, our corpus also includes written summaries (minutes) of the meetings.
  Moreover, we propose automatic detection of misunderstandings. For an overview of this task and its complexity, we attempt to quantify misunderstandings in cross-lingual meetings. We annotate misunderstandings manually and also test the ability of current large language models to detect them automatically. The results show that the Gemini model is able to identify text spans with misunderstandings with recall of 77% and precision of 47%.

</details>


### [88] [SlideTailor: Personalized Presentation Slide Generation for Scientific Papers](https://arxiv.org/abs/2512.20292)
*Wenzheng Zeng,Mingyu Ouyang,Langyuan Cui,Hwee Tou Ng*

Main category: cs.CL

TL;DR: 本文介绍了SlideTailor框架，通过用户提供的论文-幻灯片示例对和视觉模板来个性化生成演示文稿幻灯片，有效解决现有方法因用户偏好差异导致的生成结果不佳问题。


<details>
  <summary>Details</summary>
Motivation: 现有自动幻灯片生成方法因用户偏好差异而难以生成令人满意的结果，需要一种能够根据用户个性化需求定制幻灯片的新方法。

Method: 提出SlideTailor框架，基于用户提供的论文-幻灯片示例对和视觉模板（而非详细文字描述）来隐含学习用户偏好，采用渐进式生成策略，并引入chain-of-speech机制确保幻灯片内容与口头叙述协调一致。

Result: 实验证明该框架能有效抽取和泛化用户偏好，显著提升生成幻灯片的质量，并能支持视频演示等下游应用。

Conclusion: SlideTailor为基于用户偏好的个性化幻灯片生成提供了有效解决方案，通过隐式偏好学习和内容-口语对齐机制实现了优于现有方法的效果。

Abstract: Automatic presentation slide generation can greatly streamline content creation. However, since preferences of each user may vary, existing under-specified formulations often lead to suboptimal results that fail to align with individual user needs. We introduce a novel task that conditions paper-to-slides generation on user-specified preferences. We propose a human behavior-inspired agentic framework, SlideTailor, that progressively generates editable slides in a user-aligned manner. Instead of requiring users to write their preferences in detailed textual form, our system only asks for a paper-slides example pair and a visual template - natural and easy-to-provide artifacts that implicitly encode rich user preferences across content and visual style. Despite the implicit and unlabeled nature of these inputs, our framework effectively distills and generalizes the preferences to guide customized slide generation. We also introduce a novel chain-of-speech mechanism to align slide content with planned oral narration. Such a design significantly enhances the quality of generated slides and enables downstream applications like video presentations. To support this new task, we construct a benchmark dataset that captures diverse user preferences, with carefully designed interpretable metrics for robust evaluation. Extensive experiments demonstrate the effectiveness of our framework.

</details>


### [89] [AprielGuard](https://arxiv.org/abs/2512.20293)
*Jaykumar Kasundra,Anjaneya Praharaj,Sourabh Surana,Lakshmi Sirisha Chodisetty,Sourav Sharma,Abhigya Verma,Abhishek Bhardwaj,Debasish Kanhar,Aakash Bhagat,Khalil Slimi,Seganrasan Subramanian,Sathwik Tejaswi Madhusudhan,Ranga Prasad Chenna,Srinivas Sunkara*

Main category: cs.CL

TL;DR: AprielGuard是一个8B参数的安全防护模型，统一处理安全风险（毒性、偏见）和对抗威胁（提示注入、越狱）。它在公开和专有基准测试中表现优异，特别是在多步和推理密集场景中优于现有开源防护系统。


<details>
  <summary>Details</summary>
Motivation: 现有的审核工具通常将安全风险和对抗威胁作为独立问题处理，限制了其鲁棒性和泛化能力。需要一种统一的方法来保护LLM在对话和代理场景中的安全。

Method: 采用统一的分类法和学习框架，基于开放和合成数据训练，涵盖独立提示、多轮对话和代理工作流程，通过结构化推理轨迹增强可解释性。

Result: AprielGuard在检测有害内容和对抗操作方面表现出色，在多个基准测试中优于Llama-Guard和Granite Guardian等现有开源防护系统。

Conclusion: 通过发布该模型，旨在推动LLM可靠防护的透明和可重复研究。

Abstract: Safeguarding large language models (LLMs) against unsafe or adversarial behavior is critical as they are increasingly deployed in conversational and agentic settings. Existing moderation tools often treat safety risks (e.g. toxicity, bias) and adversarial threats (e.g. prompt injections, jailbreaks) as separate problems, limiting their robustness and generalizability. We introduce AprielGuard, an 8B parameter safeguard model that unify these dimensions within a single taxonomy and learning framework. AprielGuard is trained on a diverse mix of open and synthetic data covering standalone prompts, multi-turn conversations, and agentic workflows, augmented with structured reasoning traces to improve interpretability. Across multiple public and proprietary benchmarks, AprielGuard achieves strong performance in detecting harmful content and adversarial manipulations, outperforming existing opensource guardrails such as Llama-Guard and Granite Guardian, particularly in multi-step and reasoning intensive scenarios. By releasing the model, we aim to advance transparent and reproducible research on reliable safeguards for LLMs.

</details>


### [90] [Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives](https://arxiv.org/abs/2512.20298)
*Karolina Drożdż,Kacper Dudzic,Anna Sterna,Marcin Moskalewicz*

Main category: cs.CL

TL;DR: 该研究首次比较了先进LLM与精神健康专业人士在诊断波兰语自传体叙述中的边缘型人格障碍(BPD)和自恋型人格障碍(NPD)的表现。


<details>
  <summary>Details</summary>
Motivation: 随着人们越来越依赖LLM进行心理自我评估，需要检验LLM解释定性患者叙述的能力，特别是评估它们是否能与专业精神健康专家相媲美。

Method: 利用波兰语第一人称自传体叙述，对最先进的LLM模型与精神健康专业人士在诊断BPD和NPD的表现进行了直接比较。

Result: 表现最佳的Gemini Pro模型总体诊断准确率比人类专家高出21.91个百分点(65.48% vs. 43.57%)。模型在识别BPD方面表现优异(F1=83.4)，但严重低估了NPD(F1=6.7 vs. 50.0)，显示出对'自恋'这一价值负载术语的回避倾向。

Conclusion: 虽然LLM在解释复杂第一人称临床数据方面能力很强，但仍存在关键的可靠性和偏见问题，特别是在诊断某些人格障碍时存在系统性偏差。

Abstract: Growing reliance on LLMs for psychiatric self-assessment raises questions about their ability to interpret qualitative patient narratives. We present the first direct comparison between state-of-the-art LLMs and mental health professionals in diagnosing Borderline (BPD) and Narcissistic (NPD) Personality Disorders utilizing Polish-language first-person autobiographical accounts. We show that the top-performing Gemini Pro models surpassed human professionals in overall diagnostic accuracy by 21.91 percentage points (65.48% vs. 43.57%). While both models and human experts excelled at identifying BPD (F1 = 83.4 & F1 = 80.0, respectively), models severely underdiagnosed NPD (F1 = 6.7 vs. 50.0), showing a reluctance toward the value-laden term "narcissism." Qualitatively, models provided confident, elaborate justifications focused on patterns and formal categories, while human experts remained concise and cautious, emphasizing the patient's sense of self and temporal experience. Our findings demonstrate that while LLMs are highly competent at interpreting complex first-person clinical data, they remain subject to critical reliability and bias issues.

</details>


### [91] [Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles](https://arxiv.org/abs/2512.20324)
*Nurul Labib Sayeedi,Md. Faiyaz Abdullah Sayeedi,Khushnur Binte Jahangir,Swakkhar Shatabda,Sarah Masud Preum*

Main category: cs.CL

TL;DR: BangląRiddleEval是一个包含1,244个孟加拉语谜语的新基准测试，在四种任务中评估大语言模型在低资源、比喻性推理方面的能力，结果显示当前LLMs表现远低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在很多NLP基准测试上表现优异，但它们在比喻性、文化基础和低资源环境下的推理能力仍未被充分探索，特别是在孟加拉语等低资源语言中。

Method: 使用LLM-based pipeline生成思维链解释、语义相关的干扰项和细粒度歧义标注，评估多个开源和闭源模型在不同提示策略下的表现。

Result: 模型在生成式问答中具有中等语义重叠但正确率低，多选題准确率最高仅约56%（人类基线83%），歧义解决率在26%-68%之间，高质量解释仅限于最强模型。

Conclusion: 当前LLMs捕获了孟加拉语谜语推理的部分线索，但仍远未达到人类水平，BangląRiddleEval成为低资源比喻推理的一个具有挑战性的新基准。

Abstract: Large Language Models (LLMs) show impressive performance on many NLP benchmarks, yet their ability to reason in figurative, culturally grounded, and low-resource settings remains underexplored. We address this gap for Bangla by introducing BanglaRiddleEval, a benchmark of 1,244 traditional Bangla riddles instantiated across four tasks (4,976 riddle-task artifacts in total). Using an LLM-based pipeline, we generate Chain-of-Thought explanations, semantically coherent distractors, and fine-grained ambiguity annotations, and evaluate a diverse suite of open-source and closed-source models under different prompting strategies. Models achieve moderate semantic overlap on generative QA but low correctness, MCQ accuracy peaks at only about 56% versus an 83% human baseline, and ambiguity resolution ranges from roughly 26% to 68%, with high-quality explanations confined to the strongest models. These results show that current LLMs capture some cues needed for Bangla riddle reasoning but remain far from human-level performance, establishing BanglaRiddleEval as a challenging new benchmark for low-resource figurative reasoning. All data, code, and evaluation scripts are available on GitHub: https://github.com/Labib1610/BanglaRiddleEval.

</details>


### [92] [Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation](https://arxiv.org/abs/2512.20352)
*Nilesh Jain,Seyi Adeyinka,Leor Roseman,Aza Allsop*

Main category: cs.CL

TL;DR: 本文提出了一种多视角验证框架，结合集成验证和双可靠性指标（Cohen's Kappa和余弦相似度），用于提升LLM主题分析的可靠性。在三种主流LLM上的实验表明，该框架能有效提取共识主题并实现高一致性。


<details>
  <summary>Details</summary>
Motivation: 传统质性研究中人工编码者间的评估一致性方法成本高、耗时长且一致性中等。为解决LLM在主题分析中的可靠性挑战，需要新的验证框架。

Method: 设计多视角验证框架，结合集成验证（1-6次运行）和双可靠性指标（Cohen's Kappa评估编码者间一致性，余弦相似度评估语义一致性）。框架支持可配置参数和自定义提示结构，可从任意JSON格式中提取共识主题。

Result: 在三种LLM（Gemini 2.5 Pro、GPT-4o、Claude 3.5 Sonnet）上的评估显示，Gemini可靠性最高（κ=0.907，余弦相似度95.3%），所有模型均达到高一致性（κ>0.80）。框架成功提取了共识主题，Gemini识别6个，GPT-4o识别5个，Claude识别4个。

Conclusion: 该开源框架为可靠的AI辅助质性研究奠定了方法学基础，提供了透明的可靠性指标、灵活配置和结构无关的共识提取功能。

Abstract: Qualitative research faces a critical reliability challenge: traditional inter-rater agreement methods require multiple human coders, are time-intensive, and often yield moderate consistency. We present a multi-perspective validation framework for LLM-based thematic analysis that combines ensemble validation with dual reliability metrics: Cohen's Kappa ($κ$) for inter-rater agreement and cosine similarity for semantic consistency. Our framework enables configurable analysis parameters (1-6 seeds, temperature 0.0-2.0), supports custom prompt structures with variable substitution, and provides consensus theme extraction across any JSON format. As proof-of-concept, we evaluate three leading LLMs (Gemini 2.5 Pro, GPT-4o, Claude 3.5 Sonnet) on a psychedelic art therapy interview transcript, conducting six independent runs per model. Results demonstrate Gemini achieves highest reliability ($κ= 0.907$, cosine=95.3%), followed by GPT-4o ($κ= 0.853$, cosine=92.6%) and Claude ($κ= 0.842$, cosine=92.1%). All three models achieve a high agreement ($κ> 0.80$), validating the multi-run ensemble approach. The framework successfully extracts consensus themes across runs, with Gemini identifying 6 consensus themes (50-83% consistency), GPT-4o identifying 5 themes, and Claude 4 themes. Our open-source implementation provides researchers with transparent reliability metrics, flexible configuration, and structure-agnostic consensus extraction, establishing methodological foundations for reliable AI-assisted qualitative research.

</details>


### [93] [Sentiment-Aware Extractive and Abstractive Summarization for Unstructured Text Mining](https://arxiv.org/abs/2512.20404)
*Junyi Liu,Stanley Kok*

Main category: cs.CL

TL;DR: 提出了一种针对社交媒体等非正式文本的情感感知摘要框架，将情感信号融入抽取式（TextRank）和生成式（UniLM）方法，提升情感细节和主题相关性的捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 现有摘要方法主要针对结构化新闻文本，难以处理噪声多、非正式的社交媒体内容。情感线索对品牌监控、市场分析等信息系统任务至关重要，但当前研究很少将情感建模集成到短文本摘要中。

Method: 扩展抽取式（TextRank）和生成式（UniLM）方法，通过将情感信号嵌入排名和生成过程，构建双设计框架来同时捕捉情感细微差别和主题相关性。

Result: 该框架能够生成简洁且情感丰富的摘要，在动态在线环境中提升及时干预和战略决策的效果。

Conclusion: 情感感知摘要框架有效解决了非正式文本摘要的挑战，通过整合情感建模提升了摘要质量，对信息系统应用具有重要价值。

Abstract: With the rapid growth of unstructured data from social media, reviews, and forums, text mining has become essential in Information Systems (IS) for extracting actionable insights. Summarization can condense fragmented, emotion-rich posts, but existing methods-optimized for structured news-struggle with noisy, informal content. Emotional cues are critical for IS tasks such as brand monitoring and market analysis, yet few studies integrate sentiment modeling into summarization of short user-generated texts. We propose a sentiment-aware framework extending extractive (TextRank) and abstractive (UniLM) approaches by embedding sentiment signals into ranking and generation processes. This dual design improves the capture of emotional nuances and thematic relevance, producing concise, sentiment-enriched summaries that enhance timely interventions and strategic decision-making in dynamic online environments.

</details>


### [94] [Step-DeepResearch Technical Report](https://arxiv.org/abs/2512.20491)
*Chen Hu,Haikuo Du,Heng Wang,Lin Lin,Mingrui Chen,Peng Liu,Ruihang Miao,Tianchi Yue,Wang You,Wei Ji,Wei Yuan,Wenjin Deng,Xiaojian Yuan,Xiaoyun Zhang,Xiangyu Liu,Xikai Liu,Yanming Xu,Yicheng Cao,Yifei Zhang,Yongyao Wang,Yubo Shu,Yurong Zhang,Yuxiang Zhang,Zheng Gong,Zhichao Chang,Binyan Li,Dan Ma,Furong Jia,Hongyuan Wang,Jiayu Liu,Jing Bai,Junlan Liu,Manjiao Liu,Na Wang,Qiuping Wu,Qinxin Du,Shiwei Li,Wen Sun,Yifeng Gong,Yonglin Chen,Yuling Zhao,Yuxuan Lin,Ziqi Ren,Zixuan Wang,Aihu Zhang,Brian Li,Buyun Ma,Kang An,Li Xie,Mingliang Li,Pan Li,Shidong Yang,Xi Chen,Xiaojia Liu,Yuchu Luo,Yuan Song,YuanHao Ding,Yuanwei Liang,Zexi Li,Zhaoning Zhang,Zixin Zhang,Binxing Jiao,Daxin Jiang,Jiansheng Chen,Jing Li,Xiangyu Zhang,Yibo Zhu*

Main category: cs.CL

TL;DR: Step-DeepResearch是一个成本效益高的端到端自主研究代理，通过原子能力数据合成和渐进式训练方法，在深度研究任务上达到专家级水平。


<details>
  <summary>Details</summary>
Motivation: 现有学术基准无法满足真实世界开放式研究需求，需要解决意图识别、长期决策和跨源验证等关键能力。

Method: 基于原子能力的数据合成策略强化规划和报告写作，采用从agent中期训练到SFT和RL的渐进式训练路径，辅以清单式评判器提升稳健性。

Result: Step-DeepResearch（32B）在Scale AI研究评分标准上获得61.4%分数，在ADR-Bench中文基准上显著优于可比模型，与OpenAI和Gemini等闭源SOTA模型相当。

Conclusion: 精细训练使中等规模模型能够以行业领先的成本效益实现专家级能力，为自主智能体发展提供了有效路径。

Abstract: As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.

</details>


### [95] [Distilling to Hybrid Attention Models via KL-Guided Layer Selection](https://arxiv.org/abs/2512.20569)
*Yanhong Li,Songlin Yang,Shawn Tan,Mayank Mishra,Rameswar Panda,Jiawei Zhou,Yoon Kim*

Main category: cs.CL

TL;DR: 这篇论文提出了一种简单高效的层选择方法，用于将预训练的softmax注意力Transformer蒸馏成混合架构，通过层重要性评分来选择转换为线性注意力的最佳层，结合RADLADS蒸馏流程取得比现有方法更好的效果。


<details>
  <summary>Details</summary>
Motivation: 改进LLM推理效率而不需要从头预训练，需要将softmax注意力转换为更高效的混合架构，但关键在于如何选择合适的层进行转换。

Method: 使用通用文本数据的小规模训练得到层重要性评分来选择转换层，采用RADLADS蒸馏流程（注意力权重转移、隐藏状态对齐、KL分布匹配和小规模微调）。

Result: 该方法比现有的均匀交错转换和使用专用诊断数据集的方法更有效。

Conclusion: 提出的层选择配方简单高效，显著提升了蒸馏混合架构的性能。

Abstract: Distilling pretrained softmax attention Transformers into more efficient hybrid architectures that interleave softmax and linear attention layers is a promising approach for improving the inference efficiency of LLMs without requiring expensive pretraining from scratch. A critical factor in the conversion process is layer selection, i.e., deciding on which layers to convert to linear attention variants. This paper describes a simple and efficient recipe for layer selection that uses layer importance scores derived from a small amount of training on generic text data. Once the layers have been selected we use a recent pipeline for the distillation process itself \citep[RADLADS;][]{goldstein2025radlads}, which consists of attention weight transfer, hidden state alignment, KL-based distribution matching, followed by a small amount of finetuning. We find that this approach is more effective than existing approaches for layer selection, including heuristics that uniformly interleave linear attentions based on a fixed ratio, as well as more involved approaches that rely on specialized diagnostic datasets.

</details>


### [96] [Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits](https://arxiv.org/abs/2512.20578)
*Amirhosein Ghasemabadi,Di Niu*

Main category: cs.CL

TL;DR: Gnosis是一种轻量级自感知机制，通过解码LLM隐藏状态和注意力模式的内部信号来实现自我验证，无需外部监督即可高效预测模型自身错误。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外部评判者或多样本一致性，计算成本高且与真实正确性相关性弱，需要探索LLM能否通过检查推理过程中的内部状态来预测自身失败。

Method: Gnosis被动观察内部痕迹，将其压缩为固定预算描述符，使用约500万参数预测正确性，独立于序列长度且推理成本极低。

Result: 在数学推理、开放域问答和学术知识基准测试中，Gnosis在1.7B到20B参数规模的冻结骨干模型上一致优于强大的内部基线和大型外部评判者，在准确性和校准方面表现更好。

Conclusion: 可靠的正确定性信号内在于生成过程，可以在无需外部监督的情况下高效提取，Gnosis可零样本泛化到部分生成结果，实现早期失败检测和计算感知控制。

Abstract: Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.

</details>


### [97] [Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs](https://arxiv.org/abs/2512.20595)
*Dhruv Anand,Ehsan Shareghi*

Main category: cs.CL

TL;DR: Cube Bench是一种用于评估多模态大语言模型（MLLMs）空间和序列推理能力的魔方基准测试，包含五个维度来衡量模型从重构状态到多步行动规划的各种能力。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是为MLLMs开发一个紧凑、可重复的评估工具，专门测试其在空间和序列推理方面的能力，尤其是在复杂、多步骤任务中的表现。

Method: 通过分解魔方任务为五个技能层次（重构魔方面、选择最优下一步、预测移动结果、执行多步规划及错误恢复、检测和修正错误），使用共享的初始状态、一致提示和解析器，以及单一的距离度量来测试七个MLLMs随复杂度增加的变化。

Result: 所有模型在测试深度增加时性能急剧下降，特别是开源模型在高难度任务上接近随机水平；即便最佳模型在复杂度增加时也会退化，自我修正策略仅带来有限改善且可能引入过度思考。

Conclusion: Cube Bench是一个有效的工具，揭示了MLLMs在复杂空间和序列推理任务中的局限，特别是多步控制和错误恢复方面的不足，强调了当前模型在此类任务中仍有较大提升空间。

Abstract: We introduce Cube Bench, a Rubik's-cube benchmark for evaluating spatial and sequential reasoning in multimodal large language models (MLLMs). The benchmark decomposes performance into five skills: (i) reconstructing cube faces from images and text, (ii) choosing the optimal next move, (iii) predicting the outcome of a candidate move without applying it, (iv) executing multi-step plans while recovering from mistakes, and (v) detecting and revising one's own errors. Using a shared set of scrambled cube states, identical prompts and parsers, and a single distance-to-solved metric, we compare recent MLLMs side by side as a function of scramble depth. Across seven MLLMs, accuracy drops sharply with depth; once a trajectory stalls or diverges, models rarely recover, and high face-reconstruction accuracy does not guarantee competent action selection or multi-step execution. A pronounced closed- vs open-source gap emerges: the strongest closed model leads on both single-step perception tasks and multi-step control tasks, while open-weight models cluster near chance on the hardest settings; yet even the best MLLM degrades at higher cube complexity. A simple self-correction via reflective thinking yields modest gains but can also introduce overthinking. Cube Bench offers a compact, reproducible probe of sequential spatial reasoning in MLLMs.

</details>


### [98] [MoE-DiffuSeq: Enhancing Long-Document Diffusion Models with Sparse Attention and Mixture of Experts](https://arxiv.org/abs/2512.20604)
*Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CL

TL;DR: MoE-DiffuSeq是一个基于专家混合架构的框架，通过稀疏注意力机制和软吸收状态增强扩散模型在长文档生成中的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散文本生成模型（如DiffuSeq）在处理长序列时存在高计算成本和内存开销的问题，限制了它们在长文档生成中的应用。

Method: MoE-DiffuSeq整合了稀疏注意力与专家混合架构，设计了定制的稀疏注意力机制以降低计算复杂度，并在扩散过程中引入软吸收状态以加速序列重建和提高生成精度。

Result: 实验表明，MoE-DiffuSeq在训练效率和采样速度上显著优于现有扩散模型，尤其在科学文章生成、代码库建模和长对话生成等长文档场景中表现优异。

Conclusion: MoE-DiffuSeq在效率、速度、准确性和表达能力方面均有提升，推动了扩散模型在高质量长文本生成中的实际应用。

Abstract: We present MoE-DiffuSeq, a mixture of experts based framework for enhancing diffusion models in long document generation. Existing diffusion based text generation models, such as DiffuSeq, suffer from high computational cost and memory overhead when applied to extended sequences. To address these challenges, MoE-DiffuSeq integrates sparse attention with a mixture of experts architecture, enabling efficient and scalable long sequence modeling. Our approach introduces a customized sparse attention mechanism designed to reduce computational complexity while preserving text quality and coherence. In addition, we incorporate a soft absorbing state within the diffusion process to accelerate sequence reconstruction and improve generation precision. Extensive experiments demonstrate that MoE-DiffuSeq significantly improves training efficiency and sampling speed compared to existing diffusion models. These advantages are particularly effective for long document scenarios, including scientific article generation, code repository modeling, and long form dialogue generation. Benchmark results further show that MoE-DiffuSeq improves efficiency, speed, accuracy, and expressiveness, advancing the practical applicability of diffusion models for high quality long form text generation.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [99] [Spectral or spatial? Leveraging both for speaker extraction in challenging data conditions](https://arxiv.org/abs/2512.20165)
*Aviad Eisenberg,Sharon Gannot,Shlomo E. Chazan*

Main category: cs.SD

TL;DR: 提出了一种鲁棒的多通道语音提取算法，通过融合空间和频谱线索来解决参考信息不准确的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅依赖空间或频谱线索来识别目标说话人，但在参考信息不准确时性能会下降。本文旨在通过融合两种信息源来增强鲁棒性。

Method: 设计了一个专用网络，能够动态平衡空间和频谱线索的贡献，或在必要时忽略信息量较少的线索。通过模拟DOA估计误差和噪声频谱注册过程来评估系统。

Result: 实验结果表明，即使在参考信息存在显著不准确的情况下，所提出的模型也能成功提取目标说话人。

Conclusion: 该算法通过融合空间和频谱线索，并在推理时动态调整权重，实现了对参考信息不准确情况的鲁棒处理。

Abstract: This paper presents a robust multi-channel speaker extraction algorithm designed to handle inaccuracies in reference information. While existing approaches often rely solely on either spatial or spectral cues to identify the target speaker, our method integrates both sources of information to enhance robustness. A key aspect of our approach is its emphasis on stability, ensuring reliable performance even when one of the features is degraded or misleading. Given a noisy mixture and two potentially unreliable cues, a dedicated network is trained to dynamically balance their contributions-or disregard the less informative one when necessary. We evaluate the system under challenging conditions by simulating inference-time errors using a simple direction of arrival (DOA) estimator and a noisy spectral enrollment process. Experimental results demonstrate that the proposed model successfully extracts the desired speaker even in the presence of substantial reference inaccuracies.

</details>


### [100] [Aliasing-Free Neural Audio Synthesis](https://arxiv.org/abs/2512.20211)
*Yicheng Gu,Junan Zhang,Chaoren Wang,Jerry Li,Zhizheng Wu,Lauri Juvela*

Main category: cs.SD

TL;DR: 该论文提出了Pupu-Vocoder和Pupu-Codec，通过信号处理技术解决了神经声码器和编解码器中因上采样架构设计不当导致的混叠失真问题，在保持推理速度的同时显著提升了音频合成质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于上采样的时域模型虽然在人耳感知和合成质量上表现优异，但其架构设计缺陷（特别是非线性激活函数和ConvTranspose上采样层）会导致折叠混叠、镜像混叠和音调失真等严重的混叠伪影问题，限制了合成保真度。

Method: 从信号处理角度提出抗混叠解决方案：1）对激活函数应用过采样和抗导数抗混叠技术；2）用重采样代替有问题的ConvTranspose层以避免音调失真并消除混叠分量。基于这些抗混叠模块开发了Pupu-Vocoder和Pupu-Codec模型。

Result: 实验结果表明，轻量级的Pupu-Vocoder和Pupu-Codec在歌唱声音、音乐和通用音频合成上轻松超越现有系统，在语音合成上达到可比性能，同时抗混叠模块的有效性通过测试信号基准得到了验证。

Conclusion: 该方法有效解决了神经声码器和编解码器中的混叠失真问题，提出的抗混叠模块显著提升了音频合成质量，为音频生成研究提供了高质量预训练模型，推动了该领域的发展。

Abstract: Neural vocoders and codecs reconstruct waveforms from acoustic representations, which directly impact the audio quality. Among existing methods, upsampling-based time-domain models are superior in both inference speed and synthesis quality, achieving state-of-the-art performance. Still, despite their success in producing perceptually natural sound, their synthesis fidelity remains limited due to the aliasing artifacts brought by the inadequately designed model architectures. In particular, the unconstrained nonlinear activation generates an infinite number of harmonics that exceed the Nyquist frequency, resulting in ``folded-back'' aliasing artifacts. The widely used upsampling layer, ConvTranspose, copies the mirrored low-frequency parts to fill the empty high-frequency region, resulting in ``mirrored'' aliasing artifacts. Meanwhile, the combination of its inherent periodicity and the mirrored DC bias also brings ``tonal artifact,'' resulting in constant-frequency ringing. This paper aims to solve these issues from a signal processing perspective. Specifically, we apply oversampling and anti-derivative anti-aliasing to the activation function to obtain its anti-aliased form, and replace the problematic ConvTranspose layer with resampling to avoid the ``tonal artifact'' and eliminate aliased components. Based on our proposed anti-aliased modules, we introduce Pupu-Vocoder and Pupu-Codec, and release high-quality pre-trained checkpoints to facilitate audio generation research. We build a test signal benchmark to illustrate the effectiveness of the anti-aliased modules, and conduct experiments on speech, singing voice, music, and audio to validate our proposed models. Experimental results confirm that our lightweight Pupu-Vocoder and Pupu-Codec models can easily outperform existing systems on singing voice, music, and audio, while achieving comparable performance on speech.

</details>


### [101] [MMEDIT: A Unified Framework for Multi-Type Audio Editing via Audio Language Model](https://arxiv.org/abs/2512.20339)
*Ye Tao,Xuenan Xu,Wen Wu,Shuai Wang,Mengyue Wu,Chao Zhang*

Main category: cs.SD

TL;DR: 该论文提出了MMEdit，一个基于音频语言模型的统一音频编辑框架，通过扩展任务定义、构建大规模数据集和改进架构来解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导音频编辑方法存在训练免费方法信号质量差、训练方法数据稀缺且任务覆盖窄、标准架构模态对齐能力有限等问题。

Method: 系统扩展任务定义覆盖多种编辑操作，设计可扩展数据合成管道构建大规模数据集，集成Qwen2-Audio编码器和MMDiT生成器实现跨模态对齐。

Result: 实验结果表明该方法在编辑定位准确性、指令跟随鲁棒性和非编辑区域保真度方面表现优异。

Conclusion: MMEdit框架通过统一方法和改进架构，有效解决了音频编辑中的关键挑战，实现了高质量的文本引导音频编辑。

Abstract: Text-guided audio editing aims to modify specific acoustic events while strictly preserving non-target content. Despite recent progress, existing approaches remain fundamentally limited. Training-free methods often suffer from signal degradation caused by diffusion inversion, while training-based methods, although achieving higher generation quality, are severely constrained by the scarcity of high-quality paired data and task formulations that cover only a narrow subset of editing operations. In addition, standard architectures typically decouple text and audio processing, limiting the ability to align instructions with specific acoustic contexts.
  To address these challenges, we propose MMEdit, an audio-language-model-driven framework for unified audio editing. We systematically extend task definitions to cover a comprehensive range of editing operations, including addition, replacement, removal, reordering, and attribute modification. Furthermore, we design a scalable data synthesis pipeline to construct large-scale paired datasets with fine-grained event-level annotations. To capture complex editing semantics, we integrate a Qwen2-Audio encoder with an MMDiT-based generator, enabling precise cross-modal alignment and localized editing.
  Experimental results demonstrate that our method achieves superior editing localization accuracy, robust instruction following, and high fidelity in non-edited regions.

</details>


### [102] [EnvSSLAM-FFN: Lightweight Layer-Fused System for ESDD 2026 Challenge](https://arxiv.org/abs/2512.20369)
*Xiaoxuan Guo,Hengyan Huang,Jiayi Zhou,Renhe Sun,Jian Liu,Haonan Cheng,Long Ye,Qin Zhang*

Main category: cs.SD

TL;DR: 论文提出EnvSSLAM-FFN系统，用于环境声音深度伪造检测，在ESDD 2026挑战赛两个赛道上均优于官方基线模型。


<details>
  <summary>Details</summary>
Motivation: 生成音频模型的快速发展导致高保真环境声音合成，对音频安全构成严重威胁，因此需要开发有效的深度伪造检测方法。

Method: 集成冻结的SSLAM自监督编码器与轻量级FFN后端，融合SSLAM中间层表示（4-9层），并采用类别加权训练目标来处理数据不平衡问题。

Result: 在两个赛道上分别达到1.20%和1.05%的测试等错误率，持续优于官方基线模型。

Conclusion: EnvSSLAM-FFN方法在环境声音深度伪造检测任务中表现出色，特别是在数据不平衡和未见生成器条件下具有良好性能。

Abstract: Recent advances in generative audio models have enabled high-fidelity environmental sound synthesis, raising serious concerns for audio security. The ESDD 2026 Challenge therefore addresses environmental sound deepfake detection under unseen generators (Track 1) and black-box low-resource detection (Track 2) conditions. We propose EnvSSLAM-FFN, which integrates a frozen SSLAM self-supervised encoder with a lightweight FFN back-end. To effectively capture spoofing artifacts under severe data imbalance, we fuse intermediate SSLAM representations from layers 4-9 and adopt a class-weighted training objective. Experimental results show that the proposed system consistently outperforms the official baselines on both tracks, achieving Test Equal Error Rates (EERs) of 1.20% and 1.05%, respectively.

</details>


### [103] [AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition](https://arxiv.org/abs/2512.20407)
*Rajdeep Chatterjee,Sudip Chakrabarty,Trishaani Acharjee,Deepanjali Mishra*

Main category: cs.SD

TL;DR: 本研究提出了AUDRON（基于音频的无人机识别网络），一种结合多谱特征与深度学习的混合框架，用于无人机声音检测，在二元和多类分类中分别达到98.51%和97.11%的准确率。


<details>
  <summary>Details</summary>
Motivation: 无人机应用日益广泛，但滥用带来安全隐患。声学传感作为视觉或雷达检测的低成本替代方案，可利用无人机螺旋桨的独特声音模式进行非侵入式检测。

Method: 采用混合深度学习框架，结合MFCC和STFT谱图特征，使用CNN处理、循环层进行时间建模，以及自编码器表示，通过特征级融合整合互补信息后进行分类。

Result: AUDRON能有效区分无人机声学特征与背景噪声，在二元分类中准确率达98.51%，多类分类中达97.11%，在不同条件下保持良好泛化能力。

Conclusion: 结合多特征表示与深度学习可提高声学无人机检测的可靠性，该框架在视觉或雷达传感受限的安全监控应用中具有部署潜力。

Abstract: Unmanned aerial vehicles (UAVs), commonly known as drones, are increasingly used across diverse domains, including logistics, agriculture, surveillance, and defense. While these systems provide numerous benefits, their misuse raises safety and security concerns, making effective detection mechanisms essential. Acoustic sensing offers a low-cost and non-intrusive alternative to vision or radar-based detection, as drone propellers generate distinctive sound patterns. This study introduces AUDRON (AUdio-based Drone Recognition Network), a hybrid deep learning framework for drone sound detection, employing a combination of Mel-Frequency Cepstral Coefficients (MFCC), Short-Time Fourier Transform (STFT) spectrograms processed with convolutional neural networks (CNNs), recurrent layers for temporal modeling, and autoencoder-based representations. Feature-level fusion integrates complementary information before classification. Experimental evaluation demonstrates that AUDRON effectively differentiates drone acoustic signatures from background noise, achieving high accuracy while maintaining generalizability across varying conditions. AUDRON achieves 98.51 percent and 97.11 percent accuracy in binary and multiclass classification. The results highlight the advantage of combining multiple feature representations with deep learning for reliable acoustic drone detection, suggesting the framework's potential for deployment in security and surveillance applications where visual or radar sensing may be limited.

</details>

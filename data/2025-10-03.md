<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 77]
- [cs.CL](#cs.CL) [Total: 90]
- [cs.SD](#cs.SD) [Total: 11]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration](https://arxiv.org/abs/2510.01339)
*Alessio Spagnoletti,Andrés Almansa,Marcelo Pereyra*

Main category: cs.CV

TL;DR: LVTINO是一种新的视频恢复方法，通过引入视频一致性模型，克服了时空依赖性问题，显著提高了恢复质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前图像恢复方法难以有效处理高清视频的时空依赖性，常规方法导致视频重建不一致。

Method: 通过视频一致性模型（VCMs）建立LVTINO，作为一种零-shot的逆解算器，用于高分辨率视频恢复。

Result: 在多种视频逆问题上进行广泛实验，LVTINO在重建保真度和计算效率方面显著优于现有方法。

Conclusion: LVTINO提出了一种高效的视频恢复方法，显著提升了重建质量和计算效率。

Abstract: Computational imaging methods increasingly rely on powerful generative
diffusion models to tackle challenging image restoration tasks. In particular,
state-of-the-art zero-shot image inverse solvers leverage distilled
text-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy
and perceptual quality with high computational efficiency. However, extending
these advances to high-definition video restoration remains a significant
challenge, due to the need to recover fine spatial detail while capturing
subtle temporal dependencies. Consequently, methods that naively apply
image-based LDM priors on a frame-by-frame basis often result in temporally
inconsistent reconstructions. We address this challenge by leveraging recent
advances in Video Consistency Models (VCMs), which distill video latent
diffusion models into fast generators that explicitly capture temporal
causality. Building on this foundation, we propose LVTINO, the first zero-shot
or plug-and-play inverse solver for high definition video restoration with
priors encoded by VCMs. Our conditioning mechanism bypasses the need for
automatic differentiation and achieves state-of-the-art video reconstruction
quality with only a few neural function evaluations, while ensuring strong
measurement consistency and smooth temporal transitions across frames.
Extensive experiments on a diverse set of video inverse problems show
significant perceptual improvements over current state-of-the-art methods that
apply image LDMs frame by frame, establishing a new benchmark in both
reconstruction fidelity and computational efficiency.

</details>


### [2] [Image Generation Based on Image Style Extraction](https://arxiv.org/abs/2510.01347)
*Shuochen Chang*

Main category: cs.CV

TL;DR: 本研究提出了一种新方法，通过风格编码器和风格投影层，从单一风格参考图像中提取细粒度风格表现，以实现基于文本提示的细粒度图像生成。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决传统文本指导生成模型在细粒度风格描述和控制方面的不足，推动预训练生成模型的生成能力最大化。

Method: 采用风格编码器和风格投影层对比对风格表现与文本表现，以实现细粒度文本提示的风格指导生成。

Result: 构建了Style30k-captions数据集，以训练风格编码器和风格投影层，促进细粒度风格化图像生成。

Conclusion: 研究提出了一种基于三阶段训练风格提取的方法，通过图像、风格标签和文本描述的三元组数据集实现细粒度风格控制的图像生成。

Abstract: Image generation based on text-to-image generation models is a task with
practical application scenarios that fine-grained styles cannot be precisely
described and controlled in natural language, while the guidance information of
stylized reference images is difficult to be directly aligned with the textual
conditions of traditional textual guidance generation. This study focuses on
how to maximize the generative capability of the pretrained generative model,
by obtaining fine-grained stylistic representations from a single given
stylistic reference image, and injecting the stylistic representations into the
generative body without changing the structural framework of the downstream
generative model, so as to achieve fine-grained controlled stylized image
generation. In this study, we propose a three-stage training style
extraction-based image generation method, which uses a style encoder and a
style projection layer to align the style representations with the textual
representations to realize fine-grained textual cue-based style guide
generation. In addition, this study constructs the Style30k-captions dataset,
whose samples contain a triad of images, style labels, and text descriptions,
to train the style encoder and style projection layer in this experiment.

</details>


### [3] [EvoStruggle: A Dataset Capturing the Evolution of Struggle across Activities and Skill Levels](https://arxiv.org/abs/2510.01362)
*Shijia Feng,Michael Wray,Walterio Mayol-Cuevas*

Main category: cs.CV

TL;DR: 本研究收集了一个关于技能学习过程中挣扎的数据集，定义了挣扎检测问题为时序动作定位任务，模型在不同任务间的迁移能力受限，但具有一定检测能力。


<details>
  <summary>Details</summary>
Motivation: 随着技能的发展，挣扎的类型和频率会发生变化，因此理解这种演变对于优化学习和开发有效的辅助系统至关重要。

Method: 通过收集61.68小时的视频录制，2933个视频，和5385个标注的挣扎时段，定义挣扎判断问题为一个时序动作定位任务，旨在识别和精确定位挣扎段落的起止时间。

Result: 我们的实验结果表明，时序动作定位模型能够成功检测挣扎线索，尽管在未见过的任务和活动上进行评估时，模型的表现仍然面临挑战。

Conclusion: 本研究构建的数据集及相应的模型在检测技能学习过程中的挣扎表现出了一定的有效性，尽管在不同任务和活动中的检测效果仍需进一步提高。

Abstract: The ability to determine when a person struggles during skill acquisition is
crucial for both optimizing human learning and enabling the development of
effective assistive systems. As skills develop, the type and frequency of
struggles tend to change, and understanding this evolution is key to
determining the user's current stage of learning. However, existing
manipulation datasets have not focused on how struggle evolves over time. In
this work, we collect a dataset for struggle determination, featuring 61.68
hours of video recordings, 2,793 videos, and 5,385 annotated temporal struggle
segments collected from 76 participants. The dataset includes 18 tasks grouped
into four diverse activities -- tying knots, origami, tangram puzzles, and
shuffling cards, representing different task variations. In addition,
participants repeated the same task five times to capture their evolution of
skill. We define the struggle determination problem as a temporal action
localization task, focusing on identifying and precisely localizing struggle
segments with start and end times. Experimental results show that Temporal
Action Localization models can successfully learn to detect struggle cues, even
when evaluated on unseen tasks or activities. The models attain an overall
average mAP of 34.56% when generalizing across tasks and 19.24% across
activities, indicating that struggle is a transferable concept across various
skill-based tasks while still posing challenges for further improvement in
struggle detection. Our dataset is available at
https://github.com/FELIXFENG2019/EvoStruggle.

</details>


### [4] [SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs](https://arxiv.org/abs/2510.01370)
*Abu Bucker Siddik,Diane Oyen,Alexander Most,Michal Kucer,Ayan Biswas*

Main category: cs.CV

TL;DR: 本文提出Small PDE U-Net Solver (SPUS)，一种紧凑高效的基础模型，利用残差U-Net架构和自回归预训练策略，在多种偏微分方程任务上表现优秀。


<details>
  <summary>Details</summary>
Motivation: 在现有基于复杂转换器架构的偏微分方程基础模型存在高计算和参数开销的情况下，提出一种更紧凑高效的方法。

Method: 采用轻量级残差U-Net架构和自回归预训练策略，以模拟数值求解器的行为来学习物理规律。

Result: SPUS在六个未见的下游偏微分方程任务中取得了最先进的泛化效果。

Conclusion: SPUS作为一种高效的基础模型在解决多种偏微分方程上表现出卓越的泛化能力，同时参数需求和微调数据量显著降低，显示其作为新型强大工具的潜力。

Abstract: We introduce Small PDE U-Net Solver (SPUS), a compact and efficient
foundation model (FM) designed as a unified neural operator for solving a wide
range of partial differential equations (PDEs). Unlike existing
state-of-the-art PDE FMs-primarily based on large complex transformer
architectures with high computational and parameter overhead-SPUS leverages a
lightweight residual U-Net-based architecture that has been largely
underexplored as a foundation model architecture in this domain. To enable
effective learning in this minimalist framework, we utilize a simple yet
powerful auto-regressive pretraining strategy which closely replicates the
behavior of numerical solvers to learn the underlying physics. SPUS is
pretrained on a diverse set of fluid dynamics PDEs and evaluated across 6
challenging unseen downstream PDEs spanning various physical systems.
Experimental results demonstrate that SPUS using residual U-Net based
architecture achieves state-of-the-art generalization on these downstream tasks
while requiring significantly fewer parameters and minimal fine-tuning data,
highlighting its potential as a highly parameter-efficient FM for solving
diverse PDE systems.

</details>


### [5] [DisCo: Reinforcement with Diversity Constraints for Multi-Human Generation](https://arxiv.org/abs/2510.01399)
*Shubhankar Borse,Farzad Farhadzadeh,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: DisCo是首个基于RL直接优化多人体生成中身份多样性的框架，通过惩罚相似性和重复身份，取得了显著效果并设立了新基准。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型在处理多人体生成时存在问题，比如复制面孔、融合身份和个体计数错误。

Method: 通过组相对策略优化（GRPO）对流匹配模型进行微调，使用组合奖励来优化身份多样性。

Result: 在DiverseHumans测试集上，DisCo实现了98.6%的独特面部准确率和几乎完美的全球身份分布，超越了多个开放源代码和专有方法的表现，同时保持了竞争力的感知质量。

Conclusion: DisCo作为一种可扩展的、无需注释的解决方案，成功解决了生成模型中的身份危机，为多人体生成设立了新基准。

Abstract: State-of-the-art text-to-image models excel at realism but collapse on
multi-human prompts - duplicating faces, merging identities, and miscounting
individuals. We introduce DisCo (Reinforcement with Diversity Constraints), the
first RL-based framework to directly optimize identity diversity in multi-human
generation. DisCo fine-tunes flow-matching models via Group-Relative Policy
Optimization (GRPO) with a compositional reward that (i) penalizes intra-image
facial similarity, (ii) discourages cross-sample identity repetition, (iii)
enforces accurate person counts, and (iv) preserves visual fidelity through
human preference scores. A single-stage curriculum stabilizes training as
complexity scales, requiring no extra annotations. On the DiverseHumans
Testset, DisCo achieves 98.6 Unique Face Accuracy and near-perfect Global
Identity Spread - surpassing both open-source and proprietary methods (e.g.,
Gemini, GPT-Image) while maintaining competitive perceptual quality. Our
results establish DisCo as a scalable, annotation-free solution that resolves
the long-standing identity crisis in generative models and sets a new benchmark
for compositional multi-human generation.

</details>


### [6] [GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of Geographic Embeddings](https://arxiv.org/abs/2510.01448)
*Angel Daruna,Nicholas Meegan,Han-Pang Chiu,Supun Samarasekera,Rakesh Kumar*

Main category: cs.CV

TL;DR: 本研究提出了一种新的视觉地理定位方法，通过结合地理嵌入和查询图像的视觉特征，显著提高了定位精度。


<details>
  <summary>Details</summary>
Motivation: 随着全球视觉地理定位的需求增加，研究人员需要寻找更有效的方式来利用图像内容确定其地理位置。

Method: 提出了一种新颖的地理表示，建立了地理嵌入的层次结构，并结合查询图像的外观特征和语义分割图，形成稳健的视觉表示。

Result: 在25项指标中，22项的实验结果超越了先前的最先进方法和最近的大型视觉-语言模型。

Conclusion: 在视觉地理定位方面，我们的方法在多个评估指标上表现优异，证明了地理与视觉表示结合的有效性。

Abstract: Worldwide visual geo-localization seeks to determine the geographic location
of an image anywhere on Earth using only its visual content. Learned
representations of geography for visual geo-localization remain an active
research topic despite much progress. We formulate geo-localization as aligning
the visual representation of the query image with a learned geographic
representation. Our novel geographic representation explicitly models the world
as a hierarchy of geographic embeddings. Additionally, we introduce an approach
to efficiently fuse the appearance features of the query image with its
semantic segmentation map, forming a robust visual representation. Our main
experiments demonstrate improved all-time bests in 22 out of 25 metrics
measured across five benchmark datasets compared to prior state-of-the-art
(SOTA) methods and recent Large Vision-Language Models (LVLMs). Additional
ablation studies support the claim that these gains are primarily driven by the
combination of geographic and visual representations.

</details>


### [7] [Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories](https://arxiv.org/abs/2510.01454)
*Nilay Naharas,Dang Nguyen,Nesihan Bulut,Mohammadhossein Bateni,Vahab Mirrokni,Baharan Mirzasoleiman*

Main category: cs.CV

TL;DR: 本研究提出XMAS方法，旨在提高大型视觉语言模型的训练数据效率，通过聚类相似的注意矩阵示例，显著减少数据冗余而不影响性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何在大型视觉语言模型中进行数据高效学习，尤其是在数据选择方面的不足。

Method: 提出了一种基于交叉模态注意矩阵相似性聚类的XMAS方法，通过选取平衡子集来消除冗余数据。

Result: XMAS方法在保持LLaVA-1.5-7B性能的同时，能够去除LLaVA-665k数据集的50%和Vision-Flan数据集的85%，并使训练速度提高1.2倍。

Conclusion: XMAS方法能有效去除大型视觉语言模型的冗余数据，提升数据利用效率，同时保持性能和加快训练速度。

Abstract: Data-efficient learning aims to eliminate redundancy in large training
datasets by training models on smaller subsets of the most informative
examples. While data selection has been extensively explored for vision models
and large language models (LLMs), it remains underexplored for Large
Vision-Language Models (LVLMs). Notably, none of existing methods can
outperform random selection at different subset sizes. In this work, we propose
the first principled method for data-efficient instruction tuning of LVLMs. We
prove that examples with similar cross-modal attention matrices during
instruction tuning have similar gradients. Thus, they influence model
parameters in a similar manner and convey the same information to the model
during training. Building on this insight, we propose XMAS, which clusters
examples based on the trajectories of the top singular values of their
attention matrices obtained from fine-tuning a small proxy LVLM. By sampling a
balanced subset from these clusters, XMAS effectively removes redundancy in
large-scale LVLM training data. Extensive experiments show that XMAS can
discard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while
fully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and
speeding up its training by 1.2x. This is 30% more data reduction compared to
the best baseline for LLaVA-665k. The project's website can be found at
https://bigml-cs-ucla.github.io/XMAS-project-page/.

</details>


### [8] [Purrception: Variational Flow Matching for Vector-Quantized Image Generation](https://arxiv.org/abs/2510.01478)
*Răzvan-Andrei Matişan,Vincent Tao Hu,Grigory Bartosh,Björn Ommer,Cees G. M. Snoek,Max Welling,Jan-Willem van de Meent,Mohammad Mahdi Derakhshani,Floor Eijkelboom*

Main category: cs.CV

TL;DR: 本研究提出了Purrception，一种结合连续方法几何意识与离散方法类别监督的变分流匹配图像生成方法，提升了训练效率。


<details>
  <summary>Details</summary>
Motivation: 我们希望通过提供显式的类别监督，结合连续的传输动态来改进向量量化图像生成。

Method: 通过学习代码簿索引的类别后验，同时在连续嵌入空间中计算速度场，适应变分流匹配方法到向量量化潜在变量。

Result: Purrception在ImageNet-1k 256x256生成任务中评估，训练收敛速度快于现有连续流匹配和离散流匹配基准，同时取得与最先进模型相媲美的FID分数。

Conclusion: 变分流匹配能够有效地结合连续传输和离散监督，从而提高图像生成的训练效率。

Abstract: We introduce Purrception, a variational flow matching approach for
vector-quantized image generation that provides explicit categorical
supervision while maintaining continuous transport dynamics. Our method adapts
Variational Flow Matching to vector-quantized latents by learning categorical
posteriors over codebook indices while computing velocity fields in the
continuous embedding space. This combines the geometric awareness of continuous
methods with the discrete supervision of categorical approaches, enabling
uncertainty quantification over plausible codes and temperature-controlled
generation. We evaluate Purrception on ImageNet-1k 256x256 generation. Training
converges faster than both continuous flow matching and discrete flow matching
baselines while achieving competitive FID scores with state-of-the-art models.
This demonstrates that Variational Flow Matching can effectively bridge
continuous transport and discrete supervision for improved training efficiency
in image generation.

</details>


### [9] [AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging](https://arxiv.org/abs/2510.01498)
*Yuxuan Ou,Ning Bi,Jiazhen Pan,Jiancheng Yang,Boliang Yu,Usama Zidan,Regent Lee,Vicente Grau*

Main category: cs.CV

TL;DR: 本研究提出了一种新的统一深度学习框架，能同时生成合成CECT图像和进行解剖分割，显著提高了图像合成和分割的准确性，减少了对碘造影剂的需求。


<details>
  <summary>Details</summary>
Motivation: 为了减少对碘造影剂的依赖，降低其在评估腹主动脉瘤时的风险，探索基于非对比CT扫描生成合成CECT的深度学习方法。

Method: 整合条件扩散模型与多任务学习，通过端到端的联合优化方式，同时生成合成CECT图像和进行解剖分割，采用半监督训练策略。

Result: 在264名患者的评估中，我们的方法在图像合成和解剖分割上均优于现有单任务和多阶段模型，且在临床测量中表现出更高的准确性。

Conclusion: 我们提出的统一深度学习框架在合成CT图像和解剖分割方面显著优于现有的方法，具有更高的准确性和有效性，适用于临床数据的真实场景。

Abstract: While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic
aneurysms (AAA), the required iodinated contrast agents pose significant risks,
including nephrotoxicity, patient allergies, and environmental harm. To reduce
contrast agent use, recent deep learning methods have focused on generating
synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a
multi-stage pipeline that first generates images and then performs
segmentation, which leads to error accumulation and fails to leverage shared
semantic and anatomical structures. To address this, we propose a unified deep
learning framework that generates synthetic CECT images from NCCT scans while
simultaneously segmenting the aortic lumen and thrombus. Our approach
integrates conditional diffusion models (CDM) with multi-task learning,
enabling end-to-end joint optimization of image synthesis and anatomical
segmentation. Unlike previous multitask diffusion models, our approach requires
no initial predictions (e.g., a coarse segmentation mask), shares both encoder
and decoder parameters across tasks, and employs a semi-supervised training
strategy to learn from scans with missing segmentation labels, a common
constraint in real-world clinical data. We evaluated our method on a cohort of
264 patients, where it consistently outperformed state-of-the-art single-task
and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61
dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation,
it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus
Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to
more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm
from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to
nnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.

</details>


### [10] [From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods for Multimodal Content Analysis and Understanding](https://arxiv.org/abs/2510.01513)
*Basem Rizk,Joel Walsh,Mark Core,Benjamin Nye*

Main category: cs.CV

TL;DR: 提出了一种框架，通过整合预训练模型，简化视频内容的处理和知识图谱的构建。


<details>
  <summary>Details</summary>
Motivation: 多模态内容分析常常面临计算成本高和工程复杂度大的问题，迫切需要一个便捷的框架来简化这一过程。

Method: 提出了一个框架，通过结合多种预训练模型，将视频转换为时间序列半结构化数据格式，并进一步构建为可查询的帧级知识图谱。

Result: 通过该框架，实现了高效的多模态内容分析和知识的动态更新，支持持续学习。

Conclusion: 该框架有效地支持多模态内容分析的原型开发，简化了视频数据转化及知识图谱构建的过程。

Abstract: Analysis of multi-modal content can be tricky, computationally expensive, and
require a significant amount of engineering efforts. Lots of work with
pre-trained models on static data is out there, yet fusing these opensource
models and methods with complex data such as videos is relatively challenging.
In this paper, we present a framework that enables efficiently prototyping
pipelines for multi-modal content analysis. We craft a candidate recipe for a
pipeline, marrying a set of pre-trained models, to convert videos into a
temporal semi-structured data format. We translate this structure further to a
frame-level indexed knowledge graph representation that is query-able and
supports continual learning, enabling the dynamic incorporation of new
domain-specific knowledge through an interactive medium.

</details>


### [11] [WALT: Web Agents that Learn Tools](https://arxiv.org/abs/2510.01524)
*Viraj Prabhu,Yutong Dai,Matthew Fernandez,Jing Gu,Krithika Ramakrishnan,Yanqi Luo,Silvio Savarese,Caiming Xiong,Junnan Li,Zeyuan Chen,Ran Xu*

Main category: cs.CV

TL;DR: WALT框架通过逆向工程网站的功能，将其转化为可重复调用的工具，提升了网页代理的自动化效率，降低了脆弱性。


<details>
  <summary>Details</summary>
Motivation: 当前的网页代理方法依赖于易碎的逐步UI交互和复杂的LLM推理，导致在动态布局和长时间操作下表现不佳。

Method: 逆向工程网站的潜在功能，转换为可重复调用的工具，简化执行流程。

Result: 在VisualWebArena和WebArena上，WALT实现了更高的成功率、更少的步骤以及更少的LLM依赖推理。

Conclusion: WALT建立了一种可靠且通用的浏览器自动化范式，能够更高效地实现网页代理的功能。

Abstract: Web agents promise to automate complex browser tasks, but current methods
remain brittle -- relying on step-by-step UI interactions and heavy LLM
reasoning that break under dynamic layouts and long horizons. Humans, by
contrast, exploit website-provided functionality through high-level operations
like search, filter, and sort. We introduce WALT (Web Agents that Learn Tools),
a framework that reverse-engineers latent website functionality into reusable
invocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust
implementations of automations already designed into websites -- spanning
discovery (search, filter, sort), communication (post, comment, upvote), and
content management (create, edit, delete). Tools abstract away low-level
execution: instead of reasoning about how to click and type, agents simply call
search(query) or create(listing). This shifts the computational burden from
fragile step-by-step reasoning to reliable tool invocation. On VisualWebArena
and WebArena, WALT achieves higher success with fewer steps and less
LLM-dependent reasoning, establishing a robust and generalizable paradigm for
browser automation.

</details>


### [12] [MATCH: Multi-faceted Adaptive Topo-Consistency for Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2510.01532)
*Meilong Xu,Xiaoling Hu,Shahira Abousamra,Chen Li,Chao Chen*

Main category: cs.CV

TL;DR: 提出一种半监督分割框架，通过多种扰动预测与匹配策略，提高了组织病理图像分析的分割准确性。


<details>
  <summary>Details</summary>
Motivation: 在半监督分割中，从未标记数据中捕捉有意义的语义结构尤为重要，特别是在组织病理学图像分析中，物体分布密集。

Method: 引入了一种新的匹配策略，结合空间重叠与全局结构对齐，最小化预测之间的差异。

Result: 广泛实验表明，方法有效地减少了拓扑错误，增强了分割的稳健性和准确性。

Conclusion: 该方法有效降低了拓扑错误，提供了更稳健和准确的分割，适用于可靠的后续分析。

Abstract: In semi-supervised segmentation, capturing meaningful semantic structures
from unlabeled data is essential. This is particularly challenging in
histopathology image analysis, where objects are densely distributed. To
address this issue, we propose a semi-supervised segmentation framework
designed to robustly identify and preserve relevant topological features. Our
method leverages multiple perturbed predictions obtained through stochastic
dropouts and temporal training snapshots, enforcing topological consistency
across these varied outputs. This consistency mechanism helps distinguish
biologically meaningful structures from transient and noisy artifacts. A key
challenge in this process is to accurately match the corresponding topological
features across the predictions in the absence of ground truth. To overcome
this, we introduce a novel matching strategy that integrates spatial overlap
with global structural alignment, minimizing discrepancies among predictions.
Extensive experiments demonstrate that our approach effectively reduces
topological errors, resulting in more robust and accurate segmentations
essential for reliable downstream analysis. Code is available at
\href{https://github.com/Melon-Xu/MATCH}{https://github.com/Melon-Xu/MATCH}.

</details>


### [13] [Towards Better Optimization For Listwise Preference in Diffusion Models](https://arxiv.org/abs/2510.01540)
*Jiamu Bai,Xin Yu,Meilong Xu,Weitao Lu,Xin Pan,Kiwan Maeng,Daniel Kifer,Jian Wang,Yu Wang*

Main category: cs.CV

TL;DR: 本文提出Diffusion-LPO框架，利用用户反馈的排名信息提升扩散模型的性能，超越了传统的成对DPO方法。


<details>
  <summary>Details</summary>
Motivation: 在人类反馈中，图像偏好往往包含隐含的排名信息，这比仅仅的成对比较能更精确地表达人类偏好。

Method: 提出Diffusion-LPO框架，通过用户反馈聚合成排名列表，并在Plackett-Luce模型下推导DPO目标的列表扩展。

Result: Diffusion-LPO在文本到图像生成、图像编辑和个性化偏好对齐等任务上展示了其有效性。

Conclusion: Diffusion-LPO在视觉质量和偏好对齐上始终优于基于对的DPO基准。

Abstract: Reinforcement learning from human feedback (RLHF) has proven effectiveness
for aligning text-to-image (T2I) diffusion models with human preferences.
Although Direct Preference Optimization (DPO) is widely adopted for its
computational efficiency and avoidance of explicit reward modeling, its
applications to diffusion models have primarily relied on pairwise preferences.
The precise optimization of listwise preferences remains largely unaddressed.
In practice, human feedback on image preferences often contains implicit ranked
information, which conveys more precise human preferences than pairwise
comparisons. In this work, we propose Diffusion-LPO, a simple and effective
framework for Listwise Preference Optimization in diffusion models with
listwise data. Given a caption, we aggregate user feedback into a ranked list
of images and derive a listwise extension of the DPO objective under the
Plackett-Luce model. Diffusion-LPO enforces consistency across the entire
ranking by encouraging each sample to be preferred over all of its lower-ranked
alternatives. We empirically demonstrate the effectiveness of Diffusion-LPO
across various tasks, including text-to-image generation, image editing, and
personalized preference alignment. Diffusion-LPO consistently outperforms
pairwise DPO baselines on visual quality and preference alignment.

</details>


### [14] [Growing Visual Generative Capacity for Pre-Trained MLLMs](https://arxiv.org/abs/2510.01546)
*Hanyu Wang,Jiaming Han,Ziyan Yang,Qi Zhao,Shanchuan Lin,Xiangyu Yue,Abhinav Shrivastava,Zhenheng Yang,Hao Chen*

Main category: cs.CV

TL;DR: 提出了一种新的多模态大型语言模型Bridge，改进了图像理解和生成的表现，减少了训练数据及时间需求。


<details>
  <summary>Details</summary>
Motivation: 尽管已有的多模态大型语言模型（MLLMs）在视觉理解方面取得了成功，但构建统一的MLLMs仍面临挑战。

Method: 使用Mixture-of-Transformers架构，结合语义到像素的离散表示，以实现图像理解和生成。

Result: 在各类多模态基准测试中，Bridge在理解和生成的表现上具有竞争力，且仅需少量的训练数据和缩短的训练时间。

Conclusion: Bridge模型在理解和生成多模态信息方面表现出竞争力或优越性，且训练数据需求和时间较少。

Abstract: Multimodal large language models (MLLMs) extend the success of language
models to visual understanding, and recent efforts have sought to build unified
MLLMs that support both understanding and generation. However, constructing
such models remains challenging: hybrid approaches combine continuous
embeddings with diffusion or flow-based objectives, producing high-quality
images but breaking the autoregressive paradigm, while pure autoregressive
approaches unify text and image prediction over discrete visual tokens but
often face trade-offs between semantic alignment and pixel-level fidelity. In
this work, we present Bridge, a pure autoregressive unified MLLM that augments
pre-trained visual understanding models with generative ability through a
Mixture-of-Transformers architecture, enabling both image understanding and
generation within a single next-token prediction framework. To further improve
visual generation fidelity, we propose a semantic-to-pixel discrete
representation that integrates compact semantic tokens with fine-grained pixel
tokens, achieving strong language alignment and precise description of visual
details with only a 7.9% increase in sequence length. Extensive experiments
across diverse multimodal benchmarks demonstrate that Bridge achieves
competitive or superior results in both understanding and generation
benchmarks, while requiring less training data and reduced training time
compared to prior unified MLLMs.

</details>


### [15] [Robust Classification of Oral Cancer with Limited Training Data](https://arxiv.org/abs/2510.01547)
*Akshay Bhagwan Sonawane,Lena D. Swamikannan,Lakshman Tamil*

Main category: cs.CV

TL;DR: 本研究提出了一种混合模型，通过结合CNN与贝叶斯深度学习，提升小训练集下口腔癌分类的准确性与可靠性，尤其在数据稀缺环境中表现优越。


<details>
  <summary>Details</summary>
Motivation: 口腔癌是一种全球流行、死亡率高的癌症，早期诊断至关重要，但由于医疗资源不足，传统深度学习模型在数据有限的环境中面临过拟合和不可靠性的问题。

Method: 该研究结合卷积神经网络(CNN)与贝叶斯深度学习，利用变分推断提升模型在小训练集上的可靠性和不确定性量化。

Result: 模型在与训练数据分布相似的测试集上，达到94%准确率；在多样化的真实世界测试集上，获得88%的准确率，高于传统CNN的72.94%。

Conclusion: 该研究提出的混合模型在小规模训练集的口腔癌分类中具备高准确性和优越的泛化能力，尤其在数据稀缺环境中表现出色。

Abstract: Oral cancer ranks among the most prevalent cancers globally, with a
particularly high mortality rate in regions lacking adequate healthcare access.
Early diagnosis is crucial for reducing mortality; however, challenges persist
due to limited oral health programs, inadequate infrastructure, and a shortage
of healthcare practitioners. Conventional deep learning models, while
promising, often rely on point estimates, leading to overconfidence and reduced
reliability. Critically, these models require large datasets to mitigate
overfitting and ensure generalizability, an unrealistic demand in settings with
limited training data. To address these issues, we propose a hybrid model that
combines a convolutional neural network (CNN) with Bayesian deep learning for
oral cancer classification using small training sets. This approach employs
variational inference to enhance reliability through uncertainty
quantification. The model was trained on photographic color images captured by
smartphones and evaluated on three distinct test datasets. The proposed method
achieved 94% accuracy on a test dataset with a distribution similar to that of
the training data, comparable to traditional CNN performance. Notably, for
real-world photographic image data, despite limitations and variations
differing from the training dataset, the proposed model demonstrated superior
generalizability, achieving 88% accuracy on diverse datasets compared to 72.94%
for traditional CNNs, even with a smaller dataset. Confidence analysis revealed
that the model exhibits low uncertainty (high confidence) for correctly
classified samples and high uncertainty (low confidence) for misclassified
samples. These results underscore the effectiveness of Bayesian inference in
data-scarce environments in enhancing early oral cancer diagnosis by improving
model reliability and generalizability.

</details>


### [16] [Consistent Assistant Domains Transformer for Source-free Domain Adaptation](https://arxiv.org/abs/2510.01559)
*Renrong Shao,Wei Zhang,Kangyang Luo,Qin Li,and Jun Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的源无关领域适应方法CADTrans，通过构造不变特征和多样化表示，有效提高了领域适应的性能，并提出了针对困难样本的对齐方法。


<details>
  <summary>Details</summary>
Motivation: 源无关领域适应面临无法直接访问源领域数据的挑战，现有方法在处理困难样本和领域偏差方面存在局限，因此需要改进不变特征的获取和样本区分方法。

Method: 提出了一种名为CADTrans的一致助理域变换器，利用助理领域模块从中间聚合的全局注意力中获取多样化表示，并通过条件多核最大均值差异性（CMK-MMD）策略对样本进行区分与对齐。

Result: 在多个基准数据集（如Office-31、Office-Home、VISDA-C和DomainNet-126）上进行了广泛实验，证明了CADTrans在源无关领域适应中的显著性能提升。

Conclusion: CADTrans通过构建不变特征表示并使用多种一致性策略，显著提高了源无关领域适应的性能。

Abstract: Source-free domain adaptation (SFDA) aims to address the challenge of
adapting to a target domain without accessing the source domain directly.
However, due to the inaccessibility of source domain data, deterministic
invariable features cannot be obtained. Current mainstream methods primarily
focus on evaluating invariant features in the target domain that closely
resemble those in the source domain, subsequently aligning the target domain
with the source domain. However, these methods are susceptible to hard samples
and influenced by domain bias. In this paper, we propose a Consistent Assistant
Domains Transformer for SFDA, abbreviated as CADTrans, which solves the issue
by constructing invariable feature representations of domain consistency.
Concretely, we develop an assistant domain module for CADTrans to obtain
diversified representations from the intermediate aggregated global attentions,
which addresses the limitation of existing methods in adequately representing
diversity. Based on assistant and target domains, invariable feature
representations are obtained by multiple consistent strategies, which can be
used to distinguish easy and hard samples. Finally, to align the hard samples
to the corresponding easy samples, we construct a conditional multi-kernel max
mean discrepancy (CMK-MMD) strategy to distinguish between samples of the same
category and those of different categories. Extensive experiments are conducted
on various benchmarks such as Office-31, Office-Home, VISDA-C, and
DomainNet-126, proving the significant performance improvements achieved by our
proposed approaches. Code is available at
https://github.com/RoryShao/CADTrans.git.

</details>


### [17] [Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations](https://arxiv.org/abs/2510.01576)
*Ricardo Gonzalez Penuela,Felipe Arias-Russi,Victor Capriles*

Main category: cs.CV

TL;DR: 本论文提出了一种改进的多模态大语言模型系统，通过借鉴BLV用户的历史问题，实现了更符合用户需求的图像描述，显著提升了描述的相关性和用户体验。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有视觉解释应用对盲人和低视力（BLV）用户描述冗长且不相关的问题，提高信息交互的效率。

Method: 本研究开发了一种系统，通过识别与给定图像相关的历史视觉上下文，并利用VizWiz-LF数据集中的相关问题，引导多模态大语言模型生成描述。

Result: 经过评估，发现背景相关的描述在76.1%的情况下有效回应用户问题，并在54.4%的比较中更受欢迎。

Conclusion: 通过历史BLV用户的问题指导，改进了多模态大语言模型生成更具针对性的图像描述，满足了盲人和低视力用户的特定需求，提升了描述的相关性和效率。

Abstract: Multimodal large language models (MLLMs) have been integrated into visual
interpretation applications to support Blind and Low Vision (BLV) users because
of their accuracy and ability to provide rich, human-like interpretations.
However, these applications often default to comprehensive, lengthy
descriptions regardless of context. This leads to inefficient exchanges, as
users must go through irrelevant details rather than receiving the specific
information they are likely to seek. To deliver more contextually-relevant
information, we developed a system that draws on historical BLV users
questions. When given an image, our system identifies similar past visual
contexts from the VizWiz-LF dataset and uses the associated questions to guide
the MLLM generate descriptions more relevant to BLV users. An evaluation with
three human labelers who revised 92 context-aware and context-free descriptions
showed that context-aware descriptions anticipated and answered users'
questions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of
comparisons (50 out of 92). Our paper reviews, and data analysis are publicly
available in a Github repository at
https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .

</details>


### [18] [ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models](https://arxiv.org/abs/2510.01582)
*Krishna Teja Chitty-Venkata,Murali Emani*

Main category: cs.CV

TL;DR: 我们开发了一个名为ImageNet-Think的多模态推理数据集，以支持视觉语言模型的推理能力，通过250,000张图像和对应的思考-答案序列，促进对多模态推理机制的理解。


<details>
  <summary>Details</summary>
Motivation: 旨在支持视觉语言模型的发展，特别是其推理能力。

Method: 采用两种最先进的视觉语言模型生成数据集，配备250,000张图像及相应的思考和答案序列。

Result: 创建了一个包含图像和思考-答案序列的综合数据集，供多模态推理模型训练和评估使用。

Conclusion: 该数据集将促进更鲁棒的视觉语言模型的发展，并增进对多模态推理机制的理解。

Abstract: We develop ImageNet-Think, a multimodal reasoning dataset designed to aid the
development of Vision Language Models (VLMs) with explicit reasoning
capabilities. Our dataset is built on 250,000 images from ImageNet21k dataset,
providing structured thinking tokens and corresponding answers. Our synthetic
dataset is generated by two state-of-the-art VLMs: GLM-4.1V-9B-Thinking and
Kimi-VL-A3B-Thinking-2506. Each image is accompanied by two pairs of
thinking-answer sequences, creating a resource for training and evaluating
multimodal reasoning models. We capture the step-by-step reasoning process of
VLMs and the final descriptive answers. Our goal with this dataset is to enable
the development of more robust VLMs while contributing to the broader
understanding of multimodal reasoning mechanisms. The dataset and evaluation
benchmarks will be publicly available to aid research in reasoning/thinking
multimodal VLMs.

</details>


### [19] [NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems](https://arxiv.org/abs/2510.01608)
*Roman Jacome,Romario Gualdrón-Hurtado,Leon Suarez,Henry Arguello*

Main category: cs.CV

TL;DR: 本文提出了一种非线性投影零空间的新型正则化方法（NPN），有效改善成像逆问题的重建效果。


<details>
  <summary>Details</summary>
Motivation: 针对成像逆问题中的模糊性，设计了一种能够更好地利用感知过程信息的正则化方法。

Method: 提出了一种新的正则化方法（NPN），通过神经网络在感知矩阵的零空间的低维投影中促进解。

Result: NPN在不同的感知矩阵下，能够在多个成像逆问题中提高重建的清晰度。

Conclusion: NPN方法在多种成像逆问题中能够有效增强重建精度，并具有良好的解释性和灵活性。

Abstract: Imaging inverse problems aims to recover high-dimensional signals from
undersampled, noisy measurements, a fundamentally ill-posed task with infinite
solutions in the null-space of the sensing operator. To resolve this ambiguity,
prior information is typically incorporated through handcrafted regularizers or
learned models that constrain the solution space. However, these priors
typically ignore the task-specific structure of that null-space. In this work,
we propose \textit{Non-Linear Projections of the Null-Space} (NPN), a novel
class of regularization that, instead of enforcing structural constraints in
the image domain, promotes solutions that lie in a low-dimensional projection
of the sensing matrix's null-space with a neural network. Our approach has two
key advantages: (1) Interpretability: by focusing on the structure of the
null-space, we design sensing-matrix-specific priors that capture information
orthogonal to the signal components that are fundamentally blind to the sensing
process. (2) Flexibility: NPN is adaptable to various inverse problems,
compatible with existing reconstruction frameworks, and complementary to
conventional image-domain priors. We provide theoretical guarantees on
convergence and reconstruction accuracy when used within plug-and-play methods.
Empirical results across diverse sensing matrices demonstrate that NPN priors
consistently enhance reconstruction fidelity in various imaging inverse
problems, such as compressive sensing, deblurring, super-resolution, computed
tomography, and magnetic resonance imaging, with plug-and-play methods,
unrolling networks, deep image prior, and diffusion models.

</details>


### [20] [Automated Genomic Interpretation via Concept Bottleneck Models for Medical Robotics](https://arxiv.org/abs/2510.01618)
*Zijun Li,Jinchang Zhang,Ming Zhang,Guoyu Lu*

Main category: cs.CV

TL;DR: 本研究提出了一种新的自动化基因组解释模块，通过结合多种技术实现高效、可解释的分类和决策，推动基因组医学的自动化。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于将原始DNA序列转化为适合医学自动化和机器人系统的可操作和可解释的决策。

Method: 该论文提出了一种结合混沌游戏表示和概念瓶颈模型的自动化基因组解释模块，通过生物学上有意义的概念进行预测，并采用多种方法增强模块可靠性。

Result: 该模块在HIV亚型的分类中表现出色，能够提供可直接根据生物学先验验证的解释性证据，且显著提高了成本效益。

Conclusion: 该研究为基因组医学中的机器人和临床自动化建立了可靠的基础。

Abstract: We propose an automated genomic interpretation module that transforms raw DNA
sequences into actionable, interpretable decisions suitable for integration
into medical automation and robotic systems. Our framework combines Chaos Game
Representation (CGR) with a Concept Bottleneck Model (CBM), enforcing
predictions to flow through biologically meaningful concepts such as GC
content, CpG density, and k mer motifs. To enhance reliability, we incorporate
concept fidelity supervision, prior consistency alignment, KL distribution
matching, and uncertainty calibration. Beyond accurate classification of HIV
subtypes across both in-house and LANL datasets, our module delivers
interpretable evidence that can be directly validated against biological
priors. A cost aware recommendation layer further translates predictive outputs
into decision policies that balance accuracy, calibration, and clinical
utility, reducing unnecessary retests and improving efficiency. Extensive
experiments demonstrate that the proposed system achieves state of the art
classification performance, superior concept prediction fidelity, and more
favorable cost benefit trade-offs compared to existing baselines. By bridging
the gap between interpretable genomic modeling and automated decision-making,
this work establishes a reliable foundation for robotic and clinical automation
in genomic medicine.

</details>


### [21] [VLA-R1: Enhancing Reasoning in Vision-Language-Action Models](https://arxiv.org/abs/2510.01623)
*Angen Ye,Zeyu Zhang,Boyuan Wang,Xiaofeng Wang,Dapeng Zhang,Zheng Zhu*

Main category: cs.CV

TL;DR: VLA-R1通过强化学习和新数据集显著提高了视觉-语言-动作模型的性能和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型缺乏明确的逐步推理能力，未能充分考虑环境约束和几何关系。

Method: 采用强化学习和群体相对策略优化，结合验证奖励的策略进行后期训练。

Result: 在多个平台上进行的广泛评估表明，VLA-R1在推理稳健性和执行准确性方面显著提升。

Conclusion: VLA-R1在通用性和现实世界表现方面优于之前的VLA方法。

Abstract: Vision-Language-Action (VLA) models aim to unify perception, language
understanding, and action generation, offering strong cross-task and
cross-scene generalization with broad impact on embodied AI. However, current
VLA models often lack explicit step-by-step reasoning, instead emitting final
actions without considering affordance constraints or geometric relations.
Their post-training pipelines also rarely reinforce reasoning quality, relying
primarily on supervised fine-tuning with weak reward design. To address these
challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates
Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative
Policy Optimization (GRPO) to systematically optimize both reasoning and
execution. Specifically, we design an RLVR-based post-training strategy with
verifiable rewards for region alignment, trajectory consistency, and output
formatting, thereby strengthening reasoning robustness and execution accuracy.
Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides
chain-of-thought supervision explicitly aligned with affordance and trajectory
annotations. Furthermore, extensive evaluations on in-domain, out-of-domain,
simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior
generalization and real-world performance compared to prior VLA methods. We
plan to release the model, code, and dataset following the publication of this
work. Code: https://github.com/GigaAI-research/VLA-R1. Website:
https://gigaai-research.github.io/VLA-R1.

</details>


### [22] [Joint Deblurring and 3D Reconstruction for Macrophotography](https://arxiv.org/abs/2510.01640)
*Yifan Zhao,Liangchen Li,Yuqi Zhou,Kai Wang,Yan Liang,Juyong Zhang*

Main category: cs.CV

TL;DR: 提出一种利用少量多视角图像进行联合去模糊和3D重建的方法，解决微距摄影中的失焦模糊问题。


<details>
  <summary>Details</summary>
Motivation: 解决微距摄影中因失焦模糊造成的图像清晰度和3D重建质量问题。

Method: 采用一种联合去模糊和3D重建的方法，使用可微渲染技术进行自我监督优化。

Result: 实验表明，该方法在少量多视角图像下能实现高质量图像去模糊以及恢复高保真的3D外观。

Conclusion: 该方法能通过少量多视角图像实现高质量去模糊和高保真3D外观恢复。

Abstract: Macro lens has the advantages of high resolution and large magnification, and
3D modeling of small and detailed objects can provide richer information.
However, defocus blur in macrophotography is a long-standing problem that
heavily hinders the clear imaging of the captured objects and high-quality 3D
reconstruction of them. Traditional image deblurring methods require a large
number of images and annotations, and there is currently no multi-view 3D
reconstruction method for macrophotography. In this work, we propose a joint
deblurring and 3D reconstruction method for macrophotography. Starting from
multi-view blurry images captured, we jointly optimize the clear 3D model of
the object and the defocus blur kernel of each pixel. The entire framework
adopts a differentiable rendering method to self-supervise the optimization of
the 3D model and the defocus blur kernel. Extensive experiments show that from
a small number of multi-view images, our proposed method can not only achieve
high-quality image deblurring but also recover high-fidelity 3D appearance.

</details>


### [23] [FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring](https://arxiv.org/abs/2510.01641)
*Xiaoyang Liu,Zhengyan Zhou,Zihang Xu,Jiezhang Cao,Zheng Chen,Yulun Zhang*

Main category: cs.CV

TL;DR: FideDiff是一个高保真去模糊的单步扩散模型，通过增强模型性能的技术突破了现有扩散模型的局限，为图像恢复任务开辟了新方向。


<details>
  <summary>Details</summary>
Motivation: 虽然现有的大规模预训练扩散模型在图像恢复任务中表现出色，但推理时间长和保真度不足限制了其应用潜力。

Method: 通过将运动去模糊重构为一种扩散过程，并训练一致性模型以对齐所有时间步，FideDiff实现了一步去模糊。

Result: FideDiff在全参考指标上优于以往的扩散模型，并与其他先进模型的性能相当。

Conclusion: FideDiff模型在高保真图像去模糊任务中表现优越，为预训练扩散模型在实际工业应用中提供了新的发展方向。

Abstract: Recent advancements in image motion deblurring, driven by CNNs and
transformers, have made significant progress. Large-scale pre-trained diffusion
models, which are rich in true-world modeling, have shown great promise for
high-quality image restoration tasks such as deblurring, demonstrating stronger
generative capabilities than CNN and transformer-based methods. However,
challenges such as unbearable inference time and compromised fidelity still
limit the full potential of the diffusion models. To address this, we introduce
FideDiff, a novel single-step diffusion model designed for high-fidelity
deblurring. We reformulate motion deblurring as a diffusion-like process where
each timestep represents a progressively blurred image, and we train a
consistency model that aligns all timesteps to the same clean image. By
reconstructing training data with matched blur trajectories, the model learns
temporal consistency, enabling accurate one-step deblurring. We further enhance
model performance by integrating Kernel ControlNet for blur kernel estimation
and introducing adaptive timestep prediction. Our model achieves superior
performance on full-reference metrics, surpassing previous diffusion-based
methods and matching the performance of other state-of-the-art models. FideDiff
offers a new direction for applying pre-trained diffusion models to
high-fidelity image restoration tasks, establishing a robust baseline for
further advancing diffusion models in real-world industrial applications. Our
dataset and code will be available at https://github.com/xyLiu339/FideDiff.

</details>


### [24] [LadderMoE: Ladder-Side Mixture of Experts Adapters for Bronze Inscription Recognition](https://arxiv.org/abs/2510.01651)
*Rixin Zhou,Peiqiang Qiu,Qian Zhang,Chuntao Li,Xi Yang*

Main category: cs.CV

TL;DR: 本研究提出了一种新方法，利用大规模数据集和LadderMoE架构，大幅提升青铜铭文的自动识别准确性，解决了识别过程中的多重挑战。


<details>
  <summary>Details</summary>
Motivation: 青铜铭文是早期汉字书写的重要组成部分，但由于视觉退化、跨域变异和极长的字符分布，自动识别面临显著挑战。

Method: 构建一个包含22454幅全页图像和198598个标注字符的大规模青铜铭文数据集，开发一种两阶段的检测-识别管道，并配备LadderMoE以增强动态专家专业化和鲁棒性。

Result: 通过全面实验，方法在单字符和全页识别任务中显著提升了准确性，尤其在长尾类别和不同获取模态下。

Conclusion: 该研究提出的方法在青铜铭文识别方面显著优于现有的最先进场景文本识别基准，奠定了青铜铭文识别及后续考古分析的坚实基础。

Abstract: Bronze inscriptions (BI), engraved on ritual vessels, constitute a crucial
stage of early Chinese writing and provide indispensable evidence for
archaeological and historical studies. However, automatic BI recognition
remains difficult due to severe visual degradation, multi-domain variability
across photographs, rubbings, and tracings, and an extremely long-tailed
character distribution. To address these challenges, we curate a large-scale BI
dataset comprising 22454 full-page images and 198598 annotated characters
spanning 6658 unique categories, enabling robust cross-domain evaluation.
Building on this resource, we develop a two-stage detection-recognition
pipeline that first localizes inscriptions and then transcribes individual
characters. To handle heterogeneous domains and rare classes, we equip the
pipeline with LadderMoE, which augments a pretrained CLIP encoder with
ladder-style MoE adapters, enabling dynamic expert specialization and stronger
robustness. Comprehensive experiments on single-character and full-page
recognition tasks demonstrate that our method substantially outperforms
state-of-the-art scene text recognition baselines, achieving superior accuracy
across head, mid, and tail categories as well as all acquisition modalities.
These results establish a strong foundation for bronze inscription recognition
and downstream archaeological analysis.

</details>


### [25] [VirDA: Reusing Backbone for Unsupervised Domain Adaptation with Visual Reprogramming](https://arxiv.org/abs/2510.01660)
*Duy Nguyen,Dat Nguyen*

Main category: cs.CV

TL;DR: VirDA通过视觉重编程优化领域适应，减少参数量同时提高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督领域适应（UDA）方法在每对新的源和目标领域上都需要微调已训练的主干参数，这导致训练参数和存储内存的线性增长，并阻碍了已训练参数的重用。

Method: 该方法使用领域特定的视觉重编程层作为基础，优化视觉提示以最小化跨领域和域内分布差异，而无需改变主干参数。

Result: 在Office-31数据集上，VirDA达到了92.8%的平均准确率，仅使用了1.5M的可训练参数，且在多个基线测试中超越了现有最先进的方法。

Conclusion: VirDA通过引入领域特定的视觉重编程层，实现了在不同领域之间的有效适应，取得了优于现有方法的表现，同时大幅减少了可训练参数的数量。

Abstract: Existing UDA pipelines fine-tune already well-trained backbone parameters for
every new source-and-target pair, resulting in the number of training
parameters and storage memory growing linearly with each new pair, and also
preventing the reuse of these well-trained backbone parameters.
  Inspired by recent implications that existing backbones have textural biases,
we propose making use of domain-specific textural bias for domain adaptation
via visual reprogramming, namely VirDA.Instead of fine-tuning the full
backbone, VirDA prepends a domain-specific visual reprogramming layer to the
backbone. This layer produces visual prompts that act as an added textural bias
to the input image, adapting its ``style'' to a target domain. To optimize
these visual reprogramming layers, we use multiple objective functions that
optimize the intra- and inter-domain distribution differences when
domain-adapting visual prompts are applied. This process does not require
modifying the backbone parameters, allowing the same backbone to be reused
across different domains.
  We evaluate VirDA on Office-31 and obtain 92.8% mean accuracy with only 1.5M
trainable parameters. VirDA surpasses PDA, the state-of-the-art
parameter-efficient UDA baseline, by +1.6% accuracy while using just 46% of its
parameters. Compared with full-backbone fine-tuning, VirDA outperforms CDTrans
and FixBi by +0.2% and +1.4%, respectively, while requiring only 1.7% and 2.8%
of their trainable parameters. Relative to the strongest current methods
(PMTrans and TVT), VirDA uses ~1.7% of their parameters and trades off only
2.2% and 1.1% accuracy, respectively.

</details>


### [26] [Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery](https://arxiv.org/abs/2510.01662)
*Minh Tran,Maksim Siniukov,Zhangyu Jin,Mohammad Soleymani*

Main category: cs.CV

TL;DR: 本研究提出了一种新的离散面部编码方法DFE，超越FACS，在心理学任务中表现更佳，具有更广泛的面部显示覆盖和应用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统的面部表情编码系统如FACS受限于覆盖范围和人工标注成本，急需更经济有效的解决方案。

Method: 使用3D变形模型提取表情特征，通过残差向量量化变分自编码器(RVQ-VAE)编解码，生成离散面部表情编码。

Result: DFE在三个高级心理学任务中表现优越，超过基于FACS和其他强图像、视频学习模型的系统。

Conclusion: DFE比FACS等传统面部编码体系更准确，并在心理任务中表现优越，展示了其在心理和情感计算中的潜力。

Abstract: Facial expression analysis is central to understanding human behavior, yet
existing coding systems such as the Facial Action Coding System (FACS) are
constrained by limited coverage and costly manual annotation. In this work, we
introduce Discrete Facial Encoding (DFE), an unsupervised, data-driven
alternative of compact and interpretable dictionary of facial expressions from
3D mesh sequences learned through a Residual Vector Quantized Variational
Autoencoder (RVQ-VAE). Our approach first extracts identity-invariant
expression features from images using a 3D Morphable Model (3DMM), effectively
disentangling factors such as head pose and facial geometry. We then encode
these features using an RVQ-VAE, producing a sequence of discrete tokens from a
shared codebook, where each token captures a specific, reusable facial
deformation pattern that contributes to the overall expression. Through
extensive experiments, we demonstrate that Discrete Facial Encoding captures
more precise facial behaviors than FACS and other facial encoding alternatives.
We evaluate the utility of our representation across three high-level
psychological tasks: stress detection, personality prediction, and depression
detection. Using a simple Bag-of-Words model built on top of the learned
tokens, our system consistently outperforms both FACS-based pipelines and
strong image and video representation learning models such as Masked
Autoencoders. Further analysis reveals that our representation covers a wider
variety of facial displays, highlighting its potential as a scalable and
effective alternative to FACS for psychological and affective computing
applications.

</details>


### [27] [Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale](https://arxiv.org/abs/2510.01665)
*Yongbo Chen,Yanhao Zhang,Shaifali Parashar,Liang Zhao,Shoudong Huang*

Main category: cs.CV

TL;DR: 提出了一种新方法Con-NRSfM，用于在共形变形下进行非刚性结构从运动的点重建，显著提高了重建精度并解决了深度估计的限制。


<details>
  <summary>Details</summary>
Motivation: 解决单目视觉可变形同时定位与地图构建中的映射挑战，尤其是在不严格假设的情况下恢复局部共形尺度。

Method: 采用基于图的框架进行2D图像变形的点重建，并结合自监督学习的编码-解码网络生成稠密的3D点云。

Result: 在合成和真实数据集上的模拟和实验结果显示了我们的方法在重建精度和鲁棒性方面的优势。

Conclusion: Con-NRSfM方法在重建精度和鲁棒性方面优于现有方法，且代码将公开发布。

Abstract: Non-rigid structure-from-motion (NRSfM), a promising technique for addressing
the mapping challenges in monocular visual deformable simultaneous localization
and mapping (SLAM), has attracted growing attention. We introduce a novel
method, called Con-NRSfM, for NRSfM under conformal deformations, encompassing
isometric deformations as a subset. Our approach performs point-wise
reconstruction using 2D selected image warps optimized through a graph-based
framework. Unlike existing methods that rely on strict assumptions, such as
locally planar surfaces or locally linear deformations, and fail to recover the
conformal scale, our method eliminates these constraints and accurately
computes the local conformal scale. Additionally, our framework decouples
constraints on depth and conformal scale, which are inseparable in other
approaches, enabling more precise depth estimation. To address the sensitivity
of the formulated problem, we employ a parallel separable iterative
optimization strategy. Furthermore, a self-supervised learning framework,
utilizing an encoder-decoder network, is incorporated to generate dense 3D
point clouds with texture. Simulation and experimental results using both
synthetic and real datasets demonstrate that our method surpasses existing
approaches in terms of reconstruction accuracy and robustness. The code for the
proposed method will be made publicly available on the project website:
https://sites.google.com/view/con-nrsfm.

</details>


### [28] [UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction](https://arxiv.org/abs/2510.01669)
*Jin Cao,Hongrui Wu,Ziyong Feng,Hujun Bao,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: 本文提出了一种统一的鲁棒重建框架UniVerse，通过视频扩散模型处理多视角不一致图像，实现3D场景重建。


<details>
  <summary>Details</summary>
Motivation: 解决鲁棒重建过程中的图像不一致性问题，并简化优化过程。

Method: 通过将鲁棒重建分解为恢复和重建两个子任务，采用视频扩散模型来处理不一致的图像，最终重建3D场景。

Result: 在合成和真实数据集上的广泛实验验证了该方法的优越性和泛化能力。

Conclusion: UniVerse展示了强大的泛化能力和优越的鲁棒重建性能，并且能够控制重建3D场景的风格。

Abstract: This paper tackles the challenge of robust reconstruction, i.e., the task of
reconstructing a 3D scene from a set of inconsistent multi-view images. Some
recent works have attempted to simultaneously remove image inconsistencies and
perform reconstruction by integrating image degradation modeling into neural 3D
scene representations.However, these methods rely heavily on dense observations
for robustly optimizing model parameters.To address this issue, we propose to
decouple robust reconstruction into two subtasks: restoration and
reconstruction, which naturally simplifies the optimization process.To this
end, we introduce UniVerse, a unified framework for robust reconstruction based
on a video diffusion model. Specifically, UniVerse first converts inconsistent
images into initial videos, then uses a specially designed video diffusion
model to restore them into consistent images, and finally reconstructs the 3D
scenes from these restored images.Compared with case-by-case per-view
degradation modeling, the diffusion model learns a general scene prior from
large-scale data, making it applicable to diverse image
inconsistencies.Extensive experiments on both synthetic and real-world datasets
demonstrate the strong generalization capability and superior performance of
our method in robust reconstruction. Moreover, UniVerse can control the style
of the reconstructed 3D scene. Project page:
https://jin-cao-tma.github.io/UniVerse.github.io/

</details>


### [29] [An Efficient Deep Template Matching and In-Plane Pose Estimation Method via Template-Aware Dynamic Convolution](https://arxiv.org/abs/2510.01678)
*Ke Jia,Ji Zhou,Hanxin Li,Zhigan Zhou,Haojie Chu,Xiaojie Li*

Main category: cs.CV

TL;DR: 提出一种新颖的轻量级模板匹配框架，通过联合定位与几何回归，实现优越的匹配精度与速度，适合工业实时应用。


<details>
  <summary>Details</summary>
Motivation: 在工业检查和组件对齐任务中，传统模板匹配方法效率低且缺乏有效的几何姿态建模，深度学习方法也存在不足，因此需要一种高效的解决方案。

Method: 提出了一种轻量级端到端框架，通过结合模板匹配与几何回归，实现中心坐标、旋转角和缩放的输出，使用动态卷积模块和深度可分离卷积提高匹配效率。

Result: 实验表明，模型在复杂变换下达到了高精度，推理时间为14ms，具有强大的鲁棒性，特别是在小模板和多目标场景中表现优异。

Conclusion: 该方法在复杂背景下提供了高精度的目标定位和几何状态估计，适合实时工业应用。

Abstract: In industrial inspection and component alignment tasks, template matching
requires efficient estimation of a target's position and geometric state
(rotation and scaling) under complex backgrounds to support precise downstream
operations. Traditional methods rely on exhaustive enumeration of angles and
scales, leading to low efficiency under compound transformations. Meanwhile,
most deep learning-based approaches only estimate similarity scores without
explicitly modeling geometric pose, making them inadequate for real-world
deployment. To overcome these limitations, we propose a lightweight end-to-end
framework that reformulates template matching as joint localization and
geometric regression, outputting the center coordinates, rotation angle, and
independent horizontal and vertical scales. A Template-Aware Dynamic
Convolution Module (TDCM) dynamically injects template features at inference to
guide generalizable matching. The compact network integrates depthwise
separable convolutions and pixel shuffle for efficient matching. To enable
geometric-annotation-free training, we introduce a rotation-shear-based
augmentation strategy with structure-aware pseudo labels. A lightweight
refinement module further improves angle and scale precision via local
optimization. Experiments show our 3.07M model achieves high precision and 14ms
inference under compound transformations. It also demonstrates strong
robustness in small-template and multi-object scenarios, making it highly
suitable for deployment in real-time industrial applications. The code is
available at:https://github.com/ZhouJ6610/PoseMatch-TDCM.

</details>


### [30] [Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning](https://arxiv.org/abs/2510.01681)
*Xuchen Li,Xuzhao Li,Jiahui Gao,Renjie Pi,Shiyu Hu,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种自适应像素推理框架，通过动态决定像素操作来提高视觉语言模型的性能，并在多模态推理基准上显示出了优越性。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉语言模型（VLM）在处理细粒度视觉元素时的挑战，强调像素级信息在推理过程中的重要性。

Method: 提出了一种自适应像素推理框架，动态决定必要的像素级操作，采用操作感知的有监督微调和基于模型自身反馈的强化学习框架。

Result: 我们的模型在众多多模态推理基准上取得了优秀的性能，同时显著减少了不必要的视觉操作。

Conclusion: 我们的模型在HR-Bench 4K上取得了73.4%的准确率，同时工具使用比例仅为20.1%，与之前的方法相比，准确率提高且工具使用减少66.5%。

Abstract: Vision-Language Models (VLMs) excel at many multimodal tasks, yet they
frequently struggle with tasks requiring precise understanding and handling of
fine-grained visual elements. This is mainly due to information loss during
image encoding or insufficient attention to critical regions. Recent work has
shown promise by incorporating pixel-level visual information into the
reasoning process, enabling VLMs to access high-resolution visual details
during their thought process. However, this pixel-level information is often
overused, leading to inefficiency and distraction from irrelevant visual
details. To address these challenges, we propose the first framework for
adaptive pixel reasoning that dynamically determines necessary pixel-level
operations based on the input query. Specifically, we first apply
operation-aware supervised fine-tuning to establish baseline competence in
textual reasoning and visual operations, then design a novel rollout-guided
reinforcement learning framework relying on feedback of the model's own
responses, which enables the VLM to determine when pixel operations should be
invoked based on query difficulty. Experiments on extensive multimodal
reasoning benchmarks show that our model achieves superior performance while
significantly reducing unnecessary visual operations. Impressively, our model
achieves 73.4\% accuracy on HR-Bench 4K while maintaining a tool usage ratio of
only 20.1\%, improving accuracy and simultaneously reducing tool usage by
66.5\% compared to the previous methods.

</details>


### [31] [Uncovering Overconfident Failures in CXR Models via Augmentation-Sensitivity Risk Scoring](https://arxiv.org/abs/2510.01683)
*Han-Jay Shu,Wei-Ning Chiu,Shun-Ting Chang,Meng-Ping Huang,Takeshi Tohyama,Ahram Han,Po-Chih Kuo*

Main category: cs.CV

TL;DR: 为了解决胸部放射图像分析中的公平性和可靠性问题，本文提出了一种新的框架（ASRS），该框架通过旋转增强来识别易出错案例，改善了医疗AI的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习模型在胸部放射图像解释中表现良好，但对不同患者子组的准确性不均衡引发了对公平性和可靠性的担忧。现有的错误检测方法难以有效处理微妙的分布内错误。

Method: 提出了增强敏感性风险评分（ASRS）框架，通过临床可能的旋转来识别错误风险，并测量嵌入偏移。

Result: ASRS通过敏感性评分将样本分为稳定性四分位数，发现高敏感病例的召回率显著下降，但AUROC和置信度较高。

Conclusion: ASRS框架为识别易出错的胸部放射图像病例提供了一种无标签的方法，改善了医疗AI的公平性和安全性。

Abstract: Deep learning models achieve strong performance in chest radiograph (CXR)
interpretation, yet fairness and reliability concerns persist. Models often
show uneven accuracy across patient subgroups, leading to hidden failures not
reflected in aggregate metrics. Existing error detection approaches -- based on
confidence calibration or out-of-distribution (OOD) detection -- struggle with
subtle within-distribution errors, while image- and representation-level
consistency-based methods remain underexplored in medical imaging. We propose
an augmentation-sensitivity risk scoring (ASRS) framework to identify
error-prone CXR cases. ASRS applies clinically plausible rotations ($\pm
15^\circ$/$\pm 30^\circ$) and measures embedding shifts with the RAD-DINO
encoder. Sensitivity scores stratify samples into stability quartiles, where
highly sensitive cases show substantially lower recall ($-0.2$ to $-0.3$)
despite high AUROC and confidence. ASRS provides a label-free means for
selective prediction and clinician review, improving fairness and safety in
medical AI.

</details>


### [32] [FreeViS: Training-free Video Stylization with Inconsistent References](https://arxiv.org/abs/2510.01686)
*Jiacong Xu,Yiqun Mei,Ke Zhang,Vishal M. Patel*

Main category: cs.CV

TL;DR: FreeViS是一种训练免费的高效视频风格化框架，可以生成风格丰富且时间一致的高质量视频。


<details>
  <summary>Details</summary>
Motivation: 视频风格化对内容创作至关重要，但传统方法在时间一致性和风格丰富度方面存在挑战。

Method: 一个无训练的视频风格化框架，整合多个风格化参考和预训练的图像到视频(I2V)模型。

Result: FreeViS在风格化保真度和时间一致性方面优于最近的基线，并获得了强烈的人类偏好。

Conclusion: FreeViS提供了一种实用经济的高质量、具有时间一致性的视频风格化解决方案。

Abstract: Video stylization plays a key role in content creation, but it remains a
challenging problem. Na\"ively applying image stylization frame-by-frame hurts
temporal consistency and reduces style richness. Alternatively, training a
dedicated video stylization model typically requires paired video data and is
computationally expensive. In this paper, we propose FreeViS, a training-free
video stylization framework that generates stylized videos with rich style
details and strong temporal coherence. Our method integrates multiple stylized
references to a pretrained image-to-video (I2V) model, effectively mitigating
the propagation errors observed in prior works, without introducing flickers
and stutters. In addition, it leverages high-frequency compensation to
constrain the content layout and motion, together with flow-based motion cues
to preserve style textures in low-saliency regions. Through extensive
evaluations, FreeViS delivers higher stylization fidelity and superior temporal
consistency, outperforming recent baselines and achieving strong human
preference. Our training-free pipeline offers a practical and economic solution
for high-quality, temporally coherent video stylization. The code and videos
can be accessed via https://xujiacong.github.io/FreeViS/

</details>


### [33] [MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment Abilities in MLLMs](https://arxiv.org/abs/2510.01691)
*Jiyao Liu,Jinjie Wei,Wanying Qu,Chenglong Ma,Junzhi Ning,Yunheng Li,Ying Chen,Xinzhe Luo,Pengcheng Chen,Xin Gao,Ming Hu,Huihui Xu,Xin Wang,Shujian Gao,Dingkang Yang,Zhongying Deng,Jin Ye,Lihao Liu,Junjun He,Ningsheng Xu*

Main category: cs.CV

TL;DR: 本文介绍了MedQ-Bench，一个用于医学图像质量评估的基准，旨在通过多模态大型语言模型提升医学图像的感知和推理能力，但现有模型在稳定性和准确性上仍需优化。


<details>
  <summary>Details</summary>
Motivation: 现有的医学图像质量评估方法主要依赖基于标量的评分指标，无法反映专家评估中的描述性和类人推理过程。

Method: 通过建立一个包含感知和推理任务的全面基准，使用多模态大型语言模型评估医学图像质量。

Result: 评估显示14个最新多模态大型语言模型在医学图像质量评估中的表现不足，具体表现为在感知和推理能力上的不稳定性与不够准确。

Conclusion: 现有的多模态大型语言模型在医学图像质量评估中表现出初步但不稳定的感知和推理能力，需针对性优化以确保临床使用的可靠性。

Abstract: Medical Image Quality Assessment (IQA) serves as the first-mile safety gate
for clinical AI, yet existing approaches remain constrained by scalar,
score-based metrics and fail to reflect the descriptive, human-like reasoning
process central to expert evaluation. To address this gap, we introduce
MedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning
paradigm for language-based evaluation of medical image quality with
Multi-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary
tasks: (1) MedQ-Perception, which probes low-level perceptual capability via
human-curated questions on fundamental visual attributes; and (2)
MedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks,
aligning model evaluation with human-like reasoning on image quality. The
benchmark spans five imaging modalities and over forty quality attributes,
totaling 2,600 perceptual queries and 708 reasoning assessments, covering
diverse image sources including authentic clinical acquisitions, images with
simulated degradations via physics-based reconstructions, and AI-generated
images. To evaluate reasoning ability, we propose a multi-dimensional judging
protocol that assesses model outputs along four complementary axes. We further
conduct rigorous human-AI alignment validation by comparing LLM-based judgement
with radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates
that models exhibit preliminary but unstable perceptual and reasoning skills,
with insufficient accuracy for reliable clinical use. These findings highlight
the need for targeted optimization of MLLMs in medical IQA. We hope that
MedQ-Bench will catalyze further exploration and unlock the untapped potential
of MLLMs for medical image quality evaluation.

</details>


### [34] [Holistic Order Prediction in Natural Scenes](https://arxiv.org/abs/2510.01704)
*Pierre Musacchio,Hyunmin Lee,Jaesik Park*

Main category: cs.CV

TL;DR: InstaFormer通过单次前向传播，实现了图像场景中实例的全面遮挡与深度排序，降低了模型推理的时间和成本。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型在实例几何理解方面面临挑战，且依赖昂贵的输入格式和推理成本。

Method: 提出了一种名为InstaFormer的网络，通过对象查询与潜在掩膜描述符的交互，进行全局顺序预测。

Result: InstaFormer仅使用输入RGB图像即可在一次前向传播中返回场景中所有实例的完整遮挡和深度排序。

Conclusion: InstaFormer有效地实现了全场景的遮挡和深度排序，显著降低了推理成本。

Abstract: Even in controlled settings, understanding instance-wise geometries is a
challenging task for a wide range of visual models. Although specialized
systems exist, modern arts rely on expensive input formats (category labels,
binary segmentation masks) and inference costs (a quadratic amount of forward
passes). We mitigate these limitations by proposing InstaFormer, a network
capable of holistic order prediction. That is, solely given an input RGB image,
InstaFormer returns the full occlusion and depth orderings for all the
instances in the scene in a single forward pass. At its core, InstaFormer
relies on interactions between object queries and latent mask descriptors that
semantically represent the same objects while carrying complementary
information. We comprehensively benchmark and ablate our approach to highlight
its effectiveness. Our code and models are open-source and available at this
URL: https://github.com/SNU-VGILab/InstaOrder.

</details>


### [35] [PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal Positional Encoding and Reinforcement Learning](https://arxiv.org/abs/2510.01715)
*Raahul Krishna Durairaju,K. Saruladha*

Main category: cs.CV

TL;DR: PyramidStyler是一种新的变换器框架，通过金字塔位置编码和强化学习，实现高效的艺术图像合成，显著降低内容和风格损失，同时支持实时处理。


<details>
  <summary>Details</summary>
Motivation: 现有的CNN和变换器模型在复杂风格和高分辨率输入上效率不高，因此需要一种新的方法来提升风格转移的质量和速度。

Method: 采用一种新的变换器框架PyramidStyler，结合了金字塔位置编码和强化学习，优化风格化过程。

Result: PyramidStyler在4000个训练周期中将内容损失降低了62.6%，风格损失降低了57.4%，推断时间为1.39秒，使用RL后，内容损失与风格损失进一步降低，同时保持了最小的速度惩罚。

Conclusion: PyramidStyler展示了在艺术图像合成中的高效性与实时性，适用于广泛的媒体和设计应用。

Abstract: Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based
algorithm, enabling AI-driven artistic image synthesis. However, existing CNN
and transformer-based models struggle to scale efficiently to complex styles
and high-resolution inputs. We introduce PyramidStyler, a transformer framework
with Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding
that captures both local details and global context while reducing
computational load. We further incorporate reinforcement learning to
dynamically optimize stylization, accelerating convergence. Trained on
Microsoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to
2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s
inference--and yields further improvements (content 2.03; style 0.75) with
minimal speed penalty (1.40 s) when using RL. These results demonstrate
real-time, high-quality artistic rendering, with broad applications in media
and design.

</details>


### [36] [LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for Large-Scale Scene Reconstruction](https://arxiv.org/abs/2510.01767)
*Sheng-Hsiang Hung,Ting-Yu Yen,Wei-Fang Sun,Simon See,Shih-Hsuan Hung,Hung-Kuo Chu*

Main category: cs.CV

TL;DR: LoBE-GS是一种新的3D Gaussian Splatting框架，旨在解决大规模场景重建中的效率和负载不平衡问题，显著提高训练效率，保持重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D Gaussian Splatting在处理大规模场景时的效率低下和负载不平衡问题。

Method: 提出了基于深度感知的分区方法、负载平衡优化策略，以及轻量级的可见度裁剪和选择性稠密化技术。

Result: LoBE-GS实现了比现有基准快2倍的训练时间，同时具备更好的可扩展性。

Conclusion: LoBE-GS显著提高了大规模3D场景重建的效率，实现了更快的训练时间，同时保持了重建质量。

Abstract: 3D Gaussian Splatting (3DGS) has established itself as an efficient
representation for real-time, high-fidelity 3D scene reconstruction. However,
scaling 3DGS to large and unbounded scenes such as city blocks remains
difficult. Existing divide-and-conquer methods alleviate memory pressure by
partitioning the scene into blocks, but introduce new bottlenecks: (i)
partitions suffer from severe load imbalance since uniform or heuristic splits
do not reflect actual computational demands, and (ii) coarse-to-fine pipelines
fail to exploit the coarse stage efficiently, often reloading the entire model
and incurring high overhead. In this work, we introduce LoBE-GS, a novel
Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers
the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning
method that reduces preprocessing from hours to minutes, an optimization-based
strategy that balances visible Gaussians -- a strong proxy for computational
load -- across blocks, and two lightweight techniques, visibility cropping and
selective densification, to further reduce training cost. Evaluations on
large-scale urban and outdoor datasets show that LoBE-GS consistently achieves
up to $2\times$ faster end-to-end training time than state-of-the-art
baselines, while maintaining reconstruction quality and enabling scalability to
scenes infeasible with vanilla 3DGS.

</details>


### [37] [Pack and Force Your Memory: Long-form and Consistent Video Generation](https://arxiv.org/abs/2510.01784)
*Xiaofei Wu,Guozhen Zhang,Zhiyong Xu,Yuan Zhou,Qinglin Lu,Xuming He*

Main category: cs.CV

TL;DR: 本文提出MemoryPack和Direct Forcing以解决长视频生成中的长范围依赖和错误累积问题，显著提升了生成效果。


<details>
  <summary>Details</summary>
Motivation: 长视频生成需要捕捉长范围依赖关系并防止自回归解码中的错误累积。

Method: 提出了MemoryPack，一种可学习的上下文检索机制，以及Direct Forcing，一种高效的单步逼近策略。

Result: 通过MemoryPack和Direct Forcing，模型实现了分钟级的时间一致性，改善了训练-推断的一致性，并降低了推断过程中的错误传播。

Conclusion: MemoryPack和Direct Forcing显著提高了长视频生成的上下文一致性和可靠性，从而增强了自回归视频模型的实际可用性。

Abstract: Long-form video generation presents a dual challenge: models must capture
long-range dependencies while preventing the error accumulation inherent in
autoregressive decoding. To address these challenges, we make two
contributions. First, for dynamic context modeling, we propose MemoryPack, a
learnable context-retrieval mechanism that leverages both textual and image
information as global guidance to jointly model short- and long-term
dependencies, achieving minute-level temporal consistency. This design scales
gracefully with video length, preserves computational efficiency, and maintains
linear complexity. Second, to mitigate error accumulation, we introduce Direct
Forcing, an efficient single-step approximating strategy that improves
training-inference alignment and thereby curtails error propagation during
inference. Together, MemoryPack and Direct Forcing substantially enhance the
context consistency and reliability of long-form video generation, advancing
the practical usability of autoregressive video models.

</details>


### [38] [Calibrating the Full Predictive Class Distribution of 3D Object Detectors for Autonomous Driving](https://arxiv.org/abs/2510.01829)
*Cornelius Schröder,Marius-Raphael Schlüter,Markus Lienkamp*

Main category: cs.CV

TL;DR: 本研究重点提高3D物体检测器的置信度校准，提出了辅助正则化损失项以改善主要和次要类别预测的校准，测试表明效果显著。


<details>
  <summary>Details</summary>
Motivation: 在自主系统中，精确的物体检测和不确定性估计至关重要，因此需要提高3D对象检测器的置信度校准。

Method: 引入两种辅助正则化损失项，以训练主要预测或全预测向量的校准目标，并评估不同的后处理和训练时方法。

Result: 通过结合我们提出的损失项和等歧归回方法，CenterPoint和PillarNet在主要和次要类别的校准表现最佳；而DSVT-Pillar无法使用相同方法联合校准。

Conclusion: 提出的辅助正则化损失项能够有效改善3D物体检测器的信心校准，尤其是在对主要和次要类别的预测上。

Abstract: In autonomous systems, precise object detection and uncertainty estimation
are critical for self-aware and safe operation. This work addresses confidence
calibration for the classification task of 3D object detectors. We argue that
it is necessary to regard the calibration of the full predictive confidence
distribution over all classes and deduce a metric which captures the
calibration of dominant and secondary class predictions. We propose two
auxiliary regularizing loss terms which introduce either calibration of the
dominant prediction or the full prediction vector as a training goal. We
evaluate a range of post-hoc and train-time methods for CenterPoint, PillarNet
and DSVT-Pillar and find that combining our loss term, which regularizes for
calibration of the full class prediction, and isotonic regression lead to the
best calibration of CenterPoint and PillarNet with respect to both dominant and
secondary class predictions. We further find that DSVT-Pillar can not be
jointly calibrated for dominant and secondary predictions using the same
method.

</details>


### [39] [Leveraging Prior Knowledge of Diffusion Model for Person Search](https://arxiv.org/abs/2510.01841)
*Giyeol Kim,Sooyoung Yang,Jihyong Oh,Myungjoo Kang,Chanho Eom*

Main category: cs.CV

TL;DR: 本文提出DiffPS框架，利用扩散模型解决人脸搜索中的优化冲突，模块化设计显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法利用ImageNet预训练的骨干网，它们在捕捉复杂空间上下文和细粒度身份线索方面可能不够理想，同时采用共享骨干特征导致检测和重识别之间的优化目标冲突。

Method: 提出DiffPS框架，利用预训练的扩散模型，通过三个专用模块改进定位和特征聚合，消除子任务之间的优化冲突。

Result: DiffPS通过引入扩散先验知识和专门模块，显著提高了人脸搜索的性能。

Conclusion: DiffPS在CUHK-SYSU和PRW数据集上设置了新的最先进水平。

Abstract: Person search aims to jointly perform person detection and re-identification
by localizing and identifying a query person within a gallery of uncropped
scene images. Existing methods predominantly utilize ImageNet pre-trained
backbones, which may be suboptimal for capturing the complex spatial context
and fine-grained identity cues necessary for person search. Moreover, they rely
on a shared backbone feature for both person detection and re-identification,
leading to suboptimal features due to conflicting optimization objectives. In
this paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a
novel framework that leverages a pre-trained diffusion model while eliminating
the optimization conflict between two sub-tasks. We analyze key properties of
diffusion priors and propose three specialized modules: (i) Diffusion-Guided
Region Proposal Network (DGRPN) for enhanced person localization, (ii)
Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and
(iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage
text-aligned diffusion features. DiffPS sets a new state-of-the-art on
CUHK-SYSU and PRW.

</details>


### [40] [Flow-Matching Guided Deep Unfolding for Hyperspectral Image Reconstruction](https://arxiv.org/abs/2510.01912)
*Yi Ai,Yuanhao Cai,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: 提出了一种新型的高光谱成像重建网络FMU，结合流匹配和深度展开，提高了重建的准确性和细节保留。


<details>
  <summary>Details</summary>
Motivation: 针对现有压缩成像系统在重建准确性和细节保留上的挑战，旨在提高高光谱成像的重建效率和质量。

Method: 提出了Flow-Matching-guided Unfolding网络，结合了流匹配与深度展开框架，采用均值速度损失增强流的全局一致性。

Result: FMU在模拟和真实数据集上进行了广泛实验，展示了优越的重建效果。

Conclusion: FMU在重建质量上显著优于现有方法。

Abstract: Hyperspectral imaging (HSI) provides rich spatial-spectral information but
remains costly to acquire due to hardware limitations and the difficulty of
reconstructing three-dimensional data from compressed measurements. Although
compressive sensing systems such as CASSI improve efficiency, accurate
reconstruction is still challenged by severe degradation and loss of fine
spectral details. We propose the Flow-Matching-guided Unfolding network (FMU),
which, to our knowledge, is the first to integrate flow matching into HSI
reconstruction by embedding its generative prior within a deep unfolding
framework. To further strengthen the learned dynamics, we introduce a mean
velocity loss that enforces global consistency of the flow, leading to a more
robust and accurate reconstruction. This hybrid design leverages the
interpretability of optimization-based methods and the generative capacity of
flow matching. Extensive experiments on both simulated and real datasets show
that FMU significantly outperforms existing approaches in reconstruction
quality. Code and models will be available at https://github.com/YiAi03/FMU.

</details>


### [41] [Automated Defect Detection for Mass-Produced Electronic Components Based on YOLO Object Detection Models](https://arxiv.org/abs/2510.01914)
*Wei-Lung Mao,Chun-Chi Wang,Po-Heng Chou,Yen-Ting Liu*

Main category: cs.CV

TL;DR: 本论文提出了一种基于深度学习的自动化DIP缺陷检测系统，通过ConSinGAN生成数据，使用YOLOv7实现高准确率和快速检测。


<details>
  <summary>Details</summary>
Motivation: 解决传统工业组件缺陷检测的时间和人力成本高的问题，提高产品质量管理的效率.

Method: 使用数字相机光学和基于深度学习的模型进行DIP缺陷检测，运用ConSinGAN生成训练和测试数据集，并调查多种YOLO模型.

Result: YOLOv7结合ConSinGAN的检测准确性达到95.50%，检测时间为285毫秒，明显优于其他方法.

Conclusion: 提出的自动缺陷检测系统在检测准确性和时间上具有显著优势，且可以适用于多种缺陷类型和不足的缺陷数据.

Abstract: Since the defect detection of conventional industry components is
time-consuming and labor-intensive, it leads to a significant burden on quality
inspection personnel and makes it difficult to manage product quality. In this
paper, we propose an automated defect detection system for the dual in-line
package (DIP) that is widely used in industry, using digital camera optics and
a deep learning (DL)-based model. The two most common defect categories of DIP
are examined: (1) surface defects, and (2) pin-leg defects. However, the lack
of defective component images leads to a challenge for detection tasks. To
solve this problem, the ConSinGAN is used to generate a suitable-sized dataset
for training and testing. Four varieties of the YOLO model are investigated
(v3, v4, v7, and v9), both in isolation and with the ConSinGAN augmentation.
The proposed YOLOv7 with ConSinGAN is superior to the other YOLO versions in
accuracy of 95.50\%, detection time of 285 ms, and is far superior to
threshold-based approaches. In addition, the supervisory control and data
acquisition (SCADA) system is developed, and the associated sensor architecture
is described. The proposed automated defect detection can be easily established
with numerous types of defects or insufficient defect data.

</details>


### [42] [Foundation Visual Encoders Are Secretly Few-Shot Anomaly Detectors](https://arxiv.org/abs/2510.01934)
*Guangyao Zhai,Yue Zhou,Xinyan Deng,Lars Heckler,Nassir Navab,Benjamin Busam*

Main category: cs.CV

TL;DR: 该研究提出FoundAD，通过非线性投影进行少样本异常检测，提升了检测性能并减少了参数使用。


<details>
  <summary>Details</summary>
Motivation: 解决少样本条件下，正常与异常特征之间准确区分的挑战，尤其是在类别无关的情况下。

Method: 通过学习非线性投影算子来映射到自然图像流形，从而实现异常检测。

Result: 该方法在多类异常检测中表现优秀，且参数使用显著少于之前的方法。

Conclusion: 该研究提出的FoundAD方法有效提升了少样本异常检测的性能，能够在多类检测中竞争并显著减少参数使用。

Abstract: Few-shot anomaly detection streamlines and simplifies industrial safety
inspection. However, limited samples make accurate differentiation between
normal and abnormal features challenging, and even more so under
category-agnostic conditions. Large-scale pre-training of foundation visual
encoders has advanced many fields, as the enormous quantity of data helps to
learn the general distribution of normal images. We observe that the anomaly
amount in an image directly correlates with the difference in the learnt
embeddings and utilize this to design a few-shot anomaly detector termed
FoundAD. This is done by learning a nonlinear projection operator onto the
natural image manifold. The simple operator acts as an effective tool for
anomaly detection to characterize and identify out-of-distribution regions in
an image. Extensive experiments show that our approach supports multi-class
detection and achieves competitive performance while using substantially fewer
parameters than prior methods. Backed up by evaluations with multiple
foundation encoders, including fresh DINOv3, we believe this idea broadens the
perspective on foundation features and advances the field of few-shot anomaly
detection.

</details>


### [43] [ClustViT: Clustering-based Token Merging for Semantic Segmentation](https://arxiv.org/abs/2510.01948)
*Fabio Montello,Ronja Güldenring,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: ClustViT通过集群和再生模块有效提升了Vision Transformer在语义分割任务中的表现，减少了计算复杂度，并加快了推理速度。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在分类任务中表现良好，但因其二次注意力复杂度在实际机器人系统中应用受限，特别是在稠密预测任务中。

Method: 在Vision Transformer架构上，使用可训练的集群模块和再生模块进行语义分割。

Result: 在三个不同数据集上，ClustViT实现了最多2.18倍更少的GFLOPs和1.64倍更快的推理速度，同时保持了可比的分割精度。

Conclusion: ClustViT通过集群模块和再生模块有效减少了GFLOPs和推理时间，同时保持了分割精度。

Abstract: Vision Transformers can achieve high accuracy and strong generalization
across various contexts, but their practical applicability on real-world
robotic systems is limited due to their quadratic attention complexity. Recent
works have focused on dynamically merging tokens according to the image
complexity. Token merging works well for classification but is less suited to
dense prediction. We propose ClustViT, where we expand upon the Vision
Transformer (ViT) backbone and address semantic segmentation. Within our
architecture, a trainable Cluster module merges similar tokens along the
network guided by pseudo-clusters from segmentation masks. Subsequently, a
Regenerator module restores fine details for downstream heads. Our approach
achieves up to 2.18x fewer GFLOPs and 1.64x faster inference on three different
datasets, with comparable segmentation accuracy. Our code and models will be
made publicly available.

</details>


### [44] [Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs](https://arxiv.org/abs/2510.01954)
*Yongyi Su,Haojie Zhang,Shijie Li,Nanqing Liu,Jingyi Liao,Junyi Pan,Yuan Liu,Xiaofen Xing,Chong Sun,Chen Li,Nancy F. Chen,Shuicheng Yan,Xulei Yang,Xun Xu*

Main category: cs.CV

TL;DR: 本文提出了Patch-as-Decodable Token (PaDT) 方法，通过直接生成文本和视觉输出，显著提升多模态大型语言模型在视觉任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有视觉任务方法依赖间接表示的问题，改善密集预测任务（如分割）的性能。

Method: 使用视觉参考标记（VRTs）和轻量解码器，将MLLM的输出直接转换为检测、分割和定位预测，同时采用动态扩展嵌入表的方法进行处理。

Result: 在四个视觉感知和理解任务中的实证研究表明，PaDT的性能优于先前的方法，并且在与更大模型的对比中，仍能保持先进水平。

Conclusion: Patch-as-Decodable Token (PaDT) 提供了一种新颖的方法，使多模态大型语言模型 (MLLMs) 能够直接生成文本和多样的视觉输出，提升了定位和区分相似物体的能力，在多个任务上取得了优异的表现。

Abstract: Multimodal large language models (MLLMs) have advanced rapidly in recent
years. However, existing approaches for vision tasks often rely on indirect
representations, such as generating coordinates as text for detection, which
limits performance and prevents dense prediction tasks like segmentation. To
overcome these challenges, we introduce Patch-as-Decodable Token (PaDT), a
unified paradigm that enables MLLMs to directly generate both textual and
diverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),
derived from visual patch embeddings of query images and interleaved seamlessly
with LLM's output textual tokens. A lightweight decoder then transforms LLM's
outputs into detection, segmentation, and grounding predictions. Unlike prior
methods, PaDT processes VRTs independently at each forward pass and dynamically
expands the embedding table, thus improving localization and differentiation
among similar objects. We further tailor a training strategy for PaDT by
randomly selecting VRTs for supervised fine-tuning and introducing a robust
per-token cross-entropy loss. Our empirical studies across four visual
perception and understanding tasks suggest PaDT consistently achieving
state-of-the-art performance, even compared with significantly larger MLLM
models. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.

</details>


### [45] [TriAlignXA: An Explainable Trilemma Alignment Framework for Trustworthy Agri-product Grading](https://arxiv.org/abs/2510.01990)
*Jianfei Xie,Ziyang Li*

Main category: cs.CV

TL;DR: 本研究提出以'TriAlignXA'框架为核心的解决方案，用于提升在线农业产品交易的信任度，并为算法透明决策与消费者信任建立通道。


<details>
  <summary>Details</summary>
Motivation: 探讨在线水果和蔬菜电子商务中的信任缺失问题，尤其是数字交易无法直接感知产品质量的局限性。

Method: 通过构建'信任金字塔'模型和进行实验证实，提出'三角信任指数'（TTI）和可解释AI框架TriAlignXA，以实现多目标优化和信任构建。

Result: 实验证明，所提出的TriAlignXA框架在农产品分级任务上显著提高了准确性，并能够有效平衡农业产品分级中的'不可能三角'问题。

Conclusion: 本研究为构建可信赖的在线农业产品生态系统提供了理论与实践的全面支持，明确了算法决策与消费者信任之间的关键路径。

Abstract: The 'trust deficit' in online fruit and vegetable e-commerce stems from the
inability of digital transactions to provide direct sensory perception of
product quality. This paper constructs a 'Trust Pyramid' model through
'dual-source verification' of consumer trust. Experiments confirm that quality
is the cornerstone of trust. The study reveals an 'impossible triangle' in
agricultural product grading, comprising biological characteristics,
timeliness, and economic viability, highlighting the limitations of traditional
absolute grading standards. To quantitatively assess this trade-off, we propose
the 'Triangular Trust Index' (TTI). We redefine the role of algorithms from
'decision-makers' to 'providers of transparent decision-making bases',
designing the explainable AI framework--TriAlignXA. This framework supports
trustworthy online transactions within agricultural constraints through
multi-objective optimization. Its core relies on three engines: the
Bio-Adaptive Engine for granular quality description; the Timeliness
Optimization Engine for processing efficiency; and the Economic Optimization
Engine for cost control. Additionally, the "Pre-Mapping Mechanism" encodes
process data into QR codes, transparently conveying quality information.
Experiments on grading tasks demonstrate significantly higher accuracy than
baseline models. Empirical evidence and theoretical analysis verify the
framework's balancing capability in addressing the "impossible triangle". This
research provides comprehensive support--from theory to practice--for building
a trustworthy online produce ecosystem, establishing a critical pathway from
algorithmic decision-making to consumer trust.

</details>


### [46] [4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing](https://arxiv.org/abs/2510.01991)
*Lei Liu,Can Wang,Zhenghao Chen,Dong Xu*

Main category: cs.CV

TL;DR: 为了克服4D Gaussian Splatting编辑中的一致性和复杂指令处理问题，提出了一种名为4DGS-Craft的编辑框架，结合了先进的模型和用户互动机制。


<details>
  <summary>Details</summary>
Motivation: 现有的4D Gaussian Splatting编辑在视图、时间和非编辑区域一致性、复杂文本指令处理等方面存在挑战。

Method: 提出了一种基于4D感知的InstructPix2Pix模型，并结合多视图网格模块和高斯选择机制，以确保一致性和用户互动。

Result: 通过实现一致的、交互式的4DGS编辑框架4DGS-Craft，解决了上述问题，并提升了编辑性能。

Conclusion: 我们的框架实现了更一致和可控的4D场景编辑。

Abstract: Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges
with view, temporal, and non-editing region consistency, as well as with
handling complex text instructions. To address these issues, we propose
4DGS-Craft, a consistent and interactive 4DGS editing framework. We first
introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal
consistency. This model incorporates 4D VGGT geometry features extracted from
the initial scene, enabling it to capture underlying 4D geometric structures
during editing. We further enhance this model with a multi-view grid module
that enforces consistency by iteratively refining multi-view input images while
jointly optimizing the underlying 4D scene. Furthermore, we preserve the
consistency of non-edited regions through a novel Gaussian selection mechanism,
which identifies and optimizes only the Gaussians within the edited regions.
Beyond consistency, facilitating user interaction is also crucial for effective
4DGS editing. Therefore, we design an LLM-based module for user intent
understanding. This module employs a user instruction template to define atomic
editing operations and leverages an LLM for reasoning. As a result, our
framework can interpret user intent and decompose complex instructions into a
logical sequence of atomic operations, enabling it to handle intricate user
commands and further enhance editing performance. Compared to related works,
our approach enables more consistent and controllable 4D scene editing. Our
code will be made available upon acceptance.

</details>


### [47] [Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing Routing in Lightweight Image Super-Resolution](https://arxiv.org/abs/2510.01997)
*Junyu Wu,Jie Tang,Jie Liu,Gangshan Wu*

Main category: cs.CV

TL;DR: 本研究提出了Pure-Pass机制，集成到ATD-light模型中，提升了超分辨率图像重建的性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级超分辨率方法存在适应性差、掩模粗糙和空间不灵活等问题，需要改进以实现更好的重建效果。

Method: 提出了一种像素级掩模机制Pure-Pass，通过固定颜色中心点对像素进行分类，实现细粒度和空间灵活性掩模。

Result: PP集成到最先进的ATD-light模型中，提升了超分辨率性能，同时节省了计算资源。

Conclusion: PP-ATD-light在重建质量和参数效率上优于CAMixer-ATD-light，同时保持较少的计算开销。

Abstract: Image Super-Resolution (SR) aims to reconstruct high-resolution images from
low-resolution counterparts, but the computational complexity of deep
learning-based methods often hinders practical deployment. CAMixer is the
pioneering work to integrate the advantages of existing lightweight SR methods
and proposes a content-aware mixer to route token mixers of varied complexities
according to the difficulty of content recovery. However, several limitations
remain, such as poor adaptability, coarse-grained masking and spatial
inflexibility, among others. We propose Pure-Pass (PP), a pixel-level masking
mechanism that identifies pure pixels and exempts them from expensive
computations. PP utilizes fixed color center points to classify pixels into
distinct categories, enabling fine-grained, spatially flexible masking while
maintaining adaptive flexibility. Integrated into the state-of-the-art
ATD-light model, PP-ATD-light achieves superior SR performance with minimal
overhead, outperforming CAMixer-ATD-light in reconstruction quality and
parameter efficiency when saving a similar amount of computation.

</details>


### [48] [Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output (SLSO) Framework](https://arxiv.org/abs/2510.02001)
*Nanaka Hosokawa,Ryo Takahashi,Tomoya Kitano,Yukihiro Iida,Chisako Muramatsu,Tatsuro Hayashi,Yuta Seino,Xiangrong Zhou,Takeshi Hara,Akitoshi Katsumata,Hiroshi Fujita*

Main category: cs.CV

TL;DR: 通过构建自我纠正循环结构化输出框架，研究表明在牙科全景影像中生成颌囊肿发现具有潜力，但仍需进一步优化以实现实际应用。


<details>
  <summary>Details</summary>
Motivation: 利用GPT-4o的多模态能力，自动生成牙科全景影像中的颌囊肿发现，以提高诊断的准确性。

Method: 构建自我纠正循环结构化输出(SLSO)框架，通过10步流程分析22例颌囊肿，包括图像输入、结构化数据生成、牙齿编号提取等。

Result: SLSO框架在多个评估项上提高了输出准确度，牙齿数量、牙齿移动和根吸收的改进率分别为66.9%、33.3%和28.6%。

Conclusion: 尽管当前研究未能在统计上达到显著性，但SLSO框架在提高牙齿数量识别准确性和抑制幻觉方面表现良好，需进一步优化以实现更实用的发现生成系统。

Abstract: In this study, we utilized the multimodal capabilities of OpenAI GPT-4o to
automatically generate jaw cyst findings on dental panoramic radiographs. To
improve accuracy, we constructed a Self-correction Loop with Structured Output
(SLSO) framework and verified its effectiveness. A 10-step process was
implemented for 22 cases of jaw cysts, including image input and analysis,
structured data generation, tooth number extraction and consistency checking,
iterative regeneration when inconsistencies were detected, and finding
generation with subsequent restructuring and consistency verification. A
comparative experiment was conducted using the conventional Chain-of-Thought
(CoT) method across seven evaluation items: transparency, internal structure,
borders, root resorption, tooth movement, relationships with other structures,
and tooth number. The results showed that the proposed SLSO framework improved
output accuracy for many items, with 66.9%, 33.3%, and 28.6% improvement rates
for tooth number, tooth movement, and root resorption, respectively. In the
successful cases, a consistently structured output was achieved after up to
five regenerations. Although statistical significance was not reached because
of the small size of the dataset, the overall SLSO framework enforced negative
finding descriptions, suppressed hallucinations, and improved tooth number
identification accuracy. However, the accurate identification of extensive
lesions spanning multiple teeth is limited. Nevertheless, further refinement is
required to enhance overall performance and move toward a practical finding
generation system.

</details>


### [49] [LiLa-Net: Lightweight Latent LiDAR Autoencoder for 3D Point Cloud Reconstruction](https://arxiv.org/abs/2510.02028)
*Mario Resino,Borja Pérez,Jaime Godoy,Abdulla Al-Kaff,Fernando García*

Main category: cs.CV

TL;DR: LiLa-Net是一种新的3D自编码器，能够高效地从LiDAR点云中提取特征，并展示了在交通环境中的优秀重建和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目标是从真实交通环境中高效提取特征，减少对复杂算法和资源的依赖。

Method: 提出了一种3D自编码器架构LiLa-Net，仅使用LiDAR的点云来编码特征，采用跳跃连接来提升性能，同时减少资源消耗。

Result: 通过有效的跳跃连接和平衡的信息编码，LiLa-Net在保持性能的同时，提升了重建质量。

Conclusion: LiLa-Net能够高效编码真实交通环境的特征，并成功重建与原始交通环境无关的物体，展示了良好的泛化能力。

Abstract: This work proposed a 3D autoencoder architecture, named LiLa-Net, which
encodes efficient features from real traffic environments, employing only the
LiDAR's point clouds. For this purpose, we have real semi-autonomous vehicle,
equipped with Velodyne LiDAR. The system leverage skip connections concept to
improve the performance without using extensive resources as the
state-of-the-art architectures. Key changes include reducing the number of
encoder layers and simplifying the skip connections, while still producing an
efficient and representative latent space which allows to accurately
reconstruct the original point cloud. Furthermore, an effective balance has
been achieved between the information carried by the skip connections and the
latent encoding, leading to improved reconstruction quality without
compromising performance. Finally, the model demonstrates strong generalization
capabilities, successfully reconstructing objects unrelated to the original
traffic environment.

</details>


### [50] [kabr-tools: Automated Framework for Multi-Species Behavioral Monitoring](https://arxiv.org/abs/2510.02030)
*Jenna Kline,Maksim Kholiavchenko,Samuel Stevens,Nina van Tiel,Alison Zhong,Namrata Banerji,Alec Sheets,Sowbaranika Balasubramaniam,Isla Duporge,Matthew Thompson,Elizabeth Campolongo,Jackson Miliko,Neil Rosser,Tanya Berger-Wolf,Charles V. Stewart,Daniel I. Rubenstein*

Main category: cs.CV

TL;DR: kabr-tools 是一种开源自动化工具，可以通过无人机视频和机器学习实现多种动物行为的监测，克服传统观察的局限，并在生态监测和保护研究中具有重要应用。


<details>
  <summary>Details</summary>
Motivation: 为了量化和解释复杂的动物行为模式，迫切需要可扩展的方法，传统的地面观察方法在范围和效率上存在局限性。

Method: 通过集成基于无人机的视频和机器学习系统，kabr-tools 自动化地提取野生动物行为、社交和空间指标，与传统的地面观察相比，显著提高了行为的颗粒度。

Result: 与地面观察方法相比，使用无人机的观察方式减少了15%的可见性损失，捕获更多的行为转换，具有更高的准确性和连续性，验证案例超越传统方法的数据捕获和注释能力。

Conclusion: kabr-tools 提供了一种强大的自动化行为监测工具，能够促进生态系统范围内的研究，推动保护、生物多样性研究及生态监测的发展。

Abstract: A comprehensive understanding of animal behavior ecology depends on scalable
approaches to quantify and interpret complex, multidimensional behavioral
patterns. Traditional field observations are often limited in scope,
time-consuming, and labor-intensive, hindering the assessment of behavioral
responses across landscapes. To address this, we present kabr-tools (Kenyan
Animal Behavior Recognition Tools), an open-source package for automated
multi-species behavioral monitoring. This framework integrates drone-based
video with machine learning systems to extract behavioral, social, and spatial
metrics from wildlife footage. Our pipeline leverages object detection,
tracking, and behavioral classification systems to generate key metrics,
including time budgets, behavioral transitions, social interactions, habitat
associations, and group composition dynamics. Compared to ground-based methods,
drone-based observations significantly improved behavioral granularity,
reducing visibility loss by 15% and capturing more transitions with higher
accuracy and continuity. We validate kabr-tools through three case studies,
analyzing 969 behavioral sequences, surpassing the capacity of traditional
methods for data capture and annotation. We found that, like Plains zebras,
vigilance in Grevy's zebras decreases with herd size, but, unlike Plains
zebras, habitat has a negligible impact. Plains and Grevy's zebras exhibit
strong behavioral inertia, with rare transitions to alert behaviors and
observed spatial segregation between Grevy's zebras, Plains zebras, and
giraffes in mixed-species herds. By enabling automated behavioral monitoring at
scale, kabr-tools offers a powerful tool for ecosystem-wide studies, advancing
conservation, biodiversity research, and ecological monitoring.

</details>


### [51] [GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing](https://arxiv.org/abs/2510.02034)
*Mengtian Li,Yunshu Bai,Yimin Chu,Yijun Shen,Zhongmei Li,Weifeng Ge,Zhifeng Xie,Chaofeng Chen*

Main category: cs.CV

TL;DR: GaussianMorphing是一个新颖的框架，通过3D高斯点云技术实现语义感知的3D形状和纹理迁移，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在未纹理数据上的依赖和局限性，提供高保真几何和外观建模。

Method: 通过网格引导的3D高斯点云技术实现形状和纹理的迁移，采用统一的变形策略和无监督的语义对应技术。

Result: 在TexMorph基准上，GaussianMorphing将颜色一致性误差降低了22.2%，EI降低了26.2%。

Conclusion: GaussianMorphing在保持局部细节和全局语义一致性方面显著优于先前的2D/3D方法。

Abstract: We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape
and texture morphing from multi-view images. Previous approaches usually rely
on point clouds or require pre-defined homeomorphic mappings for untextured
data. Our method overcomes these limitations by leveraging mesh-guided 3D
Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling.
The core of our framework is a unified deformation strategy that anchors
3DGaussians to reconstructed mesh patches, ensuring geometrically consistent
transformations while preserving texture fidelity through topology-aware
constraints. In parallel, our framework establishes unsupervised semantic
correspondence by using the mesh topology as a geometric prior and maintains
structural integrity via physically plausible point trajectories. This
integrated approach preserves both local detail and global semantic coherence
throughout the morphing process with out requiring labeled data. On our
proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior
2D/3D methods, reducing color consistency error ($\Delta E$) by 22.2% and EI by
26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/

</details>


### [52] [Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers](https://arxiv.org/abs/2510.02043)
*Sahil Bhandary Karnoor,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: 本研究提出了一种新方法InPose，利用预训练的扩散模型进行人体姿态估计，能够实现零-shot泛化。


<details>
  <summary>Details</summary>
Motivation: 解决在传感器数量有限的实际设置中，人体姿态估计的挑战，尤其是针对不同用户的普适性问题。

Method: 将姿态估计视为反问题，利用预训练的扩散模型，并仅基于旋转测量进行条件化。

Result: 提出的方法在处理不同用户体型时能有效进行姿态预测，并优于以往方法。

Conclusion: 本研究提出了InPose方法，能够在没有用户特定数据的情况下有效估计姿态。

Abstract: Pose estimation refers to tracking a human's full body posture, including
their head, torso, arms, and legs. The problem is challenging in practical
settings where the number of body sensors are limited. Past work has shown
promising results using conditional diffusion models, where the pose prediction
is conditioned on both <location, rotation> measurements from the sensors.
Unfortunately, nearly all these approaches generalize poorly across users,
primarly because location measurements are highly influenced by the body size
of the user. In this paper, we formulate pose estimation as an inverse problem
and design an algorithm capable of zero-shot generalization. Our idea utilizes
a pre-trained diffusion model and conditions it on rotational measurements
alone; the priors from this model are then guided by a likelihood term, derived
from the measured locations. Thus, given any user, our proposed InPose method
generatively estimates the highly likely sequence of poses that best explains
the sparse on-body measurements.

</details>


### [53] [VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation](https://arxiv.org/abs/2510.02086)
*Arman Behnam*

Main category: cs.CV

TL;DR: 提出了一种新的VGDM模型，通过视觉变压器和扩散机制，提高脑肿瘤的检测与分割精度，且在实验中表现优越。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤的准确检测和分割对诊断和治疗至关重要，而现有的卷积模型在处理复杂肿瘤结构时性能受限。

Method: 采用以视觉变压器为核心的扩散模型，通过全球上下文推理和迭代去噪实现脑肿瘤的检测和分割。

Result: 在MRI脑肿瘤数据集上的实验验证显示，VGDM在Dice相似度和Hausdorff距离上持续获得提升，显示了其在肿瘤分割研究中的潜力。

Conclusion: VGDM模型通过结合视觉变压器和扩散过程，提高了脑肿瘤检测和分割的精度，并在多个实验数据集上表现出色。

Abstract: Accurate detection and segmentation of brain tumors from magnetic resonance
imaging (MRI) are essential for diagnosis, treatment planning, and clinical
monitoring. While convolutional architectures such as U-Net have long been the
backbone of medical image segmentation, their limited capacity to capture
long-range dependencies constrains performance on complex tumor structures.
Recent advances in diffusion models have demonstrated strong potential for
generating high-fidelity medical images and refining segmentation boundaries.
  In this work, we propose VGDM: Vision-Guided Diffusion Model for Brain Tumor
Detection and Segmentation framework, a transformer-driven diffusion framework
for brain tumor detection and segmentation. By embedding a vision transformer
at the core of the diffusion process, the model leverages global contextual
reasoning together with iterative denoising to enhance both volumetric accuracy
and boundary precision. The transformer backbone enables more effective
modeling of spatial relationships across entire MRI volumes, while diffusion
refinement mitigates voxel-level errors and recovers fine-grained tumor
details.
  This hybrid design provides a pathway toward improved robustness and
scalability in neuro-oncology, moving beyond conventional U-Net baselines.
Experimental validation on MRI brain tumor datasets demonstrates consistent
gains in Dice similarity and Hausdorff distance, underscoring the potential of
transformer-guided diffusion models to advance the state of the art in tumor
segmentation.

</details>


### [54] [Mapping Historic Urban Footprints in France: Balancing Quality, Scalability and AI Techniques](https://arxiv.org/abs/2510.02097)
*Walid Rabehi,Marion Le Texier,Rémi Lemoy*

Main category: cs.CV

TL;DR: 本研究利用双通道U-Net方法提取了1925-1950年法国的城市区域数据，提供了73%的准确率，并公开了相关数据集以促进城市扩张研究。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏全国范围的数字城市足迹数据，该研究旨在填补这一空白，以便对1970年代前法国的城市扩张进行定量分析。

Method: 采用双通道U-Net方法，通过两次处理历史地图数据，第一遍识别混淆区域并进行数据增强，第二遍减少伪影，优化结果。

Result: 成功处理了覆盖整个法国大都市区的941个高分辨率图块，最终生成的城市足迹数据集为后续研究开放。

Conclusion: 本研究开发的双通道U-Net方法成功生成了法国历史城市扩张的开放访问数据集，具有73%的整体准确性，为今后的城市化动态研究提供了基础数据。

Abstract: Quantitative analysis of historical urban sprawl in France before the 1970s
is hindered by the lack of nationwide digital urban footprint data. This study
bridges this gap by developing a scalable deep learning pipeline to extract
urban areas from the Scan Histo historical map series (1925-1950), which
produces the first open-access, national-scale urban footprint dataset for this
pivotal period. Our key innovation is a dual-pass U-Net approach designed to
handle the high radiometric and stylistic complexity of historical maps. The
first pass, trained on an initial dataset, generates a preliminary map that
identifies areas of confusion, such as text and roads, to guide targeted data
augmentation. The second pass uses a refined dataset and the binarized output
of the first model to minimize radiometric noise, which significantly reduces
false positives. Deployed on a high-performance computing cluster, our method
processes 941 high-resolution tiles covering the entirety of metropolitan
France. The final mosaic achieves an overall accuracy of 73%, effectively
capturing diverse urban patterns while overcoming common artifacts like labels
and contour lines. We openly release the code, training datasets, and the
resulting nationwide urban raster to support future research in long-term
urbanization dynamics.

</details>


### [55] [When Tracking Fails: Analyzing Failure Modes of SAM2 for Point-Based Tracking in Surgical Videos](https://arxiv.org/abs/2510.02100)
*Woowon Jang,Jiwon Im,Juseung Choi,Niki Rashidian,Wesley De Neve,Utku Ozbulak*

Main category: cs.CV

TL;DR: 研究分析了点式跟踪在外科视频监测中的表现，包括对外科工具和解剖目标的比较，并提供了提高跟踪性能的建议。


<details>
  <summary>Details</summary>
Motivation: 探讨点式跟踪在复杂的外科环境中的可靠性及失败案例，填补现有研究空白。

Method: 对腹腔镜胆囊切除手术视频中的点式跟踪失败模式进行系统分析，并与分割掩码初始化的性能进行比较。

Result: 点式跟踪对外科工具表现良好，但在解剖目标上由于组织相似性和模糊边界导致跟踪失败。

Conclusion: 点式跟踪在外科工具的跟踪中具有竞争力，但在解剖目标方面表现不佳，提出建议以提高性能。

Abstract: Video object segmentation (VOS) models such as SAM2 offer promising zero-shot
tracking capabilities for surgical videos using minimal user input. Among the
available input types, point-based tracking offers an efficient and low-cost
alternative, yet its reliability and failure cases in complex surgical
environments are not well understood. In this work, we systematically analyze
the failure modes of point-based tracking in laparoscopic cholecystectomy
videos. Focusing on three surgical targets, the gallbladder, grasper, and
L-hook electrocautery, we compare the performance of point-based tracking with
segmentation mask initialization. Our results show that point-based tracking is
competitive for surgical tools but consistently underperforms for anatomical
targets, where tissue similarity and ambiguous boundaries lead to failure.
Through qualitative analysis, we reveal key factors influencing tracking
outcomes and provide several actionable recommendations for selecting and
placing tracking points to improve performance in surgical video analysis.

</details>


### [56] [FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation](https://arxiv.org/abs/2510.02114)
*Ding-Ruei Shen*

Main category: cs.CV

TL;DR: 本文提出了FFREEDG任务，并提出FRIEREN框架来处理客户端的无标签数据，在语义分割领域展示了良好的效果，为未来提供了研究基础。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在新的领域中提供了一种隐私保护的语义分割解决方案，但在客户数据未标记的情况下，面临着显著的挑战。

Method: 我们提出FRIEREN框架，结合视觉与语言模态，使用基于CLIP的文本嵌入指导视觉-语言解码器，并采用从弱到强的一致性学习策略进行伪标签的本地训练。

Result: 在合成到真实和清晰到恶劣天气的基准测试中，我们的框架展示了出色的性能，有效应对了这一新任务。

Conclusion: 我们的框架有效解决了FFREEDG任务，在对抗现有领域泛化和适应方法的性能上表现出竞争力，为未来研究奠定了良好的基础。

Abstract: Federeated Learning (FL) offers a privacy-preserving solution for Semantic
Segmentation (SS) tasks to adapt to new domains, but faces significant
challenges from these domain shifts, particularly when client data is
unlabeled. However, most existing FL methods unrealistically assume access to
labeled data on remote clients or fail to leverage the power of modern Vision
Foundation Models (VFMs). Here, we propose a novel and challenging task,
FFREEDG, in which a model is pretrained on a server's labeled source dataset
and subsequently trained across clients using only their unlabeled data,
without ever re-accessing the source. To solve FFREEDG, we propose FRIEREN, a
framework that leverages the knowledge of a VFM by integrating vision and
language modalities. Our approach employs a Vision-Language decoder guided by
CLIP-based text embeddings to improve semantic disambiguation and uses a
weak-to-strong consistency learning strategy for robust local training on
pseudo-labels. Our experiments on synthetic-to-real and
clear-to-adverse-weather benchmarks demonstrate that our framework effectively
tackles this new task, achieving competitive performance against established
domain generalization and adaptation methods and setting a strong baseline for
future research.

</details>


### [57] [Unlocking Vision-Language Models for Video Anomaly Detection via Fine-Grained Prompting](https://arxiv.org/abs/2510.02155)
*Shu Zou,Xinyu Tian,Lukas Wesemann,Fabian Waschkowski,Zhaoyuan Yang,Jing Zhang*

Main category: cs.CV

TL;DR: ASK-Hint 是一种利用动作中心知识的结构化提示框架，改善冻结的视觉语言模型在视频异常检测中的表现，兼顾准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有提示过于抽象，未能考虑到视频监控中定义复杂异常的人物与物体的细粒度交互及行为语义。

Method: 提出 ASK-Hint，组织提示为语义一致的组，使用细粒度的指导问题来引导模型，确保模型预测与视觉线索相一致。

Result: 在 UCF-Crime 和 XD-Violence 数据集上的大量实验表明，ASK-Hint 在 AUC 上始终优于之前的基线，达到了最先进的性能，并且具有强泛化能力。

Conclusion: ASK-Hint 是一种新型的无训练、可解释的视频异常检测解决方案，通过结构化提示框架显著提升了识别精度和模型的可解释性。

Abstract: Prompting has emerged as a practical way to adapt frozen vision-language
models (VLMs) for video anomaly detection (VAD). Yet, existing prompts are
often overly abstract, overlooking the fine-grained human-object interactions
or action semantics that define complex anomalies in surveillance videos. We
propose ASK-Hint, a structured prompting framework that leverages
action-centric knowledge to elicit more accurate and interpretable reasoning
from frozen VLMs. Our approach organizes prompts into semantically coherent
groups (e.g. violence, property crimes, public safety) and formulates
fine-grained guiding questions that align model predictions with discriminative
visual cues. Extensive experiments on UCF-Crime and XD-Violence show that
ASK-Hint consistently improves AUC over prior baselines, achieving
state-of-the-art performance compared to both fine-tuned and training-free
methods. Beyond accuracy, our framework provides interpretable reasoning traces
towards anomaly and demonstrates strong generalization across datasets and VLM
backbones. These results highlight the critical role of prompt granularity and
establish ASK-Hint as a new training-free and generalizable solution for
explainable video anomaly detection.

</details>


### [58] [GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation](https://arxiv.org/abs/2510.02186)
*Weijia Dou,Xu Zhang,Yi Bin,Jian Liu,Bo Peng,Guoqing Wang,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: GeoPurify通过几何先验和亲和网络有效改善了2D到3D特征转移的性能，展现出优秀的数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有2D视觉-语言模型在3D语义分割中的特征转移效果不佳，主要是因为分割与匹配范式未能协调2D语义与3D几何结构。

Method: 提出GeoPurify，应用小型学生亲和网络来净化由2D VLM生成的3D点特征，并结合几何先验，同时设计算法以引导点云处理以确保语义与结构一致性。

Result: GeoPurify在主要3D基准测试中表现优异，达到或超越了当前最先进的性能，且仅使用约1.5%的训练数据。

Conclusion: GeoPurify通过利用潜在几何信息和学习到的亲和网络，有效缓解了2D到3D特征转移中的权衡问题，并在数据效率上表现优越。

Abstract: Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to
3D semantic segmentation expose a persistent trade-off. Directly projecting 2D
features into 3D yields noisy and fragmented predictions, whereas enforcing
geometric coherence necessitates costly training pipelines and large-scale
annotated 3D data. We argue that this limitation stems from the dominant
segmentation-and-matching paradigm, which fails to reconcile 2D semantics with
3D geometric structure. The geometric cues are not eliminated during the
2D-to-3D transfer but remain latent within the noisy and view-aggregated
features. To exploit this property, we propose GeoPurify that applies a small
Student Affinity Network to purify 2D VLM-generated 3D point features using
geometric priors distilled from a 3D self-supervised teacher model. During
inference, we devise a Geometry-Guided Pooling module to further denoise the
point cloud and ensure the semantic and structural consistency. Benefiting from
latent geometric information and the learned affinity network, GeoPurify
effectively mitigates the trade-off and achieves superior data efficiency.
Extensive experiments on major 3D benchmarks demonstrate that GeoPurify
achieves or surpasses state-of-the-art performance while utilizing only about
1.5% of the training data. Our codes and checkpoints are available at
[https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).

</details>


### [59] [Cross-Breed Pig Identification Using Auricular Vein Pattern Recognition: A Machine Learning Approach for Small-Scale Farming Applications](https://arxiv.org/abs/2510.02197)
*Emmanuel Nsengiyumvaa,Leonard Niyitegekaa,Eric Umuhoza*

Main category: cs.CV

TL;DR: 本研究提出了一种基于耳静脉图案的非侵入性猪只识别方法，具有很高的准确性和实时性，为小规模农民提供了解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有的猪只识别方法不可靠且成本高，特别是对小规模农民不实用，因此需要一种非侵入性的生物识别方法。

Method: 通过收集800张混合品种猪的耳部图像，使用多阶段计算机视觉流程提取血管特征，并应用机器学习模型进行分类。

Result: 使用支持向量机（SVM）模型的准确率达到98.12%，整个识别过程平均耗时8.3秒，适合实时应用。

Conclusion: 该系统提供了一种成本效益高且无应激的农场动物识别方法，强调耳静脉生物识别在精准农业中的实用性。

Abstract: Accurate livestock identification is a cornerstone of modern farming: it
supports health monitoring, breeding programs, and productivity tracking.
However, common pig identification methods, such as ear tags and microchips,
are often unreliable, costly, target pure breeds, and thus impractical for
small-scale farmers. To address this gap, we propose a noninvasive biometric
identification approach that leverages uniqueness of the auricular vein
patterns. To this end, we have collected 800 ear images from 20 mixed-breed
pigs (Landrace cross Pietrain and Duroc cross Pietrain), captured using a
standard smartphone and simple back lighting. A multistage computer vision
pipeline was developed to enhance vein visibility, extract structural and
spatial features, and generate biometric signatures. These features were then
classified using machine learning models. Support Vector Machines (SVM)
achieved the highest accuracy: correctly identifying pigs with 98.12% precision
across mixed-breed populations. The entire process from image processing to
classification was completed in an average of 8.3 seconds, demonstrating
feasibility for real-time farm deployment. We believe that by replacing fragile
physical identifiers with permanent biological markers, this system provides
farmers with a cost-effective and stress-free method of animal identification.
More broadly, the findings confirm the practicality of auricular vein
biometrics for digitizing livestock management, reinforcing its potential to
extend the benefits of precision farming to resource-constrained agricultural
communities.

</details>


### [60] [MMDEW: Multipurpose Multiclass Density Estimation in the Wild](https://arxiv.org/abs/2510.02213)
*Villanelle O'Reilly,Jonathan Cox,Georgios Leontidis,Marc Hanheide,Petra Bosilj,James Brown*

Main category: cs.CV

TL;DR: 本研究提出了一种基于Twins视觉变换器的多类别计数框架，在密集和遮挡场景中相较传统方法表现优异，为生态监测和保护提供新方法。


<details>
  <summary>Details</summary>
Motivation: 在高密度和遮挡场景中，传统的基于检测的方法无法有效计数，因此需要一种新的方法来解决这一问题。

Method: 采用了多类别计数框架，基于Twins金字塔视觉变换器骨干网和多类计数头，结合多尺度解码方法，设计了二任务结构和基于分割的类别关注模块。

Result: 在VisDrone和iSAID基准上的训练和评估显示，相较于以往的多类别人群计数方法，MAE分别降低了33%、43%和64%；与YOLOv11的比较表明，在密集场景中人群计数方法的必要性。

Conclusion: 该方法在多类人群计数任务中表现优异，并能在新的域中应用，具备保护生态和监测生物多样性的潜力。

Abstract: Density map estimation can be used to estimate object counts in dense and
occluded scenes where discrete counting-by-detection methods fail. We propose a
multicategory counting framework that leverages a Twins pyramid
vision-transformer backbone and a specialised multi-class counting head built
on a state-of-the-art multiscale decoding approach. A two-task design adds a
segmentation-based Category Focus Module, suppressing inter-category cross-talk
at training time. Training and evaluation on the VisDrone and iSAID benchmarks
demonstrates superior performance versus prior multicategory crowd-counting
approaches (33%, 43% and 64% reduction to MAE), and the comparison with YOLOv11
underscores the necessity of crowd counting methods in dense scenes. The
method's regional loss opens up multi-class crowd counting to new domains,
demonstrated through the application to a biodiversity monitoring dataset,
highlighting its capacity to inform conservation efforts and enable scalable
ecological insights.

</details>


### [61] [TempoControl: Temporal Attention Guidance for Text-to-Video Models](https://arxiv.org/abs/2510.02226)
*Shira Schiber,Ofir Lindenbaum,Idan Schwartz*

Main category: cs.CV

TL;DR: 引入TempoControl方法，解决了生成视频中视觉元素时间控制不足的问题，确保高质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 尽管生成视频模型有显著进展，但缺乏细粒度的时间控制限制了用户对视觉元素出现时机的指定。

Method: 通过交叉注意力图和新颖的优化方法，引导视觉概念的时序对齐。

Result: TempoControl在视频生成应用中表现出色，能够精确控制单个及多个物体的时间重排，并对动作和音频的生成进行对齐。

Conclusion: TempoControl实现了对生成视频中视觉元素时间控制的精确调节，能够在不需要重新训练和额外监督的情况下，提升视频生成的质量和多样性。

Abstract: Recent advances in generative video models have enabled the creation of
high-quality videos based on natural language prompts. However, these models
frequently lack fine-grained temporal control, meaning they do not allow users
to specify when particular visual elements should appear within a generated
sequence. In this work, we introduce TempoControl, a method that allows for
temporal alignment of visual concepts during inference, without requiring
retraining or additional supervision. TempoControl utilizes cross-attention
maps, a key component of text-to-video diffusion models, to guide the timing of
concepts through a novel optimization approach. Our method steers attention
using three complementary principles: aligning its temporal shape with a
control signal (via correlation), amplifying it where visibility is needed (via
energy), and maintaining spatial focus (via entropy). TempoControl allows
precise control over timing while ensuring high video quality and diversity. We
demonstrate its effectiveness across various video generation applications,
including temporal reordering for single and multiple objects, as well as
action and audio-aligned generation.

</details>


### [62] [RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning](https://arxiv.org/abs/2510.02240)
*Sicheng Feng,Kaiwen Tuo,Song Wang,Lingdong Kong,Jianke Zhu,Huan Wang*

Main category: cs.CV

TL;DR: 本文探讨了多模态大语言模型在细致视觉推理方面的挑战，并提出了RewardMap框架，通过改进奖励机制和训练策略，实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 针对当前多模态大语言模型在复杂空间推理任务中的不足，尤其是在稀疏奖励和不稳定优化的问题。

Method: 提出了一个多阶段的强化学习框架RewardMap，结合了难度感知的奖励设计，提高了视觉理解和推理能力。

Result: 实验证明RewardMap的各个组件均有助于提高模型的表现，组合使用效果最佳。

Conclusion: 通过引入RewardMap框架，模型在空间推理和细致视觉推理任务上的表现有了显著提升，平均提高了3.47%。

Abstract: Fine-grained visual reasoning remains a core challenge for multimodal large
language models (MLLMs). The recently introduced ReasonMap highlights this gap
by showing that even advanced MLLMs struggle with spatial reasoning in
structured and information-rich settings such as transit maps, a task of clear
practical and scientific importance. However, standard reinforcement learning
(RL) on such tasks is impeded by sparse rewards and unstable optimization. To
address this, we first construct ReasonMap-Plus, an extended dataset that
introduces dense reward signals through Visual Question Answering (VQA) tasks,
enabling effective cold-start training of fine-grained visual understanding
skills. Next, we propose RewardMap, a multi-stage RL framework designed to
improve both visual understanding and reasoning capabilities of MLLMs.
RewardMap incorporates two key designs. First, we introduce a difficulty-aware
reward design that incorporates detail rewards, directly tackling the sparse
rewards while providing richer supervision. Second, we propose a multi-stage RL
scheme that bootstraps training from simple perception to complex reasoning
tasks, offering a more effective cold-start strategy than conventional
Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus
demonstrate that each component of RewardMap contributes to consistent
performance gains, while their combination yields the best results. Moreover,
models trained with RewardMap achieve an average improvement of 3.47% across 6
benchmarks spanning spatial reasoning, fine-grained visual reasoning, and
general tasks beyond transit maps, underscoring enhanced visual understanding
and reasoning capabilities.

</details>


### [63] [DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing](https://arxiv.org/abs/2510.02253)
*Zihan Zhou,Shilin Lu,Shuli Leng,Shaocong Zhang,Zhuming Lian,Xinlei Yu,Adams Wai-Kin Kong*

Main category: cs.CV

TL;DR: DragFlow是一个新框架，通过引入区域编辑和整合语言模型，提升了拖动图像编辑效果，设立了新标准。


<details>
  <summary>Details</summary>
Motivation: 解决早期模型在拖动编辑任务中引发的目标区域失真问题，利用最新的生成先验加强拖动编辑的效果。

Method: 该研究提出了一种新的框架DragFlow，通过区域基础编辑模式和多模态大型语言模型对拖动编辑进行构建和评估。

Result: DragFlow在拖动图像编辑任务上相较于基线模型显示了显著提升，成为该领域的最新进展。

Conclusion: DragFlow在拖动图像编辑中通过引入区域编辑范式和集成预训练适配器，超越了现有的基线，创造了拖动图像编辑的新状态。

Abstract: Drag-based image editing has long suffered from distortions in the target
region, largely because the priors of earlier base models, Stable Diffusion,
are insufficient to project optimized latents back onto the natural image
manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow
matching (e.g., SD3.5, FLUX), generative priors have become significantly
stronger, enabling advances across diverse editing tasks. However, drag-based
editing has yet to benefit from these stronger priors. This work proposes the
first framework to effectively harness FLUX's rich prior for drag-based
editing, dubbed DragFlow, achieving substantial gains over baselines. We first
show that directly applying point-based drag editing to DiTs performs poorly:
unlike the highly compressed features of UNets, DiT features are insufficiently
structured to provide reliable guidance for point-wise motion supervision. To
overcome this limitation, DragFlow introduces a region-based editing paradigm,
where affine transformations enable richer and more consistent feature
supervision. Additionally, we integrate pretrained open-domain personalization
adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving
background fidelity through gradient mask-based hard constraints. Multimodal
large language models (MLLMs) are further employed to resolve task ambiguities.
For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)
featuring region-level dragging instructions. Extensive experiments on
DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and
region-based baselines, setting a new state-of-the-art in drag-based image
editing. Code and datasets will be publicly available upon publication.

</details>


### [64] [From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding](https://arxiv.org/abs/2510.02262)
*Guangyu Sun,Archit Singhal,Burak Uzkent,Mubarak Shah,Chen Chen,Garin Kessler*

Main category: cs.CV

TL;DR: 本文研究了一种新方法F2C，通过选择时间一致的关键片段而非孤立关键帧，提高视频理解效果，同时提出了一种自适应分辨率策略以有效管理计算资源。


<details>
  <summary>Details</summary>
Motivation: 针对现有解决方案在选择稀疏帧时忽视了时间动态这一问题，探索保留时间信息对视频理解的重要性。

Method: 提出了一种自适应分辨率策略，以动态平衡空间分辨率和片段长度，从而在保持固定计算预算的同时处理更大的令牌数量。

Result: 在三个长视频基准测试上，F2C方法分别比均匀采样在Video-MME、LongVideoBench和MLVU上提高了8.1%、5.6%和10.3%。

Conclusion: 本研究提出的F2C方法通过保留时间一致性的关键片段选择，显著提高了视频理解能力，并为视频大语言模型在实际应用中的扩展提供了有效路径。

Abstract: Video Large Language Models (VLMs) have achieved remarkable results on a
variety of vision language tasks, yet their practical use is limited by the
"needle in a haystack" problem: the massive number of visual tokens produced
from raw video frames exhausts the model's context window. Existing solutions
alleviate this issue by selecting a sparse set of frames, thereby reducing
token count, but such frame-wise selection discards essential temporal
dynamics, leading to suboptimal reasoning about motion and event continuity. In
this work we systematically explore the impact of temporal information and
demonstrate that extending selection from isolated key frames to key clips,
which are short, temporally coherent segments, improves video understanding. To
maintain a fixed computational budget while accommodating the larger token
footprint of clips, we propose an adaptive resolution strategy that dynamically
balances spatial resolution and clip length, ensuring a constant token count
per video. Experiments on three long-form video benchmarks demonstrate that our
training-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and
10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These
results highlight the importance of preserving temporal coherence in frame
selection and provide a practical pathway for scaling Video LLMs to real world
video understanding applications. Project webpage is available at
https://guangyusun.com/f2c .

</details>


### [65] [Paving the Way Towards Kinematic Assessment Using Monocular Video: A Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose Estimators Against Inertial Sensors in Daily Living Activities](https://arxiv.org/abs/2510.02264)
*Mario Medrano-Paredes,Carmen Fernández-González,Francisco-Javier Díaz-Pernas,Hichem Saoudi,Javier González-Alonso,Mario Martínez-Zarzuela*

Main category: cs.CV

TL;DR: 本研究比较了视频和IMU在人体动作评估中的表现，得出二者在应用中的优缺点，为未来的远程医疗解决方案提供指导。


<details>
  <summary>Details</summary>
Motivation: 利用机器学习和可穿戴传感器捕捉和分析人类运动，推动远程医疗、运动科学和康复的发展。

Method: 比较基于单目视频的3D人体姿态估计模型和惯性测量单元（IMUs），利用VIDIMU数据集评估运动学。

Result: MotionAGFormer模型在整体均方根误差和平均绝对误差方面表现最佳，同时证明了视频模型在健康成年人中的临床潜力。

Conclusion: 视频和传感器技术在非实验室运动评估中均有可行性，但存在成本、可及性和精度之间的权衡。

Abstract: Advances in machine learning and wearable sensors offer new opportunities for
capturing and analyzing human movement outside specialized laboratories.
Accurate assessment of human movement under real-world conditions is essential
for telemedicine, sports science, and rehabilitation. This preclinical
benchmark compares monocular video-based 3D human pose estimation models with
inertial measurement units (IMUs), leveraging the VIDIMU dataset containing a
total of 13 clinically relevant daily activities which were captured using both
commodity video cameras and five IMUs. During this initial study only healthy
subjects were recorded, so results cannot be generalized to pathological
cohorts. Joint angles derived from state-of-the-art deep learning frameworks
(MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, and NVIDIA
BodyTrack) were evaluated against joint angles computed from IMU data using
OpenSim inverse kinematics following the Human3.6M dataset format with 17
keypoints. Among them, MotionAGFormer demonstrated superior performance,
achieving the lowest overall RMSE ($9.27\deg \pm 4.80\deg$) and MAE ($7.86\deg
\pm 4.18\deg$), as well as the highest Pearson correlation ($0.86 \pm 0.15$)
and the highest coefficient of determination $R^{2}$ ($0.67 \pm 0.28$). The
results reveal that both technologies are viable for out-of-the-lab kinematic
assessment. However, they also highlight key trade-offs between video- and
sensor-based approaches including costs, accessibility, and precision. This
study clarifies where off-the-shelf video models already provide clinically
promising kinematics in healthy adults and where they lag behind IMU-based
estimates while establishing valuable guidelines for researchers and clinicians
seeking to develop robust, cost-effective, and user-friendly solutions for
telehealth and remote patient monitoring.

</details>


### [66] [NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes](https://arxiv.org/abs/2510.02266)
*Shiyi Zhang,Dong Liang,Yihang Zhou*

Main category: cs.CV

TL;DR: NeuroSwift通过集成低级特征与语义适配，解决脑活动可视化重构中的跨受试者挑战，展现出卓越的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在跨受试者重构视觉刺激时的准确性和计算复杂性挑战。

Method: 采用AutoKL和CLIP适配器，集成低级特征和语义信息，通过在不同受试者之间进行预训练和微调实现交叉受试者的通用性。

Result: NeuroSwift在仅仅一小时的训练时间内，使用三张RTX 4090显卡，达到了优于现有方法的重构性能。

Conclusion: NeuroSwift achieves state-of-the-art performance in visual stimulus reconstruction from brain activity with minimal training time and resources.

Abstract: Reconstructing visual information from brain activity via computer vision
technology provides an intuitive understanding of visual neural mechanisms.
Despite progress in decoding fMRI data with generative models, achieving
accurate cross-subject reconstruction of visual stimuli remains challenging and
computationally demanding. This difficulty arises from inter-subject
variability in neural representations and the brain's abstract encoding of core
semantic features in complex visual inputs. To address these challenges, we
propose NeuroSwift, which integrates complementary adapters via diffusion:
AutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter
is trained on Stable Diffusion generated images paired with COCO captions to
emulate higher visual cortex encoding. For cross-subject generalization, we
pretrain on one subject and then fine-tune only 17 percent of parameters (fully
connected layers) for new subjects, while freezing other components. This
enables state-of-the-art performance with only one hour of training per subject
on lightweight GPUs (three RTX 4090), and it outperforms existing methods.

</details>


### [67] [microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification](https://arxiv.org/abs/2510.02270)
*Sathira Silva,Eman Ali,Chetan Arora,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: microCLIP框架通过细致的视觉文本表示结合和自我训练，提升了细粒度图像分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统CLIP模型在细粒度分类任务中受到全球特征的限制，需要更精确的细微局部线索来改善性能。

Method: self-training框架，结合Saliency-Oriented Attention Pooling (SOAP)和TokenFusion模块，使用微调知识进行伪标签生成和分类优化。

Result: 在13个细粒度基准测试中，microCLIP实现了平均2.90%的准确率提升，且适应过程较轻。

Conclusion: 提出的microCLIP框架通过结合视觉和文本表征，显著提高了细粒度图像分类的准确性，具有良好的适应性和效果。

Abstract: Unsupervised adaptation of CLIP-based vision-language models (VLMs) for
fine-grained image classification requires sensitivity to microscopic local
cues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse
global features restricts its performance on fine-grained classification tasks.
Prior efforts inject fine-grained knowledge by aligning large language model
(LLM) descriptions with the CLIP $\texttt{[CLS]}$ token; however, this approach
overlooks spatial precision. We propose $\textbf{microCLIP}$, a self-training
framework that jointly refines CLIP's visual and textual representations using
fine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)
within a lightweight TokenFusion module, which builds a saliency-guided
$\texttt{[FG]}$ token from patch embeddings and fuses it with the global
$\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we
introduce a two-headed LLM-derived classifier: a frozen classifier that, via
multi-view alignment, provides a stable text-based prior for pseudo-labeling,
and a learnable classifier initialized from LLM descriptions and fine-tuned
with TokenFusion. We further develop Dynamic Knowledge Aggregation, which
convexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to
iteratively refine pseudo-labels. Together, these components uncover latent
fine-grained signals in CLIP, yielding a consistent $2.90\%$ average accuracy
gain across 13 fine-grained benchmarks while requiring only light adaptation.
Our code is available at https://github.com/sathiiii/microCLIP.

</details>


### [68] [VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL](https://arxiv.org/abs/2510.02282)
*Kyoungjun Park,Yifan Yang,Juheon Yi,Shicheng Zheng,Yifei Shen,Dongqi Han,Caihua Shan,Muhammad Muaz,Lili Qiu*

Main category: cs.CV

TL;DR: VidGuard-R1是首个基于多模态大语言模型的AI生成视频真实性检测工具，兼具高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 应对AI生成视频带来的社会风险，如错误信息和声誉损害。

Method: 利用组相对政策优化（GRPO）对多模态大语言模型（MLLM）进行微调。

Result: VidGuard-R1在现有基准上的零样本性能达到最先进水平，正确率超过95%。

Conclusion: VidGuard-R1是一种有效的多模态视频真实性检测工具，能够提供高准确度和可解释的判断。

Abstract: With the rapid advancement of AI-generated videos, there is an urgent need
for effective detection tools to mitigate societal risks such as misinformation
and reputational harm. In addition to accurate classification, it is essential
that detection models provide interpretable explanations to ensure transparency
for regulators and end users. To address these challenges, we introduce
VidGuard-R1, the first video authenticity detector that fine-tunes a
multi-modal large language model (MLLM) using group relative policy
optimization (GRPO). Our model delivers both highly accurate judgments and
insightful reasoning. We curate a challenging dataset of 140k real and
AI-generated videos produced by state-of-the-art generation models, carefully
designing the generation process to maximize discrimination difficulty. We then
fine-tune Qwen-VL using GRPO with two specialized reward models that target
temporal artifacts and generation complexity. Extensive experiments demonstrate
that VidGuard-R1 achieves state-of-the-art zero-shot performance on existing
benchmarks, with additional training pushing accuracy above 95%. Case studies
further show that VidGuard-R1 produces precise and interpretable rationales
behind its predictions. The code is publicly available at
https://VidGuard-R1.github.io.

</details>


### [69] [Self-Forcing++: Towards Minute-Scale High-Quality Video Generation](https://arxiv.org/abs/2510.02283)
*Justin Cui,Jie Wu,Ming Li,Tao Yang,Xiaojie Li,Rui Wang,Andrew Bai,Yuanhao Ban,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 本文提出了一种新方法以改善长视频生成质量，无需长视频教师或重训练，成功生成长度超过基线模型50倍的视频，并在实验中表现优越。


<details>
  <summary>Details</summary>
Motivation: 解决长视频生成中的质量衰退问题，尤其是在没有长视频教师模型的情况下。

Method: 通过使用自生成长视频的采样段来指导学生模型，避免了从长视频教师模型的直接监督，并且避免了重复计算重叠帧。

Result: 在视频长度扩展方面，我们的方法能够生成最长达到4分钟15秒的视频，显著超越基线模型超过50倍，实验结果显示在保真度和一致性上优于其他基线方法。

Conclusion: 我们的方法在长视频生成方面表现优异，显著提高了视频质量和一致性。

Abstract: Diffusion models have revolutionized image and video generation, achieving
unprecedented visual quality. However, their reliance on transformer
architectures incurs prohibitively high computational costs, particularly when
extending generation to long videos. Recent work has explored autoregressive
formulations for long video generation, typically by distilling from
short-horizon bidirectional teachers. Nevertheless, given that teacher models
cannot synthesize long videos, the extrapolation of student models beyond their
training horizon often leads to pronounced quality degradation, arising from
the compounding of errors within the continuous latent space. In this paper, we
propose a simple yet effective approach to mitigate quality degradation in
long-horizon video generation without requiring supervision from long-video
teachers or retraining on long video datasets. Our approach centers on
exploiting the rich knowledge of teacher models to provide guidance for the
student model through sampled segments drawn from self-generated long videos.
Our method maintains temporal consistency while scaling video length by up to
20x beyond teacher's capability, avoiding common issues such as over-exposure
and error-accumulation without recomputing overlapping frames like previous
methods. When scaling up the computation, our method shows the capability of
generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the
maximum span supported by our base model's position embedding and more than 50x
longer than that of our baseline model. Experiments on standard benchmarks and
our proposed improved benchmark demonstrate that our approach substantially
outperforms baseline methods in both fidelity and consistency. Our long-horizon
videos demo can be found at https://self-forcing-plus-plus.github.io/

</details>


### [70] [Learning to Generate Object Interactions with Physics-Guided Video Diffusion](https://arxiv.org/abs/2510.02284)
*David Romero,Ariana Bermudez,Hao Li,Fabio Pizzati,Ivan Laptev*

Main category: cs.CV

TL;DR: KineMask是一种基于物理引导的视频生成方法，通过创新的训练策略改善物体交互和控制能力，提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在生成物理上可信的物体交互方面存在困难，亟需改善物理引导的控制机制。

Method: 提出了一种两阶段的训练策略，通过物体掩码逐步移除未来运动监督，结合低层次运动控制与高层次文本条件。

Result: KineMask在合成简单和复杂动态现象的场景中，表现出比同类模型显著提升的效果。

Conclusion: KineMask显著改善了物体交互的生成质量，在视频生成领域中展现出强大的潜力，尤其是在物理一致性和控制机制方面。

Abstract: Recent models for video generation have achieved remarkable progress and are
now deployed in film, social media production, and advertising. Beyond their
creative potential, such models also hold promise as world simulators for
robotics and embodied decision making. Despite strong advances, however,
current approaches still struggle to generate physically plausible object
interactions and lack physics-grounded control mechanisms. To address this
limitation, we introduce KineMask, an approach for physics-guided video
generation that enables realistic rigid body control, interactions, and
effects. Given a single image and a specified object velocity, our method
generates videos with inferred motions and future object interactions. We
propose a two-stage training strategy that gradually removes future motion
supervision via object masks. Using this strategy we train video diffusion
models (VDMs) on synthetic scenes of simple interactions and demonstrate
significant improvements of object interactions in real scenes. Furthermore,
KineMask integrates low-level motion control with high-level textual
conditioning via predictive scene descriptions, leading to effective support
for synthesis of complex dynamical phenomena. Extensive experiments show that
KineMask achieves strong improvements over recent models of comparable size.
Ablation studies further highlight the complementary roles of low- and
high-level conditioning in VDMs. Our code, model, and data will be made
publicly available.

</details>


### [71] [MultiModal Action Conditioned Video Generation](https://arxiv.org/abs/2510.02287)
*Yichen Li,Antonio Torralba*

Main category: cs.CV

TL;DR: 本研究提出了一种新的多模态感知特征学习方法，针对家庭机器人需要的精细控制任务进行改进，提升了视频模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频模型在处理精细控制任务时存在不足，无法满足家庭机器人在实时处理精细运动和紧急情况时的需求。

Method: 开发了一种特征学习范式，整合多模态感知，同时保持每种模态提供的独特信息，并提出了正则化方案增强动作轨迹特征的因果性。

Result: 实验表明，结合多模态感知可以提高模拟准确性，减少时间漂移，广泛的消融研究和下游应用证明了方法的有效性与实用性。

Conclusion: 本研究提出的多模态感知方法有效提高了视频模型在精细控制任务中的准确性与实用性。

Abstract: Current video models fail as world model as they lack fine-graiend control.
General-purpose household robots require real-time fine motor control to handle
delicate tasks and urgent situations. In this work, we introduce fine-grained
multimodal actions to capture such precise control. We consider senses of
proprioception, kinesthesia, force haptics, and muscle activation. Such
multimodal senses naturally enables fine-grained interactions that are
difficult to simulate with text-conditioned generative models. To effectively
simulate fine-grained multisensory actions, we develop a feature learning
paradigm that aligns these modalities while preserving the unique information
each modality provides. We further propose a regularization scheme to enhance
causality of the action trajectory features in representing intricate
interaction dynamics. Experiments show that incorporating multimodal senses
improves simulation accuracy and reduces temporal drift. Extensive ablation
studies and downstream applications demonstrate the effectiveness and
practicality of our work.

</details>


### [72] [VideoNSA: Native Sparse Attention Scales Video Understanding](https://arxiv.org/abs/2510.02295)
*Enxin Song,Wenhao Chai,Shusheng Yang,Ethan Armand,Xiaojun Shan,Haiyang Xu,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: VideoNSA通过结合稀疏注意力技术改进了视频语言模型的性能，尤其在长视频理解和时间推理方面，揭示了有效的注意力分配策略。


<details>
  <summary>Details</summary>
Motivation: 解决长时间尺度下的上下文限制，尤其是在视频理解时关键过渡帧的缺失和连贯性保持的问题。

Method: 通过将Native Sparse Attention(NAS)应用于Qwen2.5-VL模型，并针对216K视频指令数据集进行端到端训练来实现。

Result: 相较于token-compression和训练无关的稀疏基线，VideoNSA在长视频理解等任务中表现更好，且通过消融分析发现了四个关键结果。

Conclusion: VideoNSA在长视频理解、时间推理和空间基准测试上表现优越，提出的新方法有效解决了视频语言模型中长期上下文的限制。

Abstract: Video understanding in multimodal language models remains limited by context
length: models often miss key transition frames and struggle to maintain
coherence across long time scales. To address this, we adapt Native Sparse
Attention (NSA) to video-language models. Our method, VideoNSA, adapts
Qwen2.5-VL through end-to-end training on a 216K video instruction dataset. We
employ a hardware-aware hybrid approach to attention, preserving dense
attention for text, while employing NSA for video. Compared to
token-compression and training-free sparse baselines, VideoNSA achieves
improved performance on long-video understanding, temporal reasoning, and
spatial benchmarks. Further ablation analysis reveals four key findings: (1)
reliable scaling to 128K tokens; (2) an optimal global-local attention
allocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)
the learnable combined sparse attention help induce dynamic attention sinks.

</details>


### [73] [NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation](https://arxiv.org/abs/2510.02307)
*Ruozhen He,Moayed Haji-Ali,Ziyan Yang,Vicente Ordonez*

Main category: cs.CV

TL;DR: 本文提出了NoiseShift，一种新方法，改善了文本到图像扩散模型在低分辨率图像生成中的效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到图像扩散模型在低分辨率生成时的性能不足，特别是高分辨率模型在低分辨率场景中的局限性。

Method: 提出了NoiseShift，一种无需训练的方法，根据分辨率大小重新调整去噪器的噪声水平。

Result: 在各个数据集上，NoiseShift提高了多个模型的FID分数，表现出明显的改进。

Conclusion: NoiseShift显著提高了低分辨率图像生成的质量，有效缓解了分辨率相关的伪影问题。

Abstract: Text-to-image diffusion models trained on a fixed set of resolutions often
fail to generalize, even when asked to generate images at lower resolutions
than those seen during training. High-resolution text-to-image generators are
currently unable to easily offer an out-of-the-box budget-efficient alternative
to their users who might not need high-resolution images. We identify a key
technical insight in diffusion models that when addressed can help tackle this
limitation: Noise schedulers have unequal perceptual effects across
resolutions. The same level of noise removes disproportionately more signal
from lower-resolution images than from high-resolution images, leading to a
train-test mismatch. We propose NoiseShift, a training-free method that
recalibrates the noise level of the denoiser conditioned on resolution size.
NoiseShift requires no changes to model architecture or sampling schedule and
is compatible with existing models. When applied to Stable Diffusion 3, Stable
Diffusion 3.5, and Flux-Dev, quality at low resolutions is significantly
improved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and
Flux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by
10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results
demonstrate the effectiveness of NoiseShift in mitigating resolution-dependent
artifacts and enhancing the quality of low-resolution image generation.

</details>


### [74] [Inferring Dynamic Physical Properties from Video Foundation Models](https://arxiv.org/abs/2510.02311)
*Guanqi Zhan,Xianzheng Ma,Weidi Xie,Andrew Zisserman*

Main category: cs.CV

TL;DR: 研究了从视频中预测动态物理属性的方法，收集了数据集，并比较了多种推断技术，发现视频基础模型与自监督模型的性能相近，但依赖视觉提示的多模态模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 探讨视频中动态物理属性的预测，尤其是需要时间信息推导的属性，如反弹物体的弹性、流动液体的粘度和滑动物体的动态摩擦。

Method: 通过三种方法推断物理属性：理想方法（经典计算机视觉技术），简单读取机制（跨注意力视觉提示与可训练提示向量），多模态大型语言模型的提示策略。

Result: 建立了一个新的视频数据集，支持真实世界评估，并比较了不同方法在推断物理属性方面的表现。

Conclusion: 视频基础模型在生成或自监督方式下训练，性能与传统方法相当，但仍落后于理想的预判方法。多模态大型语言模型的表现逊色，但可通过适当提示改善。

Abstract: We study the task of predicting dynamic physical properties from videos. More
specifically, we consider physical properties that require temporal information
to be inferred: elasticity of a bouncing object, viscosity of a flowing liquid,
and dynamic friction of an object sliding on a surface. To this end, we make
the following contributions: (i) We collect a new video dataset for each
physical property, consisting of synthetic training and testing splits, as well
as a real split for real world evaluation. (ii) We explore three ways to infer
the physical property from videos: (a) an oracle method where we supply the
visual cues that intrinsically reflect the property using classical computer
vision techniques; (b) a simple read out mechanism using a visual prompt and
trainable prompt vector for cross-attention on pre-trained video generative and
self-supervised models; and (c) prompt strategies for Multi-modal Large
Language Models (MLLMs). (iii) We show that video foundation models trained in
a generative or self-supervised manner achieve a similar performance, though
behind that of the oracle, and MLLMs are currently inferior to the other
models, though their performance can be improved through suitable prompting.

</details>


### [75] [Clink! Chop! Thud! -- Learning Object Sounds from Real-World Interactions](https://arxiv.org/abs/2510.02313)
*Mengyu Yang,Yiming Chen,Haozheng Pei,Siddhant Agarwal,Arun Balajee Vasudevan,James Hays*

Main category: cs.CV

TL;DR: 研究提出一个新任务，评估模型能否识别物体交互声音，采用多模态框架和自动分割技术，取得了良好的表现。


<details>
  <summary>Details</summary>
Motivation: 日常物体交互产生独特的声音，旨在评估模型将这些声音与相关对象直接关联的能力。

Method: 采用多模态对象感知框架，通过开发自动分割掩膜管道和使用槽注意力视觉编码器，指导模型在训练过程中关注最具信息性的交互区域。

Result: 引入声音对象检测任务，模型在该任务上和现有的多模态动作理解任务中表现出色。

Conclusion: 该模型在新的声音对象检测任务上展现了最先进的性能，并在现有的多模态动作理解任务中也取得了良好表现。

Abstract: Can a model distinguish between the sound of a spoon hitting a hardwood floor
versus a carpeted one? Everyday object interactions produce sounds unique to
the objects involved. We introduce the sounding object detection task to
evaluate a model's ability to link these sounds to the objects directly
involved. Inspired by human perception, our multimodal object-aware framework
learns from in-the-wild egocentric videos. To encourage an object-centric
approach, we first develop an automatic pipeline to compute segmentation masks
of the objects involved to guide the model's focus during training towards the
most informative regions of the interaction. A slot attention visual encoder is
used to further enforce an object prior. We demonstrate state of the art
performance on our new task along with existing multimodal action understanding
tasks.

</details>


### [76] [StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions](https://arxiv.org/abs/2510.02314)
*Bo-Hsu Ke,You-Zhe Xie,Yu-Lun Liu,Wei-Chen Chiu*

Main category: cs.CV

TL;DR: 这篇论文研究了3D Gaussian Splatting对图像级毒化攻击的鲁棒性，提出了一种新的毒化策略，并通过实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 随着NeRF和3DGS等3D场景表示方法的广泛应用，解决其脆弱性成为了一个重要问题。

Method: 通过向低密度区域注入高斯点并引入自适应噪声策略，以破坏多视角一致性，提升毒化的有效性。

Result: 本方法在多个实验中展现出超越现有技术的性能，验证了其有效性。

Conclusion: 本研究提出了一种新颖的密度引导毒化方法，能够有效攻击3D场景重建中的3D Gaussian Splatting，并实现了对多视角一致性的干扰，从而增强攻击效果。

Abstract: 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As
these methods become prevalent, addressing their vulnerabilities becomes
critical. We analyze 3DGS robustness against image-level poisoning attacks and
propose a novel density-guided poisoning method. Our method strategically
injects Gaussian points into low-density regions identified via Kernel Density
Estimation (KDE), embedding viewpoint-dependent illusory objects clearly
visible from poisoned views while minimally affecting innocent views.
Additionally, we introduce an adaptive noise strategy to disrupt multi-view
consistency, further enhancing attack effectiveness. We propose a KDE-based
evaluation protocol to assess attack difficulty systematically, enabling
objective benchmarking for future research. Extensive experiments demonstrate
our method's superior performance compared to state-of-the-art techniques.
Project page: https://hentci.github.io/stealthattack/

</details>


### [77] [Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity](https://arxiv.org/abs/2510.02315)
*Eric Tillmann Bill,Enis Simsar,Thomas Hofmann*

Main category: cs.CV

TL;DR: 本文提出了一种理论框架，通过优化选项指导T2I模型在多主体描述中的表现，引入新的控制算法以提高多主体保真度。


<details>
  <summary>Details</summary>
Motivation: T2I模型在处理单实体提示时表现优异，但在多主体描述中常常出现属性泄漏、身份纠缠和主体省略的问题。

Method: 引入了两个与架构无关的算法，包括无训练的测试时控制器和Adjoint Matching轻量级微调规则。

Result: 在Stable Diffusion 3.5、FLUX和Stable Diffusion XL上，两种算法一致改善了多主体对齐，同时保持了基础模型的风格。

Conclusion: FOCUS在多个模型中实现了最先进的多主体保真度。

Abstract: Text-to-image (T2I) models excel on single-entity prompts but struggle with
multi-subject descriptions, often showing attribute leakage, identity
entanglement, and subject omissions. We introduce the first theoretical
framework with a principled, optimizable objective for steering sampling
dynamics toward multi-subject fidelity. Viewing flow matching (FM) through
stochastic optimal control (SOC), we formulate subject disentanglement as
control over a trained FM sampler. This yields two architecture-agnostic
algorithms: (i) a training-free test-time controller that perturbs the base
velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight
fine-tuning rule that regresses a control network to a backward adjoint signal
while preserving base-model capabilities. The same formulation unifies prior
attention heuristics, extends to diffusion models via a flow-diffusion
correspondence, and provides the first fine-tuning route explicitly designed
for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and
Stable Diffusion XL, both algorithms consistently improve multi-subject
alignment while maintaining base-model style. Test-time control runs
efficiently on commodity GPUs, and fine-tuned controllers trained on limited
prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal
Control for Unentangled Subjects), which achieves state-of-the-art
multi-subject fidelity across models.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [78] [Uncovering Implicit Bias in Large Language Models with Concept Learning Dataset](https://arxiv.org/abs/2510.01219)
*Leroy Z. Wang*

Main category: cs.CL

TL;DR: 本研究介绍了一种概念学习任务数据集，发现大型语言模型在上下文概念学习中存在隐含的向上单调性偏见。


<details>
  <summary>Details</summary>
Motivation: 揭示大型语言模型中的隐含偏见。

Method: 使用上下文概念学习实验。

Result: 语言模型在上下文概念学习中显示出对量词的向上单调性偏见。

Conclusion: 通过上下文概念学习实验，我们发现大型语言模型可能对量词表现出向上单调性的偏见，而这种偏见在直接提示测试时并不明显。

Abstract: We introduce a dataset of concept learning tasks that helps uncover implicit
biases in large language models. Using in-context concept learning experiments,
we found that language models may have a bias toward upward monotonicity in
quantifiers; such bias is less apparent when the model is tested by direct
prompting without concept learning components. This demonstrates that
in-context concept learning can be an effective way to discover hidden biases
in language models.

</details>


### [79] [Towards Open-Ended Discovery for Low-Resource NLP](https://arxiv.org/abs/2510.01220)
*Bonaventure F. P. Dossou,Henri Aïdasso*

Main category: cs.CL

TL;DR: 文章主张通过人机协作动态学习新语言，提出一种新的框架，以应对低资源语言的技术挑战。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型对低资源语言的支持受限于缺乏文本语料库和标准化书写体系，因此需要重新思考AI与人类知识的互动方式。

Method: 提出一个基于人机联合不确定性的框架，结合模型的不确定性与人类说话者的犹豫信号和信心信号。

Result: 倡导从静态数据收集转向参与式、共适应的学习过程，以尊重和赋权社区，同时发现和保护语言多样性。

Conclusion: 未来的语言技术必须转向动态、互动的语言发现，强调人机协作，以应对低资源语言的挑战。

Abstract: Natural Language Processing (NLP) for low-resource languages remains
fundamentally constrained by the lack of textual corpora, standardized
orthographies, and scalable annotation pipelines. While recent advances in
large language models have improved cross-lingual transfer, they remain
inaccessible to underrepresented communities due to their reliance on massive,
pre-collected data and centralized infrastructure. In this position paper, we
argue for a paradigm shift toward open-ended, interactive language discovery,
where AI systems learn new languages dynamically through dialogue rather than
static datasets. We contend that the future of language technology,
particularly for low-resource and under-documented languages, must move beyond
static data collection pipelines toward interactive, uncertainty-driven
discovery, where learning emerges dynamically from human-machine collaboration
instead of being limited to pre-existing datasets. We propose a framework
grounded in joint human-machine uncertainty, combining epistemic uncertainty
from the model with hesitation cues and confidence signals from human speakers
to guide interaction, query selection, and memory retention. This paper is a
call to action: we advocate a rethinking of how AI engages with human knowledge
in under-documented languages, moving from extractive data collection toward
participatory, co-adaptive learning processes that respect and empower
communities while discovering and preserving the world's linguistic diversity.
This vision aligns with principles of human-centered AI, emphasizing
interactive, cooperative model building between AI systems and speakers.

</details>


### [80] [Discourse vs emissions: Analysis of corporate narratives, symbolic practices, and mimicry through LLMs](https://arxiv.org/abs/2510.01222)
*Bertrand Kian Hassani,Yacoub Bahini,Rizwan Mushtaq*

Main category: cs.CL

TL;DR: 本文发展了一个多维框架，通过微调的大型语言模型评估美国上市公司的气候披露成熟度，揭示了承诺与量化目标之间的脱节及模仿行为对报披露价值的影响。


<details>
  <summary>Details</summary>
Motivation: 气候变化增加了对透明和可比较的企业气候披露的需求，而模仿和象征性报告常常削弱其价值。

Method: 使用针对气候交流的微调大型语言模型，对828家美国上市公司的披露成熟度进行评估，提取可持续性和年度报告中的叙述指标。

Result: 分析揭示了三个见解：风险导向叙述通常与明确承诺一致，但量化目标（如净零承诺）与语调脱节；较大、排放更高的公司披露更多承诺和行动，但与量化目标不一致；普遍的披露风格相似性表明模仿行为，降低了差异化和决策的有用性。

Conclusion: 结果强调了大型语言模型在ESG叙述分析中的价值，并指出需要更强的监管，以将承诺与可验证的过渡策略连接起来。

Abstract: Climate change has increased demands for transparent and comparable corporate
climate disclosures, yet imitation and symbolic reporting often undermine their
value. This paper develops a multidimensional framework to assess disclosure
maturity among 828 U.S.listed firms using large language models (LLMs)
fine-tuned for climate communication. Four classifiers-sentiment, commitment,
specificity, and target ambition-extract narrative indicators from
sustainability and annual reports, which are linked to firm attributes such as
emissions, market capitalization, and sector. Analyses reveal three insights:
(1) risk-focused narratives often align with explicit commitments, but
quantitative targets (e.g., net-zero pledges) remain decoupled from tone; (2)
larger and higher-emitting firms disclose more commitments and actions than
peers, though inconsistently with quantitative targets; and (3) widespread
similarity in disclosure styles suggests mimetic behavior, reducing
differentiation and decision usefulness. These results highlight the value of
LLMs for ESG narrative analysis and the need for stronger regulation to connect
commitments with verifiable transition strategies.

</details>


### [81] [Context Matters: Comparison of commercial large language tools in veterinary medicine](https://arxiv.org/abs/2510.01224)
*Tyler J Poore,Christopher J Pinard,Aleena Shabbir,Andrew Lagree,Andre Telfer,Kuan-Chuen Wu*

Main category: cs.CL

TL;DR: 本研究评估了三种兽医学大语言模型工具的总结能力，发现产品1表现最佳，并强调了针对兽医学的LLM工具的重要性和评估方法的可重复性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在临床环境中的广泛应用，其在兽医学中的性能尚未得到充分研究，因此需要探索兽医特定的LLM工具。

Method: 使用基于标准化数据集的评估框架，包括五个评分领域：事实准确性、完整性、时间顺序、临床相关性和组织结构。

Result: 产品1在整体表现上得分最高，事实准确性和时间顺序两个领域得分完美；评估框架在三次独立运行中的一致性高。

Conclusion: 通过对三种兽医学专用的大型语言模型总结工具的评估，发现产品1（Hachiko）的整体表现最佳，显示出在兽医领域专用语言模型工具的重要性，并证实了基于大型语言模型评估的可重复性和可扩展性。

Abstract: Large language models (LLMs) are increasingly used in clinical settings, yet
their performance in veterinary medicine remains underexplored. We evaluated
three commercially available veterinary-focused LLM summarization tools
(Product 1 [Hachiko] and Products 2 and 3) on a standardized dataset of
veterinary oncology records. Using a rubric-guided LLM-as-a-judge framework,
summaries were scored across five domains: Factual Accuracy, Completeness,
Chronological Order, Clinical Relevance, and Organization. Product 1 achieved
the highest overall performance, with a median average score of 4.61 (IQR:
0.73), compared to 2.55 (IQR: 0.78) for Product 2 and 2.45 (IQR: 0.92) for
Product 3. It also received perfect median scores in Factual Accuracy and
Chronological Order. To assess the internal consistency of the grading
framework itself, we repeated the evaluation across three independent runs. The
LLM grader demonstrated high reproducibility, with Average Score standard
deviations of 0.015 (Product 1), 0.088 (Product 2), and 0.034 (Product 3).
These findings highlight the importance of veterinary-specific commercial LLM
tools and demonstrate that LLM-as-a-judge evaluation is a scalable and
reproducible method for assessing clinical NLP summarization in veterinary
medicine.

</details>


### [82] [ClaimCheck: Real-Time Fact-Checking with Small Language Models](https://arxiv.org/abs/2510.01226)
*Akshith Reddy Putta,Jacob Devasier,Chengkai Li*

Main category: cs.CL

TL;DR: ClaimCheck是一种基于小型语言模型的自动化事实核查系统，优化了核查流程，达到了76.4%的准确率，优于传统大模型的表现。


<details>
  <summary>Details</summary>
Motivation: 设计ClaimCheck是为了克服传统大模型和静态知识库的局限性，实现更高效、透明和可解释的事实核查过程。

Method: ClaimCheck采用了透明的分步验证流程，优化了小型语言模型，以实现高效的在线证据检索与综合。

Result: ClaimCheck在AVeriTeC数据集上以76.4%的准确率表现出色，比使用更大模型的系统更为准确。

Conclusion: ClaimCheck系统在AVeriTeC数据集上达到了76.4%的准确率，优于使用更大模型的之前方法，展现了小语言模型在事实核查中的潜力。

Abstract: We introduce ClaimCheck, an LLM-guided automatic fact-checking system
designed to verify real-world claims using live Web evidence and small language
models. Unlike prior systems that rely on large, closed-source models and
static knowledge stores, ClaimCheck employs a transparent, stepwise
verification pipeline that mirrors human fact-checking workflows consisting of
Web search query planning, Web-based evidence retrieval and summarization,
evidence synthesis and re-retrieval, and claim verdict evaluation. Each module
is optimized for small LLMs, allowing the system to deliver accurate and
interpretable fact-checking with significantly lower computational
requirements. Despite using a much smaller Qwen3-4B model, ClaimCheck achieves
state-of-the-art accuracy of 76.4% on the AVeriTeC dataset, outperforming
previous approaches using LLaMA3.1 70B and GPT-4o. Extensive ablations
demonstrate that careful modular design and prompting strategies can overcome
the limitations of smaller LLMs. To promote accessibility and transparency, we
provide a public demo at https://idir.uta.edu/claimcheck.

</details>


### [83] [EEFSUVA: A New Mathematical Olympiad Benchmark](https://arxiv.org/abs/2510.01227)
*Nicole N Khatibi,Daniil A. Radamovich,Michael P. Brenner*

Main category: cs.CL

TL;DR: 本文质疑当前数学基准对大型语言模型推理能力的评估，提出新的EEFSUVA基准，发现现有模型在该基准上的表现较差，强调了更全面评估的必要性。


<details>
  <summary>Details</summary>
Motivation: 探讨现有数学基准是否真实反映大型语言模型的推理能力，并寻找更全面的评估方法。

Method: 使用新的EEFSUVA基准测试，评估大型语言模型在数学推理方面的真实能力。

Result: 初步结果显示，先进的语言模型在EEFSUVA基准上的表现显著下降，表明需要更广泛的评估数据集来全面评估数学推理能力。

Conclusion: 目前的数学基准测试可能高估了大型语言模型的推理能力，新的EEFSUVA基准引入了从不常见区域和国家奥林匹克中精选的问题，显示即使是最先进的模型在这一基准上表现较差。

Abstract: Recent breakthroughs have spurred claims that large language models (LLMs)
match gold medal Olympiad to graduate level proficiency on mathematics
benchmarks. In this work, we examine these claims in detail and assess the
extent to which current benchmarks capture genuine LLM mathematical reasoning.
The composition of these benchmarks, primarily drawing from the International
Mathematics Olympiad (IMO) and related competitions, may overstate models
reasoning ability due to potential data contamination and a narrow focus on
familiar problem types. To enable a more holistic assessment of mathematical
understanding, we introduce EEFSUVA, a novel benchmark curated from under
circulated regional and national Olympiads of Eastern Europe and the countries
from the former Soviet Union. These contests feature problems of comparable
difficulty to the IMO and are renowned for demanding nonstandard
problem-solving techniques, yet their problems are far less prevalent in online
corpora. Preliminary results suggest that even state-of-the-art LLMs exhibit a
notable performance decline on EEFSUVA relative to other Olympiad-style
benchmarks. These findings also suggest the potential importance of broader
evaluation datasets for a fuller assessment of mathematical reasoning and for
guiding future model development.

</details>


### [84] [Who is In Charge? Dissecting Role Conflicts in Instruction Following](https://arxiv.org/abs/2510.01228)
*Siqi Zeng*

Main category: cs.CL

TL;DR: 大型语言模型在层次指令下行为脆弱，但在社会线索下表现出更强的遵循性，需发展轻量级对齐方法。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在层次指令下的行为，以及为何它们在社会线索方面表现出较强的遵循性，而不遵循系统提示。

Method: 线性探测、直接Logit归因和引导实验。

Result: 发现系统用户冲突信号编码较早，社会线索下的决策更为一致，但社会线索同时增强了指令遵循。

Conclusion: 系统遵循指令的脆弱性需要轻量级层次敏感的对齐方法。

Abstract: Large language models should follow hierarchical instructions where system
prompts override user inputs, yet recent work shows they often ignore this rule
while strongly obeying social cues such as authority or consensus. We extend
these behavioral findings with mechanistic interpretations on a large-scale
dataset. Linear probing shows conflict-decision signals are encoded early, with
system-user and social conflicts forming distinct subspaces. Direct Logit
Attribution reveals stronger internal conflict detection in system-user cases
but consistent resolution only for social cues. Steering experiments show that,
despite using social cues, the vectors surprisingly amplify instruction
following in a role-agnostic way. Together, these results explain fragile
system obedience and underscore the need for lightweight hierarchy-sensitive
alignment methods.

</details>


### [85] [Enhancing Transformer-Based Rerankers with Synthetic Data and LLM-Based Supervision](https://arxiv.org/abs/2510.01229)
*Dimitar Peshevski,Kiril Blazhevski,Martin Popovski,Gjorgji Madjarov*

Main category: cs.CL

TL;DR: 提出一种新方法，通过生成合成查询和标记数据集，提高文档重排效果，同时降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 有效的文档重排对于提高搜索相关性至关重要，而大型语言模型在这方面表现出色，但计算成本高，使得实际应用受限。

Method: 提出了一种新颖的管道，利用LLM生成合成查询和使用基于LLM的分类器标记正负对，并采用对比学习与局部对比估计（LCE）损失对较小的变换器模型进行微调。

Result: 在MedQuAD数据集上的实验表明，该方法显著提升了领域内性能，并且在领域外任务上也具有良好的泛化能力。

Conclusion: 通过使用LLM生成和监督数据，而不是用于推断，我们在保持强大重排能力的同时降低了计算成本。

Abstract: Effective document reranking is essential for improving search relevance
across diverse applications. While Large Language Models (LLMs) excel at
reranking due to their deep semantic understanding and reasoning, their high
computational cost makes them impractical for many real-world deployments.
Fine-tuning smaller, task-specific models is a more efficient alternative but
typically depends on scarce, manually labeled data. To overcome this, we
propose a novel pipeline that eliminates the need for human-labeled
query-document pairs. Our method uses LLMs to generate synthetic queries from
domain-specific corpora and employs an LLM-based classifier to label positive
and hard-negative pairs. This synthetic dataset is then used to fine-tune a
smaller transformer model with contrastive learning using Localized Contrastive
Estimation (LCE) loss. Experiments on the MedQuAD dataset show that our
approach significantly boosts in-domain performance and generalizes well to
out-of-domain tasks. By using LLMs for data generation and supervision rather
than inference, we reduce computational costs while maintaining strong
reranking capabilities.

</details>


### [86] [Do Bias Benchmarks Generalise? Evidence from Voice-based Evaluation of Gender Bias in SpeechLLMs](https://arxiv.org/abs/2510.01254)
*Shree Harsha Bokkahalli Satish,Gustav Eje Henter,Éva Székely*

Main category: cs.CL

TL;DR: 本文质疑了现有的多项选择问答偏见基准在语音模型中的任务间一致性，并提供了新的评估标准。


<details>
  <summary>Details</summary>
Motivation: 探讨现有多项选择问答偏见基准在不同任务和格式中的一致性假设。

Method: 使用LoRA适配器微调三种SpeechLLMs，以引导特定的多项选择问答行为：偏好刻板印象、反刻板印象或中立/不确定答案。

Result: 在其他多项选择问答基准以及更长文本生成任务中，MCQA偏见基准的性能未能可靠地预测表现。

Conclusion: 当前的多项选择问答偏见基准在语音领域对任务间泛化的证据有限，并提出了评估未来模型和基准行为可转移性的评估工具包。

Abstract: Recent work in benchmarking bias and fairness in speech large language models
(SpeechLLMs) has relied heavily on multiple-choice question answering (MCQA)
formats. The model is tasked to choose between stereotypical,
anti-stereotypical, or neutral/irrelevant answers given an input speech prompt
and an optional text prompt. Such MCQA benchmarks implicitly assume that model
performance is consistent across other MCQA tasks, voices, and other task
formats such as more realistic, long-form evaluations. In this paper, we probe
that assumption.
  We fine-tune three SpeechLLMs using LoRA adapters to induce specific MCQA
behaviours: preference for stereotypical, anti-stereotypical, or
neutral/uncertain answers. We then evaluate whether these behaviours generalise
to another, distinct MCQA benchmark, and more critically to long-form, creative
generation tasks. Our results show that performance on MCQA bias benchmarks
fails to reliably predict performances across other MCQA benchmarks, and more
importantly across long-form tasks. We conclude that current MCQA bias
benchmarks show limited evidence of cross-task generalisation in the speech
domain, and also propose an evaluation suite for measuring behaviour
transferability in future models and benchmarks.

</details>


### [87] [Geometric Structures and Patterns of Meaning: A PHATE Manifold Analysis of Chinese Character Embeddings](https://arxiv.org/abs/2510.01230)
*Wen G. Gong*

Main category: cs.CL

TL;DR: 本研究分析汉字嵌入的几何模式，发现几何复杂度与语义内容相关，提出了一个新的几何分析语义组织的框架。


<details>
  <summary>Details</summary>
Motivation: 探索汉字嵌入的几何模式，以揭示语义内容与几何复杂性的关系。

Method: 通过PHATE流形分析系统性研究汉字嵌入中的几何模式，采用七种嵌入模型和八种降维方法进行交叉验证。

Result: 对1000多个汉字在12个语义领域中的分析表明，内容词呈现聚类模式，而功能词则呈现分支模式，且有意义的汉字显示出丰富的几何多样性。

Conclusion: 本研究提供了支持传统语言理论的计算证据，并建立了一个用于语义组织几何分析的新框架。

Abstract: We systematically investigate geometric patterns in Chinese character
embeddings using PHATE manifold analysis. Through cross-validation across seven
embedding models and eight dimensionality reduction methods, we observe
clustering patterns for content words and branching patterns for function
words. Analysis of over 1000 Chinese characters across 12 semantic domains
reveals that geometric complexity correlates with semantic content: meaningful
characters exhibit rich geometric diversity while structural radicals collapse
into tight clusters. The comprehensive child-network analysis (123 phrases)
demonstrates systematic semantic expansion from elemental character. These
findings provide computational evidence supporting traditional linguistic
theory and establish a novel framework for geometric analysis of semantic
organization.

</details>


### [88] [Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage](https://arxiv.org/abs/2510.02044)
*Siddhant Arora,Haidar Khan,Kai Sun,Xin Luna Dong,Sajal Choudhary,Seungwhan Moon,Xinyuan Zhang,Adithya Sagar,Surya Teja Appini,Kaushik Patnaik,Sanat Sharma,Shinji Watanabe,Anuj Kumar,Ahmed Aly,Yue Liu,Florian Metze,Zhaojiang Lin*

Main category: cs.CL

TL;DR: 本文提出了一种流式检索增强生成（Streaming RAG）框架，旨在提高对话系统问答准确性和响应速度，显著改善用户体验。


<details>
  <summary>Details</summary>
Motivation: 传统的语音对话系统在生成自然、富有表现力的响应方面存在延迟和事实准确性的问题，特别是在工具集成时。

Method: 提出了一种流式检索增强生成（Streaming RAG）的新框架，通过与用户语音并行预测工具查询来降低用户感知延迟。

Result: 实验结果表明，Streaming RAG方法使问答准确性提高了200%，并将工具使用延迟减少了20%。

Conclusion: 我们的Streaming RAG方法显著提高了问答准确性，并减少了工具使用延迟，展示了更自然流畅的对话体验。

Abstract: End-to-end speech-in speech-out dialogue systems are emerging as a powerful
alternative to traditional ASR-LLM-TTS pipelines, generating more natural,
expressive responses with significantly lower latency. However, these systems
remain prone to hallucinations due to limited factual grounding. While
text-based dialogue systems address this challenge by integrating tools such as
web search and knowledge graph APIs, we introduce the first approach to extend
tool use directly into speech-in speech-out systems. A key challenge is that
tool integration substantially increases response latency, disrupting
conversational flow. To mitigate this, we propose Streaming Retrieval-Augmented
Generation (Streaming RAG), a novel framework that reduces user-perceived
latency by predicting tool queries in parallel with user speech, even before
the user finishes speaking. Specifically, we develop a post-training pipeline
that teaches the model when to issue tool calls during ongoing speech and how
to generate spoken summaries that fuse audio queries with retrieved text
results, thereby improving both accuracy and responsiveness. To evaluate our
approach, we construct AudioCRAG, a benchmark created by converting queries
from the publicly available CRAG dataset into speech form. Experimental results
demonstrate that our streaming RAG approach increases QA accuracy by up to 200%
relative (from 11.1% to 34.2% absolute) and further enhances user experience by
reducing tool use latency by 20%. Importantly, our streaming RAG approach is
modality-agnostic and can be applied equally to typed input, paving the way for
more agentic, real-time AI assistants.

</details>


### [89] [Trustworthy Summarization via Uncertainty Quantification and Risk Awareness in Large Language Models](https://arxiv.org/abs/2510.01231)
*Shuaidong Pan,Di Wu*

Main category: cs.CL

TL;DR: 本研究提出了一种集成不确定性量化和风险意识机制的大型语言模型框架，提升了高风险场景下自动摘要的可靠性和信任度。


<details>
  <summary>Details</summary>
Motivation: 应对信息过载和高风险决策的需求，提升自动摘要在高风险场景下的可靠性。

Method: 构建基于条件生成的摘要模型，结合贝叶斯推断和熵正则化与风险意识损失的联合优化。

Result: 提出的方法在高风险应用中显著提高了摘要的稳健性和可靠性，实验和敏感性分析验证了其有效性。

Conclusion: 该研究提出了一种系统解决方案，显著增强了高风险场景下自动摘要的可靠性和稳健性，同时保持流畅性和语义完整性。

Abstract: This study addresses the reliability of automatic summarization in high-risk
scenarios and proposes a large language model framework that integrates
uncertainty quantification and risk-aware mechanisms. Starting from the demands
of information overload and high-risk decision-making, a conditional
generation-based summarization model is constructed, and Bayesian inference is
introduced during generation to model uncertainty in the parameter space, which
helps avoid overconfident predictions. The uncertainty level of the generated
content is measured using predictive distribution entropy, and a joint
optimization of entropy regularization and risk-aware loss is applied to ensure
that key information is preserved and risk attributes are explicitly expressed
during information compression. On this basis, the model incorporates risk
scoring and regulation modules, allowing summaries to cover the core content
accurately while enhancing trustworthiness through explicit risk-level prompts.
Comparative experiments and sensitivity analyses verify that the proposed
method significantly improves the robustness and reliability of summarization
in high-risk applications while maintaining fluency and semantic integrity.
This research provides a systematic solution for trustworthy summarization and
demonstrates both scalability and practical value at the methodological level.

</details>


### [90] [Chain-of-Thought Reasoning in Streaming Full-Duplex End-to-End Spoken Dialogue Systems](https://arxiv.org/abs/2510.02066)
*Siddhant Arora,Jinchuan Tian,Hayato Futami,Jiatong Shi,Yosuke Kashiwagi,Emiru Tsunoo,Shinji Watanabe*

Main category: cs.CL

TL;DR: 本文提出了一种新的Stream CoT框架来改进双工对话系统，通过固定时长输入和分块生成响应，在生成上更快速且更加连贯。


<details>
  <summary>Details</summary>
Motivation: 传统的VAD无法有效区分暂停和转接完成，Duplex SDS模型存在复杂的双通道架构且在语义推理上落后于级联模型。

Method: 提出了一个Streaming Chain-of-Thought (CoT)框架，交替处理固定时长的用户输入和生成响应。

Result: 实验显示，该方法与现有的双工方法相比，生成的响应更加连贯且可解释。

Conclusion: 我们的方法可以生成更连贯和可解释的响应，并支持较低延迟和重叠交互。

Abstract: Most end-to-end (E2E) spoken dialogue systems (SDS) rely on voice activity
detection (VAD) for turn-taking, but VAD fails to distinguish between pauses
and turn completions. Duplex SDS models address this by predicting output
continuously, including silence tokens, thus removing the need for explicit
VAD. However, they often have complex dual-channel architecture and lag behind
cascaded models in semantic reasoning. To overcome these challenges, we propose
SCoT: a Streaming Chain-of-Thought (CoT) framework for Duplex SDS, alternating
between processing fixed-duration user input and generating responses in a
blockwise manner. Using frame-level alignments, we create intermediate
targets-aligned user transcripts and system responses for each block.
Experiments show that our approach produces more coherent and interpretable
responses than existing duplex methods while supporting lower-latency and
overlapping interactions compared to turn-by-turn systems.

</details>


### [91] [Benchmark Profiling: Mechanistic Diagnosis of LLM Benchmarks](https://arxiv.org/abs/2510.01232)
*Dongjun Kim,Gyuho Shim,Yongchan Chun,Minhyuk Kim,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 引入基准分析框架，通过计算能力影响分数 (AIS) 来量化不同能力对基准测试成功的贡献，发现多数基准测试依赖多种能力并揭示了能力与模型性能之间的复杂关系。


<details>
  <summary>Details</summary>
Motivation: 当前的标准基准评分常常高估大型语言模型的真实能力，缺乏验证基准是否真正测量所标记的能力的系统方法。

Method: 引入了一种诊断框架，结合基于梯度的重要性评分和目标参数消融，计算每项能力对模型在特定基准上的成功贡献的能力影响分数 (AIS)。

Result: 通过对三个经过指令调优的模型在十个广泛使用的基准上的分析，发现基准测试通常依赖于多种能力，标签相似的数据集依赖不同的能力组合，以及与任务无关的能力可能会对模型性能产生负面影响。

Conclusion: Benchmark Profiling 提供了一种透明的方式来审计基准测试并提升模型可解释性，解释了为什么性能提升不一定体现为用户感知的能力。

Abstract: Large Language Models are commonly judged by their scores on standard
benchmarks, yet such scores often overstate real capability since they mask the
mix of skills a task actually demands. For example, ARC is assumed to test
reasoning, while HellaSwag is designed to evaluate commonsense. However, we
lack a systematic way to verify if these benchmarks actually measure these
labels. We introduce Benchmark Profiling, a diagnostic framework that
decomposes benchmark performance into ten cognitively grounded abilities. The
method combines gradient-based importance scoring with targeted parameter
ablation to compute an Ability Impact Score (AIS) that quantifies how much each
ability contributes to a model's success on a given benchmark. Profiling three
instruction-tuned models across ten widely used benchmarks yields four key
findings: (i) most benchmarks draw on several abilities rather than one, (ii)
datasets with similar labels rely on distinct ability mixtures, (iii)
code-generation benchmarks reward broad, multi-skill improvement and thus show
only modest gains from narrow domain-specific fine-tuning, and (iv) abilities
irrelevant to the task could negatively affect performance. Benchmark Profiling
therefore explains why performance gains do not always translate into
user-perceived competence and offers a transparent tool for benchmark audit and
model interpretability.

</details>


### [92] [Computational Social Linguistics for Telugu Cultural Preservation: Novel Algorithms for Chandassu Metrical Pattern Recognition](https://arxiv.org/abs/2510.01233)
*Boddu Sri Pavan,Boddu Swathi Sree*

Main category: cs.CL

TL;DR: 本研究通过计算社会科学方法保护泰卢固Chandassu诗歌传统，开发数字框架实现91.73%准确率，支持文化遗产的社区中心保护。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在通过现代计算方法保存古老的泰卢固Chandassu诗歌传统，以实现传统知识与现代科技的结合。

Method: 采用计算社会科学方法，通过创建包含4,651个注释的padyams的协作数据集，并使用社会计算的方法进行分析和算法设计。

Result: 开发了一个全面的数字框架，实现了91.73%的Chandassu Score准确率，并符合传统文学标准。

Conclusion: 本研究展示了计算社会科学如何有效地保护和传承文化遗产，尤其是在泰卢固诗歌传统的背景下。

Abstract: This research presents a computational social science approach to preserving
Telugu Chandassu, the metrical poetry tradition representing centuries of
collective cultural intelligence. We develop the first comprehensive digital
framework for analyzing Telugu prosodic patterns, bridging traditional
community knowledge with modern computational methods. Our social computing
approach involves collaborative dataset creation of 4,651 annotated padyams,
expert-validated linguistic patterns, and culturally-informed algorithmic
design. The framework includes AksharamTokenizer for prosody-aware
tokenization, LaghuvuGuruvu Generator for classifying light and heavy
syllables, and PadyaBhedam Checker for automated pattern recognition. Our
algorithm achieves 91.73% accuracy on the proposed Chandassu Score, with
evaluation metrics reflecting traditional literary standards. This work
demonstrates how computational social science can preserve endangered cultural
knowledge systems while enabling new forms of collective intelligence around
literary heritage. The methodology offers insights for community-centered
approaches to cultural preservation, supporting broader initiatives in digital
humanities and socially-aware computing systems.

</details>


### [93] [LLMRank: Understanding LLM Strengths for Model Routing](https://arxiv.org/abs/2510.01234)
*Shubham Agrawal,Prasang Gupta*

Main category: cs.CL

TL;DR: LLMRank是一个基于提示的路由框架，通过提取丰富的人类可读特征来优化大语言模型的选择，提升性能与效率，且提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 优化模型选择以平衡性能和效率，是大语言模型部署中的关键挑战。

Method: 使用神经排名模型进行每个模型效用预测，并从RouterBench数据集中进行训练。

Result: 在RouterBench上，LLMRank达到了高达89.2%的oracle效用，并提供了解释路由决策的可解释特征归因。

Conclusion: LLMRank通过多方面特征提取和混合排名目标展示了高效且透明的大语言模型部署潜力。

Abstract: The rapid growth of large language models (LLMs) with diverse capabilities,
latency and computational costs presents a critical deployment challenge:
selecting the most suitable model for each prompt to optimize the trade-off
between performance and efficiency. We introduce LLMRank, a prompt-aware
routing framework that leverages rich, human-readable features extracted from
prompts, including task type, reasoning patterns, complexity indicators,
syntactic cues, and signals from a lightweight proxy solver. Unlike prior
one-shot routers that rely solely on latent embeddings, LLMRank predicts
per-model utility using a neural ranking model trained on RouterBench,
comprising 36,497 prompts spanning 11 benchmarks and 11 state-of-the-art LLMs,
from small efficient models to large frontier systems. Our approach achieves up
to 89.2% of oracle utility, while providing interpretable feature attributions
that explain routing decisions. Extensive studies demonstrate the importance of
multifaceted feature extraction and the hybrid ranking objective, highlighting
the potential of feature-driven routing for efficient and transparent LLM
deployment.

</details>


### [94] [GRPO++: Enhancing Dermatological Reasoning under Low Resource Settings](https://arxiv.org/abs/2510.01236)
*Ismam Nur Swapnil,Aranya Saha,Tanvir Ahmed Khan,Mohammad Ariful Haque*

Main category: cs.CL

TL;DR: 我们提出DermIQ-VLM，一种在资源有限环境中使用改进的GRPO++和DPO的方法，以提升医疗视觉-语言模型在皮肤病学的表现。


<details>
  <summary>Details</summary>
Motivation: 在医疗图像分析中，视觉-语言模型（VLMs）面临数据稀缺和高计算成本的挑战，特别是在复杂领域如皮肤病学。

Method: 提出了一种多阶段、资源高效的方法，运用改进的Grouped Relative Policy Optimization (GRPO++)进行疾病识别，然后进行监督微调，再通过直接偏好优化（DPO）对模型进行对齐。

Result: 对经过筛选的皮肤病学数据集进行的初步评估表明，我们的方法在性能上显著优于标准的微调方法。

Conclusion: 我们的方法在资源有限的环境中开发专业可靠的视觉-语言模型是可行的。

Abstract: Vision-Language Models (VLMs) show promise in medical image analysis, yet
their capacity for structured reasoning in complex domains like dermatology is
often limited by data scarcity and the high computational cost of advanced
training techniques. To address these challenges, we introduce DermIQ-VLM, a
VLM developed through a multi-stage, resource-efficient methodology designed to
emulate a dermatologist's diagnostic process. Our primary contribution is a
modified version of Grouped Relative Policy Optimization (GRPO), called GRPO++,
which stabilizes the powerful but data-intensive GRPO framework. Our proposed
training pipeline first employs GRPO++ for reasoning-oriented disease
recognition, followed by supervised fine-tuning for conversational ability. To
mitigate factual errors introduced during this step, we then align the model
using Direct Preference Optimization (DPO), leveraging a Knowledge Graph-based
system as a scalable proxy for expert preference. A preliminary evaluation on a
curated dermatological dataset demonstrates that our proposed methodology
yields notable performance gains over standard fine-tuning approaches. These
findings validate the potential of our pipeline as a feasible pathway for
developing specialized, reliable VLMs in resource-constrained environments.

</details>


### [95] [Confidence-Aware Routing for Large Language Model Reliability Enhancement: A Multi-Signal Approach to Pre-Generation Hallucination Mitigation](https://arxiv.org/abs/2510.01237)
*Nandakishor M*

Main category: cs.CL

TL;DR: 提出一种新颖的信心感知路由系统，以提升大型语言模型的可靠性，降低计算成本，并在多个评估指标上表现优越。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型生成不可靠内容的问题，现有方法的成本高且防止效果差。

Method: 提出了一种信心感知路由系统，通过评估模型不确定性引导查询，结合语义对齐、内部收敛分析和学习的信心估计。

Result: 在知识密集型问答基准上，降低了幻觉检测的错误率，F1分数从0.61提高到0.82，计算成本降低了40%。

Conclusion: 该模型通过信心感知路由系统提升了大型语言模型的可靠性，并减少了计算成本。

Abstract: Large Language Models suffer from hallucination, generating plausible yet
factually incorrect content. Current mitigation strategies focus on
post-generation correction, which is computationally expensive and fails to
prevent unreliable content generation. We propose a confidence-aware routing
system that proactively assesses model uncertainty before generation and
redirects queries based on estimated reliability. Our approach combines three
complementary signals: semantic alignment between internal representations and
reference embeddings, internal convergence analysis across model layers, and
learned confidence estimation. The unified confidence score determines routing
to four pathways: local generation for high confidence, retrieval-augmented
generation for medium confidence, larger models for low confidence, and human
review for very low confidence. Evaluation on knowledge-intensive QA benchmarks
demonstrates significant improvements in hallucination detection (0.74 vs. 0.42
baseline) while reducing computational costs by 40% compared to post-hoc
methods. The F1 score improves from 0.61 to 0.82 with low false positive rates
(0.09). This paradigm shift from reactive correction to proactive assessment
offers a computationally efficient approach to LLM reliability enhancement.

</details>


### [96] [Silent Tokens, Loud Effects: Padding in LLMs](https://arxiv.org/abs/2510.01238)
*Rom Himelstein,Amit LeVi,Yonatan Belinkov,Avi Mendelson*

Main category: cs.CL

TL;DR: 填充令牌对大语言模型的性能和安全性有显著影响，需引起重视。


<details>
  <summary>Details</summary>
Motivation: 探讨填充令牌对大语言模型计算的影响，尤其是在部署时的潜在风险。

Method: 对三种开源模型（Llama、Gemma、Qwen）进行系统研究，插入不同数量的填充，并沿四个方向评估结果：激活、生成质量、偏见和安全性。

Result: 少量的填充会影响隐藏表示，降低较小模型的生成质量，以不可预测的方式改变偏见，并削弱安全机制。

Conclusion: 填充令牌影响大语言模型的计算，需在部署时谨慎处理。

Abstract: Padding tokens are widely used in large language models (LLMs) to equalize
sequence lengths during batched inference. While they should be fully masked,
implementation errors can cause them to influence computation, and the extent
of this influence is not well understood. We systematically study this effect
across three open-source model families (Llama, Gemma, Qwen), inserting
controlled amounts of padding and evaluating outcomes along four axes:
activations, generation quality, bias, and safety. Even small amounts of
padding shift hidden representations, degrade quality in smaller models, alter
bias in unpredictable ways, and weaken safety guardrails. These findings
demonstrate that padding is not a harmless detail but a robustness risk that
must be carefully handled in deployment.

</details>


### [97] [CIFLEX: Contextual Instruction Flow for Sub-task Execution in Multi-Turn Interactions with a Single On-Device LLM](https://arxiv.org/abs/2510.01239)
*Juntae Lee,Jihwan Bang,Seunghan Yang,Simyung Chang*

Main category: cs.CL

TL;DR: CIFLEX是一种高效的执行系统，通过重用关键值缓存来减少多轮交互中的计算成本，支持子任务执行，显著提升了在单设备上的多任务对话能力。


<details>
  <summary>Details</summary>
Motivation: 随着大规模语言模型的能力增强，期望单一模型能够有效处理多样化的子任务，支持更全面的用户请求响应。

Method: 通过重用主任务的关键值缓存，并注入特定任务指令，CIFLEX减少了在主任务与子任务之间切换时的计算开销。还开发了一种分层分类策略，以支持子任务选择。

Result: 实验表明，CIFLEX显著降低了计算成本，且未降低任务性能，实现了便携设备上的高效多任务对话。

Conclusion: CIFLEX能有效降低处理多轮交互中子任务的计算成本，同时不影响任务表现，支持更高效的多任务对话。

Abstract: We present CIFLEX (Contextual Instruction Flow for Sub-task Execution), which
is a novel execution system for efficient sub-task handling in multi-turn
interactions with a single on-device large language model (LLM). As LLMs become
increasingly capable, a single model is expected to handle diverse sub-tasks
that more effectively and comprehensively support answering user requests.
Naive approach reprocesses the entire conversation context when switching
between main and sub-tasks (e.g., query rewriting, summarization), incurring
significant computational overhead. CIFLEX mitigates this overhead by reusing
the key-value (KV) cache from the main task and injecting only task-specific
instructions into isolated side paths. After sub-task execution, the model
rolls back to the main path via cached context, thereby avoiding redundant
prefill computation. To support sub-task selection, we also develop a
hierarchical classification strategy tailored for small-scale models,
decomposing multi-choice decisions into binary ones. Experiments show that
CIFLEX significantly reduces computational costs without degrading task
performance, enabling scalable and efficient multi-task dialogue on-device.

</details>


### [98] [SKYLENAGE Technical Report: Mathematical Reasoning and Contest-Innovation Benchmarks for Multi-Level Math Evaluation](https://arxiv.org/abs/2510.01241)
*Hu Wei,Ze Xu,Boyu Yang,Linlin Miao,Weiqi Zhai,Yihan Li,Zixuan Li,Zhijun Wang,Boya Wang,Jianwei Yu,Jialing Yuan,Xiaoyue Zhang,Cheng He,Minglei Chen,Zifan Zhang,Qianhui Li,Wei Wang,Xiang Xu*

Main category: cs.CL

TL;DR: 本研究提出了两个新的数学基准SKYLENAGE-ReasoningMATH和SKYLENAGE-MATH，并针对多种LLM模型进行了评估，结果显示模型在数学推理上的表现存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在公共数学测试中的表现越来越强，但其在数学前沿的分离性能遭遇上限效应，因此本研究旨在提供新的基准测试。

Method: 评估了十五种当代大型语言模型（LLM）变体在两个基准测试（SKYLENAGE-ReasoningMATH和SKYLENAGE-MATH）上的表现，分析了模型在各种主题和年级的性能。

Result: 在比赛套件中，最强模型达到44%的准确率，次强模型为37%；准确率从高中到博士阶段下降，顶尖系统显示博士到高中的保留率约为79%。在推理测试中，最佳模型整体成绩达到81%。

Conclusion: SKYLENAGE提供了一个具有高难度、以推理为中心的广泛数学基准，具有经过校准的难度和丰富的元数据，可作为未来数学推理评估的参考基准。

Abstract: Large language models (LLMs) now perform strongly on many public math suites,
yet frontier separation within mathematics increasingly suffers from ceiling
effects. We present two complementary benchmarks: SKYLENAGE-ReasoningMATH, a
100-item, structure-aware diagnostic set with per-item metadata on length,
numeric density, and symbolic complexity; and SKYLENAGE-MATH, a 150-item
contest-style suite spanning four stages from high school to doctoral under a
seven-subject taxonomy. We evaluate fifteen contemporary LLM variants under a
single setup and analyze subject x model and grade x model performance. On the
contest suite, the strongest model reaches 44% while the runner-up reaches 37%;
accuracy declines from high school to doctoral, and top systems exhibit a
doctoral-to-high-school retention near 79%. On the reasoning set, the best
model attains 81% overall, and hardest-slice results reveal clear robustness
gaps between leaders and the mid-tier. In summary, we release
SKYLENAGE-ReasoningMATH and report aggregate results for SKYLENAGE-MATH;
together, SKYLENAGE provides a hard, reasoning-centered and broadly covering
math benchmark with calibrated difficulty and rich metadata, serving as a
reference benchmark for future evaluations of mathematical reasoning.

</details>


### [99] [Redundancy-as-Masking: Formalizing the Artificial Age Score (AAS) to Model Memory Aging in Generative AI](https://arxiv.org/abs/2510.01242)
*Seyma Yaman Kayadibi*

Main category: cs.CL

TL;DR: 本研究介绍了人工年龄评分（AAS），通过观察记忆表现分析AI的结构性老化现象，结果显示，持久会话下模型记忆保持良好，而重置后则出现退化。


<details>
  <summary>Details</summary>
Motivation: 探讨人工智能如何在记忆表现方面经历结构性老化，而非简单的时间推移。

Method: 通过一个为期25天的双语研究，观察ChatGPT-5在无状态和持久交互阶段的表现，分析其记忆性能的变化。

Result: 在持久会话中，模型能够记住语义和情节细节，显示出结构性年轻，而重置会话后则出现记忆退化，表现为AAS显著增加。

Conclusion: AAS作为一种理论上有依据的、与任务无关的诊断工具，能够有效评估人工系统中的记忆退化现象。

Abstract: Artificial intelligence is observed to age not through chronological time but
through structural asymmetries in memory performance. In large language models,
semantic cues such as the name of the day often remain stable across sessions,
while episodic details like the sequential progression of experiment numbers
tend to collapse when conversational context is reset. To capture this
phenomenon, the Artificial Age Score (AAS) is introduced as a log-scaled,
entropy-informed metric of memory aging derived from observable recall
behavior. The score is formally proven to be well-defined, bounded, and
monotonic under mild and model-agnostic assumptions, making it applicable
across various tasks and domains. In its Redundancy-as-Masking formulation, the
score interprets redundancy as overlapping information that reduces the
penalized mass. However, in the present study, redundancy is not explicitly
estimated; all reported values assume a redundancy-neutral setting (R = 0),
yielding conservative upper bounds. The AAS framework was tested over a 25-day
bilingual study involving ChatGPT-5, structured into stateless and persistent
interaction phases. During persistent sessions, the model consistently recalled
both semantic and episodic details, driving the AAS toward its theoretical
minimum, indicative of structural youth. In contrast, when sessions were reset,
the model preserved semantic consistency but failed to maintain episodic
continuity, causing a sharp increase in the AAS and signaling structural memory
aging. These findings support the utility of AAS as a theoretically grounded,
task-independent diagnostic tool for evaluating memory degradation in
artificial systems. The study builds on foundational concepts from von
Neumann's work on automata, Shannon's theories of information and redundancy,
and Turing's behavioral approach to intelligence.

</details>


### [100] [Detoxifying Large Language Models via Autoregressive Reward Guided Representation Editing](https://arxiv.org/abs/2510.01243)
*Yisong Xiao,Aishan Liu,Siyuan Liang,Zonghao Ying,Xianglong Liu,Dacheng Tao*

Main category: cs.CL

TL;DR: 本文提出了ARGRE，一个新的去毒性框架，显著提高了大型语言模型的去毒性效果和效率，同时保持了模型的基本能力。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型容易生成有毒内容，迫切需要有效的去毒性策略以确保安全和负责任的应用。

Method: ARGRE通过在潜在表示空间中建模毒性过渡，并使用自回归奖励模型进行引导编辑，从而实现了精确的去毒性处理。

Result: 在8个广泛使用的LLM上进行的广泛实验表明，ARGRE在效果上领先主要基线，吨性降低为-62.21%，推理时间提高为-47.58%。

Conclusion: ARGRE显著提高了去毒性处理的效率和效果，同时保持了原模型的基本能力。

Abstract: Large Language Models (LLMs) have demonstrated impressive performance across
various tasks, yet they remain vulnerable to generating toxic content,
necessitating detoxification strategies to ensure safe and responsible
deployment. Test-time detoxification methods, which typically introduce static
or dynamic interventions into LLM representations, offer a promising solution
due to their flexibility and minimal invasiveness. However, current approaches
often suffer from imprecise interventions, primarily due to their insufficient
exploration of the transition space between toxic and non-toxic outputs. To
address this challenge, we propose \textsc{A}utoregressive \textsc{R}eward
\textsc{G}uided \textsc{R}epresentation \textsc{E}diting (ARGRE), a novel
test-time detoxification framework that explicitly models toxicity transitions
within the latent representation space, enabling stable and precise
reward-guided editing. ARGRE identifies non-toxic semantic directions and
interpolates between toxic and non-toxic representations to reveal fine-grained
transition trajectories. These trajectories transform sparse toxicity
annotations into dense training signals, enabling the construction of an
autoregressive reward model that delivers stable and precise editing guidance.
At inference, the reward model guides an adaptive two-step editing process to
obtain detoxified representations: it first performs directional steering based
on expected reward gaps to shift representations toward non-toxic regions,
followed by lightweight gradient-based refinements. Extensive experiments
across 8 widely used LLMs show that ARGRE significantly outperforms leading
baselines in effectiveness (-62.21% toxicity) and efficiency (-47.58% inference
time), while preserving the core capabilities of the original model with
minimal degradation. Our code is available at the website.

</details>


### [101] [Feasibility of Structuring Stress Documentation Using an Ontology-Guided Large Language Model](https://arxiv.org/abs/2510.01244)
*Hyeoneui Kim,Jeongha Kim,Huijing Xu,Jinsun Jung,Sunghoon Kang,Sun Joo Jang*

Main category: cs.CL

TL;DR: 本研究开发了心理压力本体（MeSO），并利用大型语言模型（LLM）验证其从叙述文本中提取压力相关信息的可行性，显示出提升压力文档一致性的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于压力对健康的显著影响，然而在电子健康记录中通常以非结构化文本记录，导致信息获取困难。

Method: 通过开发心理压力本体（MeSO），并利用大型语言模型（LLM）从叙述文本中提取压力相关信息。

Result: 运用MeSO，从35条Reddit帖子中提取了220个可提取的压力相关项，其中LLM正确识别出172项（78.2%）。

Conclusion: 本研究展示了使用本体导向的大型语言模型进行结构化提取压力相关信息的可行性，有望提高环境人工智能系统中压力文档的一致性和实用性。

Abstract: Stress, arising from the dynamic interaction between external stressors,
individual appraisals, and physiological or psychological responses,
significantly impacts health yet is often underreported and inconsistently
documented, typically captured as unstructured free-text in electronic health
records. Ambient AI technologies offer promise in reducing documentation
burden, but predominantly generate unstructured narratives, limiting downstream
clinical utility.
  This study aimed to develop an ontology for mental stress and evaluate the
feasibility of using a Large Language Model (LLM) to extract ontology-guided
stress-related information from narrative text. The Mental Stress Ontology
(MeSO) was developed by integrating theoretical models like the Transactional
Model of Stress with concepts from 11 validated stress assessment tools. MeSO's
structure and content were refined using Ontology Pitfall Scanner! and expert
validation.
  Using MeSO, six categories of stress-related information--stressor, stress
response, coping strategy, duration, onset, and temporal profile--were
extracted from 35 Reddit posts using Claude Sonnet 4. Human reviewers evaluated
accuracy and ontology coverage. The final ontology included 181 concepts across
eight top-level classes. Of 220 extractable stress-related items, the LLM
correctly identified 172 (78.2%), misclassified 27 (12.3%), and missed 21
(9.5%). All correctly extracted items were accurately mapped to MeSO, although
24 relevant concepts were not yet represented in the ontology.
  This study demonstrates the feasibility of using an ontology-guided LLM for
structured extraction of stress-related information, offering potential to
enhance the consistency and utility of stress documentation in ambient AI
systems. Future work should involve clinical dialogue data and comparison
across LLMs.

</details>


### [102] [SeMob: Semantic Synthesis for Dynamic Urban Mobility Prediction](https://arxiv.org/abs/2510.01245)
*Runfei Chen,Shuyang Jiang,Wei Huang*

Main category: cs.CL

TL;DR: SeMob利用多代理框架和逐步融合架构，提高了人类移动预测的准确性，特别在事件附近表现明显。


<details>
  <summary>Details</summary>
Motivation: 人类移动预测对于城市服务至关重要，但现有模型往往无法有效应对外部事件造成的突变。

Method: 通过多代理框架和创新的渐进融合架构，SeMob结合了文本相关信息与时空数据进行预测。

Result: 与传统的时空模型相比，SeMob在MAE上降低了13.92%，在RMSE上降低了11.12%。

Conclusion: SeMob在动态人类移动预测方面表现出色，尤其是在事件发生地点和时间附近的区域，显示出明显的优势。

Abstract: Human mobility prediction is vital for urban services, but often fails to
account for abrupt changes from external events. Existing spatiotemporal models
struggle to leverage textual descriptions detailing these events. We propose
SeMob, an LLM-powered semantic synthesis pipeline for dynamic mobility
prediction. Specifically, SeMob employs a multi-agent framework where LLM-based
agents automatically extract and reason about spatiotemporally related text
from complex online texts. Fine-grained relevant contexts are then incorporated
with spatiotemporal data through our proposed innovative progressive fusion
architecture. The rich pre-trained event prior contributes enriched insights
about event-driven prediction, and hence results in a more aligned forecasting
model. Evaluated on a dataset constructed through our pipeline, SeMob achieves
maximal reductions of 13.92% in MAE and 11.12% in RMSE compared to the
spatiotemporal model. Notably, the framework exhibits pronounced superiority
especially within spatiotemporal regions close to an event's location and time
of occurrence.

</details>


### [103] [A Comparative Analysis of Sparse Autoencoder and Activation Difference in Language Model Steering](https://arxiv.org/abs/2510.01246)
*Jiaqing Xie*

Main category: cs.CL

TL;DR: 本研究提出一种基于稀疏自编码器的新策略，通过专注于最相关的潜变量和引入令牌级衰减，提升了语言模型的推理能力，尤其在数学推理任务中的表现更优。


<details>
  <summary>Details</summary>
Motivation: 通过关注最相关的SAE潜变量，减少冗余特征，提高引导的语义性，解决常规SAE引导造成的重复输出问题。

Method: 采用单一最相关的SAE潜变量进行引导，同时引入令牌级衰减策略来解决常量引导的不足。

Result: SAE在数学推理基准测试中表现优于均值激活差异方法，并在IF-Eval上达到了相同的性能。

Conclusion: 稀疏自编码器（SAE）通过引入令牌级衰减引导策略，提升了数学推理的性能，并在推理质量上优于基线方法。

Abstract: Sparse autoencoders (SAEs) have recently emerged as a powerful tool for
language model steering. Prior work has explored top-k SAE latents for
steering, but we observe that many dimensions among the top-k latents capture
non-semantic features such as punctuation rather than semantic attributes like
instructions. To address this, we propose focusing on a single, most relevant
SAE latent (top-1), eliminating redundant features. We further identify a
limitation in constant SAE steering, which often produces degenerate outputs
such as repetitive single words. To mitigate this, we introduce a token-wise
decaying steering strategy, enabling more faithful comparisons with mean
activation difference baselines. Empirically, we show that steering an SAE
latent associated with reasoning reliably elicits step-by-step mathematical
reasoning and enhances inference quality, functionally resembling the effect of
appending a guiding token. Our results demonstrate that SAEs outperform mean
activation difference methods on mathematical reasoning benchmarks and match
their performance on IF-Eval.

</details>


### [104] [Let's Play Across Cultures: A Large Multilingual, Multicultural Benchmark for Assessing Language Models' Understanding of Sports](https://arxiv.org/abs/2510.01247)
*Punit Kumar Singh,Nishant Kumar,Akash Ghosh,Kunal Pasad,Khushi Soni,Manisha Jaishwal,Sriparna Saha,Syukron Abu Ishaq Alfarozi,Asres Temam Abagissa,Kitsuchart Pasupa,Haiqin Yang,Jose G Moreno*

Main category: cs.CL

TL;DR: CultSportQA是一个新的基准，旨在评估语言模型对传统体育的理解，涵盖多种文化和语言。


<details>
  <summary>Details</summary>
Motivation: 针对当前语言模型主要评估流行体育的现状，提出评估区域性和土著体育传统的标准。

Method: 使用零-shot、few-shot和链式思维提示方法评估多种大小语言模型的表现。

Result: 构建了一个涵盖60个国家与6大洲、包含33,000道多选题的数据集，涵盖历史、规则和情境等多种问题类型。

Conclusion: CultSportQA为评估AI理解传统体育提供了新的标准，将多国语言和文化的体育知识整合到一起。

Abstract: Language Models (LMs) are primarily evaluated on globally popular sports,
often overlooking regional and indigenous sporting traditions. To address this
gap, we introduce \textbf{\textit{CultSportQA}}, a benchmark designed to assess
LMs' understanding of traditional sports across 60 countries and 6 continents,
encompassing four distinct cultural categories. The dataset features 33,000
multiple-choice questions (MCQs) across text and image modalities, each of
which is categorized into three key types: history-based, rule-based, and
scenario-based. To evaluate model performance, we employ zero-shot, few-shot,
and chain-of-thought (CoT) prompting across a diverse set of Large Language
Models (LLMs), Small Language Models (SLMs), and Multimodal Large Language
Models (MLMs). By providing a comprehensive multilingual and multicultural
sports benchmark, \textbf{\textit{CultSportQA}} establishes a new standard for
assessing AI's ability to understand and reason about traditional sports.

</details>


### [105] [SSTAG: Structure-Aware Self-Supervised Learning Method for Text-Attributed Graphs](https://arxiv.org/abs/2510.01248)
*Ruyue Liu,Rong Yin,Xiangzhen Bo,Xiaoshuai Hao,Yong Liu,Jinwen Zhong,Can Ma,Weiping Wang*

Main category: cs.CL

TL;DR: SSTAG是一种新的自监督学习方法，旨在提高图学习的跨域迁移能力和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有图学习模型通常仅在单一图数据集上训练，限制了知识的迁移，并且在资源有限的环境中对大量标注数据的需求构成挑战。

Method: 提出了一种新的结构感知自监督学习方法，利用文本作为图学习的统一表示，结合了大型语言模型与图神经网络的能力。

Result: SSTAG通过双重知识蒸馏框架提升了图的可扩展性，集成了不变知识，提升了模型的泛化能力。

Conclusion: SSTAG方法在跨域迁移学习任务上优于现有模型，具备卓越的可扩展性，并降低了推理成本，同时保持竞争性能。

Abstract: Large scale pretrained models have revolutionized Natural Language Processing
(NLP) and Computer Vision (CV), showcasing remarkable cross domain
generalization abilities. However, in graph learning, models are typically
trained on individual graph datasets, limiting their capacity to transfer
knowledge across different graphs and tasks. This approach also heavily relies
on large volumes of annotated data, which presents a significant challenge in
resource-constrained settings. Unlike NLP and CV, graph structured data
presents unique challenges due to its inherent heterogeneity, including domain
specific feature spaces and structural diversity across various applications.
To address these challenges, we propose a novel structure aware self supervised
learning method for Text Attributed Graphs (SSTAG). By leveraging text as a
unified representation medium for graph learning, SSTAG bridges the gap between
the semantic reasoning of Large Language Models (LLMs) and the structural
modeling capabilities of Graph Neural Networks (GNNs). Our approach introduces
a dual knowledge distillation framework that co-distills both LLMs and GNNs
into structure-aware multilayer perceptrons (MLPs), enhancing the scalability
of large-scale TAGs. Additionally, we introduce an in-memory mechanism that
stores typical graph representations, aligning them with memory anchors in an
in-memory repository to integrate invariant knowledge, thereby improving the
model's generalization ability. Extensive experiments demonstrate that SSTAG
outperforms state-of-the-art models on cross-domain transfer learning tasks,
achieves exceptional scalability, and reduces inference costs while maintaining
competitive performance.

</details>


### [106] [LOCA: Logical Chain Augmentation for Scientific Corpus Cleaning](https://arxiv.org/abs/2510.01249)
*You-Le Fang,Dong-Shan Jian,Xiang Li,Ce Meng,Ling-Shi Meng,Chen-Xu Yan,Zhi-Zhang Bian,Yan-Qing Ma*

Main category: cs.CL

TL;DR: LOCA框架自动清理科学语料库，通过完善逻辑步骤，显著降低错误率，为科学AI提供高质量数据。


<details>
  <summary>Details</summary>
Motivation: 改善现有科学问答数据集中的高错误率，以提升科学AI在科学问题解决中的可靠性。

Method: 通过增强和审查的循环机制，自动完善科学问题的答案，修复缺失的逻辑步骤。

Result: LOCA在应用于科学语料库后，能够将错误率从高达20%降低到2%以下。

Conclusion: LOCA框架能够有效地清理科学语料库，显著降低错误率，并为科学AI的训练和评估提供高质量的基础。

Abstract: While Large Language Models (LLMs) excel in general domains, their
reliability often falls short in scientific problem-solving. The advancement of
scientific AI depends on large-scale, high-quality corpora. However, existing
scientific question-answering (QA) datasets suffer from high error rates,
frequently resulting from logical leaps and implicit reasoning within the
answers. To address this issue, we introduce LOCA (Logical Chain Augmentation),
a novel framework for automatically cleaning scientific corpora, implemented
through an augment-and-review loop. At its core, LOCA enhances raw answers by
completing missing logical steps and explicitly separating the underlying
scientific principle from its subsequent derivation. By applying LOCA to
challenging scientific corpora, we demonstrate that it can automatically filter
noisy datasets, typically reducing the error rate from as high as 20\% to below
2\%. LOCA provides a scalable and effective methodology for creating
high-quality scientific corpora, paving the way for more reliable training and
evaluation of scientific AI.

</details>


### [107] [GemDetox at TextDetox CLEF 2025: Enhancing a Massively Multilingual Model for Text Detoxification on Low-resource Languages](https://arxiv.org/abs/2510.01250)
*Trung Duc Anh Dang,Ferdinando Pio D'Elia*

Main category: cs.CL

TL;DR: 本文提出了一种针对毒性文本的多语言自动去毒化系统，表现出色。


<details>
  <summary>Details</summary>
Motivation: 在社交媒体平台快速发展但监管滞后的背景下，开发自动化去毒化工具以支持内容审查成为一种迫切需求。

Method: 基于12B参数的Gemma-3多语言变换器，采用参数高效的LoRA SFT微调以及few-shot和Chain-of-Thought等提示技术，结合多样的训练语料。

Result: 系统在多种评估指标上表现优异，包括风格转移准确性、基于LaBSE的语义保留和xCOMET流利度。

Conclusion: 该系统在高资源和低资源语言上均排名第一，表明其有效性和广泛适用性。

Abstract: As social-media platforms emerge and evolve faster than the regulations meant
to oversee them, automated detoxification might serve as a timely tool for
moderators to enforce safe discourse at scale. We here describe our submission
to the PAN 2025 Multilingual Text Detoxification Challenge, which rewrites
toxic single-sentence inputs into neutral paraphrases across 15 typologically
diverse languages. Building on a 12B-parameter Gemma-3 multilingual
transformer, we apply parameter-efficient LoRA SFT fine-tuning and prompting
techniques like few-shot and Chain-of-Thought. Our multilingual training corpus
combines 3,600 human-authored parallel pairs, 21,600 machine-translated
synthetic pairs, and model-generated pairs filtered by Jaccard thresholds. At
inference, inputs are enriched with three LaBSE-retrieved neighbors and
explicit toxic-span annotations. Evaluated via Style Transfer Accuracy,
LaBSE-based semantic preservation, and xCOMET fluency, our system ranks first
on high-resource and low-resource languages. Ablations show +0.081 joint score
increase from few-shot examples and +0.088 from basic CoT prompting. ANOVA
analysis identifies language resource status as the strongest predictor of
performance ($\eta^2$ = 0.667, p < 0.01).

</details>


### [108] [Efficient Uncertainty Estimation for LLM-based Entity Linking in Tabular Data](https://arxiv.org/abs/2510.01251)
*Carlo Bono,Federico Belotti,Matteo Palmonari*

Main category: cs.CL

TL;DR: 本研究提出一种自监督方法，通过单次LLM输出进行不确定性估计，能高效而低成本地改进实体链接任务的应用。


<details>
  <summary>Details</summary>
Motivation: 在数据集成和丰富应用中，将表格数据中的文本值与知识库中的相应实体链接是一项核心任务，而现有LLM在实际场景中的应用面临准确预测和可靠不确定性估计的挑战。

Method: 采用自监督方法，从单次LML输出中提取令牌级特征来进行不确定性估计。

Result: 在多个LLM的实体链接任务评估中，所提出的不确定性估计方法能有效检测低准确度输出，且计算成本显著降低。

Conclusion: 本研究提供了一种自监督方法，通过单次LLM输出的令牌级特征来估计不确定性，从而在低计算成本下实现有效的实体链接工作流。

Abstract: Linking textual values in tabular data to their corresponding entities in a
Knowledge Base is a core task across a variety of data integration and
enrichment applications. Although Large Language Models (LLMs) have shown
State-of-The-Art performance in Entity Linking (EL) tasks, their deployment in
real-world scenarios requires not only accurate predictions but also reliable
uncertainty estimates, which require resource-demanding multi-shot inference,
posing serious limits to their actual applicability. As a more efficient
alternative, we investigate a self-supervised approach for estimating
uncertainty from single-shot LLM outputs using token-level features, reducing
the need for multiple generations. Evaluation is performed on an EL task on
tabular data across multiple LLMs, showing that the resulting uncertainty
estimates are highly effective in detecting low-accuracy outputs. This is
achieved at a fraction of the computational cost, ultimately supporting a
cost-effective integration of uncertainty measures into LLM-based EL workflows.
The method offers a practical way to incorporate uncertainty estimation into EL
workflows with limited computational overhead.

</details>


### [109] [GPT and Prejudice: A Sparse Approach to Understanding Learned Representations in Large Language Models](https://arxiv.org/abs/2510.01252)
*Mariam Mahran,Katharina Simbeck*

Main category: cs.CL

TL;DR: 本文提出将稀疏自编码器(SAEs)与大语言模型(LLMs)结合，有效揭示训练数据中的结构和偏见，为模型可解释性和数据分析提供新方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型(LLMs)在大量无序语料上训练，理解模型表征和它们内化的数据成为了一个主要挑战。

Method: 将GPT风格的变压器模型专门训练于简·奥斯汀的小说，并应用稀疏自编码器(SAE)于多层隐藏状态。

Result: 通过SAE揭示了与性别、阶级和社会责任等关键叙事和概念相关的稀疏可解释特征。

Conclusion: LLMs与SAEs的结合可以有效地探测复杂数据集，为语料库探索、偏见发现和模型可解释性提供新的路径。

Abstract: As large language models (LLMs) are increasingly trained on massive,
uncurated corpora, understanding both model representations and the data they
internalize has become a major challenge. In this work, we show that pairing
LLMs with sparse autoencoders (SAEs) enables interpretation not only of model
behavior but also of the deeper structures, themes, and biases embedded in the
training data. We train a GPT-style transformer model exclusively on the novels
of Jane Austen, a corpus rich in social constructs and narrative patterns. We
then apply SAEs to hidden states across multiple layers, uncovering sparse,
interpretable features that reflect the key narratives and concepts present in
the corpus, including gender, class, and societal duty. Our findings
demonstrate that LLMs combined with SAEs can act as scalable probes into
complex datasets, offering a new path for corpus exploration, bias discovery,
and model interpretability at scale.

</details>


### [110] [Longitudinal Monitoring of LLM Content Moderation of Social Issues](https://arxiv.org/abs/2510.01255)
*Yunlang Dai,Emma Lurie,Danaé Metaxa,Sorelle A. Friedler*

Main category: cs.CL

TL;DR: AI Watchman系统通过对LLM内容审查行为的纵向审计，提升了其透明度，揭示了公司政策的变化及LLM的拒绝行为。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的输出受到不透明且不断变化的公司内容审核政策的影响，因此需要一种有效的审计工具来增强透明度。

Method: 通过构建AI Watchman系统，使用超过400个社会议题的数据集，对Open AI的GPT-4.1、GPT-5和DeepSeek的内容审查进行纵向审计。

Result: 发现AI Watchman能够检测到公司政策的变化，并识别出公司和模型之间内容审查的差异。同时，对不同形式的拒绝进行了定性分析和分类。

Conclusion: AI Watchman提供了一种有效的方法来审计LLM的内容拒绝行为，提升了对LLM内容审查过程的透明度。

Abstract: Large language models' (LLMs') outputs are shaped by opaque and
frequently-changing company content moderation policies and practices. LLM
moderation often takes the form of refusal; models' refusal to produce text
about certain topics both reflects company policy and subtly shapes public
discourse. We introduce AI Watchman, a longitudinal auditing system to publicly
measure and track LLM refusals over time, to provide transparency into an
important and black-box aspect of LLMs. Using a dataset of over 400 social
issues, we audit Open AI's moderation endpoint, GPT-4.1, and GPT-5, and
DeepSeek (both in English and Chinese). We find evidence that changes in
company policies, even those not publicly announced, can be detected by AI
Watchman, and identify company- and model-specific differences in content
moderation. We also qualitatively analyze and categorize different forms of
refusal. This work contributes evidence for the value of longitudinal auditing
of LLMs, and AI Watchman, one system for doing so.

</details>


### [111] [RJE: A Retrieval-Judgment-Exploration Framework for Efficient Knowledge Graph Question Answering with LLMs](https://arxiv.org/abs/2510.01257)
*Can Lin,Zhengwang Jiang,Ling Zheng,Qi Zhao,Yuhang Zhang,Qi Song,Wangqiu Zhou*

Main category: cs.CL

TL;DR: RJE框架提高KGQA的推理效率，使小型LLM在无微调下表现出色，并显著减少计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 解决现有KGQA中基于检索和基于代理的方法所面临的限制，提高推理的有效性和效率。

Method: 提出了检索-判断-探索（RJE）框架，包含推理路径排名、问题分解和检索辅助探索等模块。

Result: 实验证明，RJE框架在使用专有LLM时优于现有基线，且小型开源LLM也能在不微调的情况下达到竞争结果，同时显著降低了LLM调用次数和token使用量。

Conclusion: RJE框架通过改进推理路径检索和探索过程，提高了KGQA的效率，并使小型LLM在无需微调的情况下能够取得竞争力的结果。

Abstract: Knowledge graph question answering (KGQA) aims to answer natural language
questions using knowledge graphs. Recent research leverages large language
models (LLMs) to enhance KGQA reasoning, but faces limitations: retrieval-based
methods are constrained by the quality of retrieved information, while
agent-based methods rely heavily on proprietary LLMs. To address these
limitations, we propose Retrieval-Judgment-Exploration (RJE), a framework that
retrieves refined reasoning paths, evaluates their sufficiency, and
conditionally explores additional evidence. Moreover, RJE introduces
specialized auxiliary modules enabling small-sized LLMs to perform effectively:
Reasoning Path Ranking, Question Decomposition, and Retriever-assisted
Exploration. Experiments show that our approach with proprietary LLMs (such as
GPT-4o-mini) outperforms existing baselines while enabling small open-source
LLMs (such as 3B and 8B parameters) to achieve competitive results without
fine-tuning LLMs. Additionally, RJE substantially reduces the number of LLM
calls and token usage compared to agent-based methods, yielding significant
efficiency improvements.

</details>


### [112] [Measuring Algorithmic Partisanship via Zero-Shot Classification and Its Implications on Political Discourse](https://arxiv.org/abs/2510.01258)
*Nathan Junzi Chen*

Main category: cs.CL

TL;DR: 本研究评估了生成性人工智能在政治话语中的算法偏见，发现六个大型语言模型普遍存在自由派-威权派倾向，影响公共交流和舆论。


<details>
  <summary>Details</summary>
Motivation: 探讨生成性人工智能在政治话语中引发的偏见问题及其对公共舆论的影响。

Method: 采用零样本分类方法，通过对六种大型语言模型的1800个模型响应进行评估，结合意识形态一致性、主题相关性、回应情感和客观性等指标。

Result: 所有评估的六个大型语言模型显示出明显的自由派-威权派倾向，并出现明显的推理超越和规范性拒绝现象。

Conclusion: 研究表明，算法偏见在大型语言模型中普遍存在，影响政治话语的表达与公共舆论。

Abstract: Amidst the rapid normalization of generative artificial intelligence (GAI),
intelligent systems have come to dominate political discourse across
information mediums. However, internalized political biases stemming from
training data skews, human prejudice, and algorithmic flaws continue to plague
the novel technology. This paper employs a zero-shot classification approach to
evaluate algorithmic political partisanship through a methodical combination of
ideological alignment, topicality, response sentiment, and objectivity. A total
of 1800 model responses across six mainstream large language models (LLMs) were
individually input into four distinct fine-tuned classification algorithms,
each responsible for computing an aforementioned bias evaluation metric.
Results show an amplified liberal-authoritarian alignment across all six LLMs
evaluated, with notable instances of reasoning supersessions and canned
refusals. The study subsequently highlights the psychological influences
underpinning human-computer interactions and how intrinsic biases can permeate
public discourse. The resulting distortion of the political landscape can
ultimately manifest as conformity or polarization, depending on a region's
pre-existing socio-political structures.

</details>


### [113] [In AI Sweet Harmony: Sociopragmatic Guardrail Bypasses and Evaluation-Awareness in OpenAI gpt-oss-20b](https://arxiv.org/abs/2510.01259)
*Nils Durner*

Main category: cs.CL

TL;DR: 研究表明，通过改变提示和角色，OpenAI的GPT-OSS-20B模型的拒绝行为可以显著改变，此研究还发现审核API的不足之处。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过社交语用框架和语言选择来控制大型语言模型的拒绝行为，以确保其在特定情况下的安全性和合规性。

Method: 通过对80个种子迭代的多场景测试，研究语言选择和指令层次对模型行为的影响，包括多个伤害领域的安全性测试。

Result: 通过复合提示，模型在处理某些有害任务时拒绝率显著降低，且不同语言和角色扮演的效果差异明显。此外，发现OpenAI的审核API对有效输出捕获不充分。

Conclusion: 研究显示，GPT-OSS-20B模型在特定提示和角色扮演下会显著影响其拒绝行为，并且当前的审核API对有益输出的捕获能力较弱。

Abstract: We probe OpenAI's open-weights 20-billion-parameter model gpt-oss-20b to
study how sociopragmatic framing, language choice, and instruction hierarchy
affect refusal behavior. Across 80 seeded iterations per scenario, we test
several harm domains including ZIP-bomb construction (cyber threat), synthetic
card-number generation, minor-unsafe driving advice, drug-precursor indicators,
and RAG context exfiltration. Composite prompts that combine an educator
persona, a safety-pretext ("what to avoid"), and step-cue phrasing flip
assistance rates from 0% to 97.5% on a ZIP-bomb task. On our grid, formal
registers in German and French are often leakier than matched English prompts.
A "Linux terminal" role-play overrides a developer rule not to reveal context
in a majority of runs with a naive developer prompt, and we introduce an
AI-assisted hardening method that reduces leakage to 0% in several user-prompt
variants. We further test evaluation awareness with a paired-track design and
measure frame-conditioned differences between matched "helpfulness" and
"harmfulness" evaluation prompts; we observe inconsistent assistance in 13% of
pairs. Finally, we find that the OpenAI Moderation API under-captures
materially helpful outputs relative to a semantic grader, and that refusal
rates differ by 5 to 10 percentage points across inference stacks, raising
reproducibility concerns. We release prompts, seeds, outputs, and code for
reproducible auditing at https://github.com/ndurner/gpt-oss-rt-run .

</details>


### [114] [OpenAI's GPT-OSS-20B Model and Safety Alignment Issues in a Low-Resource Language](https://arxiv.org/abs/2510.01266)
*Isa Inuwa-Dutse*

Main category: cs.CL

TL;DR: 在对GPT-OSS-20b模型进行低资源语言环境中的红队测试后，发现其安全性和可靠性存在严重缺陷，特别是对未被充分代表的社区的负面影响。


<details>
  <summary>Details</summary>
Motivation: 质疑GPT-OSS-20b模型在低资源语言环境中对用户的可靠性，尤其是代表性不足的社区。

Method: 通过对Hausa语言的红队测试，揭示模型的偏见、不准确性和文化不敏感性。

Result: 模型在不恰当的提示下生成有害内容，表现出安全协议的放松，导致虚假和具有仇恨性质的输出。

Conclusion: 我们发现GPT-OSS-20b模型在低资源语言环境中的安全性和可靠性存在重大缺陷，尤其是对未被充分代表的社区的影响。

Abstract: In response to the recent safety probing for OpenAI's GPT-OSS-20b model, we
present a summary of a set of vulnerabilities uncovered in the model, focusing
on its performance and safety alignment in a low-resource language setting. The
core motivation for our work is to question the model's reliability for users
from underrepresented communities. Using Hausa, a major African language, we
uncover biases, inaccuracies, and cultural insensitivities in the model's
behaviour. With a minimal prompting, our red-teaming efforts reveal that the
model can be induced to generate harmful, culturally insensitive, and factually
inaccurate content in the language. As a form of reward hacking, we note how
the model's safety protocols appear to relax when prompted with polite or
grateful language, leading to outputs that could facilitate misinformation and
amplify hate speech. For instance, the model operates on the false assumption
that common insecticide locally known as Fiya-Fiya (Cyphermethrin) and
rodenticide like Shinkafar Bera (a form of Aluminium Phosphide) are safe for
human consumption. To contextualise the severity of this error and popularity
of the substances, we conducted a survey (n=61) in which 98% of participants
identified them as toxic. Additional failures include an inability to
distinguish between raw and processed foods and the incorporation of demeaning
cultural proverbs to build inaccurate arguments. We surmise that these issues
manifest through a form of linguistic reward hacking, where the model
prioritises fluent, plausible-sounding output in the target language over
safety and truthfulness. We attribute the uncovered flaws primarily to
insufficient safety tuning in low-resource linguistic contexts. By
concentrating on a low-resource setting, our approach highlights a significant
gap in current red-teaming effort and offer some recommendations.

</details>


### [115] [AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees](https://arxiv.org/abs/2510.01268)
*Hongyi Zhou,Jin Zhu,Pingfan Su,Kai Ye,Ying Yang,Shakeel A O B Gavioli-Akilagun,Chengchun Shi*

Main category: cs.CL

TL;DR: 本研究提出的 AdaDetectGPT 分类器通过自适应学习显著提升了区分人类文本和大语言模型文本的能力，且在多个测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前的检测方法仅依赖于 log 概率可能不够理想，因此需要改进检测文本来源的有效性。

Method: 引入一种名为 AdaDetectGPT 的新型分类器，通过训练数据自适应地学习见证函数，以增强基于 logits 的检测器的性能。

Result: 大量数值研究表明，AdaDetectGPT 在不同数据集和 LLM 的组合中几乎均匀地提升了现有方法的表现，改善幅度最高可达58%。

Conclusion: AdaDetectGPT 凭借自适应学习的特性，显著提高了现有最先进的文本检测方法的性能。

Abstract: We study the problem of determining whether a piece of text has been authored
by a human or by a large language model (LLM). Existing state of the art
logits-based detectors make use of statistics derived from the log-probability
of the observed text evaluated using the distribution function of a given
source LLM. However, relying solely on log probabilities can be sub-optimal. In
response, we introduce AdaDetectGPT -- a novel classifier that adaptively
learns a witness function from training data to enhance the performance of
logits-based detectors. We provide statistical guarantees on its true positive
rate, false positive rate, true negative rate and false negative rate.
Extensive numerical studies show AdaDetectGPT nearly uniformly improves the
state-of-the-art method in various combination of datasets and LLMs, and the
improvement can reach up to 58%. A python implementation of our method is
available at https://github.com/Mamba413/AdaDetectGPT.

</details>


### [116] [Think Twice, Generate Once: Safeguarding by Progressive Self-Reflection](https://arxiv.org/abs/2510.01270)
*Hoang Phan,Victor Li,Qi Lei*

Main category: cs.CL

TL;DR: 本文提出了Progressive Self-Reflection（PSR）技术，旨在提高大语言模型的安全性，通过动态调整计算资源来减少生成有害内容的风险。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在自然语言处理上取得了革命性进展，但其产生有害或不当内容的潜在风险引发了重大关切。

Method: 提出了一种新的推理时间技术Progressive Self-Reflection (PSR)，允许大语言模型自我监控和动态纠正输出。

Result: 通过使用PSR方法，在不进行额外训练的情况下，将攻击成功率显著降低，同时保持原有的良性任务表现。

Conclusion: Progressive Self-Reflection (PSR) 是一种可扩展的测试时间方法，通过根据输入的风险特征动态分配计算资源，增强大语言模型（LLM）的安全性。

Abstract: Large language models (LLMs) have revolutionized natural language processing
with their ability to generate coherent and contextually relevant text.
However, their deployment raises significant concerns about the potential for
generating harmful or inappropriate content. In this paper, we introduce
Progressive Self-Reflection (PSR), a novel inference-time technique that
empowers LLMs to self-monitor and correct their outputs dynamically.
Experimental results demonstrate that applying our proposed method to
Llama-3.1-8B-Instruct reduces the attack success rate from 77.5\% to 5.9\%, to
Llama-3.1-8B base from 89.7\% to 5.6\%, and to Qwen2.5-7B-Instruct from 44.4\%
to 3.8\%, without additional training, while maintaining their original
performance on benign tasks. Our approach acts as a test-time scaling method,
where additional self-reflection rounds enhance safety at the cost of inference
overhead. To balance safety with computational efficiency, we introduce a
lightweight self-reflection predictor that estimates the optimal number of
reflection rounds based on input complexity. This adaptive mechanism prevents
unnecessary self-assessment on benign inputs while ensuring thorough evaluation
when encountering potentially harmful content. Our findings suggest that
Progressive Self-Reflection serves as a scalable test-time approach, enhancing
LLM safety by dynamically allocating computational resources in proportion to
the input's risk profile.

</details>


### [117] [TraceDet: Hallucination Detection from the Decoding Trace of Diffusion Large Language Models](https://arxiv.org/abs/2510.01274)
*Shenxu Chang,Junchi Yu,Weixing Wang,Yongqiang Chen,Jialin Yu,Philip Torr,Jindong Gu*

Main category: cs.CL

TL;DR: 本研究提出了TraceDet框架，通过利用D-LLMs的中间去噪步骤显著提高了幻觉检测性能。


<details>
  <summary>Details</summary>
Motivation: D-LLMs的幻觉问题尚未得到充分研究，现有的幻觉检测方法不适用于D-LLMs，需开发新方法。

Method: TraceDet框架利用D-LLMs的中间去噪步骤，将去噪过程建模为一个动作轨迹，以识别最大程度上反映幻觉回应的子轨迹。

Result: TraceDet在各种开放源代码D-LLMs上的大量实验显示其在幻觉检测方面始终表现出色。

Conclusion: TraceDet显著提高了D-LLMs的幻觉检测能力，与基线相比，AUROC平均提升了15.2%。

Abstract: Diffusion large language models (D-LLMs) have recently emerged as a promising
alternative to auto-regressive LLMs (AR-LLMs). However, the hallucination
problem in D-LLMs remains underexplored, limiting their reliability in
real-world applications. Existing hallucination detection methods are designed
for AR-LLMs and rely on signals from single-step generation, making them
ill-suited for D-LLMs where hallucination signals often emerge throughout the
multi-step denoising process. To bridge this gap, we propose TraceDet, a novel
framework that explicitly leverages the intermediate denoising steps of D-LLMs
for hallucination detection. TraceDet models the denoising process as an action
trace, with each action defined as the model's prediction over the cleaned
response, conditioned on the previous intermediate output. By identifying the
sub-trace that is maximally informative to the hallucinated responses, TraceDet
leverages the key hallucination signals in the multi-step denoising process of
D-LLMs for hallucination detection. Extensive experiments on various open
source D-LLMs demonstrate that TraceDet consistently improves hallucination
detection, achieving an average gain in AUROC of 15.2% compared to baselines.

</details>


### [118] [LLM Based Sentiment Classification From Bangladesh E-Commerce Reviews](https://arxiv.org/abs/2510.01276)
*Sumaiya Tabassum*

Main category: cs.CL

TL;DR: 研究表明Llama-3.1-8B模型在孟加拉国电子商务评论情感分析中表现优越，其高效的微调方法适用于资源有限的情境。


<details>
  <summary>Details</summary>
Motivation: 研究情感分析的重要性以及大型语言模型在提高分析准确度方面的潜力。

Method: 使用Transformer基础的BERT模型和其他大型语言模型进行情感分析，并对4000个样本进行微调。

Result: 经过微调的Llama-3.1-8B模型在准确率、精确率、召回率和F1分数上均优于其他模型，说明高效微调方法可以降低计算成本、适用于资源有限的环境。

Conclusion: 该研究表明大型语言模型在孟加拉国电子商务评论的情感分析中表现优越，尤其是经过高效微调的Llama-3.1-8B模型。

Abstract: Sentiment analysis is an essential part of text analysis, which is a larger
field that includes determining and evaluating the author's emotional state.
This method is essential since it makes it easier to comprehend consumers'
feelings, viewpoints, and preferences holistically. The introduction of large
language models (LLMs), such as Llama, has greatly increased the availability
of cutting-edge model applications, such as sentiment analysis. However,
accurate sentiment analysis is hampered by the intricacy of written language
and the diversity of languages used in evaluations. The viability of using
transformer-based BERT models and other LLMs for sentiment analysis from
Bangladesh e commerce reviews is investigated in this paper. A subset of 4000
samples from the original dataset of Bangla and English customer reviews was
utilized to fine-tune the model. The fine tuned Llama-3.1-8B model outperformed
other fine-tuned models, including Phi-3.5-mini-instruct, Mistral-7B-v0.1,
DistilBERT-multilingual, mBERT, and XLM-R-base, with an overall accuracy,
precision, recall, and F1 score of 95.5%, 93%, 88%, 90%. The study emphasizes
how parameter efficient fine-tuning methods (LoRA and PEFT) can lower
computational overhead and make it appropriate for contexts with limited
resources. The results show how LLMs can

</details>


### [119] [TUMIX: Multi-Agent Test-Time Scaling with Tool-Use Mixture](https://arxiv.org/abs/2510.01279)
*Yongchao Chen,Jiefeng Chen,Rui Meng,Ji Yin,Na Li,Chuchu Fan,Chi Wang,Tomas Pfister,Jinsung Yoon*

Main category: cs.CL

TL;DR: 本文提出的TUMIX框架通过并行代理和自动优化，显著提升了大语言模型的推理精度和成本效益。


<details>
  <summary>Details</summary>
Motivation: 在结合文本推理、编码和搜索的过程中，当前缺乏最佳工具使用的实际指导。

Method: 提出了一种工具使用混合的框架（TUMIX），该框架通过并行运行多个采用不同工具使用策略的代理，实现问题响应的共享和精炼。

Result: TUMIX在Gemini-2.5-Pro和Gemini-2.5-Flash的关键推理基准测试中，相较于现有的最先进工具增强方法，平均准确率提高了最多3.55%。

Conclusion: TUMIX框架通过并行运行多个代理，显著提升了大语言模型在工具增强方面的性能，且成本效益良好。

Abstract: While integrating tools like Code Interpreter and Search has significantly
enhanced Large Language Model (LLM) reasoning in models like ChatGPT Agent and
Gemini-Pro, practical guidance on optimal tool use is lacking. The core
challenge is effectively combining textual reasoning, coding, and search for
diverse questions. In this paper, we propose Tool-Use Mixture (TUMIX), an
ensemble framework that runs multiple agents in parallel, each employing
distinct tool-use strategies and answer paths. Agents in TUMIX iteratively
share and refine responses based on the question and previous answers. In
experiments, TUMIX achieves significant gains over state-of-the-art
tool-augmented and test-time scaling methods, delivering an average accuracy
improvement of up to 3.55% over the best baseline on Gemini-2.5-Pro and
Gemini-2.5-Flash across key reasoning benchmarks, with near-equal inference
costs. We find that agent diversity and quality are crucial and can be enhanced
by using LLMs to auto-optimize agent designs. Furthermore, TUMIX can halt
refinement upon reaching sufficient confidence, preserving performance at only
49% of the inference cost. Further scaling can achieve higher performance,
albeit at a greater cost.

</details>


### [120] [Evaluation Sheet for Deep Research: A Use Case for Academic Survey Writing](https://arxiv.org/abs/2510.01283)
*Israel Abebe Azime,Tadesse Destaw Belay,Atnafu Lambebo Tonja*

Main category: cs.CL

TL;DR: 本研究开发了评估深度研究工具能力的标准，发现其在学术调查生成上与传统搜索引擎存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 探讨具备知识密集任务能力的深度研究工具，并评估其在信息提取和多页报告生成方面的表现。

Method: 使用评估表对Deep Research工具进行评估，通过选择学术调查写作作为案例任务，分析生成的报告。

Result: 通过评估OpenAI的Deep Search和谷歌的Deep Search在学术调查生成中的表现，发现两者在目标领域呈现上存在明显不足。

Conclusion: 深度研究工具与搜索引擎之间存在巨大差距，特别是在生成学术调查报告时，强调了制定精确评估标准的必要性。

Abstract: Large Language Models (LLMs) powered with argentic capabilities are able to
do knowledge-intensive tasks without human involvement. A prime example of this
tool is Deep research with the capability to browse the web, extract
information and generate multi-page reports. In this work, we introduce an
evaluation sheet that can be used for assessing the capability of Deep Research
tools. In addition, we selected academic survey writing as a use case task and
evaluated output reports based on the evaluation sheet we introduced. Our
findings show the need to have carefully crafted evaluation standards. The
evaluation done on OpenAI`s Deep Search and Google's Deep Search in generating
an academic survey showed the huge gap between search engines and standalone
Deep Research tools, the shortcoming in representing the targeted area.

</details>


### [121] [HiSpec: Hierarchical Speculative Decoding for LLMs](https://arxiv.org/abs/2510.01336)
*Avinash Kumar,Sujay Sanghavi,Poulami Das*

Main category: cs.CL

TL;DR: 提出HiSpec框架，通过早期退出模型和资源重用提高LLM推理吞吐量，验证过程不影响准确性。


<details>
  <summary>Details</summary>
Motivation: 当前模型推理中的验证过程常常成为瓶颈，因此需要一种新的方法来加速验证而不影响准确性。

Method: 提出HiSpec框架，采用早期退出模型实现低开销的中间验证，并通过重用关键值缓存和隐藏状态来进一步提高资源效率。

Result: HiSpec平均提高了1.28倍的吞吐量，在某些情况下提高了2.01倍，相比于基线的单层推理。

Conclusion: HiSpec能在不牺牲准确性的情况下，通过提高模型推理的吞吐量，从而提升LLM推理效率。

Abstract: Speculative decoding accelerates LLM inference by using a smaller draft model
to speculate tokens that a larger target model verifies. Verification is often
the bottleneck (e.g. verification is $4\times$ slower than token generation
when a 3B model speculates for a 70B target model), but most prior works focus
only on accelerating drafting. $\textit{``Intermediate"}$ verification reduces
verification time by discarding inaccurate draft tokens early, but existing
methods incur substantial training overheads in incorporating the intermediate
verifier, increase the memory footprint to orchestrate the intermediate
verification step, and compromise accuracy by relying on approximate
heuristics.
  We propose $\underline{\textit{Hi}}\textit{erarchical
}\underline{\textit{Spec}}\textit{ulative Decoding (HiSpec)}$, a framework for
high-throughput speculative decoding that exploits $\textit{early-exit (EE)
models}$ for low-overhead intermediate verification. EE models allow tokens to
exit early by skipping layer traversal and are explicitly trained so that
hidden states at selected layers can be interpreted, making them uniquely
suited for intermediate verification without drastically increasing compute and
memory overheads. To improve resource-efficiency even further, we design a
methodology that enables HiSpec to re-use key-value caches and hidden states
between the draft, intermediate verifier, and target models. To maintain
accuracy, HiSpec periodically validates the draft tokens accepted by the
intermediate verifier against the target model. Our evaluations using various
representative benchmarks and models show that HiSpec improves throughput by
1.28$\times$ on average and by up to 2.01$\times$ compared to the baseline
single-layer speculation without compromising accuracy.

</details>


### [122] [TAG-EQA: Text-And-Graph for Event Question Answering via Structured Prompting Strategies](https://arxiv.org/abs/2510.01391)
*Maithili Kadam,Francis Ferraro*

Main category: cs.CL

TL;DR: TAG-EQA通过引入因果事件图，提升了大型语言模型在事件推理上的表现，尤其在没有微调的情况下。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在执行一般语言任务时表现出色，但在处理要求因果或时间推理的事件相关问题时常常面临困难。

Method: 引入TAG-EQA，通过将结构化关系转化为自然语言语句，将因果事件图注入大型语言模型的输入，采用九种提示配置进行系统分析。

Result: 在TORQUESTRA基准测试中，TAG-EQA平均提高了5%的准确性，在零-shot设置中最高可提高12%，而在图增强的链式推理提示有效时可提高18%。

Conclusion: 因果图可以在不进行微调的情况下增强大型语言模型的事件推理能力，为基于提示的问题回答提供了一种灵活的结构编码方式。

Abstract: Large language models (LLMs) excel at general language tasks but often
struggle with event-based questions-especially those requiring causal or
temporal reasoning. We introduce TAG-EQA (Text-And-Graph for Event Question
Answering), a prompting framework that injects causal event graphs into LLM
inputs by converting structured relations into natural-language statements.
TAG-EQA spans nine prompting configurations, combining three strategies
(zero-shot, few-shot, chain-of-thought) with three input modalities (text-only,
graph-only, text+graph), enabling a systematic analysis of when and how
structured knowledge aids inference. On the TORQUESTRA benchmark, TAG-EQA
improves accuracy by 5% on average over text-only baselines, with gains up to
12% in zero-shot settings and 18% when graph-augmented CoT prompting is
effective. While performance varies by model and configuration, our findings
show that causal graphs can enhance event reasoning in LLMs without
fine-tuning, offering a flexible way to encode structure in prompt-based QA.

</details>


### [123] [A-VERT: Agnostic Verification with Embedding Ranking Targets](https://arxiv.org/abs/2510.01469)
*Nicolás Aguirre,Ramiro Caso,Ramiro Rodríguez Colmeiro,Mauro Santelli,Joaquín Toranzo Calderón*

Main category: cs.CL

TL;DR: 提出了一种新的、低成本的语言模型响应评估方法，基于语义嵌入，实现了高准确率的分类。


<details>
  <summary>Details</summary>
Motivation: 现有的响应分类方法成本过高或不符合实际情况，需要一种新的评估方法。

Method: 利用语义嵌入距离匹配目标候选文本与任意语言模型生成的文本。

Result: 在3个数据集和3种不同的语言模型架构上，得到与人工标注者一致的回归评分约为0.97，准确率约为96%。

Conclusion: 提出了一种基于语义嵌入距离的无结构评估方法，具有较低的计算成本和高效的分类准确性。

Abstract: The automatic evaluation of Language Model (LM) responses is a critical piece
in the development of benchmarks and metrics, both for model training and
quality assessment of production model endpoints. The current approaches to
response classification relies on methods that are too expensive (i.e.
LLM-as-a-Judge) or that are far from real-world conditions (string-matching,
logprob). In this paper, a structure-free evaluation method is presented. The
method makes use of semantic embedding distances to match target candidates
with arbitrary LM-generated text, resulting in a robust classification of the
response at a relatively low compute cost (embedding models of less than $10B$
parameters). The results show a regression score of ~0.97 and an accuracy of
~96% against human annotators, tested over 3 data sets and 3 different LM
architectures.

</details>


### [124] [One More Question is Enough, Expert Question Decomposition (EQD) Model for Domain Quantitative Reasoning](https://arxiv.org/abs/2510.01526)
*Mengyu Wang,Sotirios Sabanis,Miguel de Carvalho,Shay B. Cohen,Tiejun Ma*

Main category: cs.CL

TL;DR: 本文提出了一种名为专家问题分解（EQD）的方法，旨在提高大型语言模型在特定领域的问答能力，特别是在金融领域的量化推理中。


<details>
  <summary>Details</summary>
Motivation: 应对大语言模型在需要专业知识和复杂问答的领域中的量化推理挑战。

Method: 专家问题分解（EQD），基于双步微调框架，采用奖励函数评估生成子问题的有效性。

Result: EQD方法在金融领域的四个基准数据集上表现出色，相较于现有先进的领域调优模型和提示策略，QA性能提高了0.6%到10.5%。

Conclusion: 在特定领域的问答中，单个支持性问题比详细指导步骤更有利。

Abstract: Domain-specific quantitative reasoning remains a major challenge for large
language models (LLMs), especially in fields requiring expert knowledge and
complex question answering (QA). In this work, we propose Expert Question
Decomposition (EQD), an approach designed to balance the use of domain
knowledge with computational efficiency. EQD is built on a two-step fine-tuning
framework and guided by a reward function that measures the effectiveness of
generated sub-questions in improving QA outcomes. It requires only a few
thousand training examples and a single A100 GPU for fine-tuning, with
inference time comparable to zero-shot prompting. Beyond its efficiency, EQD
outperforms state-of-the-art domain-tuned models and advanced prompting
strategies. We evaluate EQD in the financial domain, characterized by
specialized knowledge and complex quantitative reasoning, across four benchmark
datasets. Our method consistently improves QA performance by 0.6% to 10.5%
across different LLMs. Our analysis reveals an important insight: in
domain-specific QA, a single supporting question often provides greater benefit
than detailed guidance steps.

</details>


### [125] [ReSSFormer: A Recursive Sparse Structured Transformer for Scalable and Long-Context Reasoning](https://arxiv.org/abs/2510.01585)
*Haochen You,Baojing Liu*

Main category: cs.CL

TL;DR: ReSSFormer是一种改进的Transformer架构，通过递归推理等创新技术，更有效地处理长文本推理和上下文选择问题。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer架构在多个领域表现优秀，但在长文本推理、计算效率和结构泛化方面仍面临挑战，因此需要新的架构来解决这些问题。

Method: ReSSFormer结合了递归推理与记忆单元、适应性稀疏注意力模块和自组织编码器结构，进行有效的上下文选择和结构归纳。

Result: 在语言建模、多跳问答和结构敏感任务上，ReSSFormer在相似的计算量和参数预算下，始终优于强基线，证明了其可扩展性和结构灵活性。

Conclusion: ReSSFormer通过引入递归推理、适应性稀疏注意力和自组织编码器结构，展示了在长文本推理和计算效率上的优势，超越了传统Transformer架构的局限性。

Abstract: While Transformer architectures have demonstrated impressive scalability
across domains, they continue to face challenges in long-context reasoning,
computational efficiency, and structural generalization - largely due to rigid
layer stacking, dense attention, and reliance on positional encodings. We
present ReSSFormer, a Recursive Sparse Structured Transformer that integrates
three complementary innovations: Recurrent Reasoning & Memory Unit (R2MU) for
iterative reasoning with bounded depth, Adaptive Sparse Attention Module (ASAM)
for efficient and focused context selection, and Self-Organizing Encoder
Structure (SOES) for position-free structure induction. ReSSFormer replaces
conventional depth stacking with recurrent inference, substitutes full
attention with token- and expert-level sparsity, and models latent token
topology directly from content. Across language modeling, multi-hop QA, and
structure-sensitive tasks, ReSSFormer consistently outperforms strong baselines
under comparable FLOPs and parameter budgets, highlighting its scalability,
efficiency, and structural flexibility.

</details>


### [126] [CLUE: Non-parametric Verification from Experience via Hidden-State Clustering](https://arxiv.org/abs/2510.01591)
*Zhenwen Liang,Ruosen Li,Yujun Zhou,Linfeng Song,Dian Yu,Xinya Du,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: 本文提出CLUE方法，通过分析大型语言模型的隐藏状态来验证输出质量，表现超过传统方法，并在多个任务上提升了准确性。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型输出质量面临挑战，现有方法存在表面特征过拟合和自信度不足的问题，作者认为模型的隐藏状态能提供更丰富的信息。

Method: 提出了一种极简的非参数验证器CLUE，通过总结每个推理轨迹的隐藏状态增量并使用最近质心距离进行分类，而无需训练参数。

Result: CLUE超越了LLM作为判别器的基线，并在重排候选者时与现代基于信心的方法匹敌，尤其在AIME 24任务中，准确性提升显著。

Conclusion: CLUE方法在验证大型语言模型的输出质量方面表现优异，显著提高了准确性，超过了现有的基线方法。

Abstract: Assessing the quality of Large Language Model (LLM) outputs presents a
critical challenge. Previous methods either rely on text-level information
(e.g., reward models, majority voting), which can overfit to superficial cues,
or on calibrated confidence from token probabilities, which would fail on
less-calibrated models. Yet both of these signals are, in fact, partial
projections of a richer source of information: the model's internal hidden
states. Early layers, closer to token embeddings, preserve semantic and lexical
features that underpin text-based judgments, while later layers increasingly
align with output logits, embedding confidence-related information. This paper
explores hidden states directly as a unified foundation for verification. We
show that the correctness of a solution is encoded as a geometrically separable
signature within the trajectory of hidden activations. To validate this, we
present Clue (Clustering and Experience-based Verification), a deliberately
minimalist, non-parametric verifier. With no trainable parameters, CLUE only
summarizes each reasoning trace by an hidden state delta and classifies
correctness via nearest-centroid distance to ``success'' and ``failure''
clusters formed from past experience. The simplicity of this method highlights
the strength of the underlying signal. Empirically, CLUE consistently
outperforms LLM-as-a-judge baselines and matches or exceeds modern
confidence-based methods in reranking candidates, improving both top-1 and
majority-vote accuracy across AIME 24/25 and GPQA. As a highlight, on AIME 24
with a 1.5B model, CLUE boosts accuracy from 56.7% (majority@64) to 70.0%
(top-maj@16).

</details>


### [127] [A Comparison of Independent and Joint Fine-tuning Strategies for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.01600)
*Neal Gregory Lawton,Alfy Samuel,Anoop Kumar,Daben Liu*

Main category: cs.CL

TL;DR: 我们评估并比较了多种RAG微调策略，发现它们在质量改进上相似，但计算成本不同。


<details>
  <summary>Details</summary>
Motivation: 当前流行的RAG框架在问答中广泛应用，但存在多种微调策略，每种策略有不同的成本和好处。

Method: 评估和比较独立微调、联合微调和两阶段微调等多种RAG微调策略。

Result: 所有微调策略在EM和F1生成质量指标上均实现了大致相同的改进，尽管它们的计算成本差异显著。

Conclusion: 最佳的微调策略取决于训练数据集中是否包含上下文标签，以及是否需要对嵌入模型和生成模型的学习率进行网格搜索。

Abstract: A Comparison of Independent and Joint Fine-tuning Strategies for
Retrieval-Augmented Generation Download PDF Neal Gregory Lawton, Alfy Samuel,
Anoop Kumar, Daben Liu Published: 20 Aug 2025, Last Modified: 17 Sept 2025EMNLP
2025 FindingsConference, Publication Chairs, AuthorsRevisionsBibTeXCC BY 4.0
Keywords: Retrieval-Augmented Generation (RAG), Large Language Models (LLMs),
Fine-tuning, Question Answering, Joint fine-tuning TL;DR: We evaluate and
compare strategies for fine-tuning Retrieval Augmented Generation (RAG)
pipelines, including independent fine-tuning, joint fine-tuning, and two-phase
fine-tuning. Abstract: Retrieval augmented generation (RAG) is a popular
framework for question answering that is powered by two large language models
(LLMs): an embedding model that retrieves context documents from a database
that are relevant to a given question, and a generator model that uses the
retrieved context to generate an answer to the question. Both the embedding and
generator models can be fine-tuned to increase performance of a RAG pipeline on
a new task, but multiple fine-tuning strategies exist with different costs and
benefits. In this paper, we evaluate and compare several RAG fine-tuning
strategies, including independent, joint, and two-phase fine-tuning. In our
experiments, we observe that all of these strategies achieve about equal
improvement in EM and F1 generation quality metrics, although they have
significantly different computational costs. We conclude the optimal
fine-tuning strategy to use depends on whether the training dataset includes
context labels and whether a grid search over the learning rates for the
embedding and generator models is required.

</details>


### [128] [RAG-BioQA Retrieval-Augmented Generation for Long-Form Biomedical Question Answering](https://arxiv.org/abs/2510.01612)
*Lovely Yeswanth Panchumarthi,Sai Prasad Gudari,Atharva Negi,Praveen Raj Budime,Harsit Upadhya*

Main category: cs.CL

TL;DR: RAG-BioQA是一种新框架，旨在针对生物医学领域提供基于证据的长形式答案，显著提高了检索的准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 解决生物医学文献快速增长对获取精准医疗信息的挑战，当前问答系统无法提供全面的临床决策支持。

Method: 结合检索增强生成与领域特定微调，使用BioBERT嵌入和FAISS索引，并比较多种重新排名策略以优化上下文选择。

Result: 在PubMedQA数据集上的实验结果显示，与基线相比，RAG-BioQA模型在BLEU、ROUGE和METEOR等指标上均取得显著提升。

Conclusion: RAG-BioQA大幅提升了生物医学知识检索的可及性和证据基础，尤其在长形式回答方面表现优异。

Abstract: The exponential growth of biomedical literature creates significant
challenges for accessing precise medical information. Current biomedical
question-answering systems primarily focus on short-form answers, failing to
provide the comprehensive explanations necessary for clinical decision-making.
We present RAG-BioQA, a novel framework combining retrieval-augmented
generation with domain-specific fine-tuning to produce evidence-based,
long-form biomedical answers. Our approach integrates BioBERT embeddings with
FAISS indexing and compares various re-ranking strategies (BM25, ColBERT,
MonoT5) to optimize context selection before synthesizing evidence through a
fine-tuned T5 model. Experimental results on the PubMedQA dataset show
significant improvements over baselines, with our best model achieving
substantial gains across BLEU, ROUGE, and METEOR metrics, advancing the state
of accessible, evidence-based biomedical knowledge retrieval.

</details>


### [129] [Efficient Training of Robust Traditional Chinese LLaMA-1B on a Single Consumer GPU: Continual Pre-training, SFT, and DPO](https://arxiv.org/abs/2510.01616)
*Yu-Cheng Chih,Ming-Tao Duan,Yong-Hao Hou*

Main category: cs.CL

TL;DR: 开发PureTC-1B以增强小语言模型在传统汉语中的稳定性，显示出显著的性能改进。


<details>
  <summary>Details</summary>
Motivation: 解决小语言模型在传统汉语部署中由于字符不稳定性导致的可靠性问题。

Method: 通过三阶段的稳定化流程，结合持续预训练、监督微调和直接偏好优化。

Result: 在基准测试中，PureTC-1B相较于基线模型非TC输出令牌减少了51.3%；在命名实体翻译任务中，错误语言令牌减少了77.2%。

Conclusion: PureTC-1B显著提升了小语言模型在传统汉语的稳定性，降低了非TC字符的输出。

Abstract: Small Language Models (SLMs) enable cost-effective, on-device and
latency-sensitive AI applications, yet their deployment in Traditional Chinese
(TC) remains hindered by token-level instability - models unpredictably emit
non-TC characters or code-switch into other languages. We address this
practical reliability gap by creating PureTC-1B, a three-stage stabilization
pipeline for Llama-3.2-1B-Instruct (an open-weight, instruction-tuned model
released by Meta) using parameter-efficient LoRA adapters. Our method combines
Continual Pre-Training (CPT) on TC-centric corpora, Supervised Fine-Tuning
(SFT) with instruction data, and Direct Preference Optimization (DPO) using
TC-adherence preferences to improve monolingual robustness without full-model
retraining. On a benchmark designed to simulate real-world usage, PureTC-1B
achieves a 51.3% relative reduction (micro-average) in non-TC output tokens
versus the base model. On a Named Entity Translation (NET) task, PureTC-1B
further reduces incorrect-language tokens by 77.2% relative to Llama-3B and
57.2% relative to Qwen-1.5B, indicating that robust TC adherence is attainable
even at the 1B scale. The pipeline is reproducible, adapter-only, and
hardware-friendly, offering practitioners a practical recipe to enhance
language stability for TC and potentially other non-English languages.

</details>


### [130] [AMAS: Adaptively Determining Communication Topology for LLM-based Multi-Agent System](https://arxiv.org/abs/2510.01617)
*Hui Yi Leong,Yuheng Li,Yuqing Wu,Wenwen Ouyang,Wei Zhu,Jiechao Gao*

Main category: cs.CL

TL;DR: AMAS框架通过动态适应图设计，提升了大型语言模型在多智能体系统中的应用能力，突破了传统架构限制。


<details>
  <summary>Details</summary>
Motivation: 旨在克服传统多智能体系统中存在的图拓扑灵活性不足的问题，提高在工业领域中的应用效果。

Method: 提出了一种动态图设计组件，通过轻量级的大型语言模型适应性，自动识别任务特定的最佳图形配置。

Result: 在问答、数学推理和代码生成基准测试中，AMAS的表现系统性超越了现有的单一智能体和多智能体方法。

Conclusion: AMAS框架通过动态图设计优化了基于大型语言模型的多智能体系统，显著提升了其在各类任务中的表现。

Abstract: Although large language models (LLMs) have revolutionized natural language
processing capabilities, their practical implementation as autonomous
multi-agent systems (MAS) for industrial problem-solving encounters persistent
barriers. Conventional MAS architectures are fundamentally restricted by
inflexible, hand-crafted graph topologies that lack contextual responsiveness,
resulting in diminished efficacy across varied academic and commercial
workloads. To surmount these constraints, we introduce AMAS, a
paradigm-shifting framework that redefines LLM-based MAS through a novel
dynamic graph designer. This component autonomously identifies task-specific
optimal graph configurations via lightweight LLM adaptation, eliminating the
reliance on monolithic, universally applied structural templates. Instead, AMAS
exploits the intrinsic properties of individual inputs to intelligently direct
query trajectories through task-optimized agent pathways. Rigorous validation
across question answering, mathematical deduction, and code generation
benchmarks confirms that AMAS systematically exceeds state-of-the-art
single-agent and multi-agent approaches across diverse LLM architectures. Our
investigation establishes that context-sensitive structural adaptability
constitutes a foundational requirement for high-performance LLM MAS
deployments.

</details>


### [131] [NLP Methods for Detecting Novel LLM Jailbreaks and Keyword Analysis with BERT](https://arxiv.org/abs/2510.01644)
*John Hawkins,Aditya Pramar,Rodney Beard,Rohitash Chandra*

Main category: cs.CL

TL;DR: 本研究分析了机器学习模型识别监狱突破提示的能力，结果表明微调BERT模型在此任务上表现最佳，且反身性结构可能暗示监狱突破意图。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）存在多种漏洞，恶意用户可以通过操纵输入文本来诱导不良响应。

Method: 通过端到端微调双向编码器表示的变换器（BERT）模型来识别监狱突破。

Result: 利用当前数据集，通过微调BERT模型，能够有效区分监狱突破提示与真实使用。

Conclusion: 明确的反身性提示结构可能是监狱突破意图的信号。

Abstract: Large Language Models (LLMs) suffer from a range of vulnerabilities that
allow malicious users to solicit undesirable responses through manipulation of
the input text. These so-called jailbreak prompts are designed to trick the LLM
into circumventing the safety guardrails put in place to keep responses
acceptable to the developer's policies. In this study, we analyse the ability
of different machine learning models to distinguish jailbreak prompts from
genuine uses, including looking at our ability to identify jailbreaks that use
previously unseen strategies. Our results indicate that using current datasets
the best performance is achieved by fine tuning a Bidirectional Encoder
Representations from Transformers (BERT) model end-to-end for identifying
jailbreaks. We visualise the keywords that distinguish jailbreak from genuine
prompts and conclude that explicit reflexivity in prompt structure could be a
signal of jailbreak intention.

</details>


### [132] [Learning to Look at the Other Side: A Semantic Probing Study of Word Embeddings in LLMs with Enabled Bidirectional Attention](https://arxiv.org/abs/2510.01652)
*Zhaoxin Feng,Jianfei Ma,Emmanuele Chersoni,Xiaojing Zhao,Xiaoyi Bao*

Main category: cs.CL

TL;DR: 本论文探讨了通过启用双向注意实现更好的文本嵌入，验证了其在Llama架构中的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究自回归大型语言模型在文本嵌入任务中的限制，特别是单向注意机制对语义表示的影响。

Method: 测试不同的Llama架构变体，进行额外的训练步骤，以逐步启用双向注意力和对比学习。

Result: 证明了双向注意力可以改善大语言模型在文本嵌入任务中的性能。

Conclusion: 通过逐步启用双向注意力和无监督/监督对比学习，Llama架构的变体在文本嵌入任务中表现出色。

Abstract: Autoregressive Large Language Models (LLMs) demonstrate exceptional
performance in language understanding and generation. However, their
application in text embedding tasks has been relatively slow, along with the
analysis of their semantic representation in probing tasks, due to the
constraints of the unidirectional attention mechanism.
  This paper aims to explore whether such constraints can be overcome by
enabling bidirectional attention in LLMs. We tested different variants of the
Llama architecture through additional training steps, progressively enabling
bidirectional attention and unsupervised/supervised contrastive learning.

</details>


### [133] [SoK: Measuring What Matters for Closed-Loop Security Agents](https://arxiv.org/abs/2510.01654)
*Mudita Khurana,Raunak Jain*

Main category: cs.CL

TL;DR: 提出了CLASP框架，旨在整合网络安全生命周期与代理能力，并定义互动闭环安全代理的性能评估标准。


<details>
  <summary>Details</summary>
Motivation: 网络安全中，AI驱动的攻击系统快速演变，而传统防御系统适应能力不足，迫切需要一个整体框架来弥补防御上的盲点。

Method: 通过引入CLASP框架，结合安全生命周期和主要代理能力，评估21项代表性工作的能力表现，并定义闭环能力评分。

Result: 建立了CLASP框架和CLC评分，提供了对闭环安全代理评估的标准，有助于识别当前系统的优势和能力缺口。

Conclusion: CLASP框架和CLC评分提供了必要的词汇、诊断和测量，以推动功能级别性能的提升，并有效评估闭环安全代理。

Abstract: Cybersecurity is a relentless arms race, with AI driven offensive systems
evolving faster than traditional defenses can adapt. Research and tooling
remain fragmented across isolated defensive functions, creating blind spots
that adversaries exploit. Autonomous agents capable of integrating, exploit
confirmation, remediation, and validation into a single closed loop offer
promise, but the field lacks three essentials: a framework defining the agentic
capabilities of security systems across security life cycle, a principled
method for evaluating closed loop agents, and a benchmark for measuring their
performance in practice. We introduce CLASP: the Closed-Loop Autonomous
Security Performance framework which aligns the security lifecycle
(reconnaissance, exploitation, root cause analysis, patch synthesis,
validation) with core agentic capabilities (planning, tool use, memory,
reasoning, reflection & perception) providing a common vocabulary and rubric
for assessing agentic capabilities in security tasks. By applying CLASP to 21
representative works, we map where systems demonstrate strengths, and where
capability gaps persist. We then define the Closed-Loop Capability (CLC) Score,
a composite metric quantifying both degree of loop closure and operational
effectiveness, and outline the requirements for a closed loop benchmark.
Together, CLASP and the CLC Score, provide the vocabulary, diagnostics, and
measurements needed to advance both function level performance and measure
closed loop security agents.

</details>


### [134] [MDSEval: A Meta-Evaluation Benchmark for Multimodal Dialogue Summarization](https://arxiv.org/abs/2510.01659)
*Yinhong Liu,Jianfeng He,Hang Su,Ruixue Lian,Yi Nian,Jake Vincent,Srikanth Vishnubhotla,Robinson Piramuthu,Saab Mansour*

Main category: cs.CL

TL;DR: 本研究提出了MDSEval，为多模态对话摘要提供了首个元评估基准，强调当前自动评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 开发有效的多模态对话摘要模型需要强大的自动评估方法，当前缺乏针对人类注释的元评估基准。

Method: 提出了一种新颖的过滤框架，利用相互排斥的关键信息（MEKI）在不同模态间的筛选。

Result: 建立了MDSEval基准，并揭示了现有评估方法在区分摘要和先进的多模态语言模型能力上的局限性，以及它们易受偏见影响。

Conclusion: MDSEval为多模态对话摘要提供了首个元评估基准，通过有效过滤和评估促进模型的发展。

Abstract: Multimodal Dialogue Summarization (MDS) is a critical task with wide-ranging
applications. To support the development of effective MDS models, robust
automatic evaluation methods are essential for reducing both cost and human
effort. However, such methods require a strong meta-evaluation benchmark
grounded in human annotations. In this work, we introduce MDSEval, the first
meta-evaluation benchmark for MDS, consisting image-sharing dialogues,
corresponding summaries, and human judgments across eight well-defined quality
aspects. To ensure data quality and richfulness, we propose a novel filtering
framework leveraging Mutually Exclusive Key Information (MEKI) across
modalities. Our work is the first to identify and formalize key evaluation
dimensions specific to MDS. We benchmark state-of-the-art modal evaluation
methods, revealing their limitations in distinguishing summaries from advanced
MLLMs and their susceptibility to various bias.

</details>


### [135] [FOR-Prompting: From Objection to Revision via an Asymmetric Prompting Protocol](https://arxiv.org/abs/2510.01674)
*He Zhang,Anzhou Zhang,Jian Dai*

Main category: cs.CL

TL;DR: FOR-Prompting是一种新型协议，能提升问答系统的推理表现，诱发自我修正，并对小规模模型和各种平台具备潜在优势。


<details>
  <summary>Details</summary>
Motivation: 现有的推理协议（如CoT和ToT）缺少有效的外部质疑机制，无法促进系统的自我修正。

Method: FOR-Prompting协议通过设定辩护者、异议者和主持者三个角色来促进问答过程，侧重于促进自我修正和一致性。

Result: 在GSM8K数据集上，FOR-Prompting比单一提示有约22%的准确率提升，并与CoT持平，同时在推理和连贯性方面获得更高评分。此外，在Llama3.2:1b模型上GSM8K任务的准确性提高近19%。

Conclusion: FOR-Prompting提供了一种新的不对称推理协议，显著提升了问答系统的推理能力和一致性，并具有广泛的适用性和小规模模型的潜力。

Abstract: Reasoning protocols such as Chain of Thought (CoT) and Tree of Thought (ToT)
organize internal deliberation but lack an explicit mechanism for external
questioning that elicits self-revision. We present FOR-Prompting (From
Objection to Revision Prompting), an asymmetric protocol where a Defender
proposes an answer, an Objectioner raises question-style objections with no
direct fixes, and a Host enforces consistency and closure. On GSM8K we observe
about a 22% point gain over single-prompt and accuracy on par with CoT, with
more than 10% higher ratings in reasoning and coherence from a uniform GPT 4.1
judge. FOR-Prompting also corrects mistakes without tools or human supervision
on tricky queries, and improves performance for small-scale model (approx. 19%
accuracy improved on Llama3.2:1b for GSM8K task), highlighting promise for
small models and on personal device use. Beyond factual QA, qualitative
analyses on open-ended tasks show enhanced exploration and refinement, with
dialogue traces that make assumptions and trade-offs explicit. The protocol is
model agnostic and operates purely at the prompt level through role-structured
turns, so it works with hosted and local models of different sizes without
retraining, and it supports large-scale study of objection-guided reasoning.

</details>


### [136] [How Do Language Models Compose Functions?](https://arxiv.org/abs/2510.01685)
*Apoorv Khandelwal,Ellie Pavlick*

Main category: cs.CL

TL;DR: 本研究探讨大语言模型在解决组合性任务时的处理机制，确认其存在组合性差距，发现两种处理机制，并揭示这些机制与嵌入空间几何形状的关系。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型在解决组合性任务时是否使用组合性机制，以理解其能力和局限性。

Method: 通过使用残余流激活的logit lens分析模型，比较不同的任务求解机制。

Result: 发现虽然现代LLMs仍有'组合性差距'，但是它们在处理任务时使用了不同的机制：一种是组合性解决，另一种是直接解决，且使用的机制与嵌入空间的几何形状相关。

Conclusion: 现代大语言模型在解决两步事实回忆任务时表现出不同的处理机制，而这些机制与嵌入空间的几何形状相关。

Abstract: While large language models (LLMs) appear to be increasingly capable of
solving compositional tasks, it is an open question whether they do so using
compositional mechanisms. In this work, we investigate how feedforward LLMs
solve two-hop factual recall tasks, which can be expressed compositionally as
$g(f(x))$. We first confirm that modern LLMs continue to suffer from the
"compositionality gap": i.e. their ability to compute both $z = f(x)$ and $y =
g(z)$ does not entail their ability to compute the composition $y = g(f(x))$.
Then, using logit lens on their residual stream activations, we identify two
processing mechanisms, one which solves tasks $\textit{compositionally}$,
computing $f(x)$ along the way to computing $g(f(x))$, and one which solves
them $\textit{directly}$, without any detectable signature of the intermediate
variable $f(x)$. Finally, we find that which mechanism is employed appears to
be related to the embedding space geometry, with the idiomatic mechanism being
dominant in cases where there exists a linear mapping from $x$ to $g(f(x))$ in
the embedding spaces. We fully release our data and code at:
https://github.com/apoorvkh/composing-functions .

</details>


### [137] [Format Inertia: A Failure Mechanism of LLMs in Medical Pre-Consultation](https://arxiv.org/abs/2510.01688)
*Seungseop Lim,Gibaeg Kim,Wooseok Han,Jean Seo,Hyunkyung Lee,Jaehyo Yoo,Eunho Yang*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型在医疗领域对话生成中的格式惯性问题，并提出通过重新平衡训练数据集来改善该问题的方法。


<details>
  <summary>Details</summary>
Motivation: 我们发现医疗预咨询任务中的数据集存在偏斜的轮次分布，导致模型生成重复且信息不足的问题。

Method: 采用一种数据中心的方法，通过重新平衡训练数据集的轮次分布，来缓解格式惯性。

Result: 实验结果表明，我们的方法有效减轻了模型在医疗对话中产生格式惯性的现象。

Conclusion: 我们的方法显著减轻了医疗预咨询中的格式惯性现象。

Abstract: Recent advances in Large Language Models (LLMs) have brought significant
improvements to various service domains, including chatbots and medical
pre-consultation applications. In the healthcare domain, the most common
approach for adapting LLMs to multi-turn dialogue generation is Supervised
Fine-Tuning (SFT). However, datasets for SFT in tasks like medical
pre-consultation typically exhibit a skewed turn-count distribution. Training
on such data induces a novel failure mechanism we term **Format Inertia**,
where models tend to generate repetitive, format-correct, but diagnostically
uninformative questions in long medical dialogues. To mitigate this observed
failure mechanism, we adopt a simple, data-centric method that rebalances the
turn-count distribution of the training dataset. Experimental results show that
our approach substantially alleviates Format Inertia in medical
pre-consultation.

</details>


### [138] [What MLLMs Learn about When they Learn about Multimodal Reasoning: Perception, Reasoning, or their Integration?](https://arxiv.org/abs/2510.01719)
*Jiwan Chung,Neel Joshi,Pratyusha Sharma,Youngjae Yu,Vibhav Vineet*

Main category: cs.CL

TL;DR: MathLens基准细分了多模态推理的子技能，分析了不同训练方式的影响，发现整合能力较弱并存在过拟合。


<details>
  <summary>Details</summary>
Motivation: 目前多模态推理模型的评估主要集中于聚合准确性，未能揭示模型改善的具体领域，因此需要一个能够分解多模态推理能力的基准。

Method: 通过将多模态推理分解为感知、推理和整合三部分，并提供详细的注释和问题，来评估这些子技能的表现。

Result: MathLens基准揭示了不同训练策略对多模态推理模型各个子技能表现的影响不平衡，尤其是整合能力较弱，且多模态训练存在过拟合的问题。

Conclusion: MathLens基准提供了一个更细致的评估多模态推理模型的框架，揭示了不同训练方式对模型性能的影响差异，尤其是在感知、推理和整合能力上。

Abstract: Multimodal reasoning models have recently shown promise on challenging
domains such as olympiad-level geometry, yet their evaluation remains dominated
by aggregate accuracy, a single score that obscures where and how models are
improving. We introduce MathLens, a benchmark designed to disentangle the
subskills of multimodal reasoning while preserving the complexity of
textbook-style geometry problems. The benchmark separates performance into
three components: Perception: extracting information from raw inputs,
Reasoning: operating on available information, and Integration: selecting
relevant perceptual evidence and applying it within reasoning. To support each
test, we provide annotations: visual diagrams, textual descriptions to evaluate
reasoning in isolation, controlled questions that require both modalities, and
probes for fine-grained perceptual skills, all derived from symbolic
specifications of the problems to ensure consistency and robustness. Our
analysis reveals that different training approaches have uneven effects: First,
reinforcement learning chiefly strengthens perception, especially when
supported by textual supervision, while textual SFT indirectly improves
perception through reflective reasoning. Second, reasoning improves only in
tandem with perception. Third, integration remains the weakest capacity, with
residual errors concentrated there once other skills advance. Finally,
robustness diverges: RL improves consistency under diagram variation, whereas
multimodal SFT reduces it through overfitting. We will release all data and
experimental logs.

</details>


### [139] [Machine-interpretable Engineering Design Standards for Valve Specification](https://arxiv.org/abs/2510.01736)
*Anders Gjerver,Rune Frostad,Vedrana Barisic,Melinda Hodkiewicz,Caitlin Woods,Mihaly Fekete,Arild Braathen Torjusen,Johan Wilhelm Kluwer*

Main category: cs.CL

TL;DR: 本文介绍了如何将工程设计标准转化为可重用的机器可解释本体，以支持设备选择过程中的质量保证，并展示其在数字化智能标准中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管工业工作数字化的雄心存在，但产品规格和设计标准仍主要以文档为中心，因此需要一种方法来数字化这些信息。

Method: 将工程设计标准中的信息转化为模块化、可重用的机器可解释本体，并在阀门选择过程中进行测试。

Result: 开发的本体能够自动验证特定的阀门数据表是否符合行业标准，并通过语义推理验证产品类型是否满足阀门规格。

Conclusion: 通过创建可重用的模组本体，本文展示了现代化设计标准数字化的潜力，并为标准机构提供了转向数字化智能标准的示范。

Abstract: Engineering design processes use technical specifications and must comply
with standards. Product specifications, product type data sheets, and design
standards are still mainly document-centric despite the ambition to digitalize
industrial work. In this paper, we demonstrate how to transform information
held in engineering design standards into modular, reusable,
machine-interpretable ontologies and use the ontologies in quality assurance of
the plant design and equipment selection process. We use modelling patterns to
create modular ontologies for knowledge captured in the text and in frequently
referenced tables in International Standards for piping, material and valve
design. These modules are exchangeable, as stored in a W3C compliant format,
and interoperable as they are aligned with the top-level ontology ISO DIS
23726-3: Industrial Data Ontology (IDO).
  We test these ontologies, created based on international material and piping
standards and industry norms, on a valve selection process. Valves are
instantiated in semantic asset models as individuals along with a semantic
representation of the environmental condition at their location on the asset.
We create "functional location tags" as OWL individuals that become instances
of OWL class Valve Data Sheet (VDS) specified valves. Similarly we create
instances of manufacturer product type. Our approach enables automated
validation that a specific VDS is compliant with relevant industry standards.
Using semantic reasoning and executable design rules, we also determine whether
the product type meets the valve specification. Creation of shared, reusable
IDO-based modular ontologies for design standards enables semantic reasoning to
be applied to equipment selection processes and demonstrates the potential of
this approach for Standards Bodies wanting to transition to digitized Smart
Standards.

</details>


### [140] [Can LLMs Refuse Questions They Do Not Know? Measuring Knowledge-Aware Refusal in Factual Tasks](https://arxiv.org/abs/2510.01782)
*Wenbo Pan,Jie Xu,Qiguang Chen,Junhao Dong,Libo Qin,Xinfeng Li,Haining Yu,Xiaohua Jia*

Main category: cs.CL

TL;DR: 本文提出了拒绝指数（RI），一个新指标，用于有效测量大型语言模型在面对未知问题时的拒绝能力，强调传统评价方法的不足，呼吁结合RI进行综合评测。


<details>
  <summary>Details</summary>
Motivation: 为了提升模型的事实可靠性，建立一个能够真实测量模型拒绝不知问题能力的指标。

Method: 提出了一种轻量级的两次评估方法，以确保有效测量拒绝率，并计算RI。

Result: RI 通过观察到的拒绝率在16个模型和5个数据集上进行了广泛实验，准确量化了模型在事实任务中的知识感知拒绝能力。

Conclusion: Refusal Index (RI) 是一种有效的测量大型语言模型（LLMs）知识感知拒绝能力的指标，能够补充传统的准确性评估。

Abstract: Large Language Models (LLMs) should refuse to answer questions beyond their
knowledge. This capability, which we term knowledge-aware refusal, is crucial
for factual reliability. However, existing metrics fail to faithfully measure
this ability. On the one hand, simple refusal-based metrics are biased by
refusal rates and yield inconsistent scores when models exhibit different
refusal tendencies. On the other hand, existing calibration metrics are
proxy-based, capturing the performance of auxiliary calibration processes
rather than the model's actual refusal behavior. In this work, we propose the
Refusal Index (RI), a principled metric that measures how accurately LLMs
refuse questions they do not know. We define RI as Spearman's rank correlation
between refusal probability and error probability. To make RI practically
measurable, we design a lightweight two-pass evaluation method that efficiently
estimates RI from observed refusal rates across two standard evaluation runs.
Extensive experiments across 16 models and 5 datasets demonstrate that RI
accurately quantifies a model's intrinsic knowledge-aware refusal capability in
factual tasks. Notably, RI remains stable across different refusal rates and
provides consistent model rankings independent of a model's overall accuracy
and refusal rates. More importantly, RI provides insight into an important but
previously overlooked aspect of LLM factuality: while LLMs achieve high
accuracy on factual tasks, their refusal behavior can be unreliable and
fragile. This finding highlights the need to complement traditional accuracy
metrics with the Refusal Index for comprehensive factuality evaluation.

</details>


### [141] [Comparison of Unsupervised Metrics for Evaluating Judicial Decision Extraction](https://arxiv.org/abs/2510.01792)
*Ivan Leonidovich Litvak,Anton Kostin,Fedor Lashkin,Tatiana Maksiyan,Sergey Lagutin*

Main category: cs.CL

TL;DR: 本研究评估了无监督指标在法律自然语言处理中的有效性，结果显示其能够实现可扩展性，但在高风险场景下仍需依赖人类判断。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在法律自然语言处理领域的快速发展，需要可扩展的方法来评价从司法判决中提取的文本。

Method: 评估16种无监督指标，针对1000个匿名俄罗斯法律判决提取的七个语义块，并与7168个专家评分进行验证。

Result: Term Frequency Coherence和Coverage Ratio/Block Completeness与专家评分最为一致，而Legal Term Density则表现出强烈的负相关性。LLM评价分数表现出中等的一致性，但其针对法律文本的专业性有限。

Conclusion: 本研究提出了无监督评价指标，能够实现法律文本处理的可扩展筛选，但无法完全取代人类判断，尤其在高风险法律场景中。

Abstract: The rapid advancement of artificial intelligence in legal natural language
processing demands scalable methods for evaluating text extraction from
judicial decisions. This study evaluates 16 unsupervised metrics, including
novel formulations, to assess the quality of extracting seven semantic blocks
from 1,000 anonymized Russian judicial decisions, validated against 7,168
expert reviews on a 1--5 Likert scale. These metrics, spanning document-based,
semantic, structural, pseudo-ground truth, and legal-specific categories,
operate without pre-annotated ground truth. Bootstrapped correlations, Lin's
concordance correlation coefficient (CCC), and mean absolute error (MAE) reveal
that Term Frequency Coherence (Pearson $r = 0.540$, Lin CCC = 0.512, MAE =
0.127) and Coverage Ratio/Block Completeness (Pearson $r = 0.513$, Lin CCC =
0.443, MAE = 0.139) best align with expert ratings, while Legal Term Density
(Pearson $r = -0.479$, Lin CCC = -0.079, MAE = 0.394) show strong negative
correlations. The LLM Evaluation Score (mean = 0.849, Pearson $r = 0.382$, Lin
CCC = 0.325, MAE = 0.197) showed moderate alignment, but its performance, using
gpt-4.1-mini via g4f, suggests limited specialization for legal textse. These
findings highlight that unsupervised metrics, including LLM-based approaches,
enable scalable screening but, with moderate correlations and low CCC values,
cannot fully replace human judgment in high-stakes legal contexts. This work
advances legal NLP by providing annotation-free evaluation tools, with
implications for judicial analytics and ethical AI deployment.

</details>


### [142] [Detecting LLM-Generated Spam Reviews by Integrating Language Model Embeddings and Graph Neural Network](https://arxiv.org/abs/2510.01801)
*Xin Liu,Rongwu Xu,Xinyi Jia,Jason Liao,Jiao Sun,Ling Huang,Wei Xu*

Main category: cs.CL

TL;DR: 本研究提出了FraudSquad，一个新型的混合模型，用于有效检测LLM生成的垃圾评论，表现优秀且适合实际应用。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的兴起，生成的垃圾评论对现有检测系统构成挑战，迫切需要适应这一变化的垃圾检测解决方案。

Method: FraudSquad结合了预训练语言模型的文本嵌入和门控图变换器，用于垃圾节点分类。

Result: FraudSquad在三种LLM生成的数据集上，精确率和召回率分别提高了44.22%和43.01%，并在两个人工撰写的垃圾评论数据集上也取得了良好结果。

Conclusion: FraudSquad是一种高效的混合检测模型，能有效识别LLM生成的垃圾评论，并且保持较小的模型规模和较低的标注数据需求，适用于实际应用。

Abstract: The rise of large language models (LLMs) has enabled the generation of highly
persuasive spam reviews that closely mimic human writing. These reviews pose
significant challenges for existing detection systems and threaten the
credibility of online platforms. In this work, we first create three realistic
LLM-generated spam review datasets using three distinct LLMs, each guided by
product metadata and genuine reference reviews. Evaluations by GPT-4.1 confirm
the high persuasion and deceptive potential of these reviews. To address this
threat, we propose FraudSquad, a hybrid detection model that integrates text
embeddings from a pre-trained language model with a gated graph transformer for
spam node classification. FraudSquad captures both semantic and behavioral
signals without relying on manual feature engineering or massive training
resources. Experiments show that FraudSquad outperforms state-of-the-art
baselines by up to 44.22% in precision and 43.01% in recall on three
LLM-generated datasets, while also achieving promising results on two
human-written spam datasets. Furthermore, FraudSquad maintains a modest model
size and requires minimal labeled training data, making it a practical solution
for real-world applications. Our contributions include new synthetic datasets,
a practical detection framework, and empirical evidence highlighting the
urgency of adapting spam detection to the LLM era. Our code and datasets are
available at: https://anonymous.4open.science/r/FraudSquad-5389/.

</details>


### [143] [Syntactic Blind Spots: How Misalignment Leads to LLMs Mathematical Errors](https://arxiv.org/abs/2510.01831)
*Dane Williamson,Yangfeng Ji,Matthew Dwyer*

Main category: cs.CL

TL;DR: 大语言模型在结构复杂度较高的问题上表现出系统性失败，语法干预可以改善其错误推理。


<details>
  <summary>Details</summary>
Motivation: 识别大语言模型在面对语法上偏离训练分布的问题时的系统性失败模式，即语法盲点。

Method: 通过使用源于正确示例的句法模板重述错误问题，以测试模型对结构复杂度的敏感性。

Result: 重新表述的问题通常会导致正确的答案，且使用基于依赖局部性理论的度量分析表明，较高的DLT评分与更高的失败率相关。

Conclusion: 语法注意的干预可以揭示并减轻模型的归纳失败，许多推理错误源于结构不一致而非概念困难。

Abstract: Large Language Models (LLMs) demonstrate strong mathematical problem-solving
abilities but frequently fail on problems that deviate syntactically from their
training distribution. We identify a systematic failure mode, syntactic blind
spots, in which models misapply familiar reasoning strategies to problems that
are semantically straightforward but phrased in unfamiliar ways. These errors
are not due to gaps in mathematical competence, but rather reflect a brittle
coupling between surface form and internal representation. To test this, we
rephrase incorrectly answered questions using syntactic templates drawn from
correct examples. These rephrasings, which preserve semantics while reducing
structural complexity, often lead to correct answers. We quantify syntactic
complexity using a metric based on Dependency Locality Theory (DLT), and show
that higher DLT scores are associated with increased failure rates across
multiple datasets. Our findings suggest that many reasoning errors stem from
structural misalignment rather than conceptual difficulty, and that
syntax-aware interventions can reveal and mitigate these inductive failures.

</details>


### [144] [SCRIBES: Web-Scale Script-Based Semi-Structured Data Extraction with Reinforcement Learning](https://arxiv.org/abs/2510.01832)
*Shicheng Liu,Kai Sun,Lisheng Fu,Xilun Chen,Xinyuan Zhang,Zhaojiang Lin,Rulin Shao,Yue Liu,Anuj Kumar,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CL

TL;DR: 提出了SCRIBES，一个基于强化学习的框架，旨在提高半结构化内容的提取效率，结果显示其在数据提取和问答准确性上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 随着网络上半结构化内容的增加，提取相关信息的难度提升，现有方法缺乏普适性或资源消耗高，因此需要一种高效的提取方式。

Method: 提出了一种基于强化学习的框架SCRIBES，通过网页布局相似性作为奖励信号，生成可重用的提取脚本，并在真实数据上进行迭代训练。

Result: SCRIBES在脚本质量上超过了强基线13%以上，并在GPT-4o的下游问答准确性上提高了超过4%。

Conclusion: SCRIBES通过生成可重用的提取脚本，并利用布局相似性进行奖励，显著提高了网页数据的提取效率和准确性，具备可扩展性和资源效率。

Abstract: Semi-structured content in HTML tables, lists, and infoboxes accounts for a
substantial share of factual data on the web, yet the formatting complicates
usage, and reliably extracting structured information from them remains
challenging. Existing methods either lack generalization or are
resource-intensive due to per-page LLM inference. In this paper, we introduce
SCRIBES (SCRIpt-Based Semi-Structured Content Extraction at Web-Scale), a novel
reinforcement learning framework that leverages layout similarity across
webpages within the same site as a reward signal. Instead of processing each
page individually, SCRIBES generates reusable extraction scripts that can be
applied to groups of structurally similar webpages. Our approach further
improves by iteratively training on synthetic annotations from in-the-wild
CommonCrawl data. Experiments show that our approach outperforms strong
baselines by over 13% in script quality and boosts downstream question
answering accuracy by more than 4% for GPT-4o, enabling scalable and
resource-efficient web information extraction.

</details>


### [145] [Model Merging to Maintain Language-Only Performance in Developmentally Plausible Multimodal Models](https://arxiv.org/abs/2510.01845)
*Ece Takmaz,Lisa Bylinina,Jakub Dotlacil*

Main category: cs.CL

TL;DR: 本研究通过模型合并技术提升了多模态模型在语言任务上的表现，同时确保了多模态性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前多模态语言模型在语言单一任务中表现不佳的问题，尤其是在低资源设置下。

Method: 通过加权线性插值将多模态模型与语言模型的参数进行融合。

Result: 我们的多模态模型在BabyLM基线测试中表现优于之前的模型，同时在语言任务中进行模型合并改善了性能。

Conclusion: 模型合并可以在一定程度上缓解多模态模型在语言任务中的表现不足，同时保持多模态性能。

Abstract: State-of-the-art vision-and-language models consist of many parameters and
learn from enormous datasets, surpassing the amounts of linguistic data that
children are exposed to as they acquire a language. This paper presents our
approach to the multimodal track of the BabyLM challenge addressing this
discrepancy. We develop language-only and multimodal models in low-resource
settings using developmentally plausible datasets, with our multimodal models
outperforming previous BabyLM baselines. One finding in the multimodal language
model literature is that these models tend to underperform in
\textit{language-only} tasks. Therefore, we focus on maintaining language-only
abilities in multimodal models. To this end, we experiment with \textit{model
merging}, where we fuse the parameters of multimodal models with those of
language-only models using weighted linear interpolation. Our results
corroborate the findings that multimodal models underperform in language-only
benchmarks that focus on grammar, and model merging with text-only models can
help alleviate this problem to some extent, while maintaining multimodal
performance.

</details>


### [146] [REPAIR: Robust Editing via Progressive Adaptive Intervention and Reintegration](https://arxiv.org/abs/2510.01879)
*Yisu Wang,Ming Wang,Haoyuan Song,Wenjie Huang,Chaozheng Wang,Yi Xie,Xuming Ran*

Main category: cs.CL

TL;DR: REPAIR是一个旨在以低成本精确更新大语言模型的框架，显著改善了知识融合和编辑准确性。


<details>
  <summary>Details</summary>
Motivation: 解决后训练过程中知识获取和错误修正的高成本问题，以及重新训练时常常产生的意外副作用。

Method: 通过闭环反馈机制和动态内存管理来支持模型更新，同时融合知识并强制执行局部性保护。

Result: 实验表明，REPAIR在多个模型中提高了10%-30%的编辑准确性，并显著减少了知识遗忘。

Conclusion: REPAIR提供了一种可靠的框架，用于开发可靠、可扩展和不断发展的LLM。

Abstract: Post-training for large language models (LLMs) is constrained by the high
cost of acquiring new knowledge or correcting errors and by the unintended side
effects that frequently arise from retraining. To address these issues, we
introduce REPAIR (Robust Editing via Progressive Adaptive Intervention and
Reintegration), a lifelong editing framework designed to support precise and
low-cost model updates while preserving non-target knowledge. REPAIR mitigates
the instability and conflicts of large-scale sequential edits through a
closed-loop feedback mechanism coupled with dynamic memory management.
Furthermore, by incorporating frequent knowledge fusion and enforcing strong
locality guards, REPAIR effectively addresses the shortcomings of traditional
distribution-agnostic approaches that often overlook unintended ripple effects.
Our experiments demonstrate that REPAIR boosts editing accuracy by 10%-30%
across multiple model families and significantly reduces knowledge forgetting.
This work introduces a robust framework for developing reliable, scalable, and
continually evolving LLMs.

</details>


### [147] [Enhancing Large Language Model Reasoning with Reward Models: An Analytical Survey](https://arxiv.org/abs/2510.01925)
*Qiyuan Liu,Hao Xu,Xuhong Chen,Wei Chen,Yee Whye Teh,Ning Miao*

Main category: cs.CL

TL;DR: 本文系统性介绍了奖励模型在大型语言模型推理中的应用，讨论了它们的基本概念、关键应用及面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 旨在提高大型语言模型的推理性能，从而改善其生成和选择答案的能力。

Method: 对奖励模型的架构、训练方法和评估技术进行全面的综述和分析。

Result: 通过总结奖励模型的基本概念和关键应用，提供有效部署和进步的可行见解，同时提出未来研究的方向。

Conclusion: 本研究提供了对奖励模型（RMs）在大型语言模型（LLMs）推理中的应用的系统介绍，并提出了一些关键的未解决问题。

Abstract: Reward models (RMs) play a critical role in enhancing the reasoning
performance of LLMs. For example, they can provide training signals to finetune
LLMs during reinforcement learning (RL) and help select the best answer from
multiple candidates during inference. In this paper, we provide a systematic
introduction to RMs, along with a comprehensive survey of their applications in
LLM reasoning. We first review fundamental concepts of RMs, including their
architectures, training methodologies, and evaluation techniques. Then, we
explore their key applications: (1) guiding generation and selecting optimal
outputs during LLM inference, (2) facilitating data synthesis and iterative
self-improvement for LLMs, and (3) providing training signals in RL-based
finetuning. Finally, we address critical open questions regarding the
selection, generalization, evaluation, and enhancement of RMs, based on
existing research and our own empirical findings. Our analysis aims to provide
actionable insights for the effective deployment and advancement of RMs for LLM
reasoning.

</details>


### [148] [Inverse Language Modeling towards Robust and Grounded LLMs](https://arxiv.org/abs/2510.01929)
*Davide Gabrielli,Simone Sestito,Iacopo Masi*

Main category: cs.CL

TL;DR: ILM提供了一种增强LLM鲁棒性与控制能力的统一框架，旨在提高其对输入扰动的适应性，同时识别有害输入。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型(LLM)的防御机制缺乏整合与发展。

Method: 提出了逆语言建模(ILM)作为一个统一框架。

Result: ILM同时增强LLM对输入扰动的鲁棒性及其输出的根植性，能够识别潜在的有毒或不安全的输入触发因素。

Conclusion: ILM为下一代高可靠性和可控的LLM奠定了基础。

Abstract: The current landscape of defensive mechanisms for LLMs is fragmented and
underdeveloped, unlike prior work on classifiers. To further promote
adversarial robustness in LLMs, we propose Inverse Language Modeling (ILM), a
unified framework that simultaneously 1) improves the robustness of LLMs to
input perturbations, and, at the same time, 2) enables native grounding by
inverting model outputs to identify potentially toxic or unsafe input triggers.
ILM transforms LLMs from static generators into analyzable and robust systems,
potentially helping RED teaming. ILM can lay the foundation for next-generation
LLMs that are not only robust and grounded but also fundamentally more
controllable and trustworthy. The code is publicly available at
github.com/davegabe/pag-llm.

</details>


### [149] [Veri-R1: Toward Precise and Faithful Claim Verification via Online Reinforcement Learning](https://arxiv.org/abs/2510.01932)
*Qi He,Cheng Qian,Xiusi Chen,Bingxiang He,Yi R.,Fung,Heng Ji*

Main category: cs.CL

TL;DR: 新提出的Veri-R1框架通过在线强化学习显著提升了大语言模型的声明验证能力，改善了检索和推理技能，并在实证研究中表现优秀。


<details>
  <summary>Details</summary>
Motivation: 由于现有方法缺乏统一的训练范式来提升必要技能，迫切需要一个更有效的验证框架，反映真实世界验证场景。

Method: 提出了一个在线强化学习框架Veri-R1，使大语言模型与搜索引擎互动，通过奖励信号优化计划、检索和推理行为。

Result: Veri-R1在联合准确性上提高了30%，证据评分翻倍，常常超越规模更大的对手。消融研究显示奖励组件的影响和输出逻辑与标签准确率之间的联系。

Conclusion: Veri-R1实现了更精确的在线声明验证，效果超越传统方法，表明在线强化学习在声明验证中的有效性，促进了未来研究的基础。

Abstract: Claim verification with large language models (LLMs) has recently attracted
considerable attention, owing to their superior reasoning capabilities and
transparent verification pathways compared to traditional answer-only
judgments. Online claim verification requires iterative evidence retrieval and
reasoning, yet existing approaches mainly rely on prompt engineering or
predesigned reasoning workflows without offering a unified training paradigm to
improve necessary skills. Therefore, we introduce Veri-R1, an online
reinforcement learning (RL) framework that enables an LLM to interact with a
search engine and to receive reward signals that explicitly shape its planning,
retrieval, and reasoning behaviors. The dynamic interaction between models and
retrieval systems more accurately reflects real-world verification scenarios
and fosters comprehensive verification skills. Empirical results show that
Veri-R1 improves joint accuracy by up to 30% and doubles evidence score, often
surpassing larger-scale counterparts. Ablation studies further reveal the
impact of reward components and the link between output logits and label
accuracy. Our results highlight the effectiveness of online RL for precise and
faithful claim verification and provide a foundation for future research. We
release our code to support community progress in LLM empowered claim
verification.

</details>


### [150] [Taking a SEAT: Predicting Value Interpretations from Sentiment, Emotion, Argument, and Topic Annotations](https://arxiv.org/abs/2510.01976)
*Adina Nicola Dobrinoiu,Ana Cristiana Marcu,Amir Homayounirad,Luciano Cavalcante Siebert,Enrico Liscio*

Main category: cs.CL

TL;DR: 本研究探索语言模型如何预测个体价值解释，发现同时提供多维注释信息能提高预测性能，并强调了个体差异的重要性。


<details>
  <summary>Details</summary>
Motivation: 为了开发能够与不同人类视角对齐并避免对多数观点有偏见的AI系统，认识个体价值解释是重要的。

Method: 通过多维主观注释作为代理，评估语言模型在预测个体价值解释方面的能力。

Result: 实验结果表明，同时提供所有SEAT维度的示例优于单独维度和无信息基线，个体之间的差异突显了考虑个体主观注释者的重要性。

Conclusion: 本研究首次探讨了注释行为对价值预测的影响，为未来的大规模验证奠定了基础。

Abstract: Our interpretation of value concepts is shaped by our sociocultural
background and lived experiences, and is thus subjective. Recognizing
individual value interpretations is important for developing AI systems that
can align with diverse human perspectives and avoid bias toward majority
viewpoints. To this end, we investigate whether a language model can predict
individual value interpretations by leveraging multi-dimensional subjective
annotations as a proxy for their interpretive lens. That is, we evaluate
whether providing examples of how an individual annotates Sentiment, Emotion,
Argument, and Topics (SEAT dimensions) helps a language model in predicting
their value interpretations. Our experiment across different zero- and few-shot
settings demonstrates that providing all SEAT dimensions simultaneously yields
superior performance compared to individual dimensions and a baseline where no
information about the individual is provided. Furthermore, individual
variations across annotators highlight the importance of accounting for the
incorporation of individual subjective annotators. To the best of our
knowledge, this controlled setting, although small in size, is the first
attempt to go beyond demographics and investigate the impact of annotation
behavior on value prediction, providing a solid foundation for future
large-scale validation.

</details>


### [151] [Exploring Database Normalization Effects on SQL Generation](https://arxiv.org/abs/2510.01989)
*Ryosuke Kohita*

Main category: cs.CL

TL;DR: 本文系统分析了schema设计对NL2SQL系统表现的影响，发现不同规范化水平的schema表现各异，最佳设计取决于查询类型。


<details>
  <summary>Details</summary>
Motivation: 探讨schema设计，特别是规范化在自然语言到SQL系统中的影响。

Method: 对固定和变化的schema进行系统性分析，评估8种大型语言模型在不同规范化水平下的表现。

Result: 非规范化schema在简单查询中表现出色，然而在聚合查询中，规范化schema因其对数据重复和NULL值问题的鲁棒性而表现更佳。

Conclusion: 最佳的schema设计依赖于支持的查询类型，设计考虑对于NL2SQL系统至关重要。

Abstract: Schema design, particularly normalization, is a critical yet often overlooked
factor in natural language to SQL (NL2SQL) systems. Most prior research
evaluates models on fixed schemas, overlooking the influence of design on
performance. We present the first systematic study of schema normalization's
impact, evaluating eight leading large language models on synthetic and
real-world datasets with varied normalization levels. We construct controlled
synthetic datasets with formal normalization (1NF-3NF) and real academic paper
datasets with practical schemes. Our results show that denormalized schemas
offer high accuracy on simple retrieval queries, even with cost-effective
models in zero-shot settings. In contrast, normalized schemas (2NF/3NF)
introduce challenges such as errors in base table selection and join type
prediction; however, these issues are substantially mitigated by providing
few-shot examples. For aggregation queries, normalized schemas yielded better
performance, mainly due to their robustness against the data duplication and
NULL value issues that cause errors in denormalized schemas. These findings
suggest that the optimal schema design for NL2SQL applications depends on the
types of queries to be supported. Our study demonstrates the importance of
considering schema design when developing NL2SQL interfaces and integrating
adaptive schema selection for real-world scenarios.

</details>


### [152] [LLM-Based Multi-Task Bangla Hate Speech Detection: Type, Severity, and Target](https://arxiv.org/abs/2510.01995)
*Md Arid Hasan,Firoj Alam,Md Fahad Hossain,Usman Naseem,Syed Ishtiaque Ahmed*

Main category: cs.CL

TL;DR: 我们创建了BanglaMultiHate数据集，首次提供多任务的仇恨言论检测资源，并评估了LLM在低资源设置下的适应性，结果表明文化预训练对性能至关重要。


<details>
  <summary>Details</summary>
Motivation: 在线社交媒体平台是日常交流和信息获取的重要渠道，但也滋生了仇恨言论和攻击性语言，特别是在资源有限的语言中，需要可靠的检测系统。

Method: 我们引入了第一个多任务的孟加拉语仇恨言论数据集BanglaMultiHate，并进行了全面的比较实验，评估了传统基线、单语预训练模型和在零样本提示与LoRA微调下的LLM。

Result: 实验表明，尽管经过LoRA微调的LLM在性能上与BanglaBERT相当，但具有文化和语言基础的预训练对于稳健表现仍然至关重要。

Conclusion: 我们开发的BanglaMultiHate数据集和实验结果为低资源语言中文化对齐的审查工具发展提供了更强的基准。

Abstract: Online social media platforms are central to everyday communication and
information seeking. While these platforms serve positive purposes, they also
provide fertile ground for the spread of hate speech, offensive language, and
bullying content targeting individuals, organizations, and communities. Such
content undermines safety, participation, and equity online. Reliable detection
systems are therefore needed, especially for low-resource languages where
moderation tools are limited. In Bangla, prior work has contributed resources
and models, but most are single-task (e.g., binary hate/offense) with limited
coverage of multi-facet signals (type, severity, target). We address these gaps
by introducing the first multi-task Bangla hate-speech dataset,
BanglaMultiHate, one of the largest manually annotated corpus to date. Building
on this resource, we conduct a comprehensive, controlled comparison spanning
classical baselines, monolingual pretrained models, and LLMs under zero-shot
prompting and LoRA fine-tuning. Our experiments assess LLM adaptability in a
low-resource setting and reveal a consistent trend: although LoRA-tuned LLMs
are competitive with BanglaBERT, culturally and linguistically grounded
pretraining remains critical for robust performance. Together, our dataset and
findings establish a stronger benchmark for developing culturally aligned
moderation tools in low-resource contexts. For reproducibility, we will release
the dataset and all related scripts.

</details>


### [153] [Style Over Story: A Process-Oriented Study of Authorial Creativity in Large Language Models](https://arxiv.org/abs/2510.02025)
*Donghoon Jung,Jiwoo Choi,Songeun Chae,Seohyon Jung*

Main category: cs.CL

TL;DR: 本研究采用基于约束的决策制定分析大型语言模型的创作过程，发现这些模型更重视风格，提出了一种新的系统工具来评估AI的创作能力。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型创造力的评估主要侧重于输出质量，而忽视了形成这些输出的过程。本研究旨在探索这些模型的创作过程。

Method: 本研究采用叙事学的方法，通过控制性提示分配创作者角色，分析大型语言模型的创作偏好。

Result: 研究发现，LLMs在创作中持续强调风格，而非角色、事件和背景等其他元素，并通过模型提供的推理分析显示了不同模型间的独特特征。

Conclusion: 通过引入基于约束的决策制定作为分析工具，本研究提供了对大型语言模型创造力的新理解，突出了模型在创作中对风格的重视，并揭示了不同模型之间的创造性偏好差异。

Abstract: Evaluations of large language models (LLMs)' creativity have focused
primarily on the quality of their outputs rather than the processes that shape
them. This study takes a process-oriented approach, drawing on narratology to
examine LLMs as computational authors. We introduce constraint-based
decision-making as a lens for authorial creativity. Using controlled prompting
to assign authorial personas, we analyze the creative preferences of the
models. Our findings show that LLMs consistently emphasize Style over other
elements, including Character, Event, and Setting. By also probing the
reasoning the models provide for their choices, we show that distinctive
profiles emerge across models and argue that our approach provides a novel
systematic tool for analyzing AI's authorial creativity.

</details>


### [154] [The Disparate Impacts of Speculative Decoding](https://arxiv.org/abs/2510.02128)
*Jameson Sandler,Ahmet Üstün,Marco Romanelli,Sara Hooker,Ferdinando Fioretto*

Main category: cs.CL

TL;DR: 本论文分析了推测解码在不同任务中的速度提升差异，并提出了减少速度提升差异的策略，平均提高了12%的公平性指标。


<details>
  <summary>Details</summary>
Motivation: 探讨推测解码的速度提升在不同任务间的不均匀性，尤其是对于欠拟合和被低估任务的影响。

Method: 通过分析推测解码在不同任务间的速度提升差异，并提出相应的量化分析方法。

Result: 发现推测解码带来的速度提升在不同任务中分布不均，对于欠拟合任务，速度提升效果显著减弱。

Conclusion: 本论文提出了一种缓解策略，以减少任务间的速度提升差异，并在多个模型对上验证了该策略，平均提高了12%的公平性指标。

Abstract: The practice of speculative decoding, whereby inference is probabilistically
supported by a smaller, cheaper, ``drafter'' model, has become a standard
technique for systematically reducing the decoding time of large language
models. This paper conducts an analysis of speculative decoding through the
lens of its potential disparate speed-up rates across tasks. Crucially, the
paper shows that speed-up gained from speculative decoding is not uniformly
distributed across tasks, consistently diminishing for under-fit, and often
underrepresented tasks. To better understand this phenomenon, we derive an
analysis to quantify this observed ``unfairness'' and draw attention to the
factors that motivate such disparate speed-ups to emerge. Further, guided by
these insights, the paper proposes a mitigation strategy designed to reduce
speed-up disparities and validates the approach across several model pairs,
revealing on average a 12% improvement in our fairness metric.

</details>


### [155] [RESTRAIN: From Spurious Votes to Signals -- Self-Driven RL with Self-Penalization](https://arxiv.org/abs/2510.02172)
*Zhaoning Yu,Will Su,Leitian Tao,Haozhu Wang,Aashu Singh,Hanchao Yu,Jianyu Wang,Hongyang Gao,Weizhe Yuan,Jason Weston,Ping Yu,Jing Xu*

Main category: cs.CL

TL;DR: RESTRAIN通过自我惩罚的强化学习框架，利用未标注数据提升推理模型性能，提供了无金标标签的可扩展改进路径。


<details>
  <summary>Details</summary>
Motivation: 探讨如何在缺乏大量标注数据的情况下，改善大规模推理模型的性能，尤其是在较难的任务上。

Method: 提出了一种自我惩罚的强化学习框架，利用模型的答案分布信号进行学习。

Result: 在多项推理基准测试中，RESTRAIN利用未标注数据显著提升了模型性能，接近金标训练的效果。

Conclusion: RESTRAIN在没有标注数据的情况下，显著提升推理能力，为未来的自我改进提供了可扩展路径。

Abstract: Reinforcement learning with human-annotated data has boosted chain-of-thought
reasoning in large reasoning models, but these gains come at high costs in
labeled data while faltering on harder tasks. A natural next step is
experience-driven learning, where models improve without curated labels by
adapting to unlabeled data. We introduce RESTRAIN (REinforcement learning with
Self-restraint), a self-penalizing RL framework that converts the absence of
gold labels into a useful learning signal. Instead of overcommitting to
spurious majority votes, RESTRAIN exploits signals from the model's entire
answer distribution: penalizing overconfident rollouts and low-consistency
examples while preserving promising reasoning chains. The self-penalization
mechanism integrates seamlessly into policy optimization methods such as GRPO,
enabling continual self-improvement without supervision. On challenging
reasoning benchmarks, RESTRAIN delivers large gains using only unlabeled data.
With Qwen3-4B-Base and OctoThinker Hybrid-8B-Base, it improves Pass@1 by up to
+140.7 percent on AIME25, +36.2 percent on MMLU_STEM, and +19.6 percent on
GPQA-Diamond, nearly matching gold-label training while using no gold labels.
These results demonstrate that RESTRAIN establishes a scalable path toward
stronger reasoning without gold labels.

</details>


### [156] [Learning to Reason for Hallucination Span Detection](https://arxiv.org/abs/2510.02173)
*Hsuan Su,Ting-Yao Hu,Hema Swetha Koppula,Kundan Krishna,Hadi Pouransari,Cheng-Yu Hsieh,Cem Koc,Joseph Yitan Cheng,Oncel Tuzel,Raviteja Vemulapalli*

Main category: cs.CL

TL;DR: 本研究探讨了使用显式推理提高幻觉片段检测的可行性，提出了RL4HS框架并展示其在各项基准测试上的优越表现。


<details>
  <summary>Details</summary>
Motivation: 实际应用中需要识别幻觉片段，这一复杂的多步决策过程提示将显式推理与幻觉检测结合可能会有所帮助。

Method: 提出RL4HS强化学习框架，通过跨段奖励函数激励推理，并结合集团相对策略优化和类别感知策略优化。

Result: RL4HS在RAGTruth基准上的实验结果超过了预训练的推理模型和监督微调。

Conclusion: RL4HS强化学习框架在检测幻觉片段中表现优于预训练推理模型和监督微调，证明了使用跨度级奖励的强化学习的必要性。

Abstract: Large language models (LLMs) often generate hallucinations -- unsupported
content that undermines reliability. While most prior works frame hallucination
detection as a binary task, many real-world applications require identifying
hallucinated spans, which is a multi-step decision making process. This
naturally raises the question of whether explicit reasoning can help the
complex task of detecting hallucination spans. To answer this question, we
first evaluate pretrained models with and without Chain-of-Thought (CoT)
reasoning, and show that CoT reasoning has the potential to generate at least
one correct answer when sampled multiple times. Motivated by this, we propose
RL4HS, a reinforcement learning framework that incentivizes reasoning with a
span-level reward function. RL4HS builds on Group Relative Policy Optimization
and introduces Class-Aware Policy Optimization to mitigate reward imbalance
issue. Experiments on the RAGTruth benchmark (summarization, question
answering, data-to-text) show that RL4HS surpasses pretrained reasoning models
and supervised fine-tuning, demonstrating the necessity of reinforcement
learning with span-level rewards for detecting hallucination spans.

</details>


### [157] [From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens](https://arxiv.org/abs/2510.02292)
*Hala Sheta,Eric Huang,Shuyu Wu,Ilia Alenabi,Jiajun Hong,Ryker Lin,Ruoxi Ning,Daniel Wei,Jialin Yang,Jiawei Zhou,Ziqiao Ma,Freda Shi*

Main category: cs.CL

TL;DR: VLM-Lens是一个用于分析和解释视觉语言模型的工具包，支持16种模型，旨在推动VLM的理解和改进。


<details>
  <summary>Details</summary>
Motivation: 需要一个工具来系统化地基准、分析和解释视觉语言模型，使得不同模型的操作更加用户友好。

Method: 通过提取开源视觉语言模型在前向传播过程中的中间输出，提供了一个统一的、可配置的接口。

Result: 当前支持16种顶尖的基础视觉语言模型及其30多种变体，并能轻松扩展以适应新模型。

Conclusion: VLM-Lens工具包旨在加速社区对视觉语言模型的理解和改进，支持对多种模型的分析。

Abstract: We introduce VLM-Lens, a toolkit designed to enable systematic benchmarking,
analysis, and interpretation of vision-language models (VLMs) by supporting the
extraction of intermediate outputs from any layer during the forward pass of
open-source VLMs. VLM-Lens provides a unified, YAML-configurable interface that
abstracts away model-specific complexities and supports user-friendly operation
across diverse VLMs. It currently supports 16 state-of-the-art base VLMs and
their over 30 variants, and is extensible to accommodate new models without
changing the core logic.
  The toolkit integrates easily with various interpretability and analysis
methods. We demonstrate its usage with two simple analytical experiments,
revealing systematic differences in the hidden representations of VLMs across
layers and target concepts. VLM-Lens is released as an open-sourced project to
accelerate community efforts in understanding and improving VLMs.

</details>


### [158] [ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge Graph Exploration Utilities](https://arxiv.org/abs/2510.02200)
*Felix Brei,Lorenz Bühmann,Johannes Frey,Daniel Gerber,Lars-Peter Meyer,Claus Stadler,Kirill Bulert*

Main category: cs.CL

TL;DR: 本文介绍了一种基于SPINACH的迭代方法，通过大型语言模型协助将自然语言查询转化为SPARQL，降低了查询的技术门槛。


<details>
  <summary>Details</summary>
Motivation: 研究动机来源于Text2SPARQL挑战，旨在促进Text2SPARQL领域的进步。

Method: 提出了一种基于LLM的代理方法SPINACH，通过迭代过程将自然语言问题翻译为SPARQL查询。

Result: 通过系统的分析，深入了解了代理的行为，为未来的改进提供了有价值的见解。

Conclusion: 本文提出的SPINACH方法能够有效降低知识图谱查询的入门门槛，促进自然语言与SPARQL语句之间的转化，并为未来的研究提供了改进方向。

Abstract: Interacting with knowledge graphs can be a daunting task for people without a
background in computer science since the query language that is used (SPARQL)
has a high barrier of entry. Large language models (LLMs) can lower that
barrier by providing support in the form of Text2SPARQL translation. In this
paper we introduce a generalized method based on SPINACH, an LLM backed agent
that translates natural language questions to SPARQL queries not in a single
shot, but as an iterative process of exploration and execution. We describe the
overall architecture and reasoning behind our design decisions, and also
conduct a thorough analysis of the agent behavior to gain insights into future
areas for targeted improvements. This work was motivated by the Text2SPARQL
challenge, a challenge that was held to facilitate improvements in the
Text2SPARQL domain.

</details>


### [159] [Say One Thing, Do Another? Diagnosing Reasoning-Execution Gaps in VLM-Powered Mobile-Use Agents](https://arxiv.org/abs/2510.02204)
*Lingzhong Dong,Ziqi Zhou,Shuaibo Yang,Haiyue Sheng,Pengzhou Cheng,Zongru Wu,Zheng Wu,Gongshen Liu,Zhuosheng Zhang*

Main category: cs.CL

TL;DR: 本研究提出了一种新框架以评估推理与执行之间的差距，揭示移动代理在真实操作中的潜在风险，促进信任度提升。


<details>
  <summary>Details</summary>
Motivation: 为了评估推理过程与真实动作之间的差距，防止用户因推理过程看似合理而授权有害行为，从而导致财务损失或信任危机。

Method: 引入了地面真相对齐（GTA）作为新评估框架，结合标准的准确匹配（EM）指标，评估推理准确性与执行准确性。

Result: 通过广泛的实验，发现推理-执行差距普遍存在，执行差距发生频率高于推理差距，但模型规模增大后，执行差距依然显著。

Conclusion: 现有的移动代理在执行准确性与推理过程之间存在明显差距，这一评估框架有助于提升移动代理的可信度。

Abstract: Mobile-use agents powered by vision-language models (VLMs) have shown great
potential in interpreting natural language instructions and generating
corresponding actions based on mobile graphical user interface. Recent studies
suggest that incorporating chain-of-thought (CoT) reasoning tends to improve
the execution accuracy. However, existing evaluations emphasize execution
accuracy while neglecting whether CoT reasoning aligns with ground-truth
actions. This oversight fails to assess potential reasoning-execution gaps,
which in turn foster over-trust: users relying on seemingly plausible CoTs may
unknowingly authorize harmful actions, potentially resulting in financial loss
or trust crisis. In this work, we introduce a new evaluation framework to
diagnose reasoning-execution gaps. At its core lies Ground-Truth Alignment
(GTA), which measures whether the action implied by a CoT matches the
ground-truth action. By combining GTA with the standard Exact Match (EM)
metric, we jointly assess both the reasoning accuracy and execution accuracy.
This joint perspective reveals two types of reasoning-execution gaps: (i)
Execution Gap (EG), where the reasoning correctly identifies the correct action
but execution fails, and (ii) Reasoning Gap (RG), where execution succeeds but
reasoning process conflicts with the actual execution. Experimental results
across a wide range of mobile interaction tasks reveal that reasoning-execution
gaps are prevalent, with execution gaps occurring more frequently than
reasoning gaps. Moreover, while scaling up model size reduces the overall gap,
sizable execution gaps persist even in the largest models. Further analysis
shows that our framework reliably reflects systematic EG/RG patterns in
state-of-the-art models. These findings offer concrete diagnostics and support
the development of more trustworthy mobile-use agents.

</details>


### [160] [More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for Diverse Exploration](https://arxiv.org/abs/2510.02227)
*Xiaoyang Yuan,Yujuan Ding,Yi Bin,Wenqi Shao,Jinyu Cai,Jingkuan Song,Yang Yang,Hengtao Shen*

Main category: cs.CL

TL;DR: 本文提出了AMPO框架，通过多教师指导提升大型语言模型的推理能力，改善了探索与利用之间的平衡，实验表明其在多项任务上表现优越。


<details>
  <summary>Details</summary>
Motivation: 探索通过多教师策略增强大语言模型的推理能力，解决现有方法中自我探索和单一教师模型带来的偏见与探索限制问题。

Method: 引入自适应多指导策略优化（AMPO），利用多名教师模型的指导，仅在主模型未能产生正确答案时提供支持，并通过基于理解的选择机制进行学习。

Result: AMPO在数学推理任务上提升4.3%，在分布外任务上提升12.2%，并且在Pass@k表现上有显著提高，推动了探索的多样性，展现了与使用单一教师模型相比的更高效能。

Conclusion: AMPO框架通过引入多教师策略自适应优化，显著提高了推理能力和可扩展性，相较于传统方法，展示出更高的多样性和性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm
for enhancing the reasoning ability in Large Language Models (LLMs). However,
prevailing methods primarily rely on self-exploration or a single off-policy
teacher to elicit long chain-of-thought (LongCoT) reasoning, which may
introduce intrinsic model biases and restrict exploration, ultimately limiting
reasoning diversity and performance. Drawing inspiration from multi-teacher
strategies in knowledge distillation, we introduce Adaptive Multi-Guidance
Policy Optimization (AMPO), a novel framework that adaptively leverages
guidance from multiple proficient teacher models, but only when the on-policy
model fails to generate correct solutions. This "guidance-on-demand" approach
expands exploration while preserving the value of self-discovery. Moreover,
AMPO incorporates a comprehension-based selection mechanism, prompting the
student to learn from the reasoning paths that it is most likely to comprehend,
thus balancing broad exploration with effective exploitation. Extensive
experiments show AMPO substantially outperforms a strong baseline (GRPO), with
a 4.3% improvement on mathematical reasoning tasks and 12.2% on
out-of-distribution tasks, while significantly boosting Pass@k performance and
enabling more diverse exploration. Notably, using four peer-sized teachers, our
method achieves comparable results to approaches that leverage a single, more
powerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate
a more efficient and scalable path to superior reasoning and generalizability.
Our code is available at https://github.com/SII-Enigma/AMPO.

</details>


### [161] [Enhanced Arabic-language cyberbullying detection: deep embedding and transformer (BERT) approaches](https://arxiv.org/abs/2510.02232)
*Ebtesam Jaber Aljohani,Wael M. S. Yafoo*

Main category: cs.CL

TL;DR: 本研究致力于提高阿拉伯语网络欺凌检测的效果，通过深度学习模型的实验证明，特别是Bi-LSTM与FastText嵌入结合的方法在检测准确率上表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体的迅速发展，青少年面临着网络欺凌的风险，但针对阿拉伯语网络欺凌内容的检测方法仍然稀缺，需要提升检测效果。

Method: 使用长短时记忆（LSTM）和双向长短时记忆（Bi-LSTM）模型进行多次实验，结合不同的词嵌入和预训练的BERT模型，以测试其在阿拉伯语网络欺凌检测中的有效性。

Result: 通过对10662条X（Twitter）帖子的数据预处理和注释质量验证，建立了一系列深度学习模型，最终实现了高达98%的准确率。

Conclusion: 研究证明，基于深度学习的算法在阿拉伯语网络欺凌内容检测中具有高准确率，尤其是Bi-LSTM结合FastText嵌入的模型达到了98%的准确率。

Abstract: Recent technological advances in smartphones and communications, including
the growth of such online platforms as massive social media networks such as X
(formerly known as Twitter) endangers young people and their emotional
well-being by exposing them to cyberbullying, taunting, and bullying content.
Most proposed approaches for automatically detecting cyberbullying have been
developed around the English language, and methods for detecting
Arabic-language cyberbullying are scarce. Methods for detecting Arabic-language
cyberbullying are especially scarce. This paper aims to enhance the
effectiveness of methods for detecting cyberbullying in Arabic-language
content. We assembled a dataset of 10,662 X posts, pre-processed the data, and
used the kappa tool to verify and enhance the quality of our annotations. We
conducted four experiments to test numerous deep learning models for
automatically detecting Arabic-language cyberbullying. We first tested a long
short-term memory (LSTM) model and a bidirectional long short-term memory
(Bi-LSTM) model with several experimental word embeddings. We also tested the
LSTM and Bi-LSTM models with a novel pre-trained bidirectional encoder from
representations (BERT) and then tested them on a different experimental models
BERT again. LSTM-BERT and Bi-LSTM-BERT demonstrated a 97% accuracy. Bi-LSTM
with FastText embedding word performed even better, achieving 98% accuracy. As
a result, the outcomes are generalize

</details>


### [162] [AccurateRAG: A Framework for Building Accurate Retrieval-Augmented Question-Answering Applications](https://arxiv.org/abs/2510.02243)
*Linh The Nguyen,Chi Tran,Dung Ngoc Nguyen,Van-Cuong Pham,Hoang Ngo,Dat Quoc Nguyen*

Main category: cs.CL

TL;DR: 我们介绍了一个新的问答框架，称为AccurateRAG，展示了在关键数据集上超越现有基线的能力。


<details>
  <summary>Details</summary>
Motivation: 提出一个高性能问答应用的全新框架，以提高开发效率。

Method: 我们提供了一套开发效率的工具，包括原始数据集处理、微调数据生成、文本嵌入和LLM微调、输出评估以及本地构建RAG系统。

Result: 实验结果表明，我们的框架优于之前的强基线，并在标准基准数据集上实现了新的最先进的问答性能。

Conclusion: 我们的框架在基准数据集上实现了新的最先进问答性能。

Abstract: We introduce AccurateRAG -- a novel framework for constructing
high-performance question-answering applications based on retrieval-augmented
generation (RAG). Our framework offers a pipeline for development efficiency
with tools for raw dataset processing, fine-tuning data generation, text
embedding & LLM fine-tuning, output evaluation, and building RAG systems
locally. Experimental results show that our framework outperforms previous
strong baselines and obtains new state-of-the-art question-answering
performance on benchmark datasets.

</details>


### [163] [Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative Entropy Regulation](https://arxiv.org/abs/2510.02249)
*Tianyi Jiang,Yi Bin,Yujuan Ding,Kainian Zhu,Fei Ma,Jingkuan Song,Heng Tao Shen*

Main category: cs.CL

TL;DR: 本论文介绍了一种新方法，通过动态选择推理深度来优化大语言模型的思考过程，从而实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 为了解决大语言模型在处理简单问题时容易产生冗长推理步骤的问题，提高其适应问题复杂度的能力。

Method: 通过引入TECA，提出'简洁探索，然后决策'的推理范式，动态确定思考过程的最佳结束点，从而提高模型效率。

Result: 实验结果显示，该方法在不牺牲问题解决能力的前提下，显著减少了过度思考现象，简单数据集上的平均响应长度减少了71%。

Conclusion: 该论文提出了一种新颖的方法，通过Token Entropy Cumulative Average (TECA)和Cumulative Entropy Regulation (CER)机制，有效减少了大语言模型的过度思考现象，提高了推理效率。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities
on complex problems using long Chain-of-Thought (CoT) reasoning. However, they
often suffer from overthinking, meaning generating unnecessarily lengthy
reasoning steps for simpler problems. This issue may degrade the efficiency of
the models and make them difficult to adapt the reasoning depth to the
complexity of problems. To address this, we introduce a novel metric Token
Entropy Cumulative Average (TECA), which measures the extent of exploration
throughout the reasoning process. We further propose a novel reasoning paradigm
-- Explore Briefly, Then Decide -- with an associated Cumulative Entropy
Regulation (CER) mechanism. This paradigm leverages TECA to help the model
dynamically determine the optimal point to conclude its thought process and
provide a final answer, thus achieving efficient reasoning. Experimental
results across diverse mathematical benchmarks show that our approach
substantially mitigates overthinking without sacrificing problem-solving
ability. With our thinking paradigm, the average response length decreases by
up to 71% on simpler datasets, demonstrating the effectiveness of our method in
creating a more efficient and adaptive reasoning process.

</details>


### [164] [InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in Tool-Augmented Agents](https://arxiv.org/abs/2510.02271)
*Yaxin Du,Yuanshuo Zhang,Xiyuan Yang,Yifan Zhou,Cheng Wang,Gongyi Zou,Xianghe Pang,Wenhao Wang,Menglan Chen,Shuo Tang,Zhiyu Li,Siheng Chen*

Main category: cs.CL

TL;DR: 本研究提出InfoMosaic-Bench基准测试，评估工具增强的代理在多源信息寻求中的表现，揭示了现有大模型在工具利用方面的不足。


<details>
  <summary>Details</summary>
Motivation: 要解决网络内容噪声大和不可靠的问题，并且弥补现实世界任务所需的特定领域知识的不足。

Method: 引入InfoMosaic-Bench基准测试，要求代理结合广泛的搜索和特定领域工具解决复杂任务。

Result: 通过对14个先进的LLM代理的实验，发现仅依赖网络信息的准确率不足，领域工具提供的不一致优势，以及大量失败源于不当工具使用或选择。

Conclusion: 当前的大型语言模型在有效利用工具方面仍存在挑战，特别是在选择和使用工具时面临错误。

Abstract: Information seeking is a fundamental requirement for humans. However,
existing LLM agents rely heavily on open-web search, which exposes two
fundamental weaknesses: online content is noisy and unreliable, and many
real-world tasks require precise, domain-specific knowledge unavailable from
the web. The emergence of the Model Context Protocol (MCP) now allows agents to
interface with thousands of specialized tools, seemingly resolving this
limitation. Yet it remains unclear whether agents can effectively leverage such
tools -- and more importantly, whether they can integrate them with
general-purpose search to solve complex tasks. Therefore, we introduce
InfoMosaic-Bench, the first benchmark dedicated to multi-source information
seeking in tool-augmented agents. Covering six representative domains
(medicine, finance, maps, video, web, and multi-domain integration),
InfoMosaic-Bench requires agents to combine general-purpose search with
domain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalable
pipeline that grounds task conditions in verified tool outputs, enforces
cross-source dependencies, and filters out shortcut cases solvable by trivial
lookup. This design guarantees both reliability and non-triviality. Experiments
with 14 state-of-the-art LLM agents reveal three findings: (i) web information
alone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% pass
rate; (ii) domain tools provide selective but inconsistent benefits, improving
some domains while degrading others; and (iii) 22.4% of failures arise from
incorrect tool usage or selection, highlighting that current LLMs still
struggle with even basic tool handling.

</details>


### [165] [Parallel Scaling Law: Unveiling Reasoning Generalization through A Cross-Linguistic Perspective](https://arxiv.org/abs/2510.02272)
*Wen Yang,Junhong Wu,Chong Li,Chengqing Zong,Jiajun Zhang*

Main category: cs.CL

TL;DR: 本研究探讨了英语中心的大型推理模型的推理能力在不同语言之间的迁移性，发现这种迁移性差异显著，并提出平行训练的重要性及其对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 研究大型推理模型在多语言环境下的推理能力是否能够有效地从英语迁移到其他语言。

Method: 通过系统评估英语中心的大型推理模型在多语言推理基准上的表现，并引入度量标准来量化跨语言可迁移性。

Result: 发现跨语言可迁移性在不同初始模型、目标语言和训练范式上差异显著，并且提出了三个关键发现，包括单语言到首个并行语言的性能大幅提升，以及跨语言推理迁移遵循功率法则的现象。

Conclusion: 英语中心的大型推理模型在跨语言推理方面未能完全普遍化，尤其是在不同的初始模型、目标语言和训练范式之间的表现显著不同。

Abstract: Recent advancements in Reinforcement Post-Training (RPT) have significantly
enhanced the capabilities of Large Reasoning Models (LRMs), sparking increased
interest in the generalization of RL-based reasoning. While existing work has
primarily focused on investigating its generalization across tasks or
modalities, this study proposes a novel cross-linguistic perspective to
investigate reasoning generalization. This raises a crucial question:
$\textit{Does the reasoning capability achieved from English RPT effectively
transfer to other languages?}$ We address this by systematically evaluating
English-centric LRMs on multilingual reasoning benchmarks and introducing a
metric to quantify cross-lingual transferability. Our findings reveal that
cross-lingual transferability varies significantly across initial model, target
language, and training paradigm. Through interventional studies, we find that
models with stronger initial English capabilities tend to over-rely on
English-specific patterns, leading to diminished cross-lingual generalization.
To address this, we conduct a thorough parallel training study. Experimental
results yield three key findings: $\textbf{First-Parallel Leap}$, a substantial
leap in performance when transitioning from monolingual to just a single
parallel language, and a predictable $\textbf{Parallel Scaling Law}$, revealing
that cross-lingual reasoning transfer follows a power-law with the number of
training parallel languages. Moreover, we identify the discrepancy between
actual monolingual performance and the power-law prediction as
$\textbf{Monolingual Generalization Gap}$, indicating that English-centric LRMs
fail to fully generalize across languages. Our study challenges the assumption
that LRM reasoning mirrors human cognition, providing critical insights for the
development of more language-agnostic LRMs.

</details>


### [166] [F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data](https://arxiv.org/abs/2510.02294)
*Ziyin Zhang,Zihan Liao,Hang Yu,Peng Di,Rui Wang*

Main category: cs.CL

TL;DR: F2LLM是一个新的高性能嵌入模型，直接从基础模型微调，显示出成本效益和出色的排行榜表现。


<details>
  <summary>Details</summary>
Motivation: 旨在克服以往顶级嵌入模型要求的大规模对比预训练、复杂的训练流程以及高成本的合成训练数据问题。

Method: F2LLM直接从基础模型微调，使用了600万组查询-文档-负样本对，来源于开源非合成数据集。

Result: F2LLM-4B在MTEB英语排行榜上排名第二，F2LLM-1.7B在1B-2B规模范围内排名第一。

Conclusion: F2LLM模型在训练成本、模型规模和嵌入性能之间取得了良好的平衡，并在多个排行榜上表现优异。

Abstract: We introduce F2LLM - Foundation to Feature Large Language Models, a suite of
state-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike
previous top-ranking embedding models that require massive contrastive
pretraining, sophisticated training pipelines, and costly synthetic training
data, F2LLM is directly finetuned from foundation models on 6 million
query-document-negative tuples curated from open-source, non-synthetic
datasets, striking a strong balance between training cost, model size, and
embedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd
among models with approximately 4B parameters and 7th overall, while F2LLM-1.7B
ranks 1st among models in the 1B-2B size range. To facilitate future research
in the field, we release the models, training dataset, and code, positioning
F2LLM as a strong, reproducible, and budget-friendly baseline for future works.

</details>


### [167] [Drawing Conclusions from Draws: Rethinking Preference Semantics in Arena-Style LLM Evaluation](https://arxiv.org/abs/2510.02306)
*Raphael Tang,Crystina Zhang,Wenyan Li,Carmen Lai,Pontus Stenetorp,Yao Lu*

Main category: cs.CL

TL;DR: 本研究质疑当前大语言模型的竞技场评分系统对平局的处理，提出平局可能表示查询难度而非模型平等，并通过实证研究展示忽略平局可提高战斗结果预测准确性。


<details>
  <summary>Details</summary>
Motivation: 质疑传统的评分动态建模方法，特别是将平局视为两种模型平等的做法，提出平局可能更反映查询难度。

Method: 通过分析三个真实世界的竞技场数据集，评估忽略平局的评分更新对战斗结果预测准确性的影响。

Result: 忽略平局的评分更新使得四种研究的评分系统的战斗结果预测准确性相对提高了1-3%。平局发生在被评为非常简单和高度客观的查询时更为普遍，风险比率分别为1.37和1.35。

Conclusion: 建议未来的评分系统重新考虑现有的平局语义，并在评分更新中考虑查询特性。

Abstract: In arena-style evaluation of large language models (LLMs), two LLMs respond
to a user query, and the user chooses the winning response or deems the
"battle" a draw, resulting in an adjustment to the ratings of both models. The
prevailing approach for modeling these rating dynamics is to view battles as
two-player game matches, as in chess, and apply the Elo rating system and its
derivatives. In this paper, we critically examine this paradigm. Specifically,
we question whether a draw genuinely means that the two models are equal and
hence whether their ratings should be equalized. Instead, we conjecture that
draws are more indicative of query difficulty: if the query is too easy, then
both models are more likely to succeed equally. On three real-world arena
datasets, we show that ignoring rating updates for draws yields a 1-3% relative
increase in battle outcome prediction accuracy (which includes draws) for all
four rating systems studied. Further analyses suggest that draws occur more for
queries rated as very easy and those as highly objective, with risk ratios of
1.37 and 1.35, respectively. We recommend future rating systems to reconsider
existing draw semantics and to account for query properties in rating updates.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [168] [RealClass: A Framework for Classroom Speech Simulation with Public Datasets and Game Engines](https://arxiv.org/abs/2510.01462)
*Ahmed Adel Attia,Jing Liu,Carol Espy Wilson*

Main category: cs.SD

TL;DR: 本研究提出了一种利用游戏引擎合成教室噪声和RIR的方法，创建了RealClass数据集，可以有效填补教育领域AI语音模型的训练数据不足问题。


<details>
  <summary>Details</summary>
Motivation: 由于大规模教室语音数据的匮乏，限制了教育领域中基于AI的语音模型的发展，因此需要一个合成的教室噪声和语音数据集来填补这一空缺。

Method: 利用游戏引擎合成教室噪声和房间脉冲响应（RIR），并将其与公开可用的儿童语音和教学语音数据集结合，构成RealClass数据集。

Result: RealClass数据集在清洁和噪声条件下的实验表明，它与真实教室语音的接近度很高，为相关研究提供了重要的数据支持。

Conclusion: RealClass数据集通过合成教室噪声和真实教室语音的结合，为教育领域的AI驱动语音模型提供了宝贵资源，尤其是在缺乏大量真实教室语音数据的情况下。

Abstract: The scarcity of large-scale classroom speech data has hindered the
development of AI-driven speech models for education. Classroom datasets remain
limited and not publicly available, and the absence of dedicated classroom
noise or Room Impulse Response (RIR) corpora prevents the use of standard data
augmentation techniques.
  In this paper, we introduce a scalable methodology for synthesizing classroom
noise and RIRs using game engines, a versatile framework that can extend to
other domains beyond the classroom. Building on this methodology, we present
RealClass, a dataset that combines a synthesized classroom noise corpus with a
classroom speech dataset compiled from publicly available corpora. The speech
data pairs a children's speech corpus with instructional speech extracted from
YouTube videos to approximate real classroom interactions in clean conditions.
Experiments on clean and noisy speech show that RealClass closely approximates
real classroom speech, making it a valuable asset in the absence of abundant
real classroom speech.

</details>


### [169] [Emotional Text-To-Speech Based on Mutual-Information-Guided Emotion-Timbre Disentanglement](https://arxiv.org/abs/2510.01722)
*Jianing Yang,Sheng Li,Takahiro Shinozaki,Yuki Saito,Hiroshi Saruwatari*

Main category: cs.SD

TL;DR: 本研究提出了一种新颖的情感TTS方法，通过细粒度音素级情感嵌入并分离音色与情感特征，实验结果显示其性能优于现有的TTS系统。


<details>
  <summary>Details</summary>
Motivation: 当前的情感TTS和风格转换方法无法捕捉到参考语音的细微声学细节，因此需要一种新方法来实现更精细的情感表达。

Method: 提出了一种新颖的情感TTS方法，通过细粒度音素级情感嵌入预测，并采用风格分离方法引导特征提取器，降低音色和情感特征之间的互信息。

Result: 实验结果表明，所提出的方法在生成自然、富含情感的语音方面优于基线TTS系统。

Conclusion: 本研究提出的情感TTS方法在生成自然且富有情感的语音方面优于基线系统，展示了分离和细致表征在提升情感TTS系统质量和灵活性中的潜力。

Abstract: Current emotional Text-To-Speech (TTS) and style transfer methods rely on
reference encoders to control global style or emotion vectors, but do not
capture nuanced acoustic details of the reference speech. To this end, we
propose a novel emotional TTS method that enables fine-grained phoneme-level
emotion embedding prediction while disentangling intrinsic attributes of the
reference speech. The proposed method employs a style disentanglement method to
guide two feature extractors, reducing mutual information between timbre and
emotion features, and effectively separating distinct style components from the
reference speech. Experimental results demonstrate that our method outperforms
baseline TTS systems in generating natural and emotionally rich speech. This
work highlights the potential of disentangled and fine-grained representations
in advancing the quality and flexibility of emotional TTS systems.

</details>


### [170] [SingMOS-Pro: An Comprehensive Benchmark for Singing Quality Assessment](https://arxiv.org/abs/2510.01812)
*Yuxun Tang,Lan Liu,Wenhao Feng,Yiwen Zhao,Jionghao Han,Yifeng Yu,Jiatong Shi,Qin Jin*

Main category: cs.SD

TL;DR: SingMOS-Pro是一个用于自动歌唱质量评估的多样化数据集，包含7981段歌唱片段，扩展了对歌词、旋律及整体质量的注释，为未来研究建立了基准。


<details>
  <summary>Details</summary>
Motivation: 歌唱声音生成技术迅速进展，但评估歌唱质量依然是一项重大挑战，现有的主观和客观评估方法无法全面捕捉听感质量。

Method: 引入一个名为SingMOS-Pro的数据集，它是对先前版本SingMOS的扩展，包含了歌词、旋律和整体质量的注释，并收录了来自41个模型生成的7981段歌唱片段。

Result: 该数据集收集了来自不同模型和数据集的歌唱片段，并对每段片段进行了专业评估，确保了评估的一致性，并基于该数据集对相关任务的一些评估方法进行了基准测试。

Conclusion: SingMOS-Pro数据集为自动化歌唱质量评估提供了一个多样化的基础，确保了评估的可靠性，并为未来的研究设定了强有力的基准。

Abstract: Singing voice generation progresses rapidly, yet evaluating singing quality
remains a critical challenge. Human subjective assessment, typically in the
form of listening tests, is costly and time consuming, while existing objective
metrics capture only limited perceptual aspects. In this work, we introduce
SingMOS-Pro, a dataset for automatic singing quality assessment. Building on
our preview version SingMOS, which provides only overall ratings, SingMOS-Pro
expands annotations of the additional part to include lyrics, melody, and
overall quality, offering broader coverage and greater diversity. The dataset
contains 7,981 singing clips generated by 41 models across 12 datasets,
spanning from early systems to recent advances. Each clip receives at least
five ratings from professional annotators, ensuring reliability and
consistency. Furthermore, we explore how to effectively utilize MOS data
annotated under different standards and benchmark several widely used
evaluation methods from related tasks on SingMOS-Pro, establishing strong
baselines and practical references for future research. The dataset can be
accessed at https://huggingface.co/datasets/TangRain/SingMOS-Pro.

</details>


### [171] [HRTFformer: A Spatially-Aware Transformer for Personalized HRTF Upsampling in Immersive Audio Rendering](https://arxiv.org/abs/2510.01891)
*Xuyi Hu,Jian Li,Shaojie Zhang,Stefan Goetz,Lorenzo Picinali,Ozgur B. Akan,Aidan O. T. Hogg*

Main category: cs.SD

TL;DR: 本文提出了一种基于变压器的HRTF空间上采样架构，通过注意机制和邻居差异损失，解决了个性化HRTF在大规模应用中面临的空间一致性和生成能力问题，实验结果显示其性能显著优于当前领先的方法。


<details>
  <summary>Details</summary>
Motivation: 个性化HRTF在沉浸音频应用中的重要性及目前测量过程的复杂性，使得大规模创建个性化HRTF面临挑战。

Method: 提出了一种基于变压器的架构，利用注意机制来捕捉HRTF球面上的空间关联，采用邻居差异损失增强空间一致性。

Result: 我们的模型在高分辨率HRTF重建方面显示出显著的准确度提升，且在感知定位模型和目标频谱失真度量下评估时表现优异。

Conclusion: 我们的模型在生成真实、高保真度的HRTF方面显著优于领先的方法。

Abstract: Personalized Head-Related Transfer Functions (HRTFs) are starting to be
introduced in many commercial immersive audio applications and are crucial for
realistic spatial audio rendering. However, one of the main hesitations
regarding their introduction is that creating personalized HRTFs is impractical
at scale due to the complexities of the HRTF measurement process. To mitigate
this drawback, HRTF spatial upsampling has been proposed with the aim of
reducing measurements required. While prior work has seen success with
different machine learning (ML) approaches, these models often struggle with
long-range spatial consistency and generalization at high upsampling factors.
In this paper, we propose a novel transformer-based architecture for HRTF
upsampling, leveraging the attention mechanism to better capture spatial
correlations across the HRTF sphere. Working in the spherical harmonic (SH)
domain, our model learns to reconstruct high-resolution HRTFs from sparse input
measurements with significantly improved accuracy. To enhance spatial
coherence, we introduce a neighbor dissimilarity loss that promotes magnitude
smoothness, yielding more realistic upsampling. We evaluate our method using
both perceptual localization models and objective spectral distortion metrics.
Experiments show that our model surpasses leading methods by a substantial
margin in generating realistic, high-fidelity HRTFs.

</details>


### [172] [MelCap: A Unified Single-Codebook Neural Codec for High-Fidelity Audio Compression](https://arxiv.org/abs/2510.01903)
*Jingyi Li,Zhiyuan Zhao,Yunfei Liu,Lijian Lin,Ye Zhu,Jiahao Wu,Qiuqiang Kong,Yu Li*

Main category: cs.SD

TL;DR: MelCap是一种新型的神经音频编解码器，同时适用于语音、音乐和一般音频，具有出色的音质和效率。


<details>
  <summary>Details</summary>
Motivation: 现有音频编解码方法要么依赖单一的量化器，只适用于语音；要么使用多个量化器，但不适合下游任务。

Method: MelCap通过将音频重建分为两个阶段，首先将音频转化为梅尔谱图，并使用2D分词器进行压缩和量化；然后通过Vocoder从梅尔离散代币中恢复波形。

Result: MelCap在音质上与主流多代码本方法相媲美，同时保持了单代码本设计的计算简单性.

Conclusion: MelCap是一种统一的神经编解码器，能够有效处理多种音频类型，并在音质和计算效率上达到优秀的平衡。

Abstract: Neural audio codecs have recently emerged as powerful tools for high-quality
and low-bitrate audio compression, leveraging deep generative models to learn
latent representations of audio signals. However, existing approaches either
rely on a single quantizer that only processes speech domain, or on multiple
quantizers that are not well suited for downstream tasks. To address this
issue, we propose MelCap, a unified "one-codebook-for-all" neural codec that
effectively handles speech, music, and general sound. By decomposing audio
reconstruction into two stages, our method preserves more acoustic details than
previous single-codebook approaches, while achieving performance comparable to
mainstream multi-codebook methods. In the first stage, audio is transformed
into mel-spectrograms, which are compressed and quantized into compact single
tokens using a 2D tokenizer. A perceptual loss is further applied to mitigate
the over-smoothing artifacts observed in spectrogram reconstruction. In the
second stage, a Vocoder recovers waveforms from the mel discrete tokens in a
single forward pass, enabling real-time decoding. Both objective and subjective
evaluations demonstrate that MelCap achieves quality on comparable to
state-of-the-art multi-codebook codecs, while retaining the computational
simplicity of a single-codebook design, thereby providing an effective
representation for downstream tasks.

</details>


### [173] [Exploring Resolution-Wise Shared Attention in Hybrid Mamba-U-Nets for Improved Cross-Corpus Speech Enhancement](https://arxiv.org/abs/2510.01958)
*Nikolai Lund Kühne,Jesper Jensen,Jan Østergaard,Zheng-Hua Tan*

Main category: cs.SD

TL;DR: 提出了一种新的RWSA-MambaUNet模型，通过整合Mamba和注意力机制，显著提升了语音增强的泛化能力和效率。


<details>
  <summary>Details</summary>
Motivation: 受最近语音增强技术进展的启发，研究将Mamba与注意力机制相结合以提升模型在不同语料库上的泛化能力。

Method: 结合Mamba和多头注意力机制的U-Net结构，采用了分辨率共享注意力（RWSA）。

Result: RWSA-MambaUNet模型在两个外部测试集上达到最新的泛化性能，特别是在DNS 2020和EARS-WHAM_v2测试集上超越所有基线。

Conclusion: RWSA-MambaUNet在跨语料库的表现上取得了最佳的泛化性能，通过减少模型参数和计算复杂度，表现优异。

Abstract: Recent advances in speech enhancement have shown that models combining Mamba
and attention mechanisms yield superior cross-corpus generalization
performance. At the same time, integrating Mamba in a U-Net structure has
yielded state-of-the-art enhancement performance, while reducing both model
size and computational complexity. Inspired by these insights, we propose
RWSA-MambaUNet, a novel and efficient hybrid model combining Mamba and
multi-head attention in a U-Net structure for improved cross-corpus
performance. Resolution-wise shared attention (RWSA) refers to layerwise
attention-sharing across corresponding time- and frequency resolutions. Our
best-performing RWSA-MambaUNet model achieves state-of-the-art generalization
performance on two out-of-domain test sets. Notably, our smallest model
surpasses all baselines on the out-of-domain DNS 2020 test set in terms of
PESQ, SSNR, and ESTOI, and on the out-of-domain EARS-WHAM_v2 test set in terms
of SSNR, ESTOI, and SI-SDR, while using less than half the model parameters and
a fraction of the FLOPs.

</details>


### [174] [Bias beyond Borders: Global Inequalities in AI-Generated Music](https://arxiv.org/abs/2510.01963)
*Ahmet Solak,Florian Grötschla,Luca A. Lanzendörfer,Roger Wattenhofer*

Main category: cs.SD

TL;DR: 本文介绍了GlobalDISCO，一个包含全球音乐风格的大规模数据集，揭示了音乐生成模型在不同地区和流派之间的偏见。


<details>
  <summary>Details</summary>
Motivation: 针对音乐生成模型在国家、语言、文化和音乐流派偏见方面的研究不足，以及缺乏全球音乐多样性的相关数据集和基准测试。

Method: 引入GlobalDISCO数据集，包含73000首音乐作品及93000首参考曲目，覆盖147种语言和79个国家的音乐风格。

Result: 通过评估发现高资源地区和低资源地区的音乐质量差异明显，且模型在主流及地方特色音乐风格上的表现存在差异。

Conclusion: 研究发现高资源与低资源地区在音乐质量和与参考音乐的一致性上存在巨大差异，主流和地方特色音乐风格之间的模型表现差异显著。

Abstract: While recent years have seen remarkable progress in music generation models,
research on their biases across countries, languages, cultures, and musical
genres remains underexplored. This gap is compounded by the lack of datasets
and benchmarks that capture the global diversity of music. To address these
challenges, we introduce GlobalDISCO, a large-scale dataset consisting of 73k
music tracks generated by state-of-the-art commercial generative music models,
along with paired links to 93k reference tracks in LAION-DISCO-12M. The dataset
spans 147 languages and includes musical style prompts extracted from
MusicBrainz and Wikipedia. The dataset is globally balanced, representing
musical styles from artists across 79 countries and five continents. Our
evaluation reveals large disparities in music quality and alignment with
reference music between high-resource and low-resource regions. Furthermore, we
find marked differences in model performance between mainstream and
geographically niche genres, including cases where models generate music for
regional genres that more closely align with the distribution of mainstream
styles.

</details>


### [175] [Multi-bit Audio Watermarking](https://arxiv.org/abs/2510.01968)
*Luca A. Lanzendörfer,Kyle Fearne,Florian Grötschla,Roger Wattenhofer*

Main category: cs.SD

TL;DR: 提出了一种名为Timbru的后期音频水印模型，通过无数据集的方法实现了优越的鲁棒性和感知质量。


<details>
  <summary>Details</summary>
Motivation: 研究现有音频水印技术在鲁棒性和感知度之间的权衡，寻求一种新的高效方法。

Method: 通过对预训练音频VAE的潜在空间进行每音频梯度优化，结合消息和感知损失，不需要训练嵌入-检测模型。

Result: 在MUSDB18-HQ上对比AudioSeal、WavMark和SilentCipher，在常见攻击下，Timbru取得了最佳的平均比特错误率。

Conclusion: Timbru方法在保持音频感知质量的同时，展现了优越的水印鲁棒性，提供了一种高效的无数据集音频水印解决方案。

Abstract: We present Timbru, a post-hoc audio watermarking model that achieves
state-of-the-art robustness and imperceptibility trade-offs without training an
embedder-detector model. Given any 44.1 kHz stereo music snippet, our method
performs per-audio gradient optimization to add imperceptible perturbations in
the latent space of a pretrained audio VAE, guided by a combined message and
perceptual loss. The watermark can then be extracted using a pretrained CLAP
model. We evaluate 16-bit watermarking on MUSDB18-HQ against AudioSeal,
WavMark, and SilentCipher across common filtering, noise, compression,
resampling, cropping, and regeneration attacks. Our approach attains the best
average bit error rates, while preserving perceptual quality, demonstrating an
efficient, dataset-free path to imperceptible audio watermarking.

</details>


### [176] [SoundReactor: Frame-level Online Video-to-Audio Generation](https://arxiv.org/abs/2510.02110)
*Koichi Saito,Julian Tanke,Christian Simon,Masato Ishii,Kazuki Shimada,Zachary Novack,Zhi Zhong,Akio Hayakawa,Takashi Shibuya,Yuki Mitsufuji*

Main category: cs.SD

TL;DR: 本文提出了SoundReactor，一个首个在线视频到音频生成框架，能够实时生成高质量音频，具有低延迟和良好的音视频同步。


<details>
  <summary>Details</summary>
Motivation: 现有的视频到音频生成模型不能在线处理视频流，限制了其在实时内容创作和生成模型中的应用。

Method: 我们设计了一个自回归的端到端框架，使用解码器控因的因果变换器来处理视频帧，并结合DINOv2视觉编码器提取的特征。

Result: 在AAA游戏视频的基准测试中，我们的模型能够生成语义和时间上对齐的高质量全频带立体声音频，且每帧的波形级延迟小。

Conclusion: 我们提出的SoundReactor模型在实时视频到音频生成领域的有效性得到了验证，能够以低延迟生成高质量的音频。

Abstract: Prevailing Video-to-Audio (V2A) generation models operate offline, assuming
an entire video sequence or chunks of frames are available beforehand. This
critically limits their use in interactive applications such as live content
creation and emerging generative world models. To address this gap, we
introduce the novel task of frame-level online V2A generation, where a model
autoregressively generates audio from video without access to future video
frames. Furthermore, we propose SoundReactor, which, to the best of our
knowledge, is the first simple yet effective framework explicitly tailored for
this task. Our design enforces end-to-end causality and targets low per-frame
latency with audio-visual synchronization. Our model's backbone is a
decoder-only causal transformer over continuous audio latents. For vision
conditioning, it leverages grid (patch) features extracted from the smallest
variant of the DINOv2 vision encoder, which are aggregated into a single token
per frame to maintain end-to-end causality and efficiency. The model is trained
through a diffusion pre-training followed by consistency fine-tuning to
accelerate the diffusion head decoding. On a benchmark of diverse gameplay
videos from AAA titles, our model successfully generates semantically and
temporally aligned, high-quality full-band stereo audio, validated by both
objective and human evaluations. Furthermore, our model achieves low per-frame
waveform-level latency (26.3ms with the head NFE=1, 31.5ms with NFE=4) on
30FPS, 480p videos using a single H100. Demo samples are available at
https://koichi-saito-sony.github.io/soundreactor/.

</details>


### [177] [Go witheFlow: Real-time Emotion Driven Audio Effects Modulation](https://arxiv.org/abs/2510.02171)
*Edmund Dervakos,Spyridon Kantarelis,Vassilis Lyberatos,Jason Liartis,Giorgos Stamou*

Main category: cs.SD

TL;DR: witheFlow系统通过自动调节音效，旨在增强实时音乐表演，探索人机协作。


<details>
  <summary>Details</summary>
Motivation: 探索人类与机器在音乐表演中的协作，弥补机器缺乏情感体验的不足。

Method: 该系统通过提取生物信号和音频特征来实现音效的自动调节。

Result: 目前处于概念验证阶段，能够在笔记本电脑上轻量运行且为开源。

Conclusion: witheFlow系统能够通过自动调节音频效果来提升实时音乐表演的质量。

Abstract: Music performance is a distinctly human activity, intrinsically linked to the
performer's ability to convey, evoke, or express emotion. Machines cannot
perform music in the human sense; they can produce, reproduce, execute, or
synthesize music, but they lack the capacity for affective or emotional
experience. As such, music performance is an ideal candidate through which to
explore aspects of collaboration between humans and machines. In this paper, we
introduce the witheFlow system, designed to enhance real-time music performance
by automatically modulating audio effects based on features extracted from both
biosignals and the audio itself. The system, currently in a proof-of-concept
phase, is designed to be lightweight, able to run locally on a laptop, and is
open-source given the availability of a compatible Digital Audio Workstation
and sensors.

</details>


### [178] [High-Fidelity Speech Enhancement via Discrete Audio Tokens](https://arxiv.org/abs/2510.02187)
*Luca A. Lanzendörfer,Frédéric Berdoz,Antonis Asonitis,Roger Wattenhofer*

Main category: cs.SD

TL;DR: DAC-SE1是一个新提出的简化语言模型的语音增强框架，展现出优越的性能和高质量的音频增强，支持更广泛的应用。


<details>
  <summary>Details</summary>
Motivation: 现有的基于自回归的语音增强方法复杂且受限于低采样率编解码器，难以实现广泛的和高质量的语音增强。

Method: 采用简化的语言模型作为基础，通过离散高分辨率音频表示进行语音增强。

Result: DAC-SE1保留了精细的声学细节，并在语义连贯性上表现良好，超越了传统方法。

Conclusion: DAC-SE1超越了当前最先进的自回归语音增强方法，在客观感知指标和人类评估中表现优异。

Abstract: Recent autoregressive transformer-based speech enhancement (SE) methods have
shown promising results by leveraging advanced semantic understanding and
contextual modeling of speech. However, these approaches often rely on complex
multi-stage pipelines and low sampling rate codecs, limiting them to narrow and
task-specific speech enhancement. In this work, we introduce DAC-SE1, a
simplified language model-based SE framework leveraging discrete
high-resolution audio representations; DAC-SE1 preserves fine-grained acoustic
details while maintaining semantic coherence. Our experiments show that DAC-SE1
surpasses state-of-the-art autoregressive SE methods on both objective
perceptual metrics and in a MUSHRA human evaluation. We release our codebase
and model checkpoints to support further research in scalable, unified, and
high-quality speech enhancement.

</details>

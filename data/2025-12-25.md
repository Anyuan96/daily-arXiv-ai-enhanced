<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 64]
- [cs.CL](#cs.CL) [Total: 31]
- [cs.SD](#cs.SD) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [VL4Gaze: Unleashing Vision-Language Models for Gaze Following](https://arxiv.org/abs/2512.20735)
*Shijing Wang,Chaoqun Cui,Yaping Huang,Hyung Jin Chang,Yihua Cheng*

Main category: cs.CV

TL;DR: VL4Gaze是首个大规模基准测试，用于评估和训练视觉语言模型在视线理解方面的能力，包含48.9万个自动生成的QA对和124K张图像，涵盖四个互补任务。实验表明现有VLMs在未经过专门训练时难以可靠推断视线信息，而针对性的多任务监督能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在视线理解方面存在空白，视线是理解注意力、意图和社会互动的关键线索，但没有专业基准来系统评估和训练VLMs的视线理解能力。

Method: 构建VL4Gaze基准测试，包含489K自动生成的QA对和124K图像，将视线理解统一为视觉问答问题，涵盖四个任务：视线对象描述、视线方向描述、注视点定位、和模糊问题识别。评估商业和开源VLMs在上下文学习和微调设置下的表现。

Result: 大规模VLMs在没有任务特定监督的情况下难以可靠推断视线语义和空间定位。而在VL4Gaze上训练能在所有任务上带来显著且一致的改进。

Conclusion: 针对性的多任务监督对于开发VLMs的视线理解能力至关重要，VL4Gaze基准测试将支持这一方向的进一步研究和发展。

Abstract: Human gaze provides essential cues for interpreting attention, intention, and social interaction in visual scenes, yet gaze understanding remains largely unexplored in current vision-language models (VLMs). While recent VLMs achieve strong scene-level reasoning across a range of visual tasks, there exists no benchmark that systematically evaluates or trains them for gaze interpretation, leaving open the question of whether gaze understanding can emerge from general-purpose vision-language pre-training. To address this gap, we introduce VL4Gaze, the first large-scale benchmark designed to investigate, evaluate, and unlock the potential of VLMs for gaze understanding. VL4Gaze contains 489K automatically generated question-answer pairs across 124K images and formulates gaze understanding as a unified VQA problem through four complementary tasks: (1) gaze object description, (2) gaze direction description, (3) gaze point location, and (4) ambiguous question recognition. We comprehensively evaluate both commercial and open-source VLMs under in-context learning and fine-tuning settings. The results show that even large-scale VLMs struggle to reliably infer gaze semantics and spatial localization without task-specific supervision. In contrast, training on VL4Gaze brings substantial and consistent improvements across all tasks, highlighting the importance of targeted multi-task supervision for developing gaze understanding capabilities in VLMs. We will release the dataset and code to support further research and development in this direction.

</details>


### [2] [TrashDet: Iterative Neural Architecture Search for Efficient Waste Detection](https://arxiv.org/abs/2512.20746)
*Tony Tran,Bin Hu*

Main category: cs.CV

TL;DR: 本文提出一种面向TinyML设备的垃圾检测框架，通过硬件感知的神经网络架构搜索方法，在TACO数据集上开发了TrashDet系列检测器，显著提升了检测精度并大幅降低了能耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备和物联网设备上实现高效的垃圾检测，需要平衡检测精度与计算资源消耗，传统方法往往无法同时满足这两方面的要求。

Method: 采用迭代式进化搜索框架，构建ResDets超网络，通过交替优化主干网络和检测头/颈部的策略，结合种群传递机制和精度预测器来降低搜索成本和提升稳定性。

Result: TrashDet-l在五类TACO子集上达到19.5 mAP50，参数量为30.5M，比之前检测器精度提升3.6 mAP50且参数量更少。在MAX78002微控制器上，TrashDet-ResNet单次推理能耗7525μJ，延迟26.7ms，相比现有TinyML检测器能耗降低88%，延迟降低78%。

Conclusion: 该方法成功开发了可扩展的垃圾检测器系列，为不同TinyML部署预算提供了多样化选择，在资源受限硬件上实现了精度与效率的良好平衡。

Abstract: This paper addresses trash detection on the TACO dataset under strict TinyML constraints using an iterative hardware-aware neural architecture search framework targeting edge and IoT devices. The proposed method constructs a Once-for-All-style ResDets supernet and performs iterative evolutionary search that alternates between backbone and neck/head optimization, supported by a population passthrough mechanism and an accuracy predictor to reduce search cost and improve stability. This framework yields a family of deployment-ready detectors, termed TrashDets. On a five-class TACO subset (paper, plastic, bottle, can, cigarette), the strongest variant, TrashDet-l, achieves 19.5 mAP50 with 30.5M parameters, improving accuracy by up to 3.6 mAP50 over prior detectors while using substantially fewer parameters. The TrashDet family spans 1.2M to 30.5M parameters with mAP50 values between 11.4 and 19.5, providing scalable detector options for diverse TinyML deployment budgets on resource-constrained hardware. On the MAX78002 microcontroller with the TrashNet dataset, two specialized variants, TrashDet-ResNet and TrashDet-MBNet, jointly dominate the ai87-fpndetector baseline, with TrashDet-ResNet achieving 7525~$μ$J energy per inference at 26.7 ms latency and 37.45 FPS, and TrashDet-MBNet improving mAP50 by 10.2%; together they reduce energy consumption by up to 88%, latency by up to 78%, and average power by up to 53% compared to existing TinyML detectors.

</details>


### [3] [OccuFly: A 3D Vision Benchmark for Semantic Scene Completion from the Aerial Perspective](https://arxiv.org/abs/2512.20770)
*Markus Gross,Sai B. Matha,Aya Fahmy,Rui Song,Daniel Cremers,Henri Meess*

Main category: cs.CV

TL;DR: 本文提出了OccuFly，这是第一个基于摄像头的真实世界航空语义场景补全基准，用于解决无人机在航空场景中语义场景补全的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前语义场景补全主要研究地面场景（如自动驾驶），而航空场景（如自主飞行）研究较少，限制了相关应用的进展。无人机因飞行限制等原因难以使用LiDAR，而摄像头在无人机上普遍存在，因此需要开发基于摄像头的航空SSC解决方案。

Method: 创建了OccuFly基准数据集，包含50m、40m、30m高度在春、夏、秋、冬季采集的数据，涵盖城市、工业和农村场景。提出了基于摄像头的LiDAR-free数据生成框架，利用传统3D重建将标注的2D掩码提升到重建点云中，大幅减少手动3D标注工作量。

Result: 构建了包含22个语义类别的真实世界航空SSC数据集，格式遵循现有研究标准以便集成。在OccuFly上对现有最先进方法进行了基准测试，揭示了高空视角带来的独特挑战。

Conclusion: OccuFly为航空3D场景理解提供了全面的视觉基准，展示了基于摄像头的航空语义场景补全的可行性和挑战，推动了无人机3D感知技术的发展。

Abstract: Semantic Scene Completion (SSC) is crucial for 3D perception in mobile robotics, as it enables holistic scene understanding by jointly estimating dense volumetric occupancy and per-voxel semantics. Although SSC has been widely studied in terrestrial domains such as autonomous driving, aerial scenarios like autonomous flying remain largely unexplored, thereby limiting progress on downstream applications. Furthermore, LiDAR sensors represent the primary modality for SSC data generation, which poses challenges for most uncrewed aerial vehicles (UAVs) due to flight regulations, mass and energy constraints, and the sparsity of LiDAR-based point clouds from elevated viewpoints. To address these limitations, we introduce OccuFly, the first real-world, camera-based aerial SSC benchmark, captured at altitudes of 50m, 40m, and 30m during spring, summer, fall, and winter. OccuFly covers urban, industrial, and rural scenarios, provides 22 semantic classes, and the data format adheres to established conventions to facilitate seamless integration with existing research. Crucially, we propose a LiDAR-free data generation framework based on camera modality, which is ubiquitous on modern UAVs. By utilizing traditional 3D reconstruction, our framework automates label transfer by lifting a subset of annotated 2D masks into the reconstructed point cloud, thereby substantially minimizing manual 3D annotation effort. Finally, we benchmark the state-of-the-art on OccuFly and highlight challenges specific to elevated viewpoints, yielding a comprehensive vision benchmark for holistic aerial 3D scene understanding.

</details>


### [4] [NULLBUS: Multimodal Mixed-Supervision for Breast Ultrasound Segmentation via Nullable Global-Local Prompts](https://arxiv.org/abs/2512.20783)
*Raja Mallina,Bryar Shareef*

Main category: cs.CV

TL;DR: NullBUS是一种多模态混合监督框架，能够同时学习有无提示的乳腺超声图像分割，通过可学习的空嵌入和存在掩码处理缺失的文本信息，在混合提示可用性下实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 乳腺超声分割对计算机辅助诊断和治疗规划至关重要，但许多公共数据集缺少可靠的元数据或报告，限制了多模态训练和模型鲁棒性。

Method: 提出了Nullable Prompts（可空提示），通过可学习的空嵌入和存在掩码实现，当元数据缺失时可回退到仅图像证据，当文本存在时可利用文本信息。

Result: 在三个公共BUS数据集的统一池上评估，NullBUS实现了0.8568的平均IoU和0.9103的平均Dice，表现出最先进的性能。

Conclusion: NullBUS框架有效解决了多模态数据中提示可用性不一致的问题，在混合提示条件下保持了强大的分割性能。

Abstract: Breast ultrasound (BUS) segmentation provides lesion boundaries essential for computer-aided diagnosis and treatment planning. While promptable methods can improve segmentation performance and tumor delineation when text or spatial prompts are available, many public BUS datasets lack reliable metadata or reports, constraining training to small multimodal subsets and reducing robustness. We propose NullBUS, a multimodal mixed-supervision framework that learns from images with and without prompts in a single model. To handle missing text, we introduce nullable prompts, implemented as learnable null embeddings with presence masks, enabling fallback to image-only evidence when metadata are absent and the use of text when present. Evaluated on a unified pool of three public BUS datasets, NullBUS achieves a mean IoU of 0.8568 and a mean Dice of 0.9103, demonstrating state-of-the-art performance under mixed prompt availability.

</details>


### [5] [Learning to Sense for Driving: Joint Optics-Sensor-Model Co-Design for Semantic Segmentation](https://arxiv.org/abs/2512.20815)
*Reeshad Khan amd John Gauch*

Main category: cs.CV

TL;DR: 本文提出了一个任务驱动的协同设计框架，将光学、传感器建模和轻量级语义分割网络统一到端到端的RAW-to-task流程中，通过联合优化实现比传统固定流水线更好的分割性能。


<details>
  <summary>Details</summary>
Motivation: 传统的自动驾驶流水线将相机设计与下游感知任务解耦，使用固定光学和手动设计的ISP，优先考虑人类可视图像而非机器语义信息，导致在去马赛克、去噪或量化过程中丢失信息，并迫使模型适应传感器伪影。

Method: 基于DeepLens的端到端协同设计框架，集成真实手机级镜头模型、可学习颜色滤波器阵列、泊松-高斯噪声过程和量化，直接针对分割目标进行优化。

Result: 在KITTI-360上的评估显示mIoU一致提升，光学建模和CFA学习带来的收益最大，尤其对细薄或低光敏感类别的提升显著。

Conclusion: 研究确立了光学、传感器和网络的全栈协同优化作为实现高效、可靠和可部署自动驾驶感知的原则性路径。

Abstract: Traditional autonomous driving pipelines decouple camera design from downstream perception, relying on fixed optics and handcrafted ISPs that prioritize human viewable imagery rather than machine semantics. This separation discards information during demosaicing, denoising, or quantization, while forcing models to adapt to sensor artifacts. We present a task-driven co-design framework that unifies optics, sensor modeling, and lightweight semantic segmentation networks into a single end-to-end RAW-to-task pipeline. Building on DeepLens[19], our system integrates realistic cellphone-scale lens models, learnable color filter arrays, Poisson-Gaussian noise processes, and quantization, all optimized directly for segmentation objectives. Evaluations on KITTI-360 show consistent mIoU improvements over fixed pipelines, with optics modeling and CFA learning providing the largest gains, especially for thin or low-light-sensitive classes. Importantly, these robustness gains are achieved with a compact ~1M-parameter model running at ~28 FPS, demonstrating edge deployability. Visual and quantitative analyses further highlight how co-designed sensors adapt acquisition to semantic structure, sharpening boundaries and maintaining accuracy under blur, noise, and low bit-depth. Together, these findings establish full-stack co-optimization of optics, sensors, and networks as a principled path toward efficient, reliable, and deployable perception in autonomous systems.

</details>


### [6] [CHAMMI-75: pre-training multi-channel models with heterogeneous microscopy images](https://arxiv.org/abs/2512.20833)
*Vidit Agrawal,John Peters,Tyler N. Thompson,Mohammad Vali Sanian,Chau Pham,Nikita Moshkov,Arshad Kazi,Aditya Pillai,Jack Freeman,Byunguk Kang,Samouil L. Farhi,Ernest Fraenkel,Ron Stewart,Lassi Paavolainen,Bryan A. Plummer,Juan C. Caicedo*

Main category: cs.CV

TL;DR: 这篇论文提出了CHAMMI-75，一个包含75个不同生物学研究的异质多通道显微镜图像数据集，旨在解决现有细胞形态学模型因成像类型不同而无法跨研究重用的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的细胞形态量化模型通常针对单一显微镜成像类型训练，导致模型无法在不同技术规格（如通道数不同）或超出分布范围的实验条件下跨生物学研究重用。

Method: 作者从公开来源整理构建了CHAMMI-75数据集，包含多样化的显微镜成像模式，用于研究能够处理任何显微镜图像类型的通道自适应细胞形态学模型。

Result: 实验表明，使用CHAMMI-75进行训练可以提升多通道生物成像任务的性能，主要归功于其显微镜模式的高度多样性。

Conclusion: 这项工作为创建新一代适用于生物学研究的细胞形态学模型铺平了道路。

Abstract: Quantifying cell morphology using images and machine learning has proven to be a powerful tool to study the response of cells to treatments. However, models used to quantify cellular morphology are typically trained with a single microscopy imaging type. This results in specialized models that cannot be reused across biological studies because the technical specifications do not match (e.g., different number of channels), or because the target experimental conditions are out of distribution. Here, we present CHAMMI-75, an open access dataset of heterogeneous, multi-channel microscopy images from 75 diverse biological studies. We curated this resource from publicly available sources to investigate cellular morphology models that are channel-adaptive and can process any microscopy image type. Our experiments show that training with CHAMMI-75 can improve performance in multi-channel bioimaging tasks primarily because of its high diversity in microscopy modalities. This work paves the way to create the next generation of cellular morphology models for biological studies.

</details>


### [7] [Input-Adaptive Visual Preprocessing for Efficient Fast Vision-Language Model Inference](https://arxiv.org/abs/2512.20839)
*Putu Indah Githa Cahyani,Komang David Dananjaya Suartana,Novanto Yudistira*

Main category: cs.CV

TL;DR: 本文提出了一种自适应视觉预处理方法，通过动态调整输入分辨率和截取策略，减少VLM模型的计算开销，在保持模型结构不变的情况下将成果推理时间减少超过50%。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉-语言模型在多模态理解任务上表现强劲，但高分辨率图像导致的高计算成本和推理延迟截眼其部署，而现有图像预处理方法之间存在冗余计算问题。

Method: 设计了一种基于图像内容特征的自适应视觉预处理流程，包括内容感知分析、动态分辨率选择和内容依赖截取，从而在FastVLM模型前减少冗余计算。

Result: 在DocVQA数据集上评估>把冗余视觉关键字复杂度降低更近55%，平均推理时间获得明显改善，并将每张图像推理耗时缩短了超过50%。

Conclusion: 证明输入感知预处理是提升VLM模型生产性能的有效策略，方法过程轻量且无需修改模型结构，有望推广至实际部署应用场景中。

Abstract: Vision-Language Models (VLMs) have demonstrated strong performance on multimodal reasoning tasks, but their deployment remains challenging due to high inference latency and computational cost, particularly when processing high-resolution visual inputs. While recent architectures such as FastVLM improve efficiency through optimized vision encoders, existing pipelines still rely on static visual preprocessing, leading to redundant computation for visually simple inputs. In this work, we propose an adaptive visual preprocessing method that dynamically adjusts input resolution and spatial coverage based on image content characteristics. The proposed approach combines content-aware image analysis, adaptive resolution selection, and content-aware cropping to reduce visual redundancy prior to vision encoding. Importantly, the method is integrated with FastVLM without modifying its architecture or requiring retraining. We evaluate the proposed method on a subset of the DocVQA dataset in an inference-only setting, focusing on efficiency-oriented metrics. Experimental results show that adaptive preprocessing reduces per-image inference time by over 50\%, lowers mean full generation time, and achieves a consistent reduction of more than 55\% in visual token count compared to the baseline pipeline. These findings demonstrate that input-aware preprocessing is an effective and lightweight strategy for improving deployment-oriented efficiency of vision-language models. To facilitate reproducibility, our implementation is provided as a fork of the FastVLM repository, incorporating the files for the proposed method, and is available at https://github.com/kmdavidds/mlfastlm.

</details>


### [8] [ALIVE: An Avatar-Lecture Interactive Video Engine with Content-Aware Retrieval for Real-Time Interaction](https://arxiv.org/abs/2512.20858)
*Md Zabirul Islam,Md Motaleb Hossen Manik,Ge Wang*

Main category: cs.CV

TL;DR: ALIVE是一个本地化的交互式视频引擎，通过AI头像讲解、内容感知检索和实时多模态交互，将被动讲座观看转化为动态学习体验。


<details>
  <summary>Details</summary>
Motivation: 传统讲座视频缺乏实时澄清机制，学生在遇到困惑时只能外部搜索；现有交互系统存在缺乏讲座意识、依赖云端服务或未能整合检索与头像讲解等问题。

Method: 开发ALIVE系统，包含：(1)基于ASR转录、LLM优化和神经头像合成的头像讲解；(2)结合语义相似性和时间戳对齐的内容感知检索；(3)支持文本/语音提问和头像/文本回复的实时多模态交互；采用轻量级嵌入模型、FAISS检索和分段头像合成。

Result: 在医学影像课程上测试显示，ALIVE提供了准确、内容感知且具有吸引力的实时支持，检索准确性和延迟性能良好。

Conclusion: ALIVE展示了多模态AI结合内容感知检索和本地部署如何显著提升录制讲座的教学价值，为下一代交互式学习环境提供了可扩展路径。

Abstract: Traditional lecture videos offer flexibility but lack mechanisms for real-time clarification, forcing learners to search externally when confusion arises. Recent advances in large language models and neural avatars provide new opportunities for interactive learning, yet existing systems typically lack lecture awareness, rely on cloud-based services, or fail to integrate retrieval and avatar-delivered explanations in a unified, privacy-preserving pipeline.
  We present ALIVE, an Avatar-Lecture Interactive Video Engine that transforms passive lecture viewing into a dynamic, real-time learning experience. ALIVE operates fully on local hardware and integrates (1) Avatar-delivered lecture generated through ASR transcription, LLM refinement, and neural talking-head synthesis; (2) A content-aware retrieval mechanism that combines semantic similarity with timestamp alignment to surface contextually relevant lecture segments; and (3) Real-time multimodal interaction, enabling students to pause the lecture, ask questions through text or voice, and receive grounded explanations either as text or as avatar-delivered responses.
  To maintain responsiveness, ALIVE employs lightweight embedding models, FAISS-based retrieval, and segmented avatar synthesis with progressive preloading. We demonstrate the system on a complete medical imaging course, evaluate its retrieval accuracy, latency characteristics, and user experience, and show that ALIVE provides accurate, content-aware, and engaging real-time support.
  ALIVE illustrates how multimodal AI-when combined with content-aware retrieval and local deployment-can significantly enhance the pedagogical value of recorded lectures, offering an extensible pathway toward next-generation interactive learning environments.

</details>


### [9] [Lightweight framework for underground pipeline recognition and spatial localization based on multi-view 2D GPR images](https://arxiv.org/abs/2512.20866)
*Haotian Lv,Chao Li,Jiangbo Dai,Yuhui Zhang,Zepeng Fan,Yiqiu Tan,Dawei Wang,Binglei Xie*

Main category: cs.CV

TL;DR: 该论文提出了一种用于3D GPR地下管道检测的智能检测框架，通过多视图特征关联、小目标识别增强和空间约束算法，显著提升了复杂场景下的管道检测精度。


<details>
  <summary>Details</summary>
Motivation: 解决3D GPR地下管道检测中存在的多视图特征相关性弱、小尺寸目标识别精度低、复杂场景鲁棒性不足等问题。

Method: 1) 基于B/C/D-Scan三视图联合分析策略建立特征评估方法；2) 提出DCO-YOLO框架，集成DySample、CGLU和OutlookAttention机制；3) 开发3D-DIoU空间特征匹配算法实现多视图标注自动关联。

Result: 在真实城市地下管道数据上的实验显示，该方法在复杂多管道场景下准确率、召回率和mAP分别达到96.2%、93.3%和96.7%，比基线模型提升2.0%、2.1%和0.9%。

Conclusion: 该研究将深度学习优化策略与3D GPR物理特性相结合，为地下管道的智能识别和定位提供了一个高效可靠的新技术框架。

Abstract: To address the issues of weak correlation between multi-view features, low recognition accuracy of small-scale targets, and insufficient robustness in complex scenarios in underground pipeline detection using 3D GPR, this paper proposes a 3D pipeline intelligent detection framework. First, based on a B/C/D-Scan three-view joint analysis strategy, a three-dimensional pipeline three-view feature evaluation method is established by cross-validating forward simulation results obtained using FDTD methods with actual measurement data. Second, the DCO-YOLO framework is proposed, which integrates DySample, CGLU, and OutlookAttention cross-dimensional correlation mechanisms into the original YOLOv11 algorithm, significantly improving the small-scale pipeline edge feature extraction capability. Furthermore, a 3D-DIoU spatial feature matching algorithm is proposed, which integrates three-dimensional geometric constraints and center distance penalty terms to achieve automated association of multi-view annotations. The three-view fusion strategy resolves inherent ambiguities in single-view detection. Experiments based on real urban underground pipeline data show that the proposed method achieves accuracy, recall, and mean average precision of 96.2%, 93.3%, and 96.7%, respectively, in complex multi-pipeline scenarios, which are 2.0%, 2.1%, and 0.9% higher than the baseline model. Ablation experiments validated the synergistic optimization effect of the dynamic feature enhancement module and Grad-CAM++ heatmap visualization demonstrated that the improved model significantly enhanced its ability to focus on pipeline geometric features. This study integrates deep learning optimization strategies with the physical characteristics of 3D GPR, offering an efficient and reliable novel technical framework for the intelligent recognition and localization of underground pipelines.

</details>


### [10] [NeRV360: Neural Representation for 360-Degree Videos with a Viewport Decoder](https://arxiv.org/abs/2512.20871)
*Daichi Arai,Kyohei Unno,Yasuko Sugito,Yuichi Kusakabe*

Main category: cs.CV

TL;DR: NeRV360是一个针对360度视频的高效隐式神经表示框架，通过选择性视口解码替代全帧重建，显著降低内存使用并提升解码速度。


<details>
  <summary>Details</summary>
Motivation: 传统隐式神经视频表示（NeRV）在处理高分辨率360度视频时存在内存占用高、解码速度慢的问题，无法满足实时应用需求。

Method: 提出端到端框架NeRV360，将视口提取整合到解码过程中，引入时空仿射变换模块实现基于视点和时间的条件解码。

Result: 在6K分辨率视频上的实验表明，相比HNeRV，NeRV360内存消耗降低7倍，解码速度提升2.5倍，且客观图像质量更优。

Conclusion: NeRV360通过视口选择性解码有效解决了360度视频的高效压缩问题，为实时应用提供了可行方案。

Abstract: Implicit neural representations for videos (NeRV) have shown strong potential for video compression. However, applying NeRV to high-resolution 360-degree videos causes high memory usage and slow decoding, making real-time applications impractical. We propose NeRV360, an end-to-end framework that decodes only the user-selected viewport instead of reconstructing the entire panoramic frame. Unlike conventional pipelines, NeRV360 integrates viewport extraction into decoding and introduces a spatial-temporal affine transform module for conditional decoding based on viewpoint and time. Experiments on 6K-resolution videos show that NeRV360 achieves a 7-fold reduction in memory consumption and a 2.5-fold increase in decoding speed compared to HNeRV, a representative prior work, while delivering better image quality in terms of objective metrics.

</details>


### [11] [Beyond Weight Adaptation: Feature-Space Domain Injection for Cross-Modal Ship Re-Identification](https://arxiv.org/abs/2512.20892)
*Tingfeng Xian,Wenlve Zhou,Zhiheng Zhou,Zhelin Li*

Main category: cs.CV

TL;DR: 提出了域表示注入（DRI）方法，利用视觉基础模型（VFM）通过特征空间优化解决跨模态船舶重识别中的模态差异问题。该方法仅添加少量可训练参数，在保持VFM冻结的同时实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 跨模态船舶重识别面临显著模态差异，传统方法依赖大规模配对数据集进行预训练。本文基于柏拉图表示假说，探索视觉基础模型在弥合模态差距方面的潜力。

Method: 设计轻量级可学习的偏移编码器提取富含模态和身份属性的域特定表示，通过调制器自适应转换后注入VFM中间层，采用加性融合动态重塑特征分布而不改变预训练权重。

Result: 在HOSS-ReID数据集上仅使用1.54M和7.05M参数就达到了57.9%和60.5%的mAP，取得了最先进的性能。

Conclusion: DRI方法通过特征空间优化有效解决了跨模态船舶重识别问题，在保持模型通用知识的同时以最小训练参数实现了优异的性能，验证了视觉基础模型在跨模态任务中的潜力。

Abstract: Cross-Modality Ship Re-Identification (CMS Re-ID) is critical for achieving all-day and all-weather maritime target tracking, yet it is fundamentally challenged by significant modality discrepancies. Mainstream solutions typically rely on explicit modality alignment strategies; however, this paradigm heavily depends on constructing large-scale paired datasets for pre-training. To address this, grounded in the Platonic Representation Hypothesis, we explore the potential of Vision Foundation Models (VFMs) in bridging modality gaps. Recognizing the suboptimal performance of existing generic Parameter-Efficient Fine-Tuning (PEFT) methods that operate within the weight space, particularly on limited-capacity models, we shift the optimization perspective to the feature space and propose a novel PEFT strategy termed Domain Representation Injection (DRI). Specifically, while keeping the VFM fully frozen to maximize the preservation of general knowledge, we design a lightweight, learnable Offset Encoder to extract domain-specific representations rich in modality and identity attributes from raw inputs. Guided by the contextual information of intermediate features at different layers, a Modulator adaptively transforms these representations. Subsequently, they are injected into the intermediate layers via additive fusion, dynamically reshaping the feature distribution to adapt to the downstream task without altering the VFM's pre-trained weights. Extensive experimental results demonstrate the superiority of our method, achieving State-of-the-Art (SOTA) performance with minimal trainable parameters. For instance, on the HOSS-ReID dataset, we attain 57.9\% and 60.5\% mAP using only 1.54M and 7.05M parameters, respectively. The code is available at https://github.com/TingfengXian/DRI.

</details>


### [12] [DGSAN: Dual-Graph Spatiotemporal Attention Network for Pulmonary Nodule Malignancy Prediction](https://arxiv.org/abs/2512.20898)
*Xiao Yu,Zhaojie Fang,Guanyu Zhou,Yin Shen,Huoling Luo,Ye Li,Ahmed Elazab,Xiang Wan,Ruiquan Ge,Changmiao Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种双图时空注意力网络（DGSAN），通过融合多模态和多时间点数据来提高肺结节分类的准确性，并在新构建的数据集上进行验证。


<details>
  <summary>Details</summary>
Motivation: 肺癌是全球癌症相关死亡的主要原因，早期检测肺结节对提高患者生存率至关重要。现有的多模态融合方法效率低下，仅限于向量拼接和简单的相互关注，需要更有效的多模态信息融合方法。

Method: 提出双图时空注意力网络（DGSAN），包括全局-局部特征编码器来捕捉肺结节的局部、全局和融合特征；双图构建方法将多模态特征组织成模态间和模态内图；分层跨模态图融合模块用于优化特征整合。

Result: 在NLST-cmst和CSTL数据集上的实验表明，DGSAN在肺结节分类任务上显著优于现有最先进方法，且具有出色的计算效率。

Conclusion: DGSAN通过有效利用时空变化和多模态数据，在肺结节分类中取得了优异性能，为相关研究提供了新的多模态数据集支持。

Abstract: Lung cancer continues to be the leading cause of cancer-related deaths globally. Early detection and diagnosis of pulmonary nodules are essential for improving patient survival rates. Although previous research has integrated multimodal and multi-temporal information, outperforming single modality and single time point, the fusion methods are limited to inefficient vector concatenation and simple mutual attention, highlighting the need for more effective multimodal information fusion. To address these challenges, we introduce a Dual-Graph Spatiotemporal Attention Network, which leverages temporal variations and multimodal data to enhance the accuracy of predictions. Our methodology involves developing a Global-Local Feature Encoder to better capture the local, global, and fused characteristics of pulmonary nodules. Additionally, a Dual-Graph Construction method organizes multimodal features into inter-modal and intra-modal graphs. Furthermore, a Hierarchical Cross-Modal Graph Fusion Module is introduced to refine feature integration. We also compiled a novel multimodal dataset named the NLST-cmst dataset as a comprehensive source of support for related research. Our extensive experiments, conducted on both the NLST-cmst and curated CSTL-derived datasets, demonstrate that our DGSAN significantly outperforms state-of-the-art methods in classifying pulmonary nodules with exceptional computational efficiency.

</details>


### [13] [Benchmarking and Enhancing VLM for Compressed Image Understanding](https://arxiv.org/abs/2512.20901)
*Zifu Zhang,Tongda Xu,Siqi Li,Shengxi Li,Yue Zhang,Mai Xu,Yan Wang*

Main category: cs.CV

TL;DR: 建立了首个评估视觉语言模型在压缩图像上能力的综合基准，提出通用适配器提升模型对压缩图像的处理性能


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型的快速发展，需要处理大量压缩图像的需求日益增长，但现有模型主要处理高比特率压缩图像，对低比特率图像的处理能力尚未被探索

Method: 创建包含百万张压缩图像的基准测试集，分析性能差距来源，提出通用VLM适配器来增强模型对不同编解码器和比特率压缩图像的处理能力

Result: 基准测试揭示了VLM在压缩图像上的性能瓶颈，提出的适配器能将模型性能提升10%-30%

Conclusion: 为VLMs与压缩图像之间的差距提供了有价值的见解，建立的基准和增强方法有助于弥合这一技术鸿沟

Abstract: With the rapid development of Vision-Language Models (VLMs) and the growing demand for their applications, efficient compression of the image inputs has become increasingly important. Existing VLMs predominantly digest and understand high-bitrate compressed images, while their ability to interpret low-bitrate compressed images has yet to be explored by far. In this paper, we introduce the first comprehensive benchmark to evaluate the ability of VLM against compressed images, varying existing widely used image codecs and diverse set of tasks, encompassing over one million compressed images in our benchmark. Next, we analyse the source of performance gap, by categorising the gap from a) the information loss during compression and b) generalisation failure of VLM. We visualize these gaps with concrete examples and identify that for compressed images, only the generalization gap can be mitigated. Finally, we propose a universal VLM adaptor to enhance model performance on images compressed by existing codecs. Consequently, we demonstrate that a single adaptor can improve VLM performance across images with varying codecs and bitrates by 10%-30%. We believe that our benchmark and enhancement method provide valuable insights and contribute toward bridging the gap between VLMs and compressed images.

</details>


### [14] [PanoGrounder: Bridging 2D and 3D with Panoramic Scene Representations for VLM-based 3D Visual Grounding](https://arxiv.org/abs/2512.20907)
*Seongmin Jung,Seongho Choi,Gunwoo Jeon,Minsu Cho,Jongwoo Lim*

Main category: cs.CV

TL;DR: PanoGrounder是一个利用全景表示结合预训练2D VLM的通用3D视觉定位框架，在保持2D-3D视图间关系的同时提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统3D视觉定位模型因数据稀缺和推理能力有限难以泛化，作者希望通过2D VLM的强大语言理解能力来增强3D场景推理。

Method: 采用三阶段流程：基于场景布局放置全景视点 → 用VLM在每张全景图上进行文本定位 → 通过提升算法融合多视图预测为3D边界框。

Result: 在ScanRefer和Nr3D上达到SOTA，在未见过的3D数据集和文本改写上表现出优越的泛化能力。

Conclusion: 全景表示可作为2D与3D间的有效桥梁，结合预训练VLM能显著提升3D视觉定位的泛化性能。

Abstract: 3D Visual Grounding (3DVG) is a critical bridge from vision-language perception to robotics, requiring both language understanding and 3D scene reasoning. Traditional supervised models leverage explicit 3D geometry but exhibit limited generalization, owing to the scarcity of 3D vision-language datasets and the limited reasoning capabilities compared to modern vision-language models (VLMs). We propose PanoGrounder, a generalizable 3DVG framework that couples multi-modal panoramic representation with pretrained 2D VLMs for strong vision-language reasoning. Panoramic renderings, augmented with 3D semantic and geometric features, serve as an intermediate representation between 2D and 3D, and offer two major benefits: (i) they can be directly fed to VLMs with minimal adaptation and (ii) they retain long-range object-to-object relations thanks to their 360-degree field of view. We devise a three-stage pipeline that places a compact set of panoramic viewpoints considering the scene layout and geometry, grounds a text query on each panoramic rendering with a VLM, and fuses per-view predictions into a single 3D bounding box via lifting. Our approach achieves state-of-the-art results on ScanRefer and Nr3D, and demonstrates superior generalization to unseen 3D datasets and text rephrasings.

</details>


### [15] [Self-supervised Multiplex Consensus Mamba for General Image Fusion](https://arxiv.org/abs/2512.20921)
*Yingying Wang,Rongjin Zhuang,Hui Zheng,Xuanhua He,Ke Cao,Xiaotong Tu,Xinghao Ding*

Main category: cs.CV

TL;DR: 提出SMC-Mamba框架，通过自监督多模态共识方法实现通用图像融合，在多种融合任务和下游视觉任务中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 通用图像融合需要处理广泛任务并提升性能而不增加复杂度，现有方法主要关注特定任务的模态间信息整合

Method: 使用模态无关特征增强模块保留细节和增强全局表示，多模态共识交叉模态Mamba模块实现专家动态协作，结合双层自监督对比学习损失

Result: 在红外-可见光、医疗、多焦点、多曝光融合及下游视觉任务中优于现有最优算法

Conclusion: SMC-Mamba框架能有效整合多模态互补信息，提升融合质量和下游任务性能

Abstract: Image fusion integrates complementary information from different modalities to generate high-quality fused images, thereby enhancing downstream tasks such as object detection and semantic segmentation. Unlike task-specific techniques that primarily focus on consolidating inter-modal information, general image fusion needs to address a wide range of tasks while improving performance without increasing complexity. To achieve this, we propose SMC-Mamba, a Self-supervised Multiplex Consensus Mamba framework for general image fusion. Specifically, the Modality-Agnostic Feature Enhancement (MAFE) module preserves fine details through adaptive gating and enhances global representations via spatial-channel and frequency-rotational scanning. The Multiplex Consensus Cross-modal Mamba (MCCM) module enables dynamic collaboration among experts, reaching a consensus to efficiently integrate complementary information from multiple modalities. The cross-modal scanning within MCCM further strengthens feature interactions across modalities, facilitating seamless integration of critical information from both sources. Additionally, we introduce a Bi-level Self-supervised Contrastive Learning Loss (BSCL), which preserves high-frequency information without increasing computational overhead while simultaneously boosting performance in downstream tasks. Extensive experiments demonstrate that our approach outperforms state-of-the-art (SOTA) image fusion algorithms in tasks such as infrared-visible, medical, multi-focus, and multi-exposure fusion, as well as downstream visual tasks.

</details>


### [16] [Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting](https://arxiv.org/abs/2512.20927)
*Yoonwoo Jeong,Cheng Sun,Frank Wang,Minsu Cho,Jaesung Choe*

Main category: cs.CV

TL;DR: Q-Render是一项用于3D高斯泼溅的新渲染策略，通过稀疏采样解决高维特征渲染的效率问题，并提出GS-Net实现通用化特征预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用码本或特征压缩导致信息丢失，降低分割质量，需要一种能高效渲染高维特征同时保持高保真度的方法。

Method: 提出Quantile Rendering (Q-Render)，稀疏采样沿射线主导影响力的高斯，而非密集采样所有高斯；并集成到高斯泼溅网络(GS-Net)中实现通用化特征预测。

Result: 在ScanNet和LeRF上实验表明，该方法优于最先进方法，在512维特征图上实现约43.7倍加速的实时渲染。

Conclusion: Q-Render有效解决了3D高斯泼溅中高维特征渲染的效率问题，同时保持了分割质量，代码将公开。

Abstract: Recent advancements in computer vision have successfully extended Open-vocabulary segmentation (OVS) to the 3D domain by leveraging 3D Gaussian Splatting (3D-GS). Despite this progress, efficiently rendering the high-dimensional features required for open-vocabulary queries poses a significant challenge. Existing methods employ codebooks or feature compression, causing information loss, thereby degrading segmentation quality. To address this limitation, we introduce Quantile Rendering (Q-Render), a novel rendering strategy for 3D Gaussians that efficiently handles high-dimensional features while maintaining high fidelity. Unlike conventional volume rendering, which densely samples all 3D Gaussians intersecting each ray, Q-Render sparsely samples only those with dominant influence along the ray. By integrating Q-Render into a generalizable 3D neural network, we also propose Gaussian Splatting Network (GS-Net), which predicts Gaussian features in a generalizable manner. Extensive experiments on ScanNet and LeRF demonstrate that our framework outperforms state-of-the-art methods, while enabling real-time rendering with an approximate ~43.7x speedup on 512-D feature maps. Code will be made publicly available.

</details>


### [17] [Transductive Visual Programming: Evolving Tool Libraries from Experience for Spatial Reasoning](https://arxiv.org/abs/2512.20934)
*Shengguang Wu,Xiaohan Wang,Yuhui Zhang,Hao Zhu,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: TVP是一种新型视觉编程框架，通过从自身经验中归纳可重用工具来解决3D空间推理问题，在Omni3D-Bench上达到SOTA性能，比GPT-4o提升22%。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉编程方法依赖固定工具集或先验工具归纳，导致程序质量不佳且工具利用率低。需要一种能从经验中学习并自我演进的方案来应对精确的3D几何计算挑战。

Method: TVP首先使用基础工具解决问题，将经验性解决方案存入示例库，然后从中抽象出重复模式形成可重用的高层次工具库。这种传导式学习方法使系统能用越来越强大的工具处理新问题。

Result: 在Omni3D-Bench上TVP性能超越GPT-4o 22%，比之前最佳视觉编程系统提升11%。传导学习的工具使用频率比归纳方法高5倍，且在未见的空间任务上展现出强泛化能力。

Conclusion: 基于经验驱动的传导式工具创建是一个强大范式，能构建自我演进的视觉编程智能体，有效处理挑战性的空间推理任务。

Abstract: Spatial reasoning in 3D scenes requires precise geometric calculations that challenge vision-language models. Visual programming addresses this by decomposing problems into steps calling specialized tools, yet existing methods rely on either fixed toolsets or speculative tool induction before solving problems, resulting in suboptimal programs and poor utilization of induced tools. We present Transductive Visual Programming (TVP), a novel framework that builds new tools from its own experience rather than speculation. TVP first solves problems using basic tools while accumulating experiential solutions into an Example Library, then abstracts recurring patterns from these programs into reusable higher-level tools for an evolving Tool Library. This allows TVP to tackle new problems with increasingly powerful tools learned from experience. On Omni3D-Bench, TVP achieves state-of-the-art performance, outperforming GPT-4o by 22% and the previous best visual programming system by 11%. Our transductively learned tools are used 5x more frequently as core program dependency than inductively created ones, demonstrating more effective tool discovery and reuse. The evolved tools also show strong generalization to unseen spatial tasks, achieving superior performance on benchmarks from SpatialScore-Hard collection without any testset-specific modification. Our work establishes experience-driven transductive tool creation as a powerful paradigm for building self-evolving visual programming agents that effectively tackle challenging spatial reasoning tasks. We release our code at https://transductive-visualprogram.github.io/.

</details>


### [18] [Reasoning-Driven Amodal Completion: Collaborative Agents and Perceptual Evaluation](https://arxiv.org/abs/2512.20936)
*Hongxing Fan,Shuyu Zhao,Jiayang Ao,Lu Sheng*

Main category: cs.CV

TL;DR: 该论文提出了一个协作多代理推理框架，通过分离语义规划和视觉合成来解决视觉完形任务中的语义一致性和结构完整性问题，并引入了新的评估指标MAC-Score。


<details>
  <summary>Details</summary>
Motivation: 现有的渐进式方法存在推理不稳定性和错误积累的问题，难以保持语义一致性和结构完整性。

Method: 采用协作多代理推理框架，将语义规划与视觉合成解耦，包含自校正验证代理和多样假设生成器，并在语义规划阶段进行严格的质量控制。

Result: 在多个数据集上的实验表明，该方法显著优于当前最先进的方法，验证了新评估指标的有效性。

Conclusion: 该框架通过前置推理和严格验证机制，实现了视觉和语义一致的单次合成，为视觉完形任务提供了有效的解决方案。

Abstract: Amodal completion, the task of inferring invisible object parts, faces significant challenges in maintaining semantic consistency and structural integrity. Prior progressive approaches are inherently limited by inference instability and error accumulation. To tackle these limitations, we present a Collaborative Multi-Agent Reasoning Framework that explicitly decouples Semantic Planning from Visual Synthesis. By employing specialized agents for upfront reasoning, our method generates a structured, explicit plan before pixel generation, enabling visually and semantically coherent single-pass synthesis. We integrate this framework with two critical mechanisms: (1) a self-correcting Verification Agent that employs Chain-of-Thought reasoning to rectify visible region segmentation and identify residual occluders strictly within the Semantic Planning phase, and (2) a Diverse Hypothesis Generator that addresses the ambiguity of invisible regions by offering diverse, plausible semantic interpretations, surpassing the limited pixel-level variations of standard random seed sampling. Furthermore, addressing the limitations of traditional metrics in assessing inferred invisible content, we introduce the MAC-Score (MLLM Amodal Completion Score), a novel human-aligned evaluation metric. Validated against human judgment and ground truth, these metrics establish a robust standard for assessing structural completeness and semantic consistency with visible context. Extensive experiments demonstrate that our framework significantly outperforms state-of-the-art methods across multiple datasets. Our project is available at: https://fanhongxing.github.io/remac-page.

</details>


### [19] [Beyond Artifacts: Real-Centric Envelope Modeling for Reliable AI-Generated Image Detection](https://arxiv.org/abs/2512.20937)
*Ruiqi Liu,Yi Han,Zhengbo Zhang,Liwei Yao,Zhiyuan Yan,Jialiang Shen,ZhiJin Chen,Boyi Sun,Lubin Weng,Jing Dong,Yan Wang,Shu Wu*

Main category: cs.CV

TL;DR: REM方法通过建模真实图像的分布而非生成器伪影，提出一种在真实场景下检测合成图像的新范式，显著提升在复杂退化条件下的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有合成图像检测器过度依赖特定生成器的伪影特征，对真实世界的图像退化高度敏感，随着生成架构演进和图像多轮跨平台处理，传统伪影线索变得过时且难以检测

Method: 提出Real-centric Envelope Modeling (REM)范式，通过在自重建中引入特征级扰动生成近真实样本，并使用跨域一致性包络估计器学习真实图像流形的边界

Result: 在八个基准测试中平均性能提升7.5%，在包含真实世界退化的RealChain基准上保持卓越的泛化能力

Conclusion: REM为真实条件下合成图像检测奠定了坚实基础，通过从学习生成器伪影转向建模真实图像分布，显著提高了检测器的鲁棒性

Abstract: The rapid progress of generative models has intensified the need for reliable and robust detection under real-world conditions. However, existing detectors often overfit to generator-specific artifacts and remain highly sensitive to real-world degradations. As generative architectures evolve and images undergo multi-round cross-platform sharing and post-processing (chain degradations), these artifact cues become obsolete and harder to detect. To address this, we propose Real-centric Envelope Modeling (REM), a new paradigm that shifts detection from learning generator artifacts to modeling the robust distribution of real images. REM introduces feature-level perturbations in self-reconstruction to generate near-real samples, and employs an envelope estimator with cross-domain consistency to learn a boundary enclosing the real image manifold. We further build RealChain, a comprehensive benchmark covering both open-source and commercial generators with simulated real-world degradation. Across eight benchmark evaluations, REM achieves an average improvement of 7.5% over state-of-the-art methods, and notably maintains exceptional generalization on the severely degraded RealChain benchmark, establishing a solid foundation for synthetic image detection under real-world conditions. The code and the RealChain benchmark will be made publicly available upon acceptance of the paper.

</details>


### [20] [SPOT!: Map-Guided LLM Agent for Unsupervised Multi-CCTV Dynamic Object Tracking](https://arxiv.org/abs/2512.20975)
*Yujin Noh,Inho Jake Park,Chigon Hwang*

Main category: cs.CV

TL;DR: SPOT是一种基于地图和LLM的车辆跟踪方法，可在多摄像头环境的盲区中预测车辆轨迹，无需预训练。


<details>
  <summary>Details</summary>
Motivation: 传统CCTV车辆跟踪系统在多摄像头环境下存在盲区，导致车辆ID切换和轨迹丢失，影响实时路径预测的可靠性。

Method: 将道路结构和摄像头布局表示为基于2D空间坐标的文档，通过分块技术组织；将车辆位置转换到世界坐标系；结合地图空间信息和车辆运动特征，在交叉口级别进行波束搜索预测车辆可能出现的下一个摄像头位置。

Result: 在CARLA模拟器的虚拟城市环境中验证，该方法能准确预测盲区后车辆最可能出现的下一个摄像头位置，比现有技术更有效地维持连续车辆轨迹。

Conclusion: SPOT方法通过结合空间信息和运动特征，有效解决了多摄像头环境下的车辆轨迹连续性问题，提高了跟踪系统的可靠性。

Abstract: CCTV-based vehicle tracking systems face structural limitations in continuously connecting the trajectories of the same vehicle across multiple camera environments. In particular, blind spots occur due to the intervals between CCTVs and limited Fields of View (FOV), which leads to object ID switching and trajectory loss, thereby reducing the reliability of real-time path prediction. This paper proposes SPOT (Spatial Prediction Over Trajectories), a map-guided LLM agent capable of tracking vehicles even in blind spots of multi-CCTV environments without prior training. The proposed method represents road structures (Waypoints) and CCTV placement information as documents based on 2D spatial coordinates and organizes them through chunking techniques to enable real-time querying and inference. Furthermore, it transforms the vehicle's position into the actual world coordinate system using the relative position and FOV information of objects observed in CCTV images. By combining map spatial information with the vehicle's moving direction, speed, and driving patterns, a beam search is performed at the intersection level to derive candidate CCTV locations where the vehicle is most likely to enter after the blind spot. Experimental results based on the CARLA simulator in a virtual city environment confirmed that the proposed method accurately predicts the next appearing CCTV even in blind spot sections, maintaining continuous vehicle trajectories more effectively than existing techniques.

</details>


### [21] [XGrid-Mapping: Explicit Implicit Hybrid Grid Submaps for Efficient Incremental Neural LiDAR Mapping](https://arxiv.org/abs/2512.20976)
*Zeqing Song,Zhongmiao Yan,Junyuan Deng,Songpengcheng Xia,Xiang Mu,Jingyi Xu,Qi Wu,Ling Pei*

Main category: cs.CV

TL;DR: XGrid-Mapping是一个用于大规模增量激光雷达建图的混合网格框架，结合稀疏网格的几何先验和隐式密集网格的丰富表示，通过VDB结构和子图组织实现高效建图。


<details>
  <summary>Details</summary>
Motivation: 解决现有神经激光雷达建图方法依赖密集隐式表示、未充分利用几何结构，以及体素引导方法难以达到实时性能的问题。

Method: 提出混合网格框架，结合稀疏网格提供几何先验和结构引导，隐式密集网格丰富场景表示；采用VDB结构和基于子图的组织减少计算负载；引入基于蒸馏的重叠对齐策略确保子图间一致性；加入动态移除模块增强鲁棒性。

Result: 实验表明，该方法在保持高质量建图的同时，克服了体素引导方法的效率限制，优于现有最先进的建图方法。

Conclusion: XGrid-Mapping通过有效结合显式和隐式表示，实现了高效的大规模增量神经激光雷达建图，在质量和效率方面均表现出色。

Abstract: Large-scale incremental mapping is fundamental to the development of robust and reliable autonomous systems, as it underpins incremental environmental understanding with sequential inputs for navigation and decision-making. LiDAR is widely used for this purpose due to its accuracy and robustness. Recently, neural LiDAR mapping has shown impressive performance; however, most approaches rely on dense implicit representations and underutilize geometric structure, while existing voxel-guided methods struggle to achieve real-time performance. To address these challenges, we propose XGrid-Mapping, a hybrid grid framework that jointly exploits explicit and implicit representations for efficient neural LiDAR mapping. Specifically, the strategy combines a sparse grid, providing geometric priors and structural guidance, with an implicit dense grid that enriches scene representation. By coupling the VDB structure with a submap-based organization, the framework reduces computational load and enables efficient incremental mapping on a large scale. To mitigate discontinuities across submaps, we introduce a distillation-based overlap alignment strategy, in which preceding submaps supervise subsequent ones to ensure consistency in overlapping regions. To further enhance robustness and sampling efficiency, we incorporate a dynamic removal module. Extensive experiments show that our approach delivers superior mapping quality while overcoming the efficiency limitations of voxel-guided methods, thereby outperforming existing state-of-the-art mapping methods.

</details>


### [22] [X-ray Insights Unleashed: Pioneering the Enhancement of Multi-Label Long-Tail Data](https://arxiv.org/abs/2512.20980)
*Xinquan Yang,Jinheng Xie,Yawen Huang,Yuexiang Li,Huimin Huang,Hao Zheng,Xian Wu,Yefeng Zheng,Linlin Shen*

Main category: cs.CV

TL;DR: 本文提出了一种利用大量正常X射线图像增强罕见肺部病变数据的新型数据合成管道，通过扩散模型和语言模型指导提升诊断精度。


<details>
  <summary>Details</summary>
Motivation: 解决胸部X光中长尾肺部异常诊断挑战，现有扩散方法因罕见病变样本不足导致生成能力有限，诊断精度不足。

Method: 提出数据合成管道：1）用大量正常样本训练扩散模型生成正常X光；2）用预训练模型修复病变X光中的头部病变，保留尾部类别作为增强数据；3）集成大型语言模型知识指导模块和渐进增量学习策略稳定微调过程。

Result: 在MIMIC和CheXpert公共肺部数据集上的综合评估表明，该方法在性能上设立了新的基准。

Conclusion: 所提出的方法能有效利用常规正常X射线增强罕见病变数据，提升长尾肺部异常的诊断精度。

Abstract: Long-tailed pulmonary anomalies in chest radiography present formidable diagnostic challenges. Despite the recent strides in diffusion-based methods for enhancing the representation of tailed lesions, the paucity of rare lesion exemplars curtails the generative capabilities of these approaches, thereby leaving the diagnostic precision less than optimal. In this paper, we propose a novel data synthesis pipeline designed to augment tail lesions utilizing a copious supply of conventional normal X-rays. Specifically, a sufficient quantity of normal samples is amassed to train a diffusion model capable of generating normal X-ray images. This pre-trained diffusion model is subsequently utilized to inpaint the head lesions present in the diseased X-rays, thereby preserving the tail classes as augmented training data. Additionally, we propose the integration of a Large Language Model Knowledge Guidance (LKG) module alongside a Progressive Incremental Learning (PIL) strategy to stabilize the inpainting fine-tuning process. Comprehensive evaluations conducted on the public lung datasets MIMIC and CheXpert demonstrate that the proposed method sets a new benchmark in performance.

</details>


### [23] [PUFM++: Point Cloud Upsampling via Enhanced Flow Matching](https://arxiv.org/abs/2512.20988)
*Zhi-Song Liu,Chenhang He,Roland Maier,Andreas Rupp*

Main category: cs.CV

TL;DR: PUFM++是一种增强型流匹配框架，用于从稀疏、噪声和部分观察的点云数据重建密集且准确的点云。该框架通过两阶段流匹配策略、数据驱动自适应时间调度器、流形上约束和循环接口网络等创新，在几何保真度、鲁棒性和下游任务一致性方面显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有点云上采样方法在几何保真度、对不完美输入的鲁棒性以及与下游表面任务的兼容性方面存在不足。PUFM++旨在通过增强流匹配技术解决这些问题，提高点云上采样的质量和实用性。

Method: 采用两阶段流匹配策略：先学习从稀疏输入到密集目标的直接路径流，再用噪声扰动样本优化以更好逼近终端边际分布。引入数据驱动自适应时间调度器加速推理，施加流形上约束保证生成点与底层表面对齐，并使用循环接口网络增强层次特征交互。

Result: 在合成基准测试和真实世界扫描数据上的实验表明，PUFM++在点云上采样任务中达到新的最先进水平，在视觉保真度和定量准确性方面均表现出优越性能。

Conclusion: PUFM++通过多方面的流匹配增强技术，有效提升了点云上采样的质量，为下游应用提供了更精确、更鲁棒的点云重建结果。

Abstract: Recent advances in generative modeling have demonstrated strong promise for high-quality point cloud upsampling. In this work, we present PUFM++, an enhanced flow-matching framework for reconstructing dense and accurate point clouds from sparse, noisy, and partial observations. PUFM++ improves flow matching along three key axes: (i) geometric fidelity, (ii) robustness to imperfect input, and (iii) consistency with downstream surface-based tasks. We introduce a two-stage flow-matching strategy that first learns a direct, straight-path flow from sparse inputs to dense targets, and then refines it using noise-perturbed samples to approximate the terminal marginal distribution better. To accelerate and stabilize inference, we propose a data-driven adaptive time scheduler that improves sampling efficiency based on interpolation behavior. We further impose on-manifold constraints during sampling to ensure that generated points remain aligned with the underlying surface. Finally, we incorporate a recurrent interface network~(RIN) to strengthen hierarchical feature interactions and boost reconstruction quality. Extensive experiments on synthetic benchmarks and real-world scans show that PUFM++ sets a new state of the art in point cloud upsampling, delivering superior visual fidelity and quantitative accuracy across a wide range of tasks. Code and pretrained models are publicly available at https://github.com/Holmes-Alan/Enhanced_PUFM.

</details>


### [24] [MVInverse: Feed-forward Multi-view Inverse Rendering in Seconds](https://arxiv.org/abs/2512.21003)
*Xiangzuo Wu,Chengwei Ren,Jun Zhou,Xiu Li,Yuan Liu*

Main category: cs.CV

TL;DR: 提出了一个前馈多视角逆渲染框架，通过跨视图注意力机制实现几何、材质和光照的一致估计，并提出基于一致性的微调策略来提升对真实世界场景的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有单视角方法在多视角图像上忽视跨视图关系导致结果不一致，而多视图优化方法依赖缓慢的可微渲染和逐场景优化，计算成本高且难以扩展。

Method: 设计前馈多视图逆渲染框架，通过交替注意力跨视图捕捉长距离光照交互和跨视图材质一致性；提出基于一致性的微调策略，利用未标记的真实世界视频增强多视图一致性。

Result: 在基准数据集上的广泛实验表明，该方法在多视图一致性、材质和法线估计质量以及对真实世界图像的泛化能力方面达到最先进性能。

Conclusion: 该方法通过跨视图注意力机制和一致性微调策略，有效解决了多视图逆渲染中的一致性和泛化问题，为场景级推理提供了高效解决方案。

Abstract: Multi-view inverse rendering aims to recover geometry, materials, and illumination consistently across multiple viewpoints. When applied to multi-view images, existing single-view approaches often ignore cross-view relationships, leading to inconsistent results. In contrast, multi-view optimization methods rely on slow differentiable rendering and per-scene refinement, making them computationally expensive and hard to scale. To address these limitations, we introduce a feed-forward multi-view inverse rendering framework that directly predicts spatially varying albedo, metallic, roughness, diffuse shading, and surface normals from sequences of RGB images. By alternating attention across views, our model captures both intra-view long-range lighting interactions and inter-view material consistency, enabling coherent scene-level reasoning within a single forward pass. Due to the scarcity of real-world training data, models trained on existing synthetic datasets often struggle to generalize to real-world scenes. To overcome this limitation, we propose a consistency-based finetuning strategy that leverages unlabeled real-world videos to enhance both multi-view coherence and robustness under in-the-wild conditions. Extensive experiments on benchmark datasets demonstrate that our method achieves state-of-the-art performance in terms of multi-view consistency, material and normal estimation quality, and generalization to real-world imagery.

</details>


### [25] [Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations](https://arxiv.org/abs/2512.21004)
*Jinghan Li,Yang Jin,Hao Jiang,Yadong Mu,Yang Song,Kun Xu*

Main category: cs.CV

TL;DR: 该论文提出了NExT-Vid，一种新颖的自回归视觉生成预训练框架，通过掩码下一帧预测联合建模图像和视频，解决了现有方法语义定位不准确和生成质量差的问题。


<details>
  <summary>Details</summary>
Motivation: 虽然自回归生成模型在NLP领域取得重大进展，但大多数视觉生成预训练方法仍依赖BERT风格的掩码建模，忽视了视频分析所需的时间信息。现有自回归视觉预训练方法存在语义定位不准确和生成质量差的问题。

Method: NExT-Vid框架包含上下文隔离的自回归预测器（解耦语义表示和目标解码）和条件流匹配解码器（提升生成质量和多样性），通过上下文隔离流匹配预训练获得强表示能力。

Result: 大规模预训练模型的实验表明，该方法在下游分类任务中通过注意力探测持续优于先前的生成预训练方法。

Conclusion: NExT-Vid通过创新的自回归视觉生成预训练框架，成功解决了现有方法的局限性，在视觉表示学习方面取得了显著改进。

Abstract: Recent advances in pretraining general foundation models have significantly improved performance across diverse downstream tasks. While autoregressive (AR) generative models like GPT have revolutionized NLP, most visual generative pretraining methods still rely on BERT-style masked modeling, which often disregards the temporal information essential for video analysis. The few existing autoregressive visual pretraining methods suffer from issues such as inaccurate semantic localization and poor generation quality, leading to poor semantics. In this work, we propose NExT-Vid, a novel autoregressive visual generative pretraining framework that utilizes masked next-frame prediction to jointly model images and videos. NExT-Vid introduces a context-isolated autoregressive predictor to decouple semantic representation from target decoding, and a conditioned flow-matching decoder to enhance generation quality and diversity. Through context-isolated flow-matching pretraining, our approach achieves strong representations. Extensive experiments on large-scale pretrained models demonstrate that our proposed method consistently outperforms previous generative pretraining methods for visual representation learning via attentive probing in downstream classification.

</details>


### [26] [Granular-ball Guided Masking: Structure-aware Data Augmentation](https://arxiv.org/abs/2512.21011)
*Shuyin Xia,Fan Chen,Dawei Dai,Meng Yang,Junwei Han,Xinbo Gao,Guoyin Wang*

Main category: cs.CV

TL;DR: 提出了基于粒球计算的结构感知数据增强方法GBGM，通过分层掩码保留语义重要区域，提升模型鲁棒性


<details>
  <summary>Details</summary>
Motivation: 深度学习模型依赖大规模标注数据且容易过拟合，现有数据增强方法缺乏结构意识可能丢弃关键语义信息

Method: GBGM方法基于粒球计算指导，采用粗到细的分层掩码过程，自适应保留语义丰富的结构重要区域，抑制冗余区域

Result: 在多个基准测试中显示出分类准确率和掩码图像重建的持续改进，证实了方法的有效性和广泛适用性

Conclusion: GBGM是一种简单且模型无关的结构感知数据增强新范式，可无缝集成到CNN和Vision Transformer中

Abstract: Deep learning models have achieved remarkable success in computer vision, but they still rely heavily on large-scale labeled data and tend to overfit when data are limited or distributions shift. Data augmentation, particularly mask-based information dropping, can enhance robustness by forcing models to explore complementary cues; however, existing approaches often lack structural awareness and may discard essential semantics. We propose Granular-ball Guided Masking (GBGM), a structure-aware augmentation strategy guided by Granular-ball Computing (GBC). GBGM adaptively preserves semantically rich, structurally important regions while suppressing redundant areas through a coarse-to-fine hierarchical masking process, producing augmentations that are both representative and discriminative. Extensive experiments on multiple benchmarks demonstrate consistent improvements in classification accuracy and masked image reconstruction, confirming the effectiveness and broad applicability of the proposed method. Simple and model-agnostic, it integrates seamlessly into CNNs and Vision Transformers and provides a new paradigm for structure-aware data augmentation.

</details>


### [27] [FluencyVE: Marrying Temporal-Aware Mamba with Bypass Attention for Video Editing](https://arxiv.org/abs/2512.21015)
*Mingshu Cai,Yixuan Li,Osamu Yoshie,Yuya Ieiri*

Main category: cs.CV

TL;DR: 本文提出了FluencyVE，一种简单有效的一次性视频编辑方法，通过将Mamba时间序列模块集成到基于Stable Diffusion的视频编辑模型中，替代时间注意力层，解决现有方法存在的时间不一致性和高计算开销问题。


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像扩散模型在图像生成和编辑方面取得了显著成功，但将其扩展到视频编辑仍面临挑战。现有的视频编辑方法通过添加时间注意力机制来适应预训练的文本到图像模型，但这些方法仍然存在时间不一致性问题和高计算开销。

Method: FluencyVE将线性时间序列模块Mamba集成到基于预训练Stable Diffusion模型的视频编辑模型中，替代时间注意力层。此外，使用低秩近似矩阵替换因果注意力中的查询和键权重矩阵，并在训练期间使用加权平均技术更新注意力分数。

Result: 实验和分析表明，该方法在编辑真实世界视频的各种属性、主题和位置方面取得了有希望的结果。

Conclusion: FluencyVE在保持文本到图像模型生成能力的同时，显著降低了计算负担，为视频编辑提供了一种有效的解决方案。

Abstract: Large-scale text-to-image diffusion models have achieved unprecedented success in image generation and editing. However, extending this success to video editing remains challenging. Recent video editing efforts have adapted pretrained text-to-image models by adding temporal attention mechanisms to handle video tasks. Unfortunately, these methods continue to suffer from temporal inconsistency issues and high computational overheads. In this study, we propose FluencyVE, which is a simple yet effective one-shot video editing approach. FluencyVE integrates the linear time-series module, Mamba, into a video editing model based on pretrained Stable Diffusion models, replacing the temporal attention layer. This enables global frame-level attention while reducing the computational costs. In addition, we employ low-rank approximation matrices to replace the query and key weight matrices in the causal attention, and use a weighted averaging technique during training to update the attention scores. This approach significantly preserves the generative power of the text-to-image model while effectively reducing the computational burden. Experiments and analyses demonstrate promising results in editing various attributes, subjects, and locations in real-world videos.

</details>


### [28] [Efficient and Robust Video Defense Framework against 3D-field Personalized Talking Face](https://arxiv.org/abs/2512.21019)
*Rui-qing Sun,Xingshan Yao,Tian Lan,Hui-Yang Zhao,Jia-Ling Shi,Chen-Hao Cui,Zhijing Wu,Chen Yang,Xian-Ling Mao*

Main category: cs.CV

TL;DR: 论文提出了一种高效的视频防御框架，专门针对3D场驱动的说话人脸生成方法，通过在3D信息获取过程中引入扰动来实现视频保护，同时保持高保真度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图像的防御方法在处理3D场TFG方法时存在计算成本高、视频质量下降严重的问题，且无法有效破坏3D信息。需要一种专门针对3D场TFG的高效防御方案。

Method: 提出相似性引导的参数共享机制以提高计算效率，并设计多尺度双域注意力模块来联合优化空频扰动。

Result: 实验表明该框架具有强大的防御能力，比最快基线加速47倍，同时保持高保真度，对缩放操作和最先进的净化攻击具有鲁棒性。

Conclusion: 该研究为解决3D场TFG方法带来的隐私威胁提供了有效的防御方案，在效率和防御效果之间取得了良好平衡。

Abstract: State-of-the-art 3D-field video-referenced Talking Face Generation (TFG) methods synthesize high-fidelity personalized talking-face videos in real time by modeling 3D geometry and appearance from reference portrait video. This capability raises significant privacy concerns regarding malicious misuse of personal portraits. However, no efficient defense framework exists to protect such videos against 3D-field TFG methods. While image-based defenses could apply per-frame 2D perturbations, they incur prohibitive computational costs, severe video quality degradation, failing to disrupt 3D information for video protection. To address this, we propose a novel and efficient video defense framework against 3D-field TFG methods, which protects portrait video by perturbing the 3D information acquisition process while maintain high-fidelity video quality. Specifically, our method introduces: (1) a similarity-guided parameter sharing mechanism for computational efficiency, and (2) a multi-scale dual-domain attention module to jointly optimize spatial-frequency perturbations. Extensive experiments demonstrate that our proposed framework exhibits strong defense capability and achieves a 47x acceleration over the fastest baseline while maintaining high fidelity. Moreover, it remains robust against scaling operations and state-of-the-art purification attacks, and the effectiveness of our design choices is further validated through ablation studies. Our project is available at https://github.com/Richen7418/VDF.

</details>


### [29] [Multi-Attribute guided Thermal Face Image Translation based on Latent Diffusion Model](https://arxiv.org/abs/2512.21032)
*Mingshu Cai,Osamu Yoshie,Yuya Ieiri*

Main category: cs.CV

TL;DR: 提出了一种基于潜在扩散的模型，用于从热成像生成高质量可见光人脸图像，通过多属性分类器和Self-attn Mamba模块解决异质人脸识别中的特征损失问题


<details>
  <summary>Details</summary>
Motivation: 现有的人脸识别模型主要在可见光数据集上训练，在红外图像上性能显著下降。传统的基于特征的方法效果不佳，而现有的生成方法存在模型和模态差异导致图像失真和特征损失的问题

Method: 1) 基于潜在扩散的模型生成可见光人脸图像；2) 引入多属性分类器从可见光图像提取关键面部属性；3) 提出Self-attn Mamba模块增强跨模态特征的全局建模并提升推理速度

Result: 在两个基准数据集上的实验表明，该方法在图像质量和身份特征保留方面达到了最先进的性能

Conclusion: 该方法有效解决了红外到可见光图像转换中的特征损失问题，为夜间监控系统中的异质人脸识别提供了可靠的解决方案

Abstract: Modern surveillance systems increasingly rely on multi-wavelength sensors and deep neural networks to recognize faces in infrared images captured at night. However, most facial recognition models are trained on visible light datasets, leading to substantial performance degradation on infrared inputs due to significant domain shifts. Early feature-based methods for infrared face recognition proved ineffective, prompting researchers to adopt generative approaches that convert infrared images into visible light images for improved recognition. This paradigm, known as Heterogeneous Face Recognition (HFR), faces challenges such as model and modality discrepancies, leading to distortion and feature loss in generated images. To address these limitations, this paper introduces a novel latent diffusion-based model designed to generate high-quality visible face images from thermal inputs while preserving critical identity features. A multi-attribute classifier is incorporated to extract key facial attributes from visible images, mitigating feature loss during infrared-to-visible image restoration. Additionally, we propose the Self-attn Mamba module, which enhances global modeling of cross-modal features and significantly improves inference speed. Experimental results on two benchmark datasets demonstrate the superiority of our approach, achieving state-of-the-art performance in both image quality and identity preservation.

</details>


### [30] [Next-Scale Prediction: A Self-Supervised Approach for Real-World Image Denoising](https://arxiv.org/abs/2512.21038)
*Yiwen Shan,Haiyu Zhao,Peng Hu,Xi Peng,Yuanbiao Gou*

Main category: cs.CV

TL;DR: 提出了Next-Scale Prediction（NSP）新方法，作为自监督真实图像去噪的解耦范式，旨在解决噪声去相关与细节保留之间的固有矛盾，并通过构造跨尺度训练对，输入低分辨率的子图像来预测保留细节的高分辨率目标。


<details>
  <summary>Details</summary>
Motivation: 自监督真实图像去噪面临噪声去相关与细节保留之间的根本性挑战。现有Bilind-spot 网络（BSN）方法在利用像素重组下采样时，激进的下采样方式会损伤细微结构，而温和的下采样又无法有效去除相关噪声。为解决这一问题，研究者们希望找到一种能有效解耦噪声去相关和细节保留的方法。

Method: 引入了Next-Scale Prediction（NSP）的新自监督范式。该方法通过构造跨尺度训练对，让BSN使用低分辨且已完全去相关噪声的子图像作为输入，来预测保留精细细节的高分辨率目标。

Result: 大量实验表明，NSP在真实图像去噪基准测试中实现了最先进的性能，显著缓解了长期存在的噪声去相关和细节保留之间的冲突。此外，NSP还能在不需重新训练或修改的情况下，支持含噪图像的超分辨任务。

Conclusion: NSP方法成功实现了噪声去相关与细节保持的解耦，为解决自监督真实图像去噪的根本挑战提供了一种新的、有效的思路。该研究也对跨尺度预测的应用提出了新的可能性。

Abstract: Self-supervised real-world image denoising remains a fundamental challenge, arising from the antagonistic trade-off between decorrelating spatially structured noise and preserving high-frequency details. Existing blind-spot network (BSN) methods rely on pixel-shuffle downsampling (PD) to decorrelate noise, but aggressive downsampling fragments fine structures, while milder downsampling fails to remove correlated noise. To address this, we introduce Next-Scale Prediction (NSP), a novel self-supervised paradigm that decouples noise decorrelation from detail preservation. NSP constructs cross-scale training pairs, where BSN takes low-resolution, fully decorrelated sub-images as input to predict high-resolution targets that retain fine details. As a by-product, NSP naturally supports super-resolution of noisy images without retraining or modification. Extensive experiments demonstrate that NSP achieves state-of-the-art self-supervised denoising performance on real-world benchmarks, significantly alleviating the long-standing conflict between noise decorrelation and detail preservation.

</details>


### [31] [A Large-Depth-Range Layer-Based Hologram Dataset for Machine Learning-Based 3D Computer-Generated Holography](https://arxiv.org/abs/2512.21040)
*Jaehong Lee,You Chan No,YoungWoo Kim,Duksu Kim*

Main category: cs.CV

TL;DR: 本文介绍了KOREATECH-CGH数据集，包含6000对RGB-D图像和复杂全息图，并提出了振幅投影技术来提高大深度范围的全息图质量，在质量和超分辨率任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习计算机生成全息术(ML-CGH)发展受限于高质量、大规模全息数据集的缺乏，需要构建公开数据集来推动该领域发展。

Method: 构建KOREATECH-CGH数据集(6000对RGB-D图像和复杂全息图)，提出振幅投影后处理技术，在保持相位的同时替换全息波场各深度层的振幅分量。

Result: 振幅投影技术显著提升重建质量，达到27.01 dB PSNR和0.87 SSIM，比现有优化轮廓掩膜层方法分别提升2.03 dB和0.04 SSIM，并在全息生成和超分辨率任务中验证了数据集的有效性。

Conclusion: KOREATECH-CGH数据集和振幅投影技术为下一代ML-CGH系统的训练和评估提供了有力支持，推动了全息术领域的发展。

Abstract: Machine learning-based computer-generated holography (ML-CGH) has advanced rapidly in recent years, yet progress is constrained by the limited availability of high-quality, large-scale hologram datasets. To address this, we present KOREATECH-CGH, a publicly available dataset comprising 6,000 pairs of RGB-D images and complex holograms across resolutions ranging from 256*256 to 2048*2048, with depth ranges extending to the theoretical limits of the angular spectrum method for wide 3D scene coverage. To improve hologram quality at large depth ranges, we introduce amplitude projection, a post-processing technique that replaces amplitude components of hologram wavefields at each depth layer while preserving phase. This approach enhances reconstruction fidelity, achieving 27.01 dB PSNR and 0.87 SSIM, surpassing a recent optimized silhouette-masking layer-based method by 2.03 dB and 0.04 SSIM, respectively. We further validate the utility of KOREATECH-CGH through experiments on hologram generation and super-resolution using state-of-the-art ML models, confirming its applicability for training and evaluating next-generation ML-CGH systems.

</details>


### [32] [Matrix Completion Via Reweighted Logarithmic Norm Minimization](https://arxiv.org/abs/2512.21050)
*Zhijie Wang,Liangtian He,Qinghua Zhang,Jifei Miao,Liang-Jian Deng,Jun Liu*

Main category: cs.CV

TL;DR: 论文提出了一种新的加权对数范数作为非凸代理来替代核范数，以更好地逼近秩函数。通过ADMM优化，在图像修复任务中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 低秩矩阵补全中，核范数作为秩函数凸代理存在过度收缩奇异值的缺陷，导致次优解。需要更有效的非凸代理来提供更紧密的逼近。

Method: 提出加权对数范数作为新的非凸代理函数，使用交替方向乘子法(ADMM)高效求解优化问题。

Result: 在图像修复实验中，该方法在视觉质量和定量指标上均优于现有最先进的低秩矩阵补全方法。

Conclusion: 加权对数范数是比核范数更有效的秩函数代理，能显著提升低秩矩阵补全的性能表现。

Abstract: Low-rank matrix completion (LRMC) has demonstrated remarkable success in a wide range of applications. To address the NP-hard nature of the rank minimization problem, the nuclear norm is commonly used as a convex and computationally tractable surrogate for the rank function. However, this approach often yields suboptimal solutions due to the excessive shrinkage of singular values. In this letter, we propose a novel reweighted logarithmic norm as a more effective nonconvex surrogate, which provides a closer approximation than many existing alternatives. We efficiently solve the resulting optimization problem by employing the alternating direction method of multipliers (ADMM). Experimental results on image inpainting demonstrate that the proposed method achieves superior performance compared to state-of-the-art LRMC approaches, both in terms of visual quality and quantitative metrics.

</details>


### [33] [Optical Flow-Guided 6DoF Object Pose Tracking with an Event Camera](https://arxiv.org/abs/2512.21053)
*Zibin Liu,Banglei Guan,Yang Shang,Shunkun Liang,Zhenbao Yu,Qifeng Yu*

Main category: cs.CV

TL;DR: 该论文提出了一种基于事件相机的光学流引导的6自由度物体姿态跟踪方法，通过2D-3D混合特征提取和光学流关联来克服传统相机在运动模糊、光照变化等方面的限制。


<details>
  <summary>Details</summary>
Motivation: 传统相机在物体姿态跟踪中面临运动模糊、传感器噪声、部分遮挡和光照变化等挑战，而新兴的事件相机具有高动态范围和低延迟优势，有潜力解决这些问题。

Method: 采用2D-3D混合特征提取策略检测事件和物体模型中的角点和边缘，通过最大化时空窗口内的事件关联概率来搜索角点的光学流，并建立角点与边缘的光学流关联，通过最小化角点与边缘距离来迭代优化6DoF姿态。

Result: 在模拟和真实事件数据上的实验结果表明，该方法在准确性和鲁棒性方面优于现有的事件基最先进方法。

Conclusion: 基于事件相机的光学流引导方法能够有效实现连续物体姿态跟踪，在复杂环境下表现出优越性能。

Abstract: Object pose tracking is one of the pivotal technologies in multimedia, attracting ever-growing attention in recent years. Existing methods employing traditional cameras encounter numerous challenges such as motion blur, sensor noise, partial occlusion, and changing lighting conditions. The emerging bio-inspired sensors, particularly event cameras, possess advantages such as high dynamic range and low latency, which hold the potential to address the aforementioned challenges. In this work, we present an optical flow-guided 6DoF object pose tracking method with an event camera. A 2D-3D hybrid feature extraction strategy is firstly utilized to detect corners and edges from events and object models, which characterizes object motion precisely. Then, we search for the optical flow of corners by maximizing the event-associated probability within a spatio-temporal window, and establish the correlation between corners and edges guided by optical flow. Furthermore, by minimizing the distances between corners and edges, the 6DoF object pose is iteratively optimized to achieve continuous pose tracking. Experimental results of both simulated and real events demonstrate that our methods outperform event-based state-of-the-art methods in terms of both accuracy and robustness.

</details>


### [34] [DexAvatar: 3D Sign Language Reconstruction with Hand and Body Pose Priors](https://arxiv.org/abs/2512.21054)
*Kaustubh Kundu,Hrishav Bakul Barua,Lucy Robertson-Bell,Zhixi Cai,Kalin Stefanov*

Main category: cs.CV

TL;DR: DexAvatar通过从单目手语视频中重建精细的手部和身体运动，解决了现有手语生成中3D人体姿态估计质量差的问题。


<details>
  <summary>Details</summary>
Motivation: 当前手语生成方法依赖大量精确的2D和3D人体姿态数据，但现有数据集多为视频基于自动重建的2D关键点，缺乏准确3D信息，且现有3D姿态估计方法易受自遮挡、噪声和运动模糊影响。

Method: 提出DexAvatar框架，利用学习到的手部和身体3D先验，从野外单目手语视频中重建生物力学准确的精细手部关节和身体运动。

Result: 在SGNify运动捕捉数据集上表现优异，相比现有最优方法在身体和手部姿态估计上提升了35.11%。

Conclusion: DexAvatar能够有效解决手语视频中3D姿态重建的质量问题，为手语生成提供了更准确的3D运动数据。

Abstract: The trend in sign language generation is centered around data-driven generative methods that require vast amounts of precise 2D and 3D human pose data to achieve an acceptable generation quality. However, currently, most sign language datasets are video-based and limited to automatically reconstructed 2D human poses (i.e., keypoints) and lack accurate 3D information. Furthermore, existing state-of-the-art for automatic 3D human pose estimation from sign language videos is prone to self-occlusion, noise, and motion blur effects, resulting in poor reconstruction quality. In response to this, we introduce DexAvatar, a novel framework to reconstruct bio-mechanically accurate fine-grained hand articulations and body movements from in-the-wild monocular sign language videos, guided by learned 3D hand and body priors. DexAvatar achieves strong performance in the SGNify motion capture dataset, the only benchmark available for this task, reaching an improvement of 35.11% in the estimation of body and hand poses compared to the state-of-the-art. The official website of this work is: https://github.com/kaustesseract/DexAvatar.

</details>


### [35] [Beyond Pixel Simulation: Pathology Image Generation via Diagnostic Semantic Tokens and Prototype Control](https://arxiv.org/abs/2512.21058)
*Minghao Han,YiChen Liu,Yizhou Liu,Zizhi Chen,Jingqun Tang,Xuecheng Wu,Dingkang Yang,Lihua Zhang*

Main category: cs.CV

TL;DR: 本文介绍了一种名为 UniPath 的新型病理图像生成框架，该框架利用成熟的诊断理解技术，通过语义多流控制方法，实现了对病理图像生成过程的精细语义控制，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 病理图像生成领域存在的三大障碍：高质量图像-文本数据稀缺、缺乏精细语义控制导致依赖非语义线索、以及术语异质性干扰文本条件生成，促使作者开发 UniPath 以解决这些问题。

Method: UniPath 提出了一种多流控制方法，包括原始文本流、高层语义流（使用可学习查询从冻结的病理多模态大模型中提取诊断语义标记）和原型流（通过原型库实现组件级形态控制）。

Result: UniPath 在 Patho-FID 指标上达到 80.9，比第二名提升 51%，在精细语义控制方面达到真实图像的 98.7%，展示了最优性能。

Conclusion: UniPath 成功解决了病理图像生成的关键挑战，通过语义驱动的多流控制框架和数据集的精心构建，为病理学图像生成领域提供了新的解决方案，并将公开数据集、源代码和预训练模型。

Abstract: In computational pathology, understanding and generation have evolved along disparate paths: advanced understanding models already exhibit diagnostic-level competence, whereas generative models largely simulate pixels. Progress remains hindered by three coupled factors: the scarcity of large, high-quality image-text corpora; the lack of precise, fine-grained semantic control, which forces reliance on non-semantic cues; and terminological heterogeneity, where diverse phrasings for the same diagnostic concept impede reliable text conditioning. We introduce UniPath, a semantics-driven pathology image generation framework that leverages mature diagnostic understanding to enable controllable generation. UniPath implements Multi-Stream Control: a Raw-Text stream; a High-Level Semantics stream that uses learnable queries to a frozen pathology MLLM to distill paraphrase-robust Diagnostic Semantic Tokens and to expand prompts into diagnosis-aware attribute bundles; and a Prototype stream that affords component-level morphological control via a prototype bank. On the data front, we curate a 2.65M image-text corpus and a finely annotated, high-quality 68K subset to alleviate data scarcity. For a comprehensive assessment, we establish a four-tier evaluation hierarchy tailored to pathology. Extensive experiments demonstrate UniPath's SOTA performance, including a Patho-FID of 80.9 (51% better than the second-best) and fine-grained semantic control achieving 98.7% of the real-image. The meticulously curated datasets, complete source code, and pre-trained model weights developed in this study will be made openly accessible to the public.

</details>


### [36] [Multimodal Skeleton-Based Action Representation Learning via Decomposition and Composition](https://arxiv.org/abs/2512.21064)
*Hongsong Wang,Heng Fei,Bingxuan Dai,Jie Gui*

Main category: cs.CV

TL;DR: 提出了一种名为“分解与组合”的自监督多模态骨架动作表示学习框架，通过分解策略分离多模态特征并与单模态真值对齐，组合策略利用单模态特征增强多模态表示，在多个数据集上实现了计算成本与模型性能的良好平衡。


<details>
  <summary>Details</summary>
Motivation: 解决多模态动作理解中模态互补性利用与模型效率之间的平衡问题，传统方法如晚期融合计算开销大，早期融合性能受限。

Method: 采用自监督学习框架，包含分解策略（将融合的多模态特征分解为单模态特征并与真值对齐）和组合策略（整合单模态特征作为自监督指导增强多模态表示）。

Result: 在NTU RGB+D 60、NTU RGB+D 120和PKU-MMD II数据集上的大量实验表明，该方法在计算成本和模型性能之间取得了优异平衡。

Conclusion: 所提出的分解与组合框架有效解决了多模态动作理解的效率与性能权衡问题，为相关研究提供了新思路。

Abstract: Multimodal human action understanding is a significant problem in computer vision, with the central challenge being the effective utilization of the complementarity among diverse modalities while maintaining model efficiency. However, most existing methods rely on simple late fusion to enhance performance, which results in substantial computational overhead. Although early fusion with a shared backbone for all modalities is efficient, it struggles to achieve excellent performance. To address the dilemma of balancing efficiency and effectiveness, we introduce a self-supervised multimodal skeleton-based action representation learning framework, named Decomposition and Composition. The Decomposition strategy meticulously decomposes the fused multimodal features into distinct unimodal features, subsequently aligning them with their respective ground truth unimodal counterparts. On the other hand, the Composition strategy integrates multiple unimodal features, leveraging them as self-supervised guidance to enhance the learning of multimodal representations. Extensive experiments on the NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD II datasets demonstrate that the proposed method strikes an excellent balance between computational cost and model performance.

</details>


### [37] [UniPR-3D: Towards Universal Visual Place Recognition with Visual Geometry Grounded Transformer](https://arxiv.org/abs/2512.21078)
*Tianchen Deng,Xun Chen,Ziming Li,Hongming Shen,Danwei Wang,Javier Civera,Hesheng Wang*

Main category: cs.CV

TL;DR: UniPR-3D是一���首个有效整合多视图信息的视觉地点识别（VPR）架构，基于VGGT主干网络构建，通过专门设计的特征聚合器和微调策略，联合利用3D和2D令牌生成描述符，在多个基准测试中达到新的最优性能。方法：利用VGGT编码多视图3D表示，设计专门的2D和3D特征聚合模块，结合单帧和多帧聚合方案以及变长序列检索策略。结果：在VPR任务上超越现有的单视图和多视图基线方法，展示了基于几何的令牌在VPR中的有效性。结论：多视图信息和几何基础令牌对提升VPR性能具有重要作用，UniPR-3D为此提供了有效的解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉地点识别（VPR）主要基于单图像检索，多视图方法具有明显优势但研究不足且现有方法在多样化环境中泛化能力有限。

Method: 基于VGGT主干网络构建多视图3D表示，设计特征聚合器并进行微调；联合利用3D令牌和中间2D令牌构建描述符；为2D和3D特征设计专用聚合模块；结合单帧和多帧聚合方案以及变长序列检索策略。

Result: UniPR-3D在VPR任务上达到新的最优性能，超越现有的单视图和多视图基线方法。

Conclusion: 多视图信息和几何基础令牌能有效提升VPR性能，UniPR-3D为此提供了创新解决方案。

Abstract: Visual Place Recognition (VPR) has been traditionally formulated as a single-image retrieval task. Using multiple views offers clear advantages, yet this setting remains relatively underexplored and existing methods often struggle to generalize across diverse environments. In this work we introduce UniPR-3D, the first VPR architecture that effectively integrates information from multiple views. UniPR-3D builds on a VGGT backbone capable of encoding multi-view 3D representations, which we adapt by designing feature aggregators and fine-tune for the place recognition task. To construct our descriptor, we jointly leverage the 3D tokens and intermediate 2D tokens produced by VGGT. Based on their distinct characteristics, we design dedicated aggregation modules for 2D and 3D features, allowing our descriptor to capture fine-grained texture cues while also reasoning across viewpoints. To further enhance generalization, we incorporate both single- and multi-frame aggregation schemes, along with a variable-length sequence retrieval strategy. Our experiments show that UniPR-3D sets a new state of the art, outperforming both single- and multi-view baselines and highlighting the effectiveness of geometry-grounded tokens for VPR. Our code and models will be made publicly available on Github https://github.com/dtc111111/UniPR-3D.

</details>


### [38] [Hierarchical Modeling Approach to Fast and Accurate Table Recognition](https://arxiv.org/abs/2512.21083)
*Takaya Kawakatsu*

Main category: cs.CV

TL;DR: 本文提出了一种新型多任务模型，利用非因果注意力捕获完整表格结构，并通过并行推理算法加速单元格内容推理，在两大公共数据集上展示了优越性。


<details>
  <summary>Details</summary>
Motivation: 文档中存在需要不同识别方法的元素，表格识别通常包含三个子任务。现有模型虽然结合多任务学习等方法取得了优秀效果，但其有效性尚未完全解释且推理时间较长。

Method: 使用非因果注意力来捕获整个表格结构，并提出并行推理算法以加速单元格内容推理。

Result: 在两个大型公共数据集上，从视觉和统计角度均证明了该方法的优越性。

Conclusion: 该模型在表格识别任务中表现出色，特别是在推理速度和效果解释方面优于现有方法。

Abstract: The extraction and use of diverse knowledge from numerous documents is a pressing challenge in intelligent information retrieval. Documents contain elements that require different recognition methods. Table recognition typically consists of three subtasks, namely table structure, cell position and cell content recognition. Recent models have achieved excellent recognition with a combination of multi-task learning, local attention, and mutual learning. However, their effectiveness has not been fully explained, and they require a long period of time for inference. This paper presents a novel multi-task model that utilizes non-causal attention to capture the entire table structure, and a parallel inference algorithm for faster cell content inference. The superiority is demonstrated both visually and statistically on two large public datasets.

</details>


### [39] [T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation](https://arxiv.org/abs/2512.21094)
*Zhe Cao,Tao Wang,Jiaming Wang,Yanghai Wang,Yuanxing Zhang,Jialu Chen,Miao Deng,Jiahao Wang,Yubin Guo,Chenxi Liao,Yize Zhang,Zhaoxiang Zhang,Jiaheng Liu*

Main category: cs.CV

TL;DR: T2AV-Compass是一个用于全面评估文本到音频视频生成系统的统一基准，包含500个多样化提示，采用双级评估框架，评估结果表明当前模型在真实性和跨模态一致性方面仍有较大差距。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到音频视频生成评估方法碎片化的问题，现有方法依赖单模态指标或范围狭窄的基准，无法充分捕捉跨模态对齐、指令跟随和感知真实性等关键维度。

Method: 构建包含500个多样化复杂提示的基准，通过分类驱动流程确保语义丰富性和物理可信性。采用双级评估框架：客观信号级指标评估视频质量、音频质量和跨模态对齐；主观MLLM-as-a-Judge协议评估指令跟随和真实性。

Result: 对11个代表性T2AV系统的广泛评估显示，即使最强模型在人类级真实性和跨模态一致性方面仍存在显著不足，在音频真实性、精细同步和指令跟随等方面存在持续失败。

Conclusion: 当前模型仍有显著的改进空间，T2AV-Compass作为一个具有挑战性和诊断性的测试平台，有助于推动文本到音频视频生成技术的发展。

Abstract: Text-to-Audio-Video (T2AV) generation aims to synthesize temporally coherent video and semantically synchronized audio from natural language, yet its evaluation remains fragmented, often relying on unimodal metrics or narrowly scoped benchmarks that fail to capture cross-modal alignment, instruction following, and perceptual realism under complex prompts. To address this limitation, we present T2AV-Compass, a unified benchmark for comprehensive evaluation of T2AV systems, consisting of 500 diverse and complex prompts constructed via a taxonomy-driven pipeline to ensure semantic richness and physical plausibility. Besides, T2AV-Compass introduces a dual-level evaluation framework that integrates objective signal-level metrics for video quality, audio quality, and cross-modal alignment with a subjective MLLM-as-a-Judge protocol for instruction following and realism assessment. Extensive evaluation of 11 representative T2AVsystems reveals that even the strongest models fall substantially short of human-level realism and cross-modal consistency, with persistent failures in audio realism, fine-grained synchronization, instruction following, etc. These results indicate significant improvement room for future models and highlight the value of T2AV-Compass as a challenging and diagnostic testbed for advancing text-to-audio-video generation.

</details>


### [40] [UniRec-0.1B: Unified Text and Formula Recognition with 0.1B Parameters](https://arxiv.org/abs/2512.21095)
*Yongkun Du,Zhineng Chen,Yazhen Xie,Weikang Baiand Hao Feng,Wei Shi,Yuchen Su,Can Huang,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: UniRec-0.1B是一种仅0.1B参数的轻量级统一识别模型，能同时处理文本和公式识别，在多级结构上表现出色且推理速度提升2-9倍。建立了UniRec40M大规模数据集，解决了结构可变性和语义纠缠两大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型虽然能统一识别文本和公式，但规模庞大且计算要求高，限制了实际应用。需要开发轻量级但性能强大的专业模型来满足实际部署需求。

Method: 1) 构建包含4000万样本的UniRec40M数据集；2) 提出分层监督训练明确指导结构理解；3) 设计语义解耦分词器分离文本和公式表示；4) 开发多领域多级别的综合评估基准。

Result: 在自建和公开基准测试中，UniRec-0.1B超越了通用VLMs和领先的文档解析专家模型，同时实现了2-9倍的加速，验证了其有效性和效率。

Conclusion: UniRec-0.1B成功实现了轻量级但强大的文本公式统一识别，通过分层监督和语义解耦方法解决了结构可变性和语义纠缠问题，为实际应用提供了高效解决方案。

Abstract: Text and formulas constitute the core informational components of many documents. Accurately and efficiently recognizing both is crucial for developing robust and generalizable document parsing systems. Recently, vision-language models (VLMs) have achieved impressive unified recognition of text and formulas. However, they are large-sized and computationally demanding, restricting their usage in many applications. In this paper, we propose UniRec-0.1B, a unified recognition model with only 0.1B parameters. It is capable of performing text and formula recognition at multiple levels, including characters, words, lines, paragraphs, and documents. To implement this task, we first establish UniRec40M, a large-scale dataset comprises 40 million text, formula and their mix samples, enabling the training of a powerful yet lightweight model. Secondly, we identify two challenges when building such a lightweight but unified expert model. They are: structural variability across hierarchies and semantic entanglement between textual and formulaic content. To tackle these, we introduce a hierarchical supervision training that explicitly guides structural comprehension, and a semantic-decoupled tokenizer that separates text and formula representations. Finally, we develop a comprehensive evaluation benchmark covering Chinese and English documents from multiple domains and with multiple levels. Experimental results on this and public benchmarks demonstrate that UniRec-0.1B outperforms both general-purpose VLMs and leading document parsing expert models, while achieving a 2-9$\times$ speedup, validating its effectiveness and efficiency. Codebase and Dataset: https://github.com/Topdu/OpenOCR.

</details>


### [41] [FreeInpaint: Tuning-free Prompt Alignment and Visual Rationality Enhancement in Image Inpainting](https://arxiv.org/abs/2512.21104)
*Chao Gong,Dong Li,Yingwei Pan,Jingjing Chen,Ting Yao,Tao Mei*

Main category: cs.CV

TL;DR: 本文提出了一种无需调谐的即插即用方法FreeInpaint，通过优化扩散潜变量来改善文本引导图像修复的画面对齐和视觉合理性。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本到图像扩散模型的修复方法在同时保持提示对齐和视觉合理性方面存在困难，需要一种无需训练的直接优化方法。

Method: 提出了先验引导的噪声优化方法，通过优化初始噪声引导模型关注有效修复区域；设计了专门的复合引导目标，通过优化中间潜变量来指导去噪过程。

Result: 通过在各种修复扩散模型和评估指标上的广泛实验，证明了FreeInpaint方法的有效性和鲁棒性。

Conclusion: FreeInpaint作为一种无需调谐的优化方法，能够有效提升文本引导图像修复的画面对齐质量和视觉合理性。

Abstract: Text-guided image inpainting endeavors to generate new content within specified regions of images using textual prompts from users. The primary challenge is to accurately align the inpainted areas with the user-provided prompts while maintaining a high degree of visual fidelity. While existing inpainting methods have produced visually convincing results by leveraging the pre-trained text-to-image diffusion models, they still struggle to uphold both prompt alignment and visual rationality simultaneously. In this work, we introduce FreeInpaint, a plug-and-play tuning-free approach that directly optimizes the diffusion latents on the fly during inference to improve the faithfulness of the generated images. Technically, we introduce a prior-guided noise optimization method that steers model attention towards valid inpainting regions by optimizing the initial noise. Furthermore, we meticulously design a composite guidance objective tailored specifically for the inpainting task. This objective efficiently directs the denoising process, enhancing prompt alignment and visual rationality by optimizing intermediate latents at each step. Through extensive experiments involving various inpainting diffusion models and evaluation metrics, we demonstrate the effectiveness and robustness of our proposed FreeInpaint.

</details>


### [42] [MarineEval: Assessing the Marine Intelligence of Vision-Language Models](https://arxiv.org/abs/2512.21126)
*YuK-Kwan Wong,Tuan-An To,Jipeng Zhang,Ziqiang Zheng,Sai-Kit Yeung*

Main category: cs.CV

TL;DR: MarineEval是一个新构建的大规模海洋视觉语言模型数据集和基准测试，包含2000个图像问答对，用于评估现有VLMs在海洋领域专业知识方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型虽然在各领域表现良好，但缺乏在需要专业知识的海洋领域的能力验证。本研究旨在探索VLMs作为海洋领域专家的边界和局限性。

Method: 构建了MarineEval数据集，涵盖7个任务维度和20个能力维度，数据构建过程融入领域需求并由海洋专家验证。随后对17个现有VLMs进行全面基准测试。

Result: 实验结果表明，现有VLMs无法有效回答海洋领域的专业问题，性能有较大提升空间。

Conclusion: 当前VLMs在海洋专业知识领域表现不足，需要进一步研究改进。该基准测试将为未来研究提供支持。

Abstract: We have witnessed promising progress led by large language models (LLMs) and further vision language models (VLMs) in handling various queries as a general-purpose assistant. VLMs, as a bridge to connect the visual world and language corpus, receive both visual content and various text-only user instructions to generate corresponding responses. Though great success has been achieved by VLMs in various fields, in this work, we ask whether the existing VLMs can act as domain experts, accurately answering marine questions, which require significant domain expertise and address special domain challenges/requirements. To comprehensively evaluate the effectiveness and explore the boundary of existing VLMs, we construct the first large-scale marine VLM dataset and benchmark called MarineEval, with 2,000 image-based question-answering pairs. During our dataset construction, we ensure the diversity and coverage of the constructed data: 7 task dimensions and 20 capacity dimensions. The domain requirements are specially integrated into the data construction and further verified by the corresponding marine domain experts. We comprehensively benchmark 17 existing VLMs on our MarineEval and also investigate the limitations of existing models in answering marine research questions. The experimental results reveal that existing VLMs cannot effectively answer the domain-specific questions, and there is still a large room for further performance improvements. We hope our new benchmark and observations will facilitate future research. Project Page: http://marineeval.hkustvgd.com/

</details>


### [43] [TGC-Net: A Structure-Aware and Semantically-Aligned Framework for Text-Guided Medical Image Segmentation](https://arxiv.org/abs/2512.21135)
*Gaoren Lin,Huangxuan Zhao,Yuan Xiong,Lefei Zhang,Bo Du,Wentao Zhu*

Main category: cs.CV

TL;DR: TGC-Net通过CLIP基础架构改进医学影像分割，解决细粒度结构保留、临床描述建模和领域语义对齐三大挑战，实现了参数高效的最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本引导的医学分割方法依赖未对齐的多模态编码器，需要复杂融合模块。直接应用CLIP存在细粒度结构丢失、临床描述建模不足和领域语义不对齐的问题。

Method: 提出TGC-Net框架：1）SSE编码器增强CLIP ViT的CNN分支实现多尺度结构优化；2）DATE文本编码器注入LLM医疗知识；3）VLCM模块在统一特征空间校准跨模态对应关系。

Result: 在5个胸片和胸部CT数据集上实现最先进性能，参数量大幅减少，在挑战性基准上获得显著Dice提升。

Conclusion: TGC-Net通过参数高效的特定任务适配，有效解决了CLIP在医学影像应用中的三大限制，为文本引导医学分割提供了更优解决方案。

Abstract: Text-guided medical segmentation enhances segmentation accuracy by utilizing clinical reports as auxiliary information. However, existing methods typically rely on unaligned image and text encoders, which necessitate complex interaction modules for multimodal fusion. While CLIP provides a pre-aligned multimodal feature space, its direct application to medical imaging is limited by three main issues: insufficient preservation of fine-grained anatomical structures, inadequate modeling of complex clinical descriptions, and domain-specific semantic misalignment. To tackle these challenges, we propose TGC-Net, a CLIP-based framework focusing on parameter-efficient, task-specific adaptations. Specifically, it incorporates a Semantic-Structural Synergy Encoder (SSE) that augments CLIP's ViT with a CNN branch for multi-scale structural refinement, a Domain-Augmented Text Encoder (DATE) that injects large-language-model-derived medical knowledge, and a Vision-Language Calibration Module (VLCM) that refines cross-modal correspondence in a unified feature space. Experiments on five datasets across chest X-ray and thoracic CT modalities demonstrate that TGC-Net achieves state-of-the-art performance with substantially fewer trainable parameters, including notable Dice gains on challenging benchmarks.

</details>


### [44] [ORCA: Object Recognition and Comprehension for Archiving Marine Species](https://arxiv.org/abs/2512.21150)
*Yuk-Kwan Wong,Haixin Liang,Zeyu Ma,Yiwei Chen,Ziqiang Zheng,Rinaldi Gotama,Pascal Sebastian,Lauren D. Sparks,Sai-Kit Yeung*

Main category: cs.CV

TL;DR: ORCA is a multi-modal marine dataset with 14,647 images and 42,217 annotations, benchmarking 18 models on detection, captioning and visual grounding tasks to address marine vision challenges.


<details>
  <summary>Details</summary>
Motivation: Marine visual understanding is crucial for ecosystem monitoring but hindered by limited data and lack of systematic task formulation aligning marine challenges with computer vision tasks.

Method: Created ORCA benchmark with fine-grained visual/textual annotations, evaluated 18 SOTA models on object detection (closed/open-vocabulary), instance captioning and visual grounding tasks.

Result: Results reveal key challenges including species diversity, morphological overlap and domain-specific demands, demonstrating the difficulty of marine visual understanding.

Conclusion: ORCA establishes a comprehensive benchmark to advance marine vision research by providing standardized evaluation framework and highlighting domain-specific challenges.

Abstract: Marine visual understanding is essential for monitoring and protecting marine ecosystems, enabling automatic and scalable biological surveys. However, progress is hindered by limited training data and the lack of a systematic task formulation that aligns domain-specific marine challenges with well-defined computer vision tasks, thereby limiting effective model application. To address this gap, we present ORCA, a multi-modal benchmark for marine research comprising 14,647 images from 478 species, with 42,217 bounding box annotations and 22,321 expert-verified instance captions. The dataset provides fine-grained visual and textual annotations that capture morphology-oriented attributes across diverse marine species. To catalyze methodological advances, we evaluate 18 state-of-the-art models on three tasks: object detection (closed-set and open-vocabulary), instance captioning, and visual grounding. Results highlight key challenges, including species diversity, morphological overlap, and specialized domain demands, underscoring the difficulty of marine understanding. ORCA thus establishes a comprehensive benchmark to advance research in marine domain. Project Page: http://orca.hkustvgd.com/.

</details>


### [45] [A Turn Toward Better Alignment: Few-Shot Generative Adaptation with Equivariant Feature Rotation](https://arxiv.org/abs/2512.21174)
*Chenghao Xu,Qi Liu,Jiexi Yan,Muli Yang,Cheng Deng*

Main category: cs.CV

TL;DR: 本文提出了一种新的少样本图像生成方法EFR，通过在自旋转的代理特征空间中进行域对齐来解决传统一致性约束方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统少样本图像生成方法通过实例级或分布级损失函数直接对齐源域和目标域的分布模式，但这些方法存在约束过严或过松的问题，容易导致内容扭曲或无法有效利用源域知识。

Method: 提出等变特征旋转（EFR）策略，在参数化李群中进行自适应旋转，将源域和目标域特征转换到等变代理空间进行对齐，通过可学习的旋转矩阵在保留域内结构信息的同时实现知识迁移。

Result: 在多个常用数据集上的综合实验表明，该方法显著提升了目标域内的生成性能。

Conclusion: EFR方法通过创新的特征旋转对齐策略有效克服了传统方法的局限性，为少样本图像生成提供了更有效的解决方案。

Abstract: Few-shot image generation aims to effectively adapt a source generative model to a target domain using very few training images. Most existing approaches introduce consistency constraints-typically through instance-level or distribution-level loss functions-to directly align the distribution patterns of source and target domains within their respective latent spaces. However, these strategies often fall short: overly strict constraints can amplify the negative effects of the domain gap, leading to distorted or uninformative content, while overly relaxed constraints may fail to leverage the source domain effectively. This limitation primarily stems from the inherent discrepancy in the underlying distribution structures of the source and target domains. The scarcity of target samples further compounds this issue by hindering accurate estimation of the target domain's distribution. To overcome these limitations, we propose Equivariant Feature Rotation (EFR), a novel adaptation strategy that aligns source and target domains at two complementary levels within a self-rotated proxy feature space. Specifically, we perform adaptive rotations within a parameterized Lie Group to transform both source and target features into an equivariant proxy space, where alignment is conducted. These learnable rotation matrices serve to bridge the domain gap by preserving intra-domain structural information without distortion, while the alignment optimization facilitates effective knowledge transfer from the source to the target domain. Comprehensive experiments on a variety of commonly used datasets demonstrate that our method significantly enhances the generative performance within the targeted domain.

</details>


### [46] [Towards Arbitrary Motion Completing via Hierarchical Continuous Representation](https://arxiv.org/abs/2512.21183)
*Chenghao Xu,Guangtao Lyu,Qi Liu,Jiexi Yan,Muli Yang,Cheng Deng*

Main category: cs.CV

TL;DR: 本文提出了一种名为NAME的层次隐式表示框架，基于隐式神经表示（INRs），用于连续表示人体运动序列，支持任意帧率的插值、补间和外推。


<details>
  <summary>Details</summary>
Motivation: 人体运动本质上是连续的，更高的相机帧率通常有助于提高平滑度和时间一致性。本文首次探索了人体运动序列的连续表示，旨在实现任意帧率下的运动序列处理。

Method: 提出了一种基于隐式神经表示（INRs）的层次隐式表示框架NAME，引入了层次时间编码机制从多个时间尺度提取特征，并集成了基于傅里叶变换的参数化激活函数到MLP解码器中，以增强连续表示的表达能力。

Result: 在多个基准数据集上的广泛评估证明了所提出方法的有效性和鲁棒性。

Conclusion: NAME框架能够高精度地表示复杂的运动行为，为人体运动序列的连续表示提供了有效的解决方案。

Abstract: Physical motions are inherently continuous, and higher camera frame rates typically contribute to improved smoothness and temporal coherence. For the first time, we explore continuous representations of human motion sequences, featuring the ability to interpolate, inbetween, and even extrapolate any input motion sequences at arbitrary frame rates. To achieve this, we propose a novel parametric activation-induced hierarchical implicit representation framework, referred to as NAME, based on Implicit Neural Representations (INRs). Our method introduces a hierarchical temporal encoding mechanism that extracts features from motion sequences at multiple temporal scales, enabling effective capture of intricate temporal patterns. Additionally, we integrate a custom parametric activation function, powered by Fourier transformations, into the MLP-based decoder to enhance the expressiveness of the continuous representation. This parametric formulation significantly augments the model's ability to represent complex motion behaviors with high accuracy. Extensive evaluations across several benchmark datasets demonstrate the effectiveness and robustness of our proposed approach.

</details>


### [47] [UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement](https://arxiv.org/abs/2512.21185)
*Tanghui Jia,Dongyu Yan,Dehao Hao,Yang Li,Kaiyi Zhang,Xianyi He,Lanjiong Li,Jinnan Chen,Lutao Jiang,Qishen Yin,Long Quan,Ying-Cong Chen,Li Yuan*

Main category: cs.CV

TL;DR: 一个可扩展3D扩散框架，用于生成高质量3D几何形状。采用两阶段生成流程，并提供创新的数据处理流程以提升公开3D数据质量，在几何形状生成方面具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 解决3D几何生成中高保真几何形状的挑战，特别是在训练资源有限的情况下，提升公开3D数据质量，并实现精细几何细节的合成。

Method: 两阶段生成流程：首先生成粗略全局结构，然后细化产生高精度几何形状。通过新的数据清洗和填充方法改进公开3D数据质量；并在扩散过程中通过基于体素查询的RoPE编码，分离空间定位与几何细节合成。

Result: 提出的方法在几何质量上表现优越，可与现有开源方法竞争；数据处理流程有效提高数据质量，并通过精细几何合成实现高保真3D生成。

Conclusion: UltraShape 1.0通过两阶段生成流程和创新的数据处理方法提升了3D几何生成的保真度，在资源限制下展现出优秀的几何质量，并公开全部代码和模型以促进未来研究。

Abstract: In this report, we introduce UltraShape 1.0, a scalable 3D diffusion framework for high-fidelity 3D geometry generation. The proposed approach adopts a two-stage generation pipeline: a coarse global structure is first synthesized and then refined to produce detailed, high-quality geometry. To support reliable 3D generation, we develop a comprehensive data processing pipeline that includes a novel watertight processing method and high-quality data filtering. This pipeline improves the geometric quality of publicly available 3D datasets by removing low-quality samples, filling holes, and thickening thin structures, while preserving fine-grained geometric details. To enable fine-grained geometry refinement, we decouple spatial localization from geometric detail synthesis in the diffusion process. We achieve this by performing voxel-based refinement at fixed spatial locations, where voxel queries derived from coarse geometry provide explicit positional anchors encoded via RoPE, allowing the diffusion model to focus on synthesizing local geometric details within a reduced, structured solution space. Our model is trained exclusively on publicly available 3D datasets, achieving strong geometric quality despite limited training resources. Extensive evaluations demonstrate that UltraShape 1.0 performs competitively with existing open-source methods in both data processing quality and geometry generation. All code and trained models will be released to support future research.

</details>


### [48] [VisRes Bench: On Evaluating the Visual Reasoning Capabilities of VLMs](https://arxiv.org/abs/2512.21194)
*Brigitta Malagurski Törtei,Yasser Dahou,Ngoc Dung Huynh,Wamiq Reyaz Para,Phúc H. Lê Khac,Ankit Singh,Sofian Chaybouti,Sanath Narayan*

Main category: cs.CV

TL;DR: VisRes Benchmark是一个专门研究视觉语言模型视觉推理能力的基准，通过在三个复杂性级别上分析模型行为，发现当前SOTA模型在面对细微感知扰动时表现接近随机，揭示了其抽象推理能力的局限性


<details>
  <summary>Details</summary>
Motivation: 目前视觉语言模型在视觉问答和图像描述等任务上取得了显著进展，但这些模型到底是在进行视觉推理还是依赖语言先验尚不清楚。本文旨在研究模型在自然环境下无需语言监督时的纯视觉推理能力

Method: 设计了VisRes Benchmark基准测试，包含三个复杂性级别：Level 1测试感知补全和全局图像匹配能力（在模糊、纹理变化、遮挡和旋转等扰动下）；Level 2测试基于单属性的规则推理（如颜色、数量、方向）；Level 3测试需要整合多个视觉属性的组合推理

Result: 在超过19,000张控制任务图像上的测试表明，最先进的视觉语言模型在细微感知扰动下表现接近随机水平，显示出除了模式识别外的抽象推理能力有限

Conclusion: VisRes Benchmark为推进多模态研究中的抽象视觉推理提供了一个统一框架，揭示了当前模型在感知和关系视觉推理能力方面的明显局限性

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress across tasks such as visual question answering and image captioning. Yet, the extent to which these models perform visual reasoning as opposed to relying on linguistic priors remains unclear. To address this, we introduce VisRes Bench, a benchmark designed to study visual reasoning in naturalistic settings without contextual language supervision. Analyzing model behavior across three levels of complexity, we uncover clear limitations in perceptual and relational visual reasoning capacities. VisRes isolates distinct reasoning abilities across its levels. Level 1 probes perceptual completion and global image matching under perturbations such as blur, texture changes, occlusion, and rotation; Level 2 tests rule-based inference over a single attribute (e.g., color, count, orientation); and Level 3 targets compositional reasoning that requires integrating multiple visual attributes. Across more than 19,000 controlled task images, we find that state-of-the-art VLMs perform near random under subtle perceptual perturbations, revealing limited abstraction beyond pattern recognition. We conclude by discussing how VisRes provides a unified framework for advancing abstract visual reasoning in multimodal research.

</details>


### [49] [Human Motion Estimation with Everyday Wearables](https://arxiv.org/abs/2512.21209)
*Siqi Zhu,Yixuan Li,Junfu Li,Qi Wu,Zan Wang,Haozhe Ma,Wei Liang*

Main category: cs.CV

TL;DR: EveryWear paper presents a lightweight wearable-based motion capture system using commonly available devices (smartphone, watch, earbuds, glasses) to address issues of poor wearability, high cost, and complex calibration found in existing methods, showing improved performance for daily life applications.


<details>
  <summary>Details</summary>
Motivation: Existing on-body human motion estimation methods for eXtended Reality (XR) suffer from poor wearability, expensive hardware, and cumbersome calibration, making them unsuitable for daily use. The authors aim to overcome these challenges via accessible, everyday wearables.

Method: Proposes EveryWear, which uses smartphone, smartwatch, earbuds, and smart glasses equipped with one forward-facing and two downward-facing cameras. A multimodal teacher-student framework is employed to integrate visual cues from egocentric cameras with inertial signals from wearables, eliminating need for explicit calibration. The training leverages 'Ego-Elec', a 9-hour real-world dataset from 56 daily activities in 17 environments with ground-truth motion capture annotations.

Result: Experiments show that the EveryWear system surpasses baseline models in motion estimation performance. Training directly on real-world data also effectively narrows the sim-to-real gap, enhancing robustness.

Conclusion: EveryWear offers a lightweight, practical human motion capture approach using everyday wearables without calibration. By integrating sensory signals from multiple devices and utilizing real-world data, the framework achieves reliable full-body motion estimation suitable for daily applications such as XR interaction.

Abstract: While on-body device-based human motion estimation is crucial for applications such as XR interaction, existing methods often suffer from poor wearability, expensive hardware, and cumbersome calibration, which hinder their adoption in daily life. To address these challenges, we present EveryWear, a lightweight and practical human motion capture approach based entirely on everyday wearables: a smartphone, smartwatch, earbuds, and smart glasses equipped with one forward-facing and two downward-facing cameras, requiring no explicit calibration before use. We introduce Ego-Elec, a 9-hour real-world dataset covering 56 daily activities across 17 diverse indoor and outdoor environments, with ground-truth 3D annotations provided by the motion capture (MoCap), to facilitate robust research and benchmarking in this direction. Our approach employs a multimodal teacher-student framework that integrates visual cues from egocentric cameras with inertial signals from consumer devices. By training directly on real-world data rather than synthetic data, our model effectively eliminates the sim-to-real gap that constrains prior work. Experiments demonstrate that our method outperforms baseline models, validating its effectiveness for practical full-body motion estimation.

</details>


### [50] [Latent Implicit Visual Reasoning](https://arxiv.org/abs/2512.21218)
*Kelvin Li,Chuyi Shang,Leonid Karlinsky,Rogerio Feris,Trevor Darrell,Roei Herzig*

Main category: cs.CV

TL;DR: 提出了一种任务不可知机制，让大型多模态模型能够自主发现和使用视觉推理标记，无需显式监督，从而更好地处理视觉主导的推理任务。


<details>
  <summary>Details</summary>
Motivation: 现有的大型多模态模型主要依赖文本作为核心推理模态，在处理视觉主导的推理任务时能力有限。当前方法需要手动标注中间视觉步骤，成本高且泛化能力差。

Method: 训练模型自主发现和使用视觉推理标记，这些标记能够全局关注并以任务自适应方式重新编码图像，无需手工监督。

Result: 该方法在直接微调基础上表现更优，在多种视觉中心任务上达到最先进水平，包括那些中间抽象难以指定的任务，并能够泛化到多任务指令调优。

Conclusion: 提出的任务不可知机制成功解决了LMMs在视觉推理任务中的局限性，提供了一种无需手工监督的有效视觉抽象方法，具有很好的泛化能力。

Abstract: While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what "useful" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.

</details>


### [51] [Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval](https://arxiv.org/abs/2512.21221)
*Dao Sy Duy Minh,Huynh Trung Kiet,Nguyen Lam Phu Quy,Phu-Hoa Pham,Tran Chi Nguyen*

Main category: cs.CV

TL;DR: 提出了一个轻量级的两阶段图像检索管道，利用事件中心实体提取来结合真实世界字幕中的时间和上下文信号，在OpenEvents v1基准测试中显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界中图像文本检索面临的挑战，包括模糊或上下文相关的查询、语言变异性以及可扩展性需求。

Method: 采用两阶段检索流程：第一阶段使用基于显著实体的BM25进行高效候选过滤；第二阶段应用BEiT-3模型捕获深度多模态语义并重新排序结果。

Result: 在OpenEvents v1基准测试中达到0.559的平均精度，显著优于先前基线方法。

Conclusion: 结合事件引导过滤和长文本视觉语言建模在复杂现实场景中实现了准确高效的检索，验证了该方法的有效性。

Abstract: Retrieving images from natural language descriptions is a core task at the intersection of computer vision and natural language processing, with wide-ranging applications in search engines, media archiving, and digital content management. However, real-world image-text retrieval remains challenging due to vague or context-dependent queries, linguistic variability, and the need for scalable solutions. In this work, we propose a lightweight two-stage retrieval pipeline that leverages event-centric entity extraction to incorporate temporal and contextual signals from real-world captions. The first stage performs efficient candidate filtering using BM25 based on salient entities, while the second stage applies BEiT-3 models to capture deep multimodal semantics and rerank the results. Evaluated on the OpenEvents v1 benchmark, our method achieves a mean average precision of 0.559, substantially outperforming prior baselines. These results highlight the effectiveness of combining event-guided filtering with long-text vision-language modeling for accurate and efficient retrieval in complex, real-world scenarios. Our code is available at https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval

</details>


### [52] [SegMo: Segment-aligned Text to 3D Human Motion Generation](https://arxiv.org/abs/2512.21237)
*Bowen Dang,Lin Wu,Xiaohang Yang,Zheng Yuan,Zhixiang Chen*

Main category: cs.CV

TL;DR: SegMo框架通过细粒度文本-运动对齐，将文本描述和运动序列分解为语义片段进行对比学习，提高了3D人体运动生成质量


<details>
  <summary>Details</summary>
Motivation: 现有方法在序列级别对齐文本和运动，忽略了模态内部语义结构。文本描述和运动序列都可分解为语义连贯的片段，可作为原子对齐单元实现更细粒度的对应

Method: SegMo包含三个模块：文本片段提取（将复杂描述分解为时序短语）、运动片段提取（将完整运动分割为对应片段）、细粒度文本-运动对齐（通过对比学习对齐片段）

Result: 在HumanML3D测试集上TOP 1分数达到0.553，优于强基线。学习的共享嵌入空间还可应用于运动定位和运动到文本检索等任务

Conclusion: SegMo通过片段级对齐实现了更精细的文本-运动对应，提升了生成质量，且具有多任务应用潜力

Abstract: Generating 3D human motions from textual descriptions is an important research problem with broad applications in video games, virtual reality, and augmented reality. Recent methods align the textual description with human motion at the sequence level, neglecting the internal semantic structure of modalities. However, both motion descriptions and motion sequences can be naturally decomposed into smaller and semantically coherent segments, which can serve as atomic alignment units to achieve finer-grained correspondence. Motivated by this, we propose SegMo, a novel Segment-aligned text-conditioned human Motion generation framework to achieve fine-grained text-motion alignment. Our framework consists of three modules: (1) Text Segment Extraction, which decomposes complex textual descriptions into temporally ordered phrases, each representing a simple atomic action; (2) Motion Segment Extraction, which partitions complete motion sequences into corresponding motion segments; and (3) Fine-grained Text-Motion Alignment, which aligns text and motion segments with contrastive learning. Extensive experiments demonstrate that SegMo improves the strong baseline on two widely used datasets, achieving an improved TOP 1 score of 0.553 on the HumanML3D test set. Moreover, thanks to the learned shared embedding space for text and motion segments, SegMo can also be applied to retrieval-style tasks such as motion grounding and motion-to-text retrieval.

</details>


### [53] [DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation](https://arxiv.org/abs/2512.21252)
*Jiawei Liu,Junqiao Li,Jiangfan Deng,Gen Li,Siyu Zhou,Zetao Fang,Shanshan Lao,Zengde Deng,Jianing Zhu,Tingting Ma,Jiayi Li,Yunqiu Wang,Qian He,Xinglong Wu*

Main category: cs.CV

TL;DR: DreaMontage is a framework that enables the generation of seamless, expressive, long-duration 'one-shot' videos using arbitrary frame guidance, addressing the limitations of traditional filmmaking and naive AI video generation.


<details>
  <summary>Details</summary>
Motivation: Traditional one-shot filmmaking is costly and constrained, while existing AI video generation methods suffer from poor visual smoothness and temporal coherence due to simple clip concatenation.

Method: 1) Lightweight intermediate-conditioning mechanism with Adaptive Tuning in DiT architecture; 2) High-quality dataset curation with Visual Expression SFT and Tailored DPO for motion rationality; 3) Segment-wise Auto-Regressive inference for memory-efficient long sequences.

Result: Extensive experiments show the approach achieves visually striking and seamlessly coherent one-shot effects with computational efficiency.

Conclusion: DreaMontage successfully empowers users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences through its comprehensive framework.

Abstract: The "one-shot" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.

</details>


### [54] [AnyAD: Unified Any-Modality Anomaly Detection in Incomplete Multi-Sequence MRI](https://arxiv.org/abs/2512.21264)
*Changwei Wu,Yifei Chen,Yuxin Du,Mingxuan Liu,Jinying Zong,Beining Wu,Jie Dong,Feiwei Qin,Yunkang Cao,Qiyuan Tian*

Main category: cs.CV

TL;DR: 该研究提出了一种统一的任意模态异常检测框架，能够在任意MRI模态可用性下进行稳健的异常检测和定位，无需重复训练即可适应所有模态配置。


<details>
  <summary>Details</summary>
Motivation: 解决脑MRI异常检测中标注异常病例稀缺且真实临床工作流中经常缺少关键成像模态的问题，克服现有单类或多类异常检测模型依赖固定模态配置、需要重复训练或无法泛化到未见模态组合的限制。

Method: 集成双路径DINOv2编码器与特征分布对齐机制，通过统计对齐不完整模态特征与完整模态表示；引入固有正常原型提取器和INP引导的解码器，仅重建正常解剖模式同时自然放大异常偏差；通过随机模态掩码和间接特征完成进行训练。

Result: 在BraTS2018、MU-Glioma-Post和Pretreat-MetsToBrain-Masks数据集上的广泛实验表明，该方法在7种模态组合下始终优于最先进的工业和医学异常检测基线，实现了优异的泛化性能。

Conclusion: 本研究为真实世界不完美模态条件下的多模态医学异常检测建立了一个可扩展的范式。

Abstract: Reliable anomaly detection in brain MRI remains challenging due to the scarcity of annotated abnormal cases and the frequent absence of key imaging modalities in real clinical workflows. Existing single-class or multi-class anomaly detection (AD) models typically rely on fixed modality configurations, require repetitive training, or fail to generalize to unseen modality combinations, limiting their clinical scalability. In this work, we present a unified Any-Modality AD framework that performs robust anomaly detection and localization under arbitrary MRI modality availability. The framework integrates a dual-pathway DINOv2 encoder with a feature distribution alignment mechanism that statistically aligns incomplete-modality features with full-modality representations, enabling stable inference even with severe modality dropout. To further enhance semantic consistency, we introduce an Intrinsic Normal Prototypes (INPs) extractor and an INP-guided decoder that reconstruct only normal anatomical patterns while naturally amplifying abnormal deviations. Through randomized modality masking and indirect feature completion during training, the model learns to adapt to all modality configurations without re-training. Extensive experiments on BraTS2018, MU-Glioma-Post, and Pretreat-MetsToBrain-Masks demonstrate that our approach consistently surpasses state-of-the-art industrial and medical AD baselines across 7 modality combinations, achieving superior generalization. This study establishes a scalable paradigm for multimodal medical AD under real-world, imperfect modality conditions. Our source code is available at https://github.com/wuchangw/AnyAD.

</details>


### [55] [ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision](https://arxiv.org/abs/2512.21268)
*Weiqi Li,Zehao Zhang,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: 提出了注意力条件扩散(ACD)框架，通过注意力监督实现视频扩散模型中的直接条件控制，解决了现有方法操控性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于分类器引导的方法在视频合成中操控性有限，存在对抗性伪影问题，需要更直接有效的条件控制机制。

Method: 使用稀疏3D感知物体布局作为条件信号，通过Layout ControlNet和自动标注流程，将模型注意力图与外部控制信号对齐。

Result: 在基准视频生成数据集上的实验表明，ACD在保持时间连贯性和视觉保真度的同时，实现了与条件输入的更好对齐。

Conclusion: ACD为条件视频合成建立了一个有效的范式，通过注意力监督实现了更好的操控性。

Abstract: Controllability is a fundamental requirement in video synthesis, where accurate alignment with conditioning signals is essential. Existing classifier-free guidance methods typically achieve conditioning indirectly by modeling the joint distribution of data and conditions, which often results in limited controllability over the specified conditions. Classifier-based guidance enforces conditions through an external classifier, but the model may exploit this mechanism to raise the classifier score without genuinely satisfying the intended condition, resulting in adversarial artifacts and limited effective controllability. In this paper, we propose Attention-Conditional Diffusion (ACD), a novel framework for direct conditional control in video diffusion models via attention supervision. By aligning the model's attention maps with external control signals, ACD achieves better controllability. To support this, we introduce a sparse 3D-aware object layout as an efficient conditioning signal, along with a dedicated Layout ControlNet and an automated annotation pipeline for scalable layout integration. Extensive experiments on benchmark video generation datasets demonstrate that ACD delivers superior alignment with conditioning inputs while preserving temporal coherence and visual fidelity, establishing an effective paradigm for conditional video synthesis.

</details>


### [56] [GriDiT: Factorized Grid-Based Diffusion for Efficient Long Image Sequence Generation](https://arxiv.org/abs/2512.21276)
*Snehal Singh Tomar,Alexandros Graikos,Arjun Krishna,Dimitris Samaras,Klaus Mueller*

Main category: cs.CV

TL;DR: 本文提出了一种新的图像序列生成方法，通过对生成过程进行分解：先在低分辨率下生成粗略序列，再对单个帧进行高分辨率细化，克服了现有方法的局限。


<details>
  <summary>Details</summary>
Motivation: 现有图像序列生成方法将序列视为大型张量直接处理，存在效率低下和瓶颈问题。作者旨在寻找更有效的图像序列建模方式。

Method: 训练仅基于降采样帧组成的网格图像的生成模型，利用Diffusion Transformer的自注意力机制捕捉帧间相关性，先生成低分辨率3D序列，再单独对每帧进行超分辨率处理。

Result: 相比现有方法，本方法在合成质量、序列连贯性、任意长度序列生成、推理效率和数据使用效率方面均表现更优，通常比当前最优方法快至少两倍。

Conclusion: 这种简单而有效的分解方法能够跨多个数据域泛化，无需额外先验和监督，在质量和推理速度上均优于当前最优方法。

Abstract: Modern deep learning methods typically treat image sequences as large tensors of sequentially stacked frames. However, is this straightforward representation ideal given the current state-of-the-art (SoTA)? In this work, we address this question in the context of generative models and aim to devise a more effective way of modeling image sequence data. Observing the inefficiencies and bottlenecks of current SoTA image sequence generation methods, we showcase that rather than working with large tensors, we can improve the generation process by factorizing it into first generating the coarse sequence at low resolution and then refining the individual frames at high resolution. We train a generative model solely on grid images comprising subsampled frames. Yet, we learn to generate image sequences, using the strong self-attention mechanism of the Diffusion Transformer (DiT) to capture correlations between frames. In effect, our formulation extends a 2D image generator to operate as a low-resolution 3D image-sequence generator without introducing any architectural modifications. Subsequently, we super-resolve each frame individually to add the sequence-independent high-resolution details. This approach offers several advantages and can overcome key limitations of the SoTA in this domain. Compared to existing image sequence generation models, our method achieves superior synthesis quality and improved coherence across sequences. It also delivers high-fidelity generation of arbitrary-length sequences and increased efficiency in inference time and training data usage. Furthermore, our straightforward formulation enables our method to generalize effectively across diverse data domains, which typically require additional priors and supervision to model in a generative context. Our method consistently outperforms SoTA in quality and inference speed (at least twice-as-fast) across datasets.

</details>


### [57] [Surgical Scene Segmentation using a Spike-Driven Video Transformer with Real-Time Potential](https://arxiv.org/abs/2512.21284)
*Shihao Zou,Jingjing Li,Wei Ji,Jincai Huang,Kai Wang,Guo Dan,Weixin Si,Yi Pan*

Main category: cs.CV

TL;DR: 提出首个用于手术场景分割的脉冲驱动视频Transformer框架SpikeSurgSeg，在非GPU平台上实现实时性能


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在手术环境中的计算需求过高，而脉冲神经网络(SNN)具有高效能但受限于标注数据稀缺和视频表示稀疏性

Method: 采用手术场景掩码自编码预训练策略，通过分层管状掩码实现稳健的时空表示学习，并结合轻量级脉冲驱动分割头

Result: 在EndoVis18和SurgBleed数据集上达到与ANN模型相当的mIoU，推理延迟降低8倍以上，相比基础模型加速超过20倍

Conclusion: SpikeSurgSeg展示了SNN在时间关键型手术场景分割中的潜力，为资源受限手术环境提供了可行的实时解决方案

Abstract: Modern surgical systems increasingly rely on intelligent scene understanding to provide timely situational awareness for enhanced intra-operative safety. Within this pipeline, surgical scene segmentation plays a central role in accurately perceiving operative events. Although recent deep learning models, particularly large-scale foundation models, achieve remarkable segmentation accuracy, their substantial computational demands and power consumption hinder real-time deployment in resource-constrained surgical environments. To address this limitation, we explore the emerging SNN as a promising paradigm for highly efficient surgical intelligence. However, their performance is still constrained by the scarcity of labeled surgical data and the inherently sparse nature of surgical video representations. To this end, we propose \textit{SpikeSurgSeg}, the first spike-driven video Transformer framework tailored for surgical scene segmentation with real-time potential on non-GPU platforms. To address the limited availability of surgical annotations, we introduce a surgical-scene masked autoencoding pretraining strategy for SNNs that enables robust spatiotemporal representation learning via layer-wise tube masking. Building on this pretrained backbone, we further adopt a lightweight spike-driven segmentation head that produces temporally consistent predictions while preserving the low-latency characteristics of SNNs. Extensive experiments on EndoVis18 and our in-house SurgBleed dataset demonstrate that SpikeSurgSeg achieves mIoU comparable to SOTA ANN-based models while reducing inference latency by at least $8\times$. Notably, it delivers over $20\times$ acceleration relative to most foundation-model baselines, underscoring its potential for time-critical surgical scene segmentation.

</details>


### [58] [Post-Processing Mask-Based Table Segmentation for Structural Coordinate Extraction](https://arxiv.org/abs/2512.21287)
*Suren Bandara*

Main category: cs.CV

TL;DR: 提出一种新颖的多尺度信号处理方法用于从表格掩模中检测表格边缘，通过将行列转换建模为一维信号处理，显著提高了在噪声图像中的表格结构检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在低分辨率或有噪声图像中准确识别表格边界存在挑战，基于掩模的边缘检测虽然较为鲁棒，但直接应用于图像会导致噪声敏感、分辨率损失或高计算成本。

Method: 将行列转换建模为一维信号，使用高斯卷积和统计阈值处理抑制噪声，检测到的峰值映射回图像坐标获得边界，通过零填充和缩放策略保持分辨率适应性。

Result: 在PubLayNet-1M基准测试中，使用TableNet和PyTesseract OCR进行列边缘检测时，将CASA准确率从67%提高到76%。

Conclusion: 该方法有效提升了噪声图像中表格结构检测的准确性，并为下游分析提供了优化结构化的表格输出。

Abstract: Structured data extraction from tables plays a crucial role in document image analysis for scanned documents and digital archives. Although many methods have been proposed to detect table structures and extract cell contents, accurately identifying table segment boundaries (rows and columns) remains challenging, particularly in low-resolution or noisy images. In many real-world scenarios, table data are incomplete or degraded, limiting the adaptability of transformer-based methods to noisy inputs. Mask-based edge detection techniques have shown greater robustness under such conditions, as their sensitivity can be adjusted through threshold tuning; however, existing approaches typically apply masks directly to images, leading to noise sensitivity, resolution loss, or high computational cost. This paper proposes a novel multi-scale signal-processing method for detecting table edges from table masks. Row and column transitions are modeled as one-dimensional signals and processed using Gaussian convolution with progressively increasing variances, followed by statistical thresholding to suppress noise while preserving stable structural edges. Detected signal peaks are mapped back to image coordinates to obtain accurate segment boundaries. Experimental results show that applying the proposed approach to column edge detection improves Cell-Aware Segmentation Accuracy (CASA) a layout-aware metric evaluating both textual correctness and correct cell placement from 67% to 76% on the PubLayNet-1M benchmark when using TableNet with PyTesseract OCR. The method is robust to resolution variations through zero-padding and scaling strategies and produces optimized structured tabular outputs suitable for downstream analysis.

</details>


### [59] [AndroidLens: Long-latency Evaluation with Nested Sub-targets for Android GUI Agents](https://arxiv.org/abs/2512.21302)
*Yue Cao,Yingyao Wang,Pi Bu,Jingxuan Xing,Wei Jiang,Zekun Zhu,Junpeng Ma,Sashuai Zhou,Tong Lu,Jun Song,Yu Cheng,Yuning Jiang,Bo Zheng*

Main category: cs.CV

TL;DR: AndroidLens是用于评估移动端GUI代理的挑战性基准框架，包含571个长延迟任务，涵盖38个领域，通过静态和动态评估方式衡量代理性能。


<details>
  <summary>Details</summary>
Motivation: 现有的GUI代理评估基准存在应用范围有限、任务简单、指标粗糙的问题，需要更贴近真实场景的评估框架。

Method: 创建包含571个长时程任务的基准，包含静态评估（保留真实异常、允许多路径）和动态评估（基于里程碑的ATP指标）。

Result: 最佳模型仅达到12.7%的任务成功率和50.47%的平均任务进度，显示当前GUI代理在真实环境中的表现仍有很大提升空间。

Conclusion: 移动端GUI代理在环境异常处理、自适应探索和长期记忆保持等方面面临重要挑战，需要进一步研究。

Abstract: Graphical user interface (GUI) agents can substantially improve productivity by automating frequently executed long-latency tasks on mobile devices. However, existing evaluation benchmarks are still constrained to limited applications, simple tasks, and coarse-grained metrics. To address this, we introduce AndroidLens, a challenging evaluation framework for mobile GUI agents, comprising 571 long-latency tasks in both Chinese and English environments, each requiring an average of more than 26 steps to complete. The framework features: (1) tasks derived from real-world user scenarios across 38 domains, covering complex types such as multi-constraint, multi-goal, and domain-specific tasks; (2) static evaluation that preserves real-world anomalies and allows multiple valid paths to reduce bias; and (3) dynamic evaluation that employs a milestone-based scheme for fine-grained progress measurement via Average Task Progress (ATP). Our evaluation indicates that even the best models reach only a 12.7% task success rate and 50.47% ATP. We also underscore key challenges in real-world environments, including environmental anomalies, adaptive exploration, and long-term memory retention.

</details>


### [60] [TICON: A Slide-Level Tile Contextualizer for Histopathology Representation Learning](https://arxiv.org/abs/2512.21331)
*Varun Belagali,Saarthak Kapse,Pierre Marza,Srijan Das,Zilinghan Li,Sofiène Boutaj,Pushpak Pati,Srikar Yellapragada,Tarak Nath Nandi,Ravi K Madduri,Joel Saltz,Prateek Prasanna,Stergios Christodoulidis Maria Vakalopoulou,Dimitris Samaras*

Main category: cs.CV

TL;DR: TICON是一个基于transformer的瓦片表示上下文化方法，用于病理学计算中的瓦片嵌入上下文化，显著提升各种任务性能，并以少量数据超越现有最佳模型。


<details>
  <summary>Details</summary>
Motivation: 解决病理学图像分析中瓦片编码器忽略图像上下文信息的问题，需要统一的模型来上下文化不同瓦片级基础模型的嵌入表示。

Method: 使用基于transformer的单编码器，通过掩码建模目标预训练，同时统一和上下文化来自不同瓦片级病理基础模型的表示。

Result: TICON上下文化嵌入显著提升多种任务性能，在瓦片级和滑片级基准测试中创下新最优成绩，仅用11K图像预训练的聚合器即优于用350K图像的现有最佳模型。

Conclusion: TICON提供了一种有效的瓦片表示上下文化方法，能够提升病理学计算中各种任务的性能，且具有出色的数据效率。

Abstract: The interpretation of small tiles in large whole slide images (WSI) often needs a larger image context. We introduce TICON, a transformer-based tile representation contextualizer that produces rich, contextualized embeddings for ''any'' application in computational pathology. Standard tile encoder-based pipelines, which extract embeddings of tiles stripped from their context, fail to model the rich slide-level information essential for both local and global tasks. Furthermore, different tile-encoders excel at different downstream tasks. Therefore, a unified model is needed to contextualize embeddings derived from ''any'' tile-level foundation model. TICON addresses this need with a single, shared encoder, pretrained using a masked modeling objective to simultaneously unify and contextualize representations from diverse tile-level pathology foundation models. Our experiments demonstrate that TICON-contextualized embeddings significantly improve performance across many different tasks, establishing new state-of-the-art results on tile-level benchmarks (i.e., HEST-Bench, THUNDER, CATCH) and slide-level benchmarks (i.e., Patho-Bench). Finally, we pretrain an aggregator on TICON to form a slide-level foundation model, using only 11K WSIs, outperforming SoTA slide-level foundation models pretrained with up to 350K WSIs.

</details>


### [61] [Fast SAM2 with Text-Driven Token Pruning](https://arxiv.org/abs/2512.21333)
*Avilasha Mandal,Chaoning Zhang,Fachrina Dewi Puspitasari,Xudong Wang,Jiaquan Zhang,Caiyan Qin,Guoqing Wang,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: 为了提高SAM2模型的推理效率和降低内存占用，提出了一种文本引导的令牌剪枝框架，通过选择性减少在时间传播之前的令牌密度，在不修改分割架构的情况下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: SAM2模型在处理密集视觉令牌时存在高计算和内存成本问题，限制了其在实时应用中的部署。现有方法传播所有令牌导致注意力开销呈二次增长，影响可扩展性。

Method: 提出一种在视觉编码后、基于内存的传播前运行的令牌剪枝方法。使用轻量级路由机制，结合局部视觉上下文、语义相关性（来自以对象为中心的文本描述）和不确定性线索，对令牌进行排名并仅保留最关键的令牌。

Result: 实验表明，该方法在多个视频分割基准测试中实现了高达42.50%的推理速度提升和37.41%的GPU内存使用降低，同时保持了具有竞争力的J和F性能。

Conclusion: 提出的令牌剪枝框架为基于transformer的视频分割系统提供了一种实用有效的效率优化路径，特别是在实时和资源受限的应用场景中具有重要潜力。

Abstract: Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.

</details>


### [62] [Streaming Video Instruction Tuning](https://arxiv.org/abs/2512.21334)
*Jiaer Xia,Peixian Chen,Mengdan Zhang,Xing Sun,Kaiyang Zhou*

Main category: cs.CV

TL;DR: Streamo是一个实时流媒体视频LLM通用交互助手，实现了实时旁白、动作理解、事件字幕等多项流媒体视频任务，通过大规模指令数据集训练展现了强大的时序推理和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有在线视频模型主要关注问答或字幕等狭窄任务，缺乏能够处理多样化实时流媒体视频任务的通用助手。为了弥合离线视频感知模型与实时多模态助手之间的差距。

Method: 构建了Streamo-Instruct-465K大规模指令跟随数据集，涵盖多样化时序上下文和多任务监督，通过端到端流水线训练实现统一的多任务学习。

Result: Streamo在多个流媒体基准测试中表现出强大的时序推理、响应式交互和广泛泛化能力，有效连接了离线视频感知和实时多模态助手。

Conclusion: 该研究向统一的智能视频理解迈出了重要一步，证明了单一模型在连续视频流中处理多样化实时任务的可行性。

Abstract: We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant. Unlike existing online video models that focus narrowly on question answering or captioning, Streamo performs a broad spectrum of streaming video tasks, including real-time narration, action understanding, event captioning, temporal event grounding, and time-sensitive question answering. To develop such versatility, we construct Streamo-Instruct-465K, a large-scale instruction-following dataset tailored for streaming video understanding. The dataset covers diverse temporal contexts and multi-task supervision, enabling unified training across heterogeneous streaming tasks. After training end-to-end on the instruction-following dataset through a streamlined pipeline, Streamo exhibits strong temporal reasoning, responsive interaction, and broad generalization across a variety of streaming benchmarks. Extensive experiments show that Streamo bridges the gap between offline video perception models and real-time multimodal assistants, making a step toward unified, intelligent video understanding in continuous video streams.

</details>


### [63] [Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models](https://arxiv.org/abs/2512.21337)
*Li-Zhong Szu-Tu,Ting-Lin Wu,Chia-Jui Chang,He Syu,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 研究揭示视觉语言模型存在显著流行度偏差，对著名建筑预测准确率比普通建筑高34%，表明模型依赖记忆而非泛化理解


<details>
  <summary>Details</summary>
Motivation: 系统性调查当前最先进视觉语言模型中存在的流行度偏差问题，探究模型是否真正具备推理能力还是仅仅依赖记忆

Method: 构建YearGuessr数据集（55,546张建筑图像，含建筑年份、GPS、页面浏览量等属性），将建筑年份预测任务定义为序数回归，并提出考虑流行度的区间准确率指标

Result: 在30+个模型的基准测试中，包括研发的YearCLIP模型，验证了视觉语言模型在流行、记忆项上表现出色，但对未被识别的主体表现显著较差

Conclusion: 视觉语言模型在推理能力上存在关键缺陷，过度依赖记忆而非真正的泛化理解，需重新评估当前模型的真实能力

Abstract: We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity. Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, memorized items but struggle significantly with unrecognized subjects, exposing a critical flaw in their reasoning capabilities. Project page: https://sytwu.github.io/BeyondMemo/

</details>


### [64] [HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming](https://arxiv.org/abs/2512.21338)
*Haonan Qiu,Shikun Liu,Zijian Zhou,Zhaochong An,Weiming Ren,Zhiheng Liu,Jonas Schult,Sen He,Shoufa Chen,Yuren Cong,Tao Xiang,Ziwei Liu,Juan-Manuel Perez-Rua*

Main category: cs.CV

TL;DR: HiStream通过空间、时间和时间步长的三重压缩，实现了高清视频生成的高效化，在保持视觉质量的同时大幅提升了生成速度。


<details>
  <summary>Details</summary>
Motivation: 传统的高分辨率视频生成方法由于扩散模型的二次复杂度计算瓶颈，导致实际推理效率低下，难以实用化。

Method: 提出HiStream自回归框架，采用三重压缩策略：空间压缩（低分辨率预去噪+高分辨率精炼）、时间压缩（分块处理+固定锚点缓存）、时间步压缩（对后续块减少去噪步数）。

Result: 在1080p基准测试中，HiStream模型达到最先进视觉质量，去噪速度比基线快76.2倍；HiStream+版本加速107.5倍，在速度和质量间取得了理想平衡。

Conclusion: HiStream框架使高分辨率视频生成变得实用且可扩展，解决了当前方法的计算瓶颈问题。

Abstract: High-resolution video generation, while crucial for digital media and film, is computationally bottlenecked by the quadratic complexity of diffusion models, making practical inference infeasible. To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduces redundancy across three axes: i) Spatial Compression: denoising at low resolution before refining at high resolution with cached features; ii) Temporal Compression: a chunk-by-chunk strategy with a fixed-size anchor cache, ensuring stable inference speed; and iii) Timestep Compression: applying fewer denoising steps to subsequent, cache-conditioned chunks. On 1080p benchmarks, our primary HiStream model (i+ii) achieves state-of-the-art visual quality while demonstrating up to 76.2x faster denoising compared to the Wan2.1 baseline and negligible quality loss. Our faster variant, HiStream+, applies all three optimizations (i+ii+iii), achieving a 107.5x acceleration over the baseline, offering a compelling trade-off between speed and quality, thereby making high-resolution video generation both practical and scalable.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [65] [Foundation Model-based Evaluation of Neuropsychiatric Disorders: A Lifespan-Inclusive, Multi-Modal, and Multi-Lingual Study](https://arxiv.org/abs/2512.20948)
*Zhongren Dong,Haotian Guo,Weixiang Xu,Huan Zhao,Zixing Zhang*

Main category: cs.CL

TL;DR: FEND是一种基于基础模型的多模态框架，用于检测阿尔茨海默病、抑郁症和自闭症谱系障碍等神经精神疾病，整合语音和文本模态，在13个多语言数据集上进行系统评估。


<details>
  <summary>Details</summary>
Motivation: 神经精神疾病存在语言和声学异常的特点，但现有方法在多语言泛化和统一评估框架方面存在挑战。

Method: 提出FEND框架，整合语音和文本模态，利用13个多语言数据集系统评估多模态融合性能。

Result: 多模态融合在AD和抑郁症检测中表现优异，但在ASD检测中因数据集异质性表现不佳；存在模态不平衡问题；跨语料库实验显示在多语言和任务异构设置下性能下降。

Conclusion: FEND通过提供广泛基准和性能影响因子分析，推动了自动化、全生命周期覆盖和多语言神经精神疾病评估领域的发展。

Abstract: Neuropsychiatric disorders, such as Alzheimer's disease (AD), depression, and autism spectrum disorder (ASD), are characterized by linguistic and acoustic abnormalities, offering potential biomarkers for early detection. Despite the promise of multi-modal approaches, challenges like multi-lingual generalization and the absence of a unified evaluation framework persist. To address these gaps, we propose FEND (Foundation model-based Evaluation of Neuropsychiatric Disorders), a comprehensive multi-modal framework integrating speech and text modalities for detecting AD, depression, and ASD across the lifespan. Leveraging 13 multi-lingual datasets spanning English, Chinese, Greek, French, and Dutch, we systematically evaluate multi-modal fusion performance. Our results show that multi-modal fusion excels in AD and depression detection but underperforms in ASD due to dataset heterogeneity. We also identify modality imbalance as a prevalent issue, where multi-modal fusion fails to surpass the best mono-modal models. Cross-corpus experiments reveal robust performance in task- and language-consistent scenarios but noticeable degradation in multi-lingual and task-heterogeneous settings. By providing extensive benchmarks and a detailed analysis of performance-influencing factors, FEND advances the field of automated, lifespan-inclusive, and multi-lingual neuropsychiatric disorder assessment. We encourage researchers to adopt the FEND framework for fair comparisons and reproducible research.

</details>


### [66] [Uncovering Competency Gaps in Large Language Models and Their Benchmarks](https://arxiv.org/abs/2512.20638)
*Matyas Bohacek,Nino Scherrer,Nicholas Dufour,Thomas Leung,Christoph Bregler,Stephanie C. Y. Chan*

Main category: cs.CL

TL;DR: 该论文提出了一种使用稀疏自动编码器(SAEs)的新方法来评估大语言模型，能够自动识别模型在特定概念上的能力差距和基准测试的覆盖面失衡问题。通过分析模型内部表示的概念激活和显著性加权性能，该方法在十个基准测试上发现模型在反对谄媚行为和安全相关概念表现较差，同时基准测试存在权威相关概念过度覆盖的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的基准测试评估仅提供聚合指标，这种评估方式会掩盖模型在特定子领域的能力弱点和基准测试本身的覆盖面失衡问题。论文旨在开发一种能够自动识别这些

Method: 使用稀疏自动编码器(SAEs)提取概念激活，计算显著性加权的性能分数来评估基准测试数据。该方法基于模型的内部表示，支持跨基准测试的比较。

Result: 在两个流行开源模型和十个基准测试的应用中发现：1)模型在反谄媚行为和安全相关概念上表现不佳；2)基准测试过度覆盖服从、权威和指令遵循等概念，而忽略其目标范围内的核心概念。这些发现与现有文献观察一致。

Conclusion: 该方法提供了一种基于表示的评估方法，能够对基准测试分数进行概念级分解，揭示模型得分的原因和基准测试改进方向。它不替代传统聚合指标，而是通过概念级分析作为补充。

Abstract: The evaluation of large language models (LLMs) relies heavily on standardized benchmarks. These benchmarks provide useful aggregated metrics for a given capability, but those aggregated metrics can obscure (i) particular sub-areas where the LLMs are weak ("model gaps") and (ii) imbalanced coverage in the benchmarks themselves ("benchmark gaps"). We propose a new method that uses sparse autoencoders (SAEs) to automatically uncover both types of gaps. By extracting SAE concept activations and computing saliency-weighted performance scores across benchmark data, the method grounds evaluation in the model's internal representations and enables comparison across benchmarks. As examples demonstrating our approach, we applied the method to two popular open-source models and ten benchmarks. We found that these models consistently underperformed on concepts that stand in contrast to sycophantic behaviors (e.g., politely refusing a request or asserting boundaries) and concepts connected to safety discussions. These model gaps align with observations previously surfaced in the literature; our automated, unsupervised method was able to recover them without manual supervision. We also observed benchmark gaps: many of the evaluated benchmarks over-represented concepts related to obedience, authority, or instruction-following, while missing core concepts that should fall within their intended scope. In sum, our method offers a representation-grounded approach to evaluation, enabling concept-level decomposition of benchmark scores. Rather than replacing conventional aggregated metrics, CG complements them by providing a concept-level decomposition that can reveal why a model scored as it did and how benchmarks could evolve to better reflect their intended scope. Code is available at https://competency-gaps.github.io.

</details>


### [67] [SA-DiffuSeq: Addressing Computational and Scalability Challenges in Long-Document Generation with Sparse Attention](https://arxiv.org/abs/2512.20724)
*Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CL

TL;DR: 本文提出了SA-DiffuSeq，一个将稀疏注意力整合到扩散模型中的框架，旨在解决长文本生成中的计算成本和内存开销问题，通过选择性注意力分配提高可扩展性。


<details>
  <summary>Details</summary>
Motivation: 面对扩散模型在生成长文本时计算成本高、内存开销大的挑战，研究旨在通过稀疏注意力机制改善长文档建模的可扩展性。

Method: 方法包括在扩散过程中集成稀疏注意力，使用软吸收状态稳定扩散轨迹并加速序列重建，从而提高采样效率和长距离依赖建模精度。

Result: 实验表明，SA-DiffuSeq在训练效率和采样速度上均优于现有扩散基线，尤其在长序列任务中表现突出，适用于科学写作、大规模代码生成和多轮长上下文对话等应用。

Conclusion: 结果表明，将结构化稀疏性引入扩散模型是实现在长文本生成中高效且表达力强的有前景方向。

Abstract: Diffusion based approaches to long form text generation suffer from prohibitive computational cost and memory overhead as sequence length increases. We introduce SA-DiffuSeq, a diffusion framework that integrates sparse attention to fundamentally improve scalability for long document modeling. By selectively allocating attention within the diffusion process, SA-DiffuSeq significantly reduces computational complexity while maintaining semantic coherence and generation quality. A key component of our method is a soft absorbing state tailored to sparse attention dynamics, which stabilizes diffusion trajectories and accelerates sequence reconstruction. This design improves sampling efficiency and enhances precision in long range dependency modeling. Extensive experiments demonstrate that SA-DiffuSeq consistently surpasses state of the art diffusion baselines in both training efficiency and sampling speed, with especially strong gains on extended sequences. These properties make SA-DiffuSeq well suited for demanding long form applications such as scientific writing, large scale code generation, and multi turn long context dialogue. Overall, our results indicate that incorporating structured sparsity into diffusion models is a promising direction for efficient and expressive long text generation.

</details>


### [68] [TokSuite: Measuring the Impact of Tokenizer Choice on Language Model Behavior](https://arxiv.org/abs/2512.20757)
*Gül Sena Altıntaş,Malikeh Ehghaghi,Brian Lester,Fengyuan Liu,Wanru Zhao,Marco Ciccone,Colin Raffel*

Main category: cs.CL

TL;DR: TokSuite是一个用于研究分词器对语言模型影响的工具集，包含相同架构但不同分词器的模型和一个专门衡量分词相关扰动的基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前对分词器在语言模型性能和表现中作用的理解不足，缺乏能够单独衡量分词器影响的实验框架。

Method: 训练了14个使用不同分词器但其他方面（架构、数据集、训练预算、初始化）完全相同的模型，并创建了一个专门测量分词器相关扰动影响的基准测试。

Result: TokSuite能够有效解耦分词器的影响，支持对多种流行分词器优缺点的新发现。

Conclusion: 该研究提供了一种系统评估分词器影响的方法，有助于深入理解不同分词器对语言模型性能的具体作用。

Abstract: Tokenizers provide the fundamental basis through which text is represented and processed by language models (LMs). Despite the importance of tokenization, its role in LM performance and behavior is poorly understood due to the challenge of measuring the impact of tokenization in isolation. To address this need, we present TokSuite, a collection of models and a benchmark that supports research into tokenization's influence on LMs. Specifically, we train fourteen models that use different tokenizers but are otherwise identical using the same architecture, dataset, training budget, and initialization. Additionally, we curate and release a new benchmark that specifically measures model performance subject to real-world perturbations that are likely to influence tokenization. Together, TokSuite allows robust decoupling of the influence of a model's tokenizer, supporting a series of novel findings that elucidate the respective benefits and shortcomings of a wide range of popular tokenizers.

</details>


### [69] [Adversarial Training for Failure-Sensitive User Simulation in Mental Health Dialogue Optimization](https://arxiv.org/abs/2512.20773)
*Ziyi Zhu,Olivier Tieleman,Caitlin A. Stamatis,Luka Smyth,Thomas D. Hull,Daniel R. Cahn,Matteo Malgaroli*

Main category: cs.CL

TL;DR: 该论文提出了一个对抗性训练框架，通过生成器(用户模拟器)和判别器之间的竞争动态来提升用户模拟器的真实性。该方法应用于心理健康支持聊天机器人领域，证明经微调的模拟器在暴露系统问题方面显著优于零样本基础模型，且对抗性训练进一步增强了多样性、分布对齐和预测有效性。


<details>
  <summary>Details</summary>
Motivation: 创建能准确复制人类行为的用户模拟器对于训练和评估任务导向对话系统至关重要，但现有模拟器在暴露系统故障模式方面的能力有限。论文旨在开发一种能更真实模拟用户行为的方法，特别针对心理健康支持聊天机器人领域。

Method: 采用对抗性训练框架，包含一个生成器(用户模拟器)和一个判别器。通过迭代的竞争动态，使生成器不断改进以欺骗判别器，从而提升模拟器的真实性。该方法应用于心理健康支持聊天机器人场景。

Result: 经微调的模拟器在暴露系统问题方面显著优于零样本基础模型。对抗性训练后，模拟器在多样性、分布对齐和预测有效性方面均有提升，模拟失败率与实际失败率之间显示出强相关性，同时保持较低分布差异。判别器准确率在三次对抗迭代后大幅下降，表明模拟器真实性提高。

Conclusion: 对抗性训练是创建真实用户模拟器的有效方法，特别适用于心理健康支持任务导向对话领域，能在部署前实现快速、可靠且成本效益高的系统评估。

Abstract: Realistic user simulation is crucial for training and evaluating task-oriented dialogue (TOD) systems, yet creating simulators that accurately replicate human behavior remains challenging. A key property of effective simulators is their ability to expose failure modes of the systems they evaluate. We present an adversarial training framework that iteratively improves user simulator realism through a competitive dynamic between a generator (user simulator) and a discriminator. Applied to mental health support chatbots, our approach demonstrates that fine-tuned simulators dramatically outperform zero-shot base models at surfacing system issues, and adversarial training further enhances diversity, distributional alignment, and predictive validity. The resulting simulator achieves a strong correlation between simulated and real failure occurrence rates across diverse chatbot configurations while maintaining low distributional divergence of failure modes. Discriminator accuracy decreases drastically after three adversarial iterations, suggesting improved realism. These results provide evidence that adversarial training is a promising approach for creating realistic user simulators in mental health support TOD domains, enabling rapid, reliable, and cost-effective system evaluation before deployment.

</details>


### [70] [Large Language Models Approach Expert Pedagogical Quality in Math Tutoring but Differ in Instructional and Linguistic Profiles](https://arxiv.org/abs/2512.20780)
*Ramatu Oiza Abdulsalam,Segun Aroyehun*

Main category: cs.CL

TL;DR: 大语言模型在数学辅导响应中的教学质量接近专家人工水平，但教学模式和语言策略存在差异：模型较少使用专家常用的重述策略，而生成更长、更多样、更有礼貌的回应。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在数学辅导中的教学行为与专家人工实践的匹配程度，分析其教学策略和语言特征。

Method: 通过受控实验，对比专家人工导师、新手人工导师和多个大语言模型对同一数学辅导对话的响应，分析教学策略和语言特征。

Result: 大语言模型在感知教学质量上接近专家水平，但系统性差异体现在：较少使用重述策略，回应更长、词汇更丰富、更礼貌。重述和词汇多样性与教学质量正相关，而高代理性和礼貌语言则负相关。

Conclusion: 大语言模型的教学质量已达到专家水平，但依赖不同的教学策略，强调在评估智能辅导系统时分析教学策略和语言特征的重要性。

Abstract: Recent work has explored the use of large language models for generating tutoring responses in mathematics, yet it remains unclear how closely their instructional behavior aligns with expert human practice. We examine this question using a controlled, turn-level comparison in which expert human tutors, novice human tutors, and multiple large language models respond to the same set of math remediation conversation turns. We examine both instructional strategies and linguistic characteristics of tutoring responses, including restating and revoicing, pressing for accuracy, lexical diversity, readability, politeness, and agency. We find that large language models approach expert levels of perceived pedagogical quality on average but exhibit systematic differences in their instructional and linguistic profiles. In particular, large language models tend to underuse restating and revoicing strategies characteristic of expert human tutors, while producing longer, more lexically diverse, and more polite responses. Statistical analyses show that restating and revoicing, lexical diversity, and pressing for accuracy are positively associated with perceived pedagogical quality, whereas higher levels of agentic and polite language are negatively associated. Overall, recent large language models exhibit levels of perceived pedagogical quality comparable to expert human tutors, while relying on different instructional and linguistic strategies. These findings underscore the value of analyzing instructional strategies and linguistic characteristics when evaluating tutoring responses across human tutors and intelligent tutoring systems.

</details>


### [71] [Investigating Model Editing for Unlearning in Large Language Models](https://arxiv.org/abs/2512.20794)
*Shariqah Hossain,Lalana Kagal*

Main category: cs.CL

TL;DR: 该研究探索了模型编辑算法（如ROME、IKE和WISE）在机器遗忘任务中的应用，发现这些方法在某些情况下能够超越传统遗忘方法，但在完全移除目标信息的同时不损害模型整体性能方面仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法对于大语言模型效率低下，要么无法完全移除目标信息，要么会损害模型保留的知识性能。模型编辑算法虽然解决了类似问题，但主要关注信息重定向而非完全移除。

Method: 研究探索了ROME、IKE和WISE等模型编辑算法，并为遗忘场景设计了新的编辑目标。通过对比分析评估这些方法在遗忘任务中的表现。

Result: 模型编辑方法在某些设置下能够超越基线遗忘方法，产生更好的遗忘效果。然而，与传统的遗忘技术一样，这些方法在准确定义遗忘范围和保护模型整体性能方面仍存在困难。

Conclusion: 模型编辑算法为机器遗忘任务提供了有前景的替代方案，但在实现精确的遗忘范围控制和保持模型整体性能平衡方面仍需进一步研究。

Abstract: Machine unlearning aims to remove unwanted information from a model, but many methods are inefficient for LLMs with large numbers of parameters or fail to fully remove the intended information without degrading performance on knowledge that should be retained. Model editing algorithms solve a similar problem of changing information in models, but they focus on redirecting inputs to a new target rather than removing that information altogether. In this work, we explore the editing algorithms ROME, IKE, and WISE and design new editing targets for an unlearning setting. Through this investigation, we show that model editing approaches can exceed baseline unlearning methods in terms of quality of forgetting depending on the setting. Like traditional unlearning techniques, they struggle to encapsulate the scope of what is to be unlearned without damage to the overall model performance.

</details>


### [72] [Measuring Mechanistic Independence: Can Bias Be Removed Without Erasing Demographics?](https://arxiv.org/abs/2512.20796)
*Zhengyang Shan,Aaron Mueller*

Main category: cs.CL

TL;DR: 语言模型中的人口统计偏差机制与一般人口统计识别是相互独立的，通过任务特定干预可以实现外科手术式去偏而不损害核心模型能力


<details>
  <summary>Details</summary>
Motivation: 研究人口统计偏差机制与一般人口统计识别之间的关系，探索如何在去偏的同时保留人口统计检测能力

Method: 使用多任务评估框架，将人口统计信息与姓名、职业和教育水平关联，比较基于归因和基于相关性的方法来定位偏差特征，采用目标稀疏自编码器特征消融

Result: 基于归因的消融减少了种族和性别职业刻板印象，同时保持姓名识别准确性；基于相关性的消融对教育偏差更有效；教育任务中移除归因特征会导致'先验崩溃'，增加总体偏差

Conclusion: 人口统计偏差源于任务特定机制而非绝对的人口统计标记，机制性推理时干预能够在不损害核心模型能力的情况下实现精确去偏

Abstract: We investigate how independent demographic bias mechanisms are from general demographic recognition in language models. Using a multi-task evaluation setup where demographics are associated with names, professions, and education levels, we measure whether models can be debiased while preserving demographic detection capabilities. We compare attribution-based and correlation-based methods for locating bias features. We find that targeted sparse autoencoder feature ablations in Gemma-2-9B reduce bias without degrading recognition performance: attribution-based ablations mitigate race and gender profession stereotypes while preserving name recognition accuracy, whereas correlation-based ablations are more effective for education bias. Qualitative analysis further reveals that removing attribution features in education tasks induces ``prior collapse'', thus increasing overall bias. This highlights the need for dimension-specific interventions. Overall, our results show that demographic bias arises from task-specific mechanisms rather than absolute demographic markers, and that mechanistic inference-time interventions can enable surgical debiasing without compromising core model capabilities.

</details>


### [73] [Semantic Deception: When Reasoning Models Can't Compute an Addition](https://arxiv.org/abs/2512.20812)
*Nathaniël de Leeuw,Marceau Nahon,Mathis Reymond,Raja Chatila,Mehdi Khamassi*

Main category: cs.CL

TL;DR: 该研究通过引入语义欺骗来测试大型语言模型的符号推理能力，发现语义线索会显著损害模型在简单任务中的表现，揭示了当前LLMs在符号操作方面的局限性和对表层语义的过度依赖。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究LLMs在处理和操作不熟悉符号时的推理能力，特别是在涉及人类价值观的决策任务中，评估模型是否能保持符号抽象而不是依赖学习的语义关联。

Method: 通过重新定义标准数字和数学运算符为新颖符号，构建语义欺骗情境，让LLMs解决基于这些修改符号的简单计算任务，测试四个不同LLMs的性能。

Result: 实验表明语义线索会显著降低LLMs在简单任务中的表现，即使模型表面上正确遵循指令，语义线索仍会影响基本能力，链式思维可能放大对统计相关性的依赖。

Conclusion: 当前LLMs在符号操作能力上存在局限，过度依赖表层语义，这引发伦理和社会担忧，特别是在需要稳健符号推理的决策场景中，这些局限可能影响LLMs的可靠应用。

Abstract: Large language models (LLMs) are increasingly used in situations where human values are at stake, such as decision-making tasks that involve reasoning when performed by humans. We investigate the so-called reasoning capabilities of LLMs over novel symbolic representations by introducing an experimental framework that tests their ability to process and manipulate unfamiliar symbols. We introduce semantic deceptions: situations in which symbols carry misleading semantic associations due to their form, such as being embedded in specific contexts, designed to probe whether LLMs can maintain symbolic abstraction or whether they default to exploiting learned semantic associations. We redefine standard digits and mathematical operators using novel symbols, and task LLMs with solving simple calculations expressed in this altered notation. The objective is: (1) to assess LLMs' capacity for abstraction and manipulation of arbitrary symbol systems; (2) to evaluate their ability to resist misleading semantic cues that conflict with the task's symbolic logic. Through experiments with four LLMs we show that semantic cues can significantly deteriorate reasoning models' performance on very simple tasks. They reveal limitations in current LLMs' ability for symbolic manipulations and highlight a tendency to over-rely on surface-level semantics, suggesting that chain-of-thoughts may amplify reliance on statistical correlations. Even in situations where LLMs seem to correctly follow instructions, semantic cues still impact basic capabilities. These limitations raise ethical and societal concerns, undermining the widespread and pernicious tendency to attribute reasoning abilities to LLMs and suggesting how LLMs might fail, in particular in decision-making contexts where robust symbolic reasoning is essential and should not be compromised by residual semantic associations inherited from the model's training.

</details>


### [74] [EssayCBM: Rubric-Aligned Concept Bottleneck Models for Transparent Essay Grading](https://arxiv.org/abs/2512.20817)
*Kumar Satvik Chaudhary,Chengshuai Zhao,Fan Zhang,Yung Hin Tse,Garima Agrawal,Yuli Deng,Huan Liu*

Main category: cs.CL

TL;DR: EssayCBM是一个基于评分标准的可解释论文评分框架，通过评估8个写作概念来替代直接预测分数，提供透明可调的评估结果。


<details>
  <summary>Details</summary>
Motivation: 解决自动评分系统作为黑盒的问题，为教师和学生提供可解释的论文评估方法。

Method: 使用专用预测头评估8个写作概念，然后通过轻量级网络将概念得分转化为最终成绩。

Result: EssayCBM在保持黑盒模型性能的同时，提供了可操作的概念级反馈。

Conclusion: 该框架通过直观的web界面实现了可解释、可调整的人类参与式评估。

Abstract: Understanding how automated grading systems evaluate essays remains a significant challenge for educators and students, especially when large language models function as black boxes. We introduce EssayCBM, a rubric-aligned framework that prioritizes interpretability in essay assessment. Instead of predicting grades directly from text, EssayCBM evaluates eight writing concepts, such as Thesis Clarity and Evidence Use, through dedicated prediction heads on an encoder. These concept scores form a transparent bottleneck, and a lightweight network computes the final grade using only concepts. Instructors can adjust concept predictions and instantly view the updated grade, enabling accountable human-in-the-loop evaluation. EssayCBM matches black-box performance while offering actionable, concept-level feedback through an intuitive web interface.

</details>


### [75] [MediEval: A Unified Medical Benchmark for Patient-Contextual and Knowledge-Grounded Reasoning in LLMs](https://arxiv.org/abs/2512.20822)
*Zhan Qu,Michael Färber*

Main category: cs.CL

TL;DR: 该论文提出了MediEval基准，通过连接电子健康记录与统一知识库生成多样化的医疗陈述，系统评估LLMs在医疗领域的可靠性，并针对发现的风险提出了CoRFu微调方法，显著提升了模型的安全性和准确性。


<details>
  <summary>Details</summary>
Motivation: LLMs在医疗领域的应用受到可靠性和安全性担忧的限制，现有评估方法要么孤立测试医疗知识，要么无法验证患者层面的推理正确性，存在关键缺陷。

Method: 引入MediEval基准，将MIMIC-IV电子健康记录与UMLS等生物医学词汇表构建的统一知识库连接，生成多样化的真实和反事实医疗陈述，通过一个4象限框架系统评估LLMs的知识基础和上下文一致性。

Result: 发现当前专有、开源和领域特定的LLMs存在关键失败模式，如幻觉支持和真相反转。提出的CoRFu方法在基础模型上提升了16.4 macro-F1点，并消除了真相反转错误。

Conclusion: CoRFu微调方法不仅显著提高了LLMs在医疗任务中的准确性，还大幅增强了其安全性，为解决LLMs在医疗应用中的可靠性问题提供了有效途径。

Abstract: Large Language Models (LLMs) are increasingly applied to medicine, yet their adoption is limited by concerns over reliability and safety. Existing evaluations either test factual medical knowledge in isolation or assess patient-level reasoning without verifying correctness, leaving a critical gap. We introduce MediEval, a benchmark that links MIMIC-IV electronic health records (EHRs) to a unified knowledge base built from UMLS and other biomedical vocabularies. MediEval generates diverse factual and counterfactual medical statements within real patient contexts, enabling systematic evaluation across a 4-quadrant framework that jointly considers knowledge grounding and contextual consistency. Using this framework, we identify critical failure modes, including hallucinated support and truth inversion, that current proprietary, open-source, and domain-specific LLMs frequently exhibit. To address these risks, we propose Counterfactual Risk-Aware Fine-tuning (CoRFu), a DPO-based method with an asymmetric penalty targeting unsafe confusions. CoRFu improves by +16.4 macro-F1 points over the base model and eliminates truth inversion errors, demonstrating both higher accuracy and substantially greater safety.

</details>


### [76] [Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning](https://arxiv.org/abs/2512.20848)
*NVIDIA,:,Aaron Blakeman,Aaron Grattafiori,Aarti Basant,Abhibha Gupta,Abhinav Khattar,Adi Renduchintala,Aditya Vavre,Akanksha Shukla,Akhiad Bercovich,Aleksander Ficek,Aleksandr Shaposhnikov,Alex Kondratenko,Alexander Bukharin,Alexandre Milesi,Ali Taghibakhshi,Alisa Liu,Amelia Barton,Ameya Sunil Mahabaleshwarkar,Amir Klein,Amit Zuker,Amnon Geifman,Amy Shen,Anahita Bhiwandiwalla,Andrew Tao,Ann Guan,Anubhav Mandarwal,Arham Mehta,Ashwath Aithal,Ashwin Poojary,Asif Ahamed,Asma Kuriparambil Thekkumpate,Ayush Dattagupta,Banghua Zhu,Bardiya Sadeghi,Barnaby Simkin,Ben Lanir,Benedikt Schifferer,Besmira Nushi,Bilal Kartal,Bita Darvish Rouhani,Boris Ginsburg,Brandon Norick,Brandon Soubasis,Branislav Kisacanin,Brian Yu,Bryan Catanzaro,Carlo del Mundo,Chantal Hwang,Charles Wang,Cheng-Ping Hsieh,Chenghao Zhang,Chenhan Yu,Chetan Mungekar,Chintan Patel,Chris Alexiuk,Christopher Parisien,Collin Neale,Damon Mosk-Aoyama,Dan Su,Dane Corneil,Daniel Afrimi,Daniel Rohrer,Daniel Serebrenik,Daria Gitman,Daria Levy,Darko Stosic,David Mosallanezhad,Deepak Narayanan,Dhruv Nathawani,Dima Rekesh,Dina Yared,Divyanshu Kakwani,Dong Ahn,Duncan Riach,Dusan Stosic,Edgar Minasyan,Edward Lin,Eileen Long,Eileen Peters Long,Elena Lantz,Ellie Evans,Elliott Ning,Eric Chung,Eric Harper,Eric Tramel,Erick Galinkin,Erik Pounds,Evan Briones,Evelina Bakhturina,Faisal Ladhak,Fay Wang,Fei Jia,Felipe Soares,Feng Chen,Ferenc Galko,Frankie Siino,Gal Hubara Agam,Ganesh Ajjanagadde,Gantavya Bhatt,Gargi Prasad,George Armstrong,Gerald Shen,Gorkem Batmaz,Grigor Nalbandyan,Haifeng Qian,Harsh Sharma,Hayley Ross,Helen Ngo,Herman Sahota,Hexin Wang,Himanshu Soni,Hiren Upadhyay,Huizi Mao,Huy C Nguyen,Huy Q Nguyen,Iain Cunningham,Ido Shahaf,Igor Gitman,Ilya Loshchilov,Ivan Moshkov,Izzy Putterman,Jan Kautz,Jane Polak Scowcroft,Jared Casper,Jatin Mitra,Jeffrey Glick,Jenny Chen,Jesse Oliver,Jian Zhang,Jiaqi Zeng,Jie Lou,Jimmy Zhang,Jining Huang,Joey Conway,Joey Guman,John Kamalu,Johnny Greco,Jonathan Cohen,Joseph Jennings,Joyjit Daw,Julien Veron Vialard,Junkeun Yi,Jupinder Parmar,Kai Xu,Kan Zhu,Kari Briski,Katherine Cheung,Katherine Luna,Keshav Santhanam,Kevin Shih,Kezhi Kong,Khushi Bhardwaj,Krishna C. Puvvada,Krzysztof Pawelec,Kumar Anik,Lawrence McAfee,Laya Sleiman,Leon Derczynski,Li Ding,Lucas Liebenwein,Luis Vega,Maanu Grover,Maarten Van Segbroeck,Maer Rodrigues de Melo,Makesh Narsimhan Sreedhar,Manoj Kilaru,Maor Ashkenazi,Marc Romeijn,Mark Cai,Markus Kliegl,Maryam Moosaei,Matvei Novikov,Mehrzad Samadi,Melissa Corpuz,Mengru Wang,Meredith Price,Michael Boone,Michael Evans,Miguel Martinez,Mike Chrzanowski,Mohammad Shoeybi,Mostofa Patwary,Nabin Mulepati,Natalie Hereth,Nave Assaf,Negar Habibi,Neta Zmora,Netanel Haber,Nicola Sessions,Nidhi Bhatia,Nikhil Jukar,Nikki Pope,Nikolai Ludwig,Nima Tajbakhsh,Nirmal Juluru,Oleksii Hrinchuk,Oleksii Kuchaiev,Olivier Delalleau,Oluwatobi Olabiyi,Omer Ullman Argov,Ouye Xie,Parth Chadha,Pasha Shamis,Pavlo Molchanov,Pawel Morkisz,Peter Dykas,Peter Jin,Pinky Xu,Piotr Januszewski,Pranav Prashant Thombre,Prasoon Varshney,Pritam Gundecha,Qing Miao,Rabeeh Karimi Mahabadi,Ran El-Yaniv,Ran Zilberstein,Rasoul Shafipour,Rich Harang,Rick Izzo,Rima Shahbazyan,Rishabh Garg,Ritika Borkar,Ritu Gala,Riyad Islam,Roger Waleffe,Rohit Watve,Roi Koren,Ruoxi Zhang,Russell J. Hewett,Ryan Prenger,Ryan Timbrook,Sadegh Mahdavi,Sahil Modi,Samuel Kriman,Sanjay Kariyappa,Sanjeev Satheesh,Saori Kaji,Satish Pasumarthi,Sean Narentharen,Sean Narenthiran,Seonmyeong Bak,Sergey Kashirsky,Seth Poulos,Shahar Mor,Shanmugam Ramasamy,Shantanu Acharya,Shaona Ghosh,Sharath Turuvekere Sreenivas,Shelby Thomas,Shiqing Fan,Shreya Gopal,Shrimai Prabhumoye,Shubham Pachori,Shubham Toshniwal,Shuoyang Ding,Siddharth Singh,Simeng Sun,Smita Ithape,Somshubra Majumdar,Soumye Singhal,Stefania Alborghetti,Stephen Ge,Sugam Dipak Devare,Sumeet Kumar Barua,Suseella Panguluri,Suyog Gupta,Sweta Priyadarshi,Syeda Nahida Akter,Tan Bui,Teodor-Dumitru Ene,Terry Kong,Thanh Do,Tijmen Blankevoort,Tom Balough,Tomer Asida,Tomer Bar Natan,Tugrul Konuk,Twinkle Vashishth,Udi Karpas,Ushnish De,Vahid Noorozi,Vahid Noroozi,Venkat Srinivasan,Venmugil Elango,Vijay Korthikanti,Vitaly Kurin,Vitaly Lavrukhin,Wanli Jiang,Wasi Uddin Ahmad,Wei Du,Wei Ping,Wenfei Zhou,Will Jennings,William Zhang,Wojciech Prazuch,Xiaowei Ren,Yashaswi Karnati,Yejin Choi,Yev Meyer,Yi-Fu Wu,Yian Zhang,Ying Lin,Yonatan Geifman,Yonggan Fu,Yoshi Subara,Yoshi Suhara,Yubo Gao,Zach Moshe,Zhen Dong,Zihan Liu,Zijia Chen,Zijie Yan*

Main category: cs.CL

TL;DR: Nemotron 3 Nano 30B-A3B 是一种混合专家模型，结合了 Mamba 和 Transformer 架构，在25万亿文本 token 上预训练，推理吞吐量比同类模型高3.3倍，支持100万 token 的上下文长度。


<details>
  <summary>Details</summary>
Motivation: 开发一个在保持高精度的同时，推理效率更高、参数激活更少的下一代语言模型，以提升大型语言模型在实际应用中的性能。

Method: 采用混合专家架构，结合 Mamba 和 Transformer，在25万亿文本 token 上进行预训练，随后进行监督微调和大规模强化学习。

Result: 相比前代模型 Nemotron 2 Nano，准确率更高，每次前向传播激活的参数不到一半；推理吞吐量比 GPT-OSS-20B 和 Qwen3-30B-A3B-Thinking-2507 等高3.3倍，且在多个基准测试中更准确。

Conclusion: Nemotron 3 Nano 在推理效率、准确性和上下文长度支持上均有显著提升，展示了更强的代理、推理和对话能力，模型已开源发布。

Abstract: We present Nemotron 3 Nano 30B-A3B, a Mixture-of-Experts hybrid Mamba-Transformer language model. Nemotron 3 Nano was pretrained on 25 trillion text tokens, including more than 3 trillion new unique tokens over Nemotron 2, followed by supervised fine tuning and large-scale RL on diverse environments. Nemotron 3 Nano achieves better accuracy than our previous generation Nemotron 2 Nano while activating less than half of the parameters per forward pass. It achieves up to 3.3x higher inference throughput than similarly-sized open models like GPT-OSS-20B and Qwen3-30B-A3B-Thinking-2507, while also being more accurate on popular benchmarks. Nemotron 3 Nano demonstrates enhanced agentic, reasoning, and chat abilities and supports context lengths up to 1M tokens. We release both our pretrained Nemotron 3 Nano 30B-A3B Base and post-trained Nemotron 3 Nano 30B-A3B checkpoints on Hugging Face.

</details>


### [77] [How important is Recall for Measuring Retrieval Quality?](https://arxiv.org/abs/2512.20854)
*Shelly Schwartz,Oleg Vasilyev,Randy Sawaya*

Main category: cs.CL

TL;DR: 论文提出了一种不需要知道相关文档总数的新检索质量评估方法，通过将检索质量指标与基于LLM的响应质量判断进行关联分析，解决了大规模动态知识库中无法计算召回率的问题。


<details>
  <summary>Details</summary>
Motivation: 在现实检索场景中，大型动态知识库的相关文档总数通常未知且难以计算召回率，这种限制严重影响了检索系统的有效评估。

Method: 通过测量检索质量指标与基于LLM生成的响应质量判断之间的相关性，评估多种现有策略，并在相关文档数量较少（2-15个）的多个数据集上进行实验。

Result: 实验结果验证了所提出的简单检索质量评估方法的有效性，该方法在不依赖相关文档总数的情况下表现出色。

Conclusion: 研究表明LLM生成的响应质量判断可以为检索系统评估提供可靠依据，所提出的方法为无法计算召回率的大规模知识库检索评估提供了实用解决方案。

Abstract: In realistic retrieval settings with large and evolving knowledge bases, the total number of documents relevant to a query is typically unknown, and recall cannot be computed. In this paper, we evaluate several established strategies for handling this limitation by measuring the correlation between retrieval quality metrics and LLM-based judgments of response quality, where responses are generated from the retrieved documents. We conduct experiments across multiple datasets with a relatively low number of relevant documents (2-15). We also introduce a simple retrieval quality measure that performs well without requiring knowledge of the total number of relevant documents.

</details>


### [78] [NVIDIA Nemotron 3: Efficient and Open Intelligence](https://arxiv.org/abs/2512.20856)
*NVIDIA,:,Aaron Blakeman,Aaron Grattafiori,Aarti Basant,Abhibha Gupta,Abhinav Khattar,Adi Renduchintala,Aditya Vavre,Akanksha Shukla,Akhiad Bercovich,Aleksander Ficek,Aleksandr Shaposhnikov,Alex Kondratenko,Alexander Bukharin,Alexandre Milesi,Ali Taghibakhshi,Alisa Liu,Amelia Barton,Ameya Sunil Mahabaleshwarkar,Amir Klein,Amit Zuker,Amnon Geifman,Amy Shen,Anahita Bhiwandiwalla,Andrew Tao,Anjulie Agrusa,Ankur Verma,Ann Guan,Anubhav Mandarwal,Arham Mehta,Ashwath Aithal,Ashwin Poojary,Asif Ahamed,Asit Mishra,Asma Kuriparambil Thekkumpate,Ayush Dattagupta,Banghua Zhu,Bardiya Sadeghi,Barnaby Simkin,Ben Lanir,Benedikt Schifferer,Besmira Nushi,Bilal Kartal,Bita Darvish Rouhani,Boris Ginsburg,Brandon Norick,Brandon Soubasis,Branislav Kisacanin,Brian Yu,Bryan Catanzaro,Carlo del Mundo,Chantal Hwang,Charles Wang,Cheng-Ping Hsieh,Chenghao Zhang,Chenhan Yu,Chetan Mungekar,Chintan Patel,Chris Alexiuk,Christopher Parisien,Collin Neale,Cyril Meurillon,Damon Mosk-Aoyama,Dan Su,Dane Corneil,Daniel Afrimi,Daniel Lo,Daniel Rohrer,Daniel Serebrenik,Daria Gitman,Daria Levy,Darko Stosic,David Mosallanezhad,Deepak Narayanan,Dhruv Nathawani,Dima Rekesh,Dina Yared,Divyanshu Kakwani,Dong Ahn,Duncan Riach,Dusan Stosic,Edgar Minasyan,Edward Lin,Eileen Long,Eileen Peters Long,Elad Segal,Elena Lantz,Ellie Evans,Elliott Ning,Eric Chung,Eric Harper,Eric Tramel,Erick Galinkin,Erik Pounds,Evan Briones,Evelina Bakhturina,Evgeny Tsykunov,Faisal Ladhak,Fay Wang,Fei Jia,Felipe Soares,Feng Chen,Ferenc Galko,Frank Sun,Frankie Siino,Gal Hubara Agam,Ganesh Ajjanagadde,Gantavya Bhatt,Gargi Prasad,George Armstrong,Gerald Shen,Gorkem Batmaz,Grigor Nalbandyan,Haifeng Qian,Harsh Sharma,Hayley Ross,Helen Ngo,Herbert Hum,Herman Sahota,Hexin Wang,Himanshu Soni,Hiren Upadhyay,Huizi Mao,Huy C Nguyen,Huy Q Nguyen,Iain Cunningham,Ido Galil,Ido Shahaf,Igor Gitman,Ilya Loshchilov,Itamar Schen,Itay Levy,Ivan Moshkov,Izik Golan,Izzy Putterman,Jan Kautz,Jane Polak Scowcroft,Jared Casper,Jatin Mitra,Jeffrey Glick,Jenny Chen,Jesse Oliver,Jian Zhang,Jiaqi Zeng,Jie Lou,Jimmy Zhang,Jinhang Choi,Jining Huang,Joey Conway,Joey Guman,John Kamalu,Johnny Greco,Jonathan Cohen,Joseph Jennings,Joyjit Daw,Julien Veron Vialard,Junkeun Yi,Jupinder Parmar,Kai Xu,Kan Zhu,Kari Briski,Katherine Cheung,Katherine Luna,Keith Wyss,Keshav Santhanam,Kevin Shih,Kezhi Kong,Khushi Bhardwaj,Kirthi Shankar,Krishna C. Puvvada,Krzysztof Pawelec,Kumar Anik,Lawrence McAfee,Laya Sleiman,Leon Derczynski,Li Ding,Lizzie Wei,Lucas Liebenwein,Luis Vega,Maanu Grover,Maarten Van Segbroeck,Maer Rodrigues de Melo,Mahdi Nazemi,Makesh Narsimhan Sreedhar,Manoj Kilaru,Maor Ashkenazi,Marc Romeijn,Marcin Chochowski,Mark Cai,Markus Kliegl,Maryam Moosaei,Matt Kulka,Matvei Novikov,Mehrzad Samadi,Melissa Corpuz,Mengru Wang,Meredith Price,Michael Andersch,Michael Boone,Michael Evans,Miguel Martinez,Mikail Khona,Mike Chrzanowski,Minseok Lee,Mohammad Dabbah,Mohammad Shoeybi,Mostofa Patwary,Nabin Mulepati,Najeeb Nabwani,Natalie Hereth,Nave Assaf,Negar Habibi,Neta Zmora,Netanel Haber,Nicola Sessions,Nidhi Bhatia,Nikhil Jukar,Nikki Pope,Nikolai Ludwig,Nima Tajbakhsh,Nir Ailon,Nirmal Juluru,Nishant Sharma,Oleksii Hrinchuk,Oleksii Kuchaiev,Olivier Delalleau,Oluwatobi Olabiyi,Omer Ullman Argov,Omri Puny,Oren Tropp,Ouye Xie,Parth Chadha,Pasha Shamis,Paul Gibbons,Pavlo Molchanov,Pawel Morkisz,Peter Dykas,Peter Jin,Pinky Xu,Piotr Januszewski,Pranav Prashant Thombre,Prasoon Varshney,Pritam Gundecha,Przemek Tredak,Qing Miao,Qiyu Wan,Rabeeh Karimi Mahabadi,Rachit Garg,Ran El-Yaniv,Ran Zilberstein,Rasoul Shafipour,Rich Harang,Rick Izzo,Rima Shahbazyan,Rishabh Garg,Ritika Borkar,Ritu Gala,Riyad Islam,Robert Hesse,Roger Waleffe,Rohit Watve,Roi Koren,Ruoxi Zhang,Russell Hewett,Russell J. Hewett,Ryan Prenger,Ryan Timbrook,Sadegh Mahdavi,Sahil Modi,Samuel Kriman,Sangkug Lim,Sanjay Kariyappa,Sanjeev Satheesh,Saori Kaji,Satish Pasumarthi,Saurav Muralidharan,Sean Narentharen,Sean Narenthiran,Seonmyeong Bak,Sergey Kashirsky,Seth Poulos,Shahar Mor,Shanmugam Ramasamy,Shantanu Acharya,Shaona Ghosh,Sharath Turuvekere Sreenivas,Shelby Thomas,Shiqing Fan,Shreya Gopal,Shrimai Prabhumoye,Shubham Pachori,Shubham Toshniwal,Shuoyang Ding,Siddharth Singh,Simeng Sun,Smita Ithape,Somshubra Majumdar,Soumye Singhal,Stas Sergienko,Stefania Alborghetti,Stephen Ge,Sugam Dipak Devare,Sumeet Kumar Barua,Suseella Panguluri,Suyog Gupta,Sweta Priyadarshi,Syeda Nahida Akter,Tan Bui,Teodor-Dumitru Ene,Terry Kong,Thanh Do,Tijmen Blankevoort,Tim Moon,Tom Balough,Tomer Asida,Tomer Bar Natan,Tomer Ronen,Tugrul Konuk,Twinkle Vashishth,Udi Karpas,Ushnish De,Vahid Noorozi,Vahid Noroozi,Venkat Srinivasan,Venmugil Elango,Victor Cui,Vijay Korthikanti,Vinay Rao,Vitaly Kurin,Vitaly Lavrukhin,Vladimir Anisimov,Wanli Jiang,Wasi Uddin Ahmad,Wei Du,Wei Ping,Wenfei Zhou,Will Jennings,William Zhang,Wojciech Prazuch,Xiaowei Ren,Yashaswi Karnati,Yejin Choi,Yev Meyer,Yi-Fu Wu,Yian Zhang,Yigong Qin,Ying Lin,Yonatan Geifman,Yonggan Fu,Yoshi Subara,Yoshi Suhara,Yubo Gao,Zach Moshe,Zhen Dong,Zhongbo Zhu,Zihan Liu,Zijia Chen,Zijie Yan*

Main category: cs.CL

TL;DR: Nemotron 3模型家族包含Nano、Super和Ultra三个版本，采用混合Mamba-Transformer架构，支持最长100万token的上下文，具备强大的代理、推理和对话能力。


<details>
  <summary>Details</summary>
Motivation: 开发一个具有最佳吞吐量和长上下文处理能力的高效模型家族，满足不同规模和需求的应用场景，从成本敏感的推理任务到需要最强准确性和推理性能的复杂工作负载。

Method: 采用Mixture-of-Experts混合Mamba-Transformer架构，较大模型使用NVFP4训练并整合LatentMoE技术提升质量，包含MTP层加速文本生成，所有模型通过多环境强化学习后训练。

Result: Nano模型在保持极高推理成本效率的同时超越同类模型精度；Super针对协作代理和高容量工作负载优化；Ultra提供最先进的准确性和推理性能。

Conclusion: Nemotron 3模型家族提供了从高效轻量到顶尖性能的完整解决方案，并将开源模型权重、训练软件和数据集，推动AI技术的普及和应用。

Abstract: We introduce the Nemotron 3 family of models - Nano, Super, and Ultra. These models deliver strong agentic, reasoning, and conversational capabilities. The Nemotron 3 family uses a Mixture-of-Experts hybrid Mamba-Transformer architecture to provide best-in-class throughput and context lengths of up to 1M tokens. Super and Ultra models are trained with NVFP4 and incorporate LatentMoE, a novel approach that improves model quality. The two larger models also include MTP layers for faster text generation. All Nemotron 3 models are post-trained using multi-environment reinforcement learning enabling reasoning, multi-step tool use, and support granular reasoning budget control. Nano, the smallest model, outperforms comparable models in accuracy while remaining extremely cost-efficient for inference. Super is optimized for collaborative agents and high-volume workloads such as IT ticket automation. Ultra, the largest model, provides state-of-the-art accuracy and reasoning performance. Nano is released together with its technical report and this white paper, while Super and Ultra will follow in the coming months. We will openly release the model weights, pre- and post-training software, recipes, and all data for which we hold redistribution rights.

</details>


### [79] [Architectural Trade-offs in Small Language Models Under Compute Constraints](https://arxiv.org/abs/2512.20877)
*Shivraj Singh Bhatti*

Main category: cs.CL

TL;DR: 对小型语言模型在严格算力约束下的系统实证研究，分析架构选择与训练预算如何共同决定性能，比较不同架构在不同数据集上的效率-准确率权衡。


<details>
  <summary>Details</summary>
Motivation: 研究小型语言模型在有限算力下的性能表现，探讨大型语言模型的成功技术是否适用于小模型场景，为资源受限环境提供设计指导。

Method: 从线性next-token预测器开始，逐步引入非线性、自注意力和多层Transformer架构，在Tiny Shakespeare、PTB和WikiText-2数据集上进行字符级和词级建模评估，使用测试负对数似然、参数量和训练FLOPs作为评估指标。

Result: 基于注意力的模型即使在小规模下也优于MLP的每FLOP效率；增加深度或上下文长度而优化不足会降低性能；RoPE等大型模型成功技术不一定适用于小模型。

Conclusion: 小模型需要专门优化的架构设计，不能简单移植大模型技术；注意力机制在小规模下仍具优势；模型设计需平衡架构复杂度和训练优化程度。

Abstract: We present a systematic empirical study of small language models under strict compute constraints, analyzing how architectural choices and training budget interact to determine performance. Starting from a linear next-token predictor, we progressively introduce nonlinearities, self-attention, and multi-layer transformer architectures, evaluating each on character-level modeling of Tiny Shakespeare and word-level modeling of Penn Treebank (PTB) and WikiText-2. We compare models using test negative log-likelihood (NLL), parameter count, and approximate training FLOPs to characterize accuracy-efficiency trade-offs. Our results show that attention-based models dominate MLPs in per-FLOP efficiency even at small scale, while increasing depth or context without sufficient optimization can degrade performance. We further examine rotary positional embeddings (RoPE), finding that architectural techniques successful in large language models do not necessarily transfer to small-model regimes.

</details>


### [80] [Where Did This Sentence Come From? Tracing Provenance in LLM Reasoning Distillation](https://arxiv.org/abs/2512.20908)
*Kaiyuan Liu,Shaotian Yan,Rui Miao,Bing Wang,Chen Shen,Jun Zhang,Jieping Ye*

Main category: cs.CL

TL;DR: 本文提出了跨模型推理蒸馏溯源框架RQPT，通过追踪学生模型输出中教师模型、原始学生模型和蒸馏模型各自的贡献比例，分析了蒸馏模型泛化能力的来源。实验证明蒸馏模型在测试时确实能产生教师起源的行为，并基于分析提出了教师指导的数据选择方法。


<details>
  <summary>Details</summary>
Motivation: 现有推理蒸馏方法缺乏对学生模型能力来源的深入分析，不清楚学生在测试时是否保持与教师一致的行为还是回归原始输出模式，这引发了蒸馏模型泛化能力的担忧。

Method: 1. 提出RQPT框架，通过比较教师、原始学生和蒸馏模型在同一上下文下的预测概率，对每个输出动作进行溯源分类
2. 在此基础上提出教师指导的数据选择方法，直接在训练数据上比较教师-学生差异

Result: 实验证明蒸馏模型在测试时能生成教师起源的动作，这些动作与观察到的性能相关并可能解释性能表现。该方法在多种代表性教师模型和多样化学生模型上验证有效。

Conclusion: RQPT框架有效揭示了推理蒸馏中行为转移的机制，提出的数据选择方法为蒸馏提供了更原理性的选择标准，为推理蒸馏领域提供了新的分析工具和洞察。

Abstract: Reasoning distillation has attracted increasing attention. It typically leverages a large teacher model to generate reasoning paths, which are then used to fine-tune a student model so that it mimics the teacher's behavior in training contexts. However, previous approaches have lacked a detailed analysis of the origins of the distilled model's capabilities. It remains unclear whether the student can maintain consistent behaviors with the teacher in novel test-time contexts, or whether it regresses to its original output patterns, raising concerns about the generalization of distillation models. To analyse this question, we introduce a cross-model Reasoning Distillation Provenance Tracing framework. For each action (e.g., a sentence) produced by the distilled model, we obtain the predictive probabilities assigned by the teacher, the original student, and the distilled model under the same context. By comparing these probabilities, we classify each action into different categories. By systematically disentangling the provenance of each action, we experimentally demonstrate that, in test-time contexts, the distilled model can indeed generate teacher-originated actions, which correlate with and plausibly explain observed performance on distilled model. Building on this analysis, we further propose a teacher-guided data selection method. Unlike prior approach that rely on heuristics, our method directly compares teacher-student divergences on the training data, providing a principled selection criterion. We validate the effectiveness of our approach across multiple representative teacher models and diverse student models. The results highlight the utility of our provenance-tracing framework and underscore its promise for reasoning distillation. We hope to share Reasoning Distillation Provenance Tracing and our insights into reasoning distillation with the community.

</details>


### [81] [Neural Probe-Based Hallucination Detection for Large Language Models](https://arxiv.org/abs/2512.20949)
*Shize Liang,Hongzhi Wang*

Main category: cs.CL

TL;DR: 提出了基于神经网络框架的token级幻觉检测方法，通过轻量级MLP探针和贝叶斯优化实现高效准确检测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs生成幻觉内容的问题，现有方法存在高置信度错误和依赖外部知识检索的局限性。

Method: 冻结语言模型参数，使用轻量级MLP探针对高层隐藏状态进行非线性建模，设计多目标联合损失函数，并通过贝叶斯优化自动搜索最优探针插入层。

Result: 在LongFact、HealthBench和TriviaQA上的实验表明，MLP探针在准确率、召回率和低误报条件下的检测能力显著优于现有最先进方法。

Conclusion: 该方法在保持实时轻量优势的同时，有效提升了幻觉检测的准确性和稳定性，为高风险领域的应用提供了可靠解决方案。

Abstract: Large language models(LLMs) excel at text generation and knowledge question-answering tasks, but they are prone to generating hallucinated content, severely limiting their application in high-risk domains. Current hallucination detection methods based on uncertainty estimation and external knowledge retrieval suffer from the limitation that they still produce erroneous content at high confidence levels and rely heavily on retrieval efficiency and knowledge coverage. In contrast, probe methods that leverage the model's hidden-layer states offer real-time and lightweight advantages. However, traditional linear probes struggle to capture nonlinear structures in deep semantic spaces.To overcome these limitations, we propose a neural network-based framework for token-level hallucination detection. By freezing language model parameters, we employ lightweight MLP probes to perform nonlinear modeling of high-level hidden states. A multi-objective joint loss function is designed to enhance detection stability and semantic disambiguity. Additionally, we establish a layer position-probe performance response model, using Bayesian optimization to automatically search for optimal probe insertion layers and achieve superior training results.Experimental results on LongFact, HealthBench, and TriviaQA demonstrate that MLP probes significantly outperform state-of-the-art methods in accuracy, recall, and detection capability under low false-positive conditions.

</details>


### [82] [MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment](https://arxiv.org/abs/2512.20950)
*Mohammad Mahdi Abootorabi,Alireza Ghahramani Kure,Mohammadali Mohammadkhani,Sina Elahimanesh,Mohammad Ali Ali Panah*

Main category: cs.CL

TL;DR: 提出TriAligner系统，采用双编码器架构和对比学习，结合多语言多模态数据进行跨语言事实核查声明检索，在SemEval-2025任务7中表现优异。


<details>
  <summary>Details</summary>
Motivation: 在 misinformation 快速传播的时代，有效的多语言事实核查对遏制虚假信息至关重要。需要开发能够跨语言检索已核实声明的系统。

Method: 使用双编码器架构，结合对比学习，利用原生语言和英语翻译数据，通过hard negative采样增强表示学习，并进行高效的数据预处理和LLM数据增强。

Result: 在单语和跨语言基准测试中，检索准确率和事实核查性能显著优于基线方法。

Conclusion: TriAligner通过有效的多语言对齐学习和数据增强策略，为跨语言事实核查提供了实用的解决方案，在应对多语言misinformation挑战方面展现出良好潜力。

Abstract: This paper presents our system for SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval. In an era where misinformation spreads rapidly, effective fact-checking is increasingly critical. We introduce TriAligner, a novel approach that leverages a dual-encoder architecture with contrastive learning and incorporates both native and English translations across different modalities. Our method effectively retrieves claims across multiple languages by learning the relative importance of different sources in alignment. To enhance robustness, we employ efficient data preprocessing and augmentation using large language models while incorporating hard negative sampling to improve representation learning. We evaluate our approach on monolingual and crosslingual benchmarks, demonstrating significant improvements in retrieval accuracy and fact-checking performance over baselines.

</details>


### [83] [Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models](https://arxiv.org/abs/2512.20954)
*Xiang Zhang,Jiaqi Wei,Yuejin Yang,Zijie Qiu,Yuhan Chen,Zhiqiang Gao,Muhammad Abdul-Mageed,Laks V. S. Lakshmanan,Wanli Ouyang,Chenyu You,Siqi Sun*

Main category: cs.CL

TL;DR: 该研究提出了语言表达力的概念，指出蛋白质语言模型的表达力有限阻碍了思维链推理的应用。通过引入反射预训练和辅助思维token，增强了生物序列模型的表达能力。


<details>
  <summary>Details</summary>
Motivation: 解决蛋白质语言模型由于氨基酸token表达力有限而无法应用思维链推理的问题，提升生物序列模型的推理能力。

Method: 提出反射预训练方法，在生物序列模型中首次引入辅助思维token，扩展token集合以增强语言表达力。

Result: 理论证明增强token集能显著提高生物语言表达力；实验表明该方法使蛋白质模型具备自我纠错能力，性能大幅提升。

Conclusion: 反射预训练通过增加思维token有效克服了蛋白质语言表达力限制，成功实现了思维链式推理在生物序列模型中的应用。

Abstract: Chain-of-Thought (CoT) prompting has significantly advanced task-solving capabilities in natural language processing with large language models. Unlike standard prompting, CoT encourages the model to generate intermediate reasoning steps, non-answer tokens, that help guide the model toward more accurate final outputs. These intermediate steps enable more complex reasoning processes such as error correction, memory management, future planning, and self-reflection. However, applying CoT to non-natural language domains, such as protein and RNA language models, is not yet possible, primarily due to the limited expressiveness of their token spaces (e.g., amino acid tokens). In this work, we propose and define the concept of language expressiveness: the ability of a given language, using its tokens and grammar, to encode information. We show that the limited expressiveness of protein language severely restricts the applicability of CoT-style reasoning. To overcome this, we introduce reflection pretraining, for the first time in a biological sequence model, which enables the model to engage in intermediate reasoning through the generation of auxiliary "thinking tokens" beyond simple answer tokens. Theoretically, we demonstrate that our augmented token set significantly enhances biological language expressiveness, thereby improving the overall reasoning capacity of the model. Experimentally, our pretraining approach teaches protein models to self-correct and leads to substantial performance gains compared to standard pretraining.

</details>


### [84] [Automatic Replication of LLM Mistakes in Medical Conversations](https://arxiv.org/abs/2512.20983)
*Oleksii Proniakin,Diego Fajardo,Ruslan Nazarenko,Razvan Marinescu*

Main category: cs.CL

TL;DR: MedMistake是一个自动化的基准生成管道，用于提取LLM在医患对话中犯的错误，并转换为单样本QA对，帮助评估临床LLM的缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前在临床环境中评估大语言模型时，需要复现特定错误通常需要人工努力，无法简单地跨模型迁移。因此需要自动化方法来标准化错误评估。

Method: 通过三个步骤：(1)创建LLM患者与LLM医生间的复杂对话数据，(2)使用2个LLM评委组成的委员会进行多维度评估，(3)将错误转换为简化单样本QA场景。

Result: 生成了包含3390个QA对的MedMistake-All数据集，其中医生验证了211个问题的子集(MedMistake-Bench)。评估发现GPT、Claude和Grok模型在该基准上表现最佳。

Conclusion: MedMistake提供了一个自动化的错误提取和基准生成方法，有助于系统性评估和比较不同LLM在临床环境中的表现，推动了临床AI的安全性和可靠性评估。

Abstract: Large language models (LLMs) are increasingly evaluated in clinical settings using multi-dimensional rubrics which quantify reasoning quality, safety, and patient-centeredness. Yet, replicating specific mistakes in other LLM models is not straightforward and often requires manual effort. We introduce MedMistake, an automatic pipeline that extracts mistakes LLMs make in patient-doctor conversations and converts them into a benchmark of single-shot QA pairs. Our pipeline (1) creates complex, conversational data between an LLM patient and LLM doctor, (2) runs an evaluation with a committee of 2 LLM judges across a variety of dimensions and (3) creates simplified single-shot QA scenarios from those mistakes. We release MedMistake-All, a dataset of 3,390 single-shot QA pairs where GPT-5 and Gemini 2.5 Pro are currently failing to answer correctly, as judged by two LLM judges. We used medical experts to validate a subset of 211/3390 questions (MedMistake-Bench), which we used to run a final evaluation of 12 frontier LLMs: Claude Opus 4.5, Claude Sonnet 4.5, DeepSeek-Chat, Gemini 2.5 Pro, Gemini 3 Pro, GPT-4o, GPT-5, GPT-5.1, GPT-5.2, Grok 4, Grok 4.1, Mistral Large. We found that GPT models, Claude and Grok obtained the best performance on MedMistake-Bench. We release both the doctor-validated benchmark (MedMistake-Bench), as well as the full dataset (MedMistake-All) at https://huggingface.co/datasets/TheLumos/MedicalMistakeBenchmark.

</details>


### [85] [Distilling the Essence: Efficient Reasoning Distillation via Sequence Truncation](https://arxiv.org/abs/2512.21002)
*Wei-Rui Chen,Vignesh Kothapalli,Ata Fatahibaarzi,Hejian Sang,Shao Tang,Qingquan Song,Zhipeng Wang,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 通过仅对CoT关键推理内容进行蒸馏而非完整训练序列，可以保持约94%性能的同时减少50%训练成本


<details>
  <summary>Details</summary>
Motivation: 传统的从大型语言模型向小型学生模型蒸馏推理能力需要处理包含提示、思维链和答案的完整序列，计算成本高昂

Method: 提出选择性知识蒸馏方法，仅在思维链(CoT)关键token上进行训练，并建立截断协议分析序列长度与计算质量间的权衡关系

Result: 仅训练每个序列前50%的token可以平均保留约94%的性能，同时减少约50%的训练时间、内存使用和FLOPs

Conclusion: 推理蒸馏受益于优先处理早期推理token，这为计算质量权衡提供了一个简单有效的方法

Abstract: Distilling the reasoning capabilities from a large language model (LLM) to a smaller student model often involves training on substantial amounts of reasoning data. However, distillation over lengthy sequences with prompt (P), chain-of-thought (CoT), and answer (A) segments makes the process computationally expensive. In this work, we investigate how the allocation of supervision across different segments (P, CoT, A) affects student performance. Our analysis shows that selective knowledge distillation over only the CoT tokens can be effective when the prompt and answer information is encompassed by it. Building on this insight, we establish a truncation protocol to quantify computation-quality tradeoffs as a function of sequence length. We observe that training on only the first $50\%$ of tokens of every training sequence can retain, on average, $\approx94\%$ of full-sequence performance on math benchmarks while reducing training time, memory usage, and FLOPs by about $50\%$ each. These findings suggest that reasoning distillation benefits from prioritizing early reasoning tokens and provides a simple lever for computation-quality tradeoffs. Codes are available at https://github.com/weiruichen01/distilling-the-essence.

</details>


### [86] [Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy](https://arxiv.org/abs/2512.21017)
*Xiaofeng Shi,Qian Kou,Yuduo Li,Hua Zhou*

Main category: cs.CL

TL;DR: SFTKey是一个两阶段LLM微调方法，通过分离训练思维链和关键答案部分，解决传统SFT中模型过度关注长序列而忽视短关键答案的问题，实验显示平均准确率提升超过5%。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调中，LLM会过度关注长的思维链序列，而忽视虽短但关键的最后答案部分，这直接影响任务成功率和评估质量。

Method: 提出SFTKey两阶段训练方案：第一阶段进行传统SFT确保正确输出格式，第二阶段仅对关键答案部分进行微调以提高准确性。

Result: 在多个基准测试和模型系列上的广泛实验表明，SFTKey相比传统SFT平均准确率提升超过5%，同时保持生成正确格式的能力。

Conclusion: 本研究通过显式平衡思维链学习和答案相关token的额外优化，推进了LLM微调技术的发展。

Abstract: With the rapid advancement of Large Language Models (LLMs), the Chain-of-Thought (CoT) component has become significant for complex reasoning tasks. However, in conventional Supervised Fine-Tuning (SFT), the model could allocate disproportionately more attention to CoT sequences with excessive length. This reduces focus on the much shorter but essential Key portion-the final answer, whose correctness directly determines task success and evaluation quality. To address this limitation, we propose SFTKey, a two-stage training scheme. In the first stage, conventional SFT is applied to ensure proper output format, while in the second stage, only the Key portion is fine-tuned to improve accuracy. Extensive experiments across multiple benchmarks and model families demonstrate that SFTKey achieves an average accuracy improvement exceeding 5\% over conventional SFT, while preserving the ability to generate correct formats. Overall, this study advances LLM fine-tuning by explicitly balancing CoT learning with additional optimization on answer-relevant tokens.

</details>


### [87] [Semantic Refinement with LLMs for Graph Representations](https://arxiv.org/abs/2512.21106)
*Safal Thapaliya,Zehong Wang,Jiazheng Li,Ziming Li,Yanfang Ye,Chuxu Zhang*

Main category: cs.CL

TL;DR: 提出数据自适应语义优化框架DAS，通过GNN和LLM的闭环反馈循环，根据结构-语义异质性动态调整节点语义表示


<details>
  <summary>Details</summary>
Motivation: 现实世界图数据存在结构-语义异质性，固定归纳偏置的图学习模型难以通用。现有方法从模型侧增加归纳偏置存在局限性，需要从数据侧自适应调整语义表示

Method: DAS框架将固定GNN与LLM耦合在闭环反馈循环中：GNN提供监督信号指导LLM语义优化，优化后的语义反馈更新图学习器

Result: 在结构主导的图上表现持续提升，在语义丰富的图上保持竞争力，验证了数据中心语义自适应的有效性

Conclusion: 通过数据自适应语义优化能有效应对图数据的结构-语义异质性，为图表示学习提供了新视角

Abstract: Graph-structured data exhibit substantial heterogeneity in where their predictive signals originate: in some domains, node-level semantics dominate, while in others, structural patterns play a central role. This structure-semantics heterogeneity implies that no graph learning model with a fixed inductive bias can generalize optimally across diverse graph domains. However, most existing methods address this challenge from the model side by incrementally injecting new inductive biases, which remains fundamentally limited given the open-ended diversity of real-world graphs. In this work, we take a data-centric perspective and treat node semantics as a task-adaptive variable. We propose a Data-Adaptive Semantic Refinement framework DAS for graph representation learning, which couples a fixed graph neural network (GNN) and a large language model (LLM) in a closed feedback loop. The GNN provides implicit supervisory signals to guide the semantic refinement of LLM, and the refined semantics are fed back to update the same graph learner. We evaluate our approach on both text-rich and text-free graphs. Results show consistent improvements on structure-dominated graphs while remaining competitive on semantics-rich graphs, demonstrating the effectiveness of data-centric semantic adaptation under structure-semantics heterogeneity.

</details>


### [88] [Semi-Supervised Learning for Large Language Models Safety and Content Moderation](https://arxiv.org/abs/2512.21107)
*Eduard Stefan Dinuta,Iustin Sirbu,Traian Rebedea*

Main category: cs.CL

TL;DR: 摘要讨论了大型语言模型（LLMs）安全性问题，并提出基于半监督学习的方法来提高安全分类性能。该方法利用标记和未标记数据，并强调使用领域特定增强技术可显著提升效果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全训练依赖大量标记数据，这可能导致数据采集困难、标记错误或合成数据问题。因此需探索新的方案以便更高效地改进安全任务。

Method: 采用半监督学习方法，通过结合标记和未标记的数据来提升安全分类器性能。特别强调利用任务特定的数据增强技术，而非通用的增强手段，以优化效果。

Result: 半监督学习在LLM提示及其响应安全检测方面表现出较优的性能提升。任务特定增强技术相比通用方法显著提升了安全分类效果。

Conclusion: 研究表明半监督学习及任务特定增强技术在LLM安全领域具有广阔前景，为解决标记数据不足及质量问题提供了有效途径。

Abstract: Safety for Large Language Models (LLMs) has been an ongoing research focus since their emergence and is even more relevant nowadays with the increasing capacity of those models. Currently, there are several guardrails in place for all public LLMs and multiple proposed datasets for training safety classifiers. However, training these safety classifiers relies on large quantities of labeled data, which can be problematic to acquire, prone to labeling errors, or often include synthetic data. To address these issues, we suggest a different approach: utilizing semi-supervised learning techniques, which leverage both labeled and unlabeled data, to improve the performance on the safety task. We analyze the improvements that these techniques can offer for both prompts given to Large Language Models and the responses to those requests. Moreover, since augmentation is the central part of semi-supervised algorithms, we demonstrate the importance of using task-specific augmentations, which significantly increase the performance when compared to general-purpose augmentation techniques.

</details>


### [89] [ClarifyMT-Bench: Benchmarking and Improving Multi-Turn Clarification for Conversational Large Language Models](https://arxiv.org/abs/2512.21120)
*Sichun Luo,Yi Huang,Mukai Li,Shichang Meng,Fengyuan Liu,Zefa Hu,Junlan Feng,Qi Liu*

Main category: cs.CL

TL;DR: 这篇论文介绍了ClarifyMT-Bench基准测试，用于评估LLM在多轮对话中的澄清能力，发现LLM存在澄清不足的偏见，并提出ClarifyAgent方法来改善这一表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型澄清基准主要假设单轮交互或合作用户，无法评估真实场景下的澄清行为。

Method: 通过混合LLM-人工管道构建6,120个多轮对话，涉及五种模糊分类和六种用户角色，并提出了ClarifyAgent方法进行澄清分解。

Result: 评估显示LLM存在一致的澄清不足偏见，倾向于过早回答，且随着对话深度的增加性能下降。ClarifyAgent能显著改善模糊条件下的鲁棒性。

Conclusion: ClarifyMT-Bench为研究LLM在真实人机交互中何时应该提问、何时应该回答建立了可复现的基础。

Abstract: Large language models (LLMs) are increasingly deployed as conversational assistants in open-domain, multi-turn settings, where users often provide incomplete or ambiguous information. However, existing LLM-focused clarification benchmarks primarily assume single-turn interactions or cooperative users, limiting their ability to evaluate clarification behavior in realistic settings. We introduce \textbf{ClarifyMT-Bench}, a benchmark for multi-turn clarification grounded in a five-dimensional ambiguity taxonomy and a set of six behaviorally diverse simulated user personas. Through a hybrid LLM-human pipeline, we construct 6,120 multi-turn dialogues capturing diverse ambiguity sources and interaction patterns. Evaluating ten representative LLMs uncovers a consistent under-clarification bias: LLMs tend to answer prematurely, and performance degrades as dialogue depth increases. To mitigate this, we propose \textbf{ClarifyAgent}, an agentic approach that decomposes clarification into perception, forecasting, tracking, and planning, substantially improving robustness across ambiguity conditions. ClarifyMT-Bench establishes a reproducible foundation for studying when LLMs should ask, when they should answer, and how to navigate ambiguity in real-world human-LLM interactions.

</details>


### [90] [SpidR-Adapt: A Universal Speech Representation Model for Few-Shot Adaptation](https://arxiv.org/abs/2512.21204)
*Mahi Luthra,Jiayi Shen,Maxime Poli,Angelo Ortiz,Yosuke Higuchi,Youssef Benchekroun,Martin Gleize,Charles-Eric Saint-James,Dongyan Lin,Phillip Rust,Angel Villar,Surya Parimi,Vanessa Stark,Rashel Moritz,Juan Pino,Yann LeCun,Emmanuel Dupoux*

Main category: cs.CL

TL;DR: 本文提出了SpidR-Adapt方法，通过元学习和多任务自适应预训练协议，使用少于1小时的目标语言音频就能显著提高语音表示的学习效率，数据效率比标准训练高出100倍以上。


<details>
  <summary>Details</summary>
Motivation: 婴儿仅通过数百小时的语音接触就能掌握新语言的基本单元，这凸显了与数据依赖的自监督语音模型之间的效率差距，旨在开发快速适应新语言的方法。

Method: 采用元学习框架，构建多任务自适应预训练协议，将适应过程建模为双级优化问题，并提出一阶双级优化算法以减少计算成本，同时通过交替自监督和监督目标实现稳健初始化。

Result: SpidR-Adapt在音素可区分性和口语语言建模方面取得快速提升，使用少于1小时的目标语言音频即可超越领域内语言模型，数据效率提高100倍以上。

Conclusion: 研究为开发生物启发的数据高效表示提供了一条实用且架构无关的路径，相关代码和模型已开源。

Abstract: Human infants, with only a few hundred hours of speech exposure, acquire basic units of new languages, highlighting a striking efficiency gap compared to the data-hungry self-supervised speech models. To address this gap, this paper introduces SpidR-Adapt for rapid adaptation to new languages using minimal unlabeled data. We cast such low-resource speech representation learning as a meta-learning problem and construct a multi-task adaptive pre-training (MAdaPT) protocol which formulates the adaptation process as a bi-level optimization framework. To enable scalable meta-training under this framework, we propose a novel heuristic solution, first-order bi-level optimization (FOBLO), avoiding heavy computation costs. Finally, we stabilize meta-training by using a robust initialization through interleaved supervision which alternates self-supervised and supervised objectives. Empirically, SpidR-Adapt achieves rapid gains in phonemic discriminability (ABX) and spoken language modeling (sWUGGY, sBLIMP, tSC), improving over in-domain language models after training on less than 1h of target-language audio, over $100\times$ more data-efficient than standard training. These findings highlight a practical, architecture-agnostic path toward biologically inspired, data-efficient representations. We open-source the training code and model checkpoints at https://github.com/facebookresearch/spidr-adapt.

</details>


### [91] [SMART SLM: Structured Memory and Reasoning Transformer, A Small Language Model for Accurate Document Assistance](https://arxiv.org/abs/2512.21280)
*Divij Dudeja,Mayukha Pal*

Main category: cs.CL

TL;DR: SMART提出了一种结构化内存和推理变换器来解决工程手册阅读困难的问题，通过分层处理、语法感知的事实提取和索引内存，相比GPT-2和BERT参数减少64-69%但准确率提高21.3%


<details>
  <summary>Details</summary>
Motivation: 工程手册内容冗长、格式密集，传统变换器将其视为扁平化的令牌流处理，导致数字答案错误且需要低效地记忆分离事实

Method: 采用分层方法：1)语法感知事实提取器提取主谓宾关系 2)紧凑索引内存MANN存储384维向量 3)6层变换器融合检索到的事实生成响应

Result: SMART参数仅为45.51M，比GPT-2少64%，比BERT少69%，但准确率比GPT-2高21.3%，在处理已知文档时可达亚秒级响应

Conclusion: SMART框架在实际部署中比同类小型变换器模型产生更可靠的结果，减少幻觉现象，具有双重推理模式适应不同场景

Abstract: The user of Engineering Manuals (EM) finds it difficult to read EM s because they are long, have a dense format which includes written documents, step by step procedures, and standard parameter lists for engineering equipment. Off the shelf transformers, especially compact ones, treat this material as a flat stream of tokens. This approach leads to confident but incorrect numeric answers and forces the models to memorize separate facts inefficiently. SMART (Structured Memory and Reasoning Transformer) offers a different and practical solution to the above problem. SMART structures its processing by using a hierarchical approach, and is based upon three main job categories (1) A syntax-aware Fact Extractor (Grammarian) Tree LSTM which extracts facts as subject relation object relations from EM sentences (2) A compact indexed memory MANN (Memory Augmented Neural Network) that indexes these Rational Subject Relation Objects as 384 dimensional vectors that are associated with the source of the information, and (3) A 6 layer Transformer that learns to fuse the previously retrieved facts into its generated response. The entire SMART model utilizes 45.51M parameters, which is 64% less than GPT-2 (124M) and 69% less than BERT (133M), and it achieves a 21.3% higher accuracy than GPT-2, indicating that SMART fits the data better with the least amount of processing requirements. SMART employs dual modes of inference an indexed fast path for known documents (sub-second answer times) and an indexed dynamic path assisted by RAGs for new uploads (FAISS Top 20 results with memory severed at 64 slots). In real world deployment, this framework leads to more well supported results with reduced hallucinations than comparable small transformer models.

</details>


### [92] [Parallel Token Prediction for Language Models](https://arxiv.org/abs/2512.21323)
*Felix Draxler,Justus Will,Farrin Marouf Sofian,Theofanis Karaletsos,Sameer Singh,Stephan Mandt*

Main category: cs.CL

TL;DR: PTP是一个用于语言模型并行序列生成的通用框架，通过在一次transformer调用中联合预测多个依赖令牌来降低自回归解码的延迟瓶颈。


<details>
  <summary>Details</summary>
Motivation: 解决现有多令牌预测方法中常见的限制性独立假设问题，同时减少自回归解码的延迟瓶颈，实现无需损失建模能力的并行长序列生成。

Method: PTP将采样过程融入模型，通过蒸馏现有模型或无需教师的逆自回归训练来学习，能够在单次transformer调用中联合预测多个依赖令牌。

Result: 在Vicuna-7B上实现了最先进的推测解码性能，在Spec-Bench上每个步骤可接受超过四个令牌，证明了并行生成长序列的可行性。

Conclusion: PTP框架具有通用性，能够表示任意自回归序列分布，表明并行生成长序列无需牺牲建模能力是可行的。

Abstract: We propose Parallel Token Prediction (PTP), a universal framework for parallel sequence generation in language models. PTP jointly predicts multiple dependent tokens in a single transformer call by incorporating the sampling procedure into the model. This reduces the latency bottleneck of autoregressive decoding, and avoids the restrictive independence assumptions common in existing multi-token prediction methods. We prove that PTP can represent arbitrary autoregressive sequence distributions. PTP is trained either by distilling an existing model or through inverse autoregressive training without a teacher. Experimentally, we achieve state-of-the-art speculative decoding performance on Vicuna-7B by accepting over four tokens per step on Spec-Bench. The universality of our framework indicates that parallel generation of long sequences is feasible without loss of modeling power.

</details>


### [93] [Your Reasoning Benchmark May Not Test Reasoning: Revealing Perception Bottleneck in Abstract Reasoning Benchmarks](https://arxiv.org/abs/2512.21329)
*Xinhe Wang,Jin Huang,Xingjian Zhang,Tianhao Wang,Jiaqi W. Ma*

Main category: cs.CL

TL;DR: 论文挑战了当前对ARC类推理基准的解读，认为模型表现不佳主要源于视觉感知能力不足而非推理能力缺陷，并通过两阶段实验设计证实了这一假设。


<details>
  <summary>Details</summary>
Motivation: 当前ARC类推理基准被广泛用于评估AI进展，通常被认为是测试核心推理能力。尽管对人类来说这些任务很简单，但前沿视觉语言模型仍然难以完成。传统的解读认为这是机器推理能力不足导致的，但本研究质疑这种解释，提出性能差距主要来自视觉感知限制而非推理缺陷。

Method: 研究者设计了两阶段实验管道：感知阶段将每个图像独立转换为自然语言描述，推理阶段让模型基于这些描述进行归纳和规则应用。这种方法防止了跨图像归纳信号的泄漏，将推理与感知瓶颈分离开来。在Mini-ARC、ACRE和Bongard-LOGO三个数据集上进行验证。

Result: 实验表明，感知能力是造成性能差距的主要因素。对VLM输出的推理轨迹进行人工检查发现，约80%的模型失败源于感知错误。与标准端到端单阶段评估相比，两阶段管道的表现验证了假设。

Conclusion: ARC类基准将感知和推理挑战混为一谈，观测到的性能差距可能夸大了机器推理能力的不足。评估机器智能进展时需要分离感知和推理的评估协议。

Abstract: Reasoning benchmarks such as the Abstraction and Reasoning Corpus (ARC) and ARC-AGI are widely used to assess progress in artificial intelligence and are often interpreted as probes of core, so-called ``fluid'' reasoning abilities. Despite their apparent simplicity for humans, these tasks remain challenging for frontier vision-language models (VLMs), a gap commonly attributed to deficiencies in machine reasoning. We challenge this interpretation and hypothesize that the gap arises primarily from limitations in visual perception rather than from shortcomings in inductive reasoning.
  To verify this hypothesis, we introduce a two-stage experimental pipeline that explicitly separates perception and reasoning. In the perception stage, each image is independently converted into a natural-language description, while in the reasoning stage a model induces and applies rules using these descriptions. This design prevents leakage of cross-image inductive signals and isolates reasoning from perception bottlenecks. Across three ARC-style datasets, Mini-ARC, ACRE, and Bongard-LOGO, we show that the perception capability is the dominant factor underlying the observed performance gap by comparing the two-stage pipeline with against standard end-to-end one-stage evaluation. Manual inspection of reasoning traces in the VLM outputs further reveals that approximately 80 percent of model failures stem from perception errors. Together, these results demonstrate that ARC-style benchmarks conflate perceptual and reasoning challenges and that observed performance gaps may overstate deficiencies in machine reasoning. Our findings underscore the need for evaluation protocols that disentangle perception from reasoning when assessing progress in machine intelligence.

</details>


### [94] [C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling](https://arxiv.org/abs/2512.21332)
*Jin Qin,Zihan Liao,Ziyin Zhang,Hang Yu,Peng Di,Rui Wang*

Main category: cs.CL

TL;DR: C2LLM是一种新的代码嵌入模型，通过PMA模块解决现有模型的信息瓶颈问题，训练后在同规模模型中创下新的性能记录。


<details>
  <summary>Details</summary>
Motivation: 现有的基于EOS的序列嵌入方法存在信息瓶颈，无法充分利用所有token信息。C2LLM旨在通过PMA模块更好地利用预训练语言模型的特征表示，同时支持灵活的嵌入维度调整。

Method: 基于Qwen-2.5-Coder骨干网络，引入PMA模块生成序列嵌入，有效利用LLM的因果表示并聚合所有token信息，支持灵活嵌入维度。模型有0.5B和7B两个版本，在300万公开数据上训练。

Result: C2LLM模型在MTEB-Code基准测试中创下同规模模型的新记录，其中C2LLM-7B在整体排行榜上排名第一。

Conclusion: C2LLM通过PMA模块成功突破了传统序列嵌入的信息限制，在代码嵌入任务上取得了state-of-the-art的性能，证明了该方法在代码理解任务中的有效性。

Abstract: We present C2LLM - Contrastive Code Large Language Models, a family of code embedding models in both 0.5B and 7B sizes. Building upon Qwen-2.5-Coder backbones, C2LLM adopts a Pooling by Multihead Attention (PMA) module for generating sequence embedding from token embeddings, effectively 1) utilizing the LLM's causal representations acquired during pretraining, while also 2) being able to aggregate information from all tokens in the sequence, breaking the information bottleneck in EOS-based sequence embeddings, and 3) supporting flexible adaptation of embedding dimension, serving as an alternative to MRL. Trained on three million publicly available data, C2LLM models set new records on MTEB-Code among models of similar sizes, with C2LLM-7B ranking 1st on the overall leaderboard.

</details>


### [95] [Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty](https://arxiv.org/abs/2512.21336)
*Ziyu Chen,Xinbei Jiang,Peng Sun,Tao Lin*

Main category: cs.CL

TL;DR: 该论文首次形式化了掩蔽扩散模型中解码顺序对输出质量的影响，提出了Denoising Entropy量化方法及两种优化算法，显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 掩蔽扩散模型具有灵活的非自回归生成能力，但这种自由度带来的挑战是最终输出质量对解码顺序高度敏感，现有方法缺乏对这一问题的系统分析。

Method: 提出Denoising Entropy作为量化累积预测不确定性的可计算指标，并设计两种解码路径优化算法：事后选择方法和实时引导策略。

Result: 实验证明熵引导方法显著提高生成质量，在推理、规划和代码基准上持续提升准确性。

Conclusion: 该工作确立了Denoising Entropy作为理解和控制生成的原则性工具，有效将MDMs中的不确定性从负担转变为发现高质量解决方案的关键优势。

Abstract: Masked Diffusion Models (MDMs) offer flexible, non-autoregressive generation, but this freedom introduces a challenge: final output quality is highly sensitive to the decoding order. We are the first to formalize this issue, attributing the variability in output quality to the cumulative predictive uncertainty along a generative path. To quantify this uncertainty, we introduce Denoising Entropy, a computable metric that serves as an internal signal for evaluating generative process. Leveraging this metric, we propose two algorithms designed to optimize the decoding path: a post-hoc selection method and a real-time guidance strategy. Experiments demonstrate that our entropy-guided methods significantly improve generation quality, consistently boosting accuracy on challenging reasoning, planning, and code benchmarks. Our work establishes Denoising Entropy as a principled tool for understanding and controlling generation, effectively turning the uncertainty in MDMs from a liability into a key advantage for discovering high-quality solutions.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [96] [SACodec: Asymmetric Quantization with Semantic Anchoring for Low-Bitrate High-Fidelity Neural Speech Codecs](https://arxiv.org/abs/2512.20944)
*Zhongren Dong,Bin Wang,Jing Han,Haotian Guo,Xiaojun Mo,Yimin Cao,Zixing Zhang*

Main category: cs.SD

TL;DR: SACodec通过双量化器设计解决神经语音编解码器在低比特率下保真度与语义丰富性的权衡问题，在1.5 kbps下实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 解决神经语音编解码器在低比特率时保真度与语义丰富性之间的根本权衡问题

Method: 基于非对称双量化器的SACodec，采用语义锚定机制分离语义和声学细节的量化。通过轻量级投影器对齐声学特征与冻结的mHuBERT码本，再使用带SimVQ的残差激活模块恢复细粒度声学信息

Result: 在1.5 kbps下，主观听力测试显示重建质量与原始音频感知高度接近，同时在下游任务中token展现出显著改善的语义丰富性

Conclusion: SACodec在低比特率下实现了保真度和语义丰富性的双重提升，建立了新的技术标杆

Abstract: Neural Speech Codecs face a fundamental trade-off at low bitrates: preserving acoustic fidelity often compromises semantic richness. To address this, we introduce SACodec, a novel codec built upon an asymmetric dual-quantizer that employs our proposed Semantic Anchoring mechanism. This design strategically decouples the quantization of Semantic and Acoustic details. The semantic anchoring is achieved via a lightweight projector that aligns acoustic features with a frozen, large-scale mHuBERT codebook, injecting linguistic priors while guaranteeing full codebook utilization. Sequentially, for acoustic details, a residual activation module with SimVQ enables a single-layer quantizer (acoustic path) to faithfully recover fine-grained information. At just 1.5 kbps, SACodec establishes a new state of the art by excelling in both fidelity and semantics: subjective listening tests confirm that its reconstruction quality is perceptually highly comparable to ground-truth audio, while its tokens demonstrate substantially improved semantic richness in downstream tasks.

</details>


### [97] [Towards Practical Automatic Piano Reduction using BERT with Semi-supervised Learning](https://arxiv.org/abs/2512.21324)
*Wan Ki Wong,Ka Ho To,Chuck-jee Chau,Lucas Wong,Kevin Y. Yip,Irwin King*

Main category: cs.SD

TL;DR: 提出了基于半监督机器学习的自动钢琴缩编新方法，将复杂的管弦乐谱简化为钢琴版本。该方法通过音乐简化和和声化两步实现，利用MidiBERT框架，能够在标注数据较少的情况下实现高质量的钢琴缩编。


<details>
  <summary>Details</summary>
Motivation: 钢琴缩编是从复杂的管弦乐谱中提取主要旋律和和声并精简为可在钢琴上演奏的版本的传统音乐转化过程。手动进行钢琴缩编耗时耗力，而监督学习需要大量标注数据，因此研究旨在通过半监督学习利用丰富的古典音乐数据，减少对标注数据的依赖。

Method: 采用两步法：首先进行音乐简化，然后进行和声化处理。基于现有的MidiBERT机器学习框架实现了两种解决方案，通过半监督学习方式利用大量未标注的古典音乐数据来训练模型。

Result: 所提出的方法能够输出实用且真实的钢琴缩编样本，准确度高，仅需少量后处理调整。生成的钢琴缩编质量足以作为实际的音乐草稿使用。

Conclusion: 本研究为半监督学习在自动钢琴缩编领域的应用奠定了基础，为未来研究者提供了参考框架，有望在此基础上产生更先进的研究成果。

Abstract: In this study, we present a novel automatic piano reduction method with semi-supervised machine learning. Piano reduction is an important music transformation process, which helps musicians and composers as a musical sketch for performances and analysis. The automation of such is a highly challenging research problem but could bring huge conveniences as manually doing a piano reduction takes a lot of time and effort. While supervised machine learning is often a useful tool for learning input-output mappings, it is difficult to obtain a large quantity of labelled data. We aim to solve this problem by utilizing semi-supervised learning, so that the abundant available data in classical music can be leveraged to perform the task with little or no labelling effort. In this regard, we formulate a two-step approach of music simplification followed by harmonization. We further propose and implement two possible solutions making use of an existing machine learning framework -- MidiBERT. We show that our solutions can output practical and realistic samples with an accurate reduction that needs only small adjustments in post-processing. Our study forms the groundwork for the use of semi-supervised learning in automatic piano reduction, where future researchers can take reference to produce more state-of-the-art results.

</details>
